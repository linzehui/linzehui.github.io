<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/favicon.ico?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="人一己千">
<meta property="og:type" content="website">
<meta property="og:title" content="Weekly Review">
<meta property="og:url" content="http://www.linzehui.me/index.html">
<meta property="og:site_name" content="Weekly Review">
<meta property="og:description" content="人一己千">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Weekly Review">
<meta name="twitter:description" content="人一己千">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.linzehui.me/"/>





  <title>Weekly Review</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Weekly Review</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/02/碎片知识/每周碎片知识13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/02/碎片知识/每周碎片知识13/" itemprop="url">每周碎片知识13</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-02T15:32:14+08:00">
                2018-12-02
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-02T11:34:20+08:00">
                2018-12-02
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-attention"><a href="#1️⃣-attention" class="headerlink" title="1️⃣[attention]"></a>1️⃣[attention]</h3><p>所有attention的总结：<br><img src="/images/15437180657954.jpg" width="70%" height="50%"><br><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank" rel="noopener">Attention? Attention!</a></p>
<hr>
<h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>①torch.no_grad能够显著减少内存使用，model.eval不能。因为eval不会关闭历史追踪。</p>
<blockquote>
<p>model.eval() will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval model instead of training mode.<br>torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script).</p>
</blockquote>
<p>Reference:<br><a href="https://discuss.pytorch.org/t/does-model-eval-with-torch-set-grad-enabled-is-train-have-the-same-effect-for-grad-history/17183/3" target="_blank" rel="noopener">Does model.eval() &amp; with torch.set_grad_enabled(is_train) have the same effect for grad history?</a></p>
<p><a href="https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615" target="_blank" rel="noopener">‘model.eval()’ vs ‘with torch.no_grad()’</a></p>
<p>②torch.full(…) returns a tensor filled with value.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/02/代码相关/代码记录11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/02/代码相关/代码记录11/" itemprop="url">代码片段记录11</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-02T10:48:30+08:00">
                2018-12-02
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-02T11:35:31+08:00">
                2018-12-02
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="①"><a href="#①" class="headerlink" title="①"></a>①</h2><p>需求：对于两个向量$a$、$b$，$a,b \in R^d$，定义一种减法，有：</p>
<script type="math/tex; mode=display">a-b=M</script><p>其中$M \in R^{d\times d}$，$M_{ij}=a_i-b_j$</p>
<p>在代码中实际的维度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=torch.rand(batch_size,sequence_len,dim)</span><br><span class="line">b=torch.rand(batch_size,sequence_len,dim)</span><br></pre></td></tr></table></figure>
<p>方法①：for循环</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">M=torch.zeros(bz,seq_len,seq_len)</span><br><span class="line"><span class="keyword">for</span> b_i <span class="keyword">in</span> range(bz):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(seq_len):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(seq_len):</span><br><span class="line">            M_ij=torch.norm(a[b_i][i]-b[b_i][j])</span><br><span class="line">            M[b][i][j]=M_ij</span><br></pre></td></tr></table></figure>
<p>方法②：矩阵运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=a.unsqueeze(<span class="number">2</span>)  <span class="comment"># bz,seq_len,1,dim</span></span><br><span class="line">b=b.unsqueeze(<span class="number">1</span>)  <span class="comment"># bz,1,seq_lens,dim</span></span><br><span class="line">M=torch.norm(a-b,dim=<span class="number">-1</span>)   <span class="comment"># will broadcast</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="②"><a href="#②" class="headerlink" title="②"></a>②</h2><p>需求，生成一个mask矩阵，每一行有一段连续的位置填充1，其中每一行填充1的开始位置和结束位置都不同。具体来说，先生成一个中心位置center，则开始位置为center-window；结束位置为center+window。其中开始位置和结束位置不能越界，也即不小于0和大于行的总长度。<br>如：<br><img src="/images/15437208061953.jpg" width="25%" height="50%"></p>
<p>思路：<br>①先生成n行每行对应的随机中心位置，然后再获得左和右边界</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">centers=torch.randint(low=<span class="number">0</span>,high=query_len,size=(query_len,),dtype=torch.long)</span><br><span class="line"></span><br><span class="line">left=centers-self.window</span><br><span class="line">left=torch.max(left,torch.LongTensor([<span class="number">0</span>])).unsqueeze(<span class="number">1</span>)   <span class="comment"># query_len,1</span></span><br><span class="line"></span><br><span class="line">right=centers+self.window</span><br><span class="line">right=torch.min(right,torch.LongTensor([query_len<span class="number">-1</span>])).unsqueeze(<span class="number">1</span>)  <span class="comment"># query_len,1</span></span><br></pre></td></tr></table></figure>
<p>②生成一个每行都用[0,n-1]填充的矩阵，[0,n-1]表示的是该元素的index，亦即：<br><img src="/images/15437212363142.jpg" width="25%" height="50%"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">range_matrix=torch.range(<span class="number">0</span>,query_len<span class="number">-1</span>,dtype=torch.long).unsqueeze(<span class="number">0</span>).expand(query_len,<span class="number">-1</span>)  <span class="comment"># query_len,query_len</span></span><br></pre></td></tr></table></figure>
<p>③利用&lt;=和&gt;=获得一个左边界和右边界矩阵，左边界矩阵表示在该左边界的左边都是填充的1；右边界矩阵表示在该右边界右边都是填充的1。再进行异或操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">range_matrix=torch.range(<span class="number">0</span>,query_len<span class="number">-1</span>,dtype=torch.long).unsqueeze(<span class="number">0</span>).expand(query_len,<span class="number">-1</span>)  <span class="comment"># query_len,query_len</span></span><br><span class="line">left_matrix=range_matrix&lt;=left</span><br><span class="line">right_matrix=range_matrix&lt;=right</span><br><span class="line">final_matrix=left_matrix^right_matrix</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/02/论文/每周论文7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/02/论文/每周论文7/" itemprop="url">每周论文7</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-02T08:48:30+08:00">
                2018-12-02
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-02T11:47:07+08:00">
                2018-12-02
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1️⃣-Convolutional-Self-Attention-Network"><a href="#1️⃣-Convolutional-Self-Attention-Network" class="headerlink" title="1️⃣[Convolutional Self-Attention Network]"></a>1️⃣[Convolutional Self-Attention Network]</h2><p>对self-attention进行改进，引入CNN的local-bias，也即对query的邻近词进行attention而不是所有词；将self-attention扩展到2D，也即让不同的head之间也有attention交互。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>1️⃣the normalization in Softmax may inhibits the attention to neighboring information 也即邻居的信息更重要，要加强邻居的重要性</p>
<p>2️⃣features can be better captured by modeling dependencies across different channels 对于不同的channel/head也增加他们之间的交互。</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="/images/15437126353758.jpg" width="80%" height="50%"></p>
<p>对于1D的convolution：选取中心词周围一个window：<br><img src="/images/15437128149700.jpg" width="28%" height="50%"></p>
<p>对于2D的convolution，则有：<br><img src="/images/15437128476725.jpg" width="45%" height="50%"></p>
<p>在具体实践中，只对前三层添加local bias，这是因为modeling locality在底层更有效，对于高层应该捕获更远的信息。</p>
<hr>
<h2 id="2️⃣-Modeling-Localness-for-Self-Attention-Networks"><a href="#2️⃣-Modeling-Localness-for-Self-Attention-Networks" class="headerlink" title="2️⃣[Modeling Localness for Self-Attention Networks]"></a>2️⃣[Modeling Localness for Self-Attention Networks]</h2><p>和上文一样，引入local bias对self-attention进行改进，从而提升了翻译表现。和上文是同一作者，发在EMNLP上。</p>
<h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>1️⃣self-attention存在的问题：虽然能够增加长程关注，但因此会导致注意力的分散，对邻居的信号会忽略。实践证明，对local bias建模在self-attention有提升。</p>
<p>2️⃣从直觉上来说，在翻译模型中，当目标词i与源语言词j有对齐关系时，我们希望词i能同时对词j周围的词进行对齐，使得能够捕获上下文信息，如phrase的信息。</p>
<h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>在原来的公式上添加G：<br><img src="/images/15437133932791.jpg" width="45%" height="50%"><br>也即：<br><img src="/images/15437134105761.jpg" width="70%" height="50%"></p>
<p>G是一个alignment position matrix（对齐位置矩阵），元素ij代表目标词i与源语言词j之间的紧密程度。<br>我们每次根据目标词i预测一个源语言的中心词，则$G_{ij}$则为：</p>
<p><img src="/images/15437135769000.jpg" width="23%" height="50%"></p>
<p>$P_i$就是对于目标词j而言源语言的中心词。 $\sigma$ 手动设定，通常是$\frac{D}{2}$，D代表窗口大小。</p>
<p>也即最终我们需要计算的是，中心词$P_i$和窗口$D$。</p>
<h4 id="计算-P-i"><a href="#计算-P-i" class="headerlink" title="计算$P_i$"></a>计算$P_i$</h4><p>利用对应的目标词i的query即可：<br><img src="/images/15437138514005.jpg" width="28%" height="50%"><br>$p_i$是一个实数。</p>
<h4 id="计算window-size"><a href="#计算window-size" class="headerlink" title="计算window size"></a>计算window size</h4><p>①固定窗口，将其作为一个超参。</p>
<p>②Layer-Speciﬁc Window<br>将该层所有的key平均，计算出一个共享的window size：<br><img src="/images/15437139914993.jpg" width="28%" height="50%"></p>
<p>③Query-Speciﬁc Window<br>每个query都有自己的window size<br><img src="/images/15437140367683.jpg" width="30%" height="50%"></p>
<h3 id="实验分析与结论"><a href="#实验分析与结论" class="headerlink" title="实验分析与结论"></a>实验分析与结论</h3><p>①将model locality用于低层效果会更好，这是因为低层对相邻建模，而越高层越关注更远的词。</p>
<p><img src="/images/15437141387365.jpg" width="50%" height="50%"></p>
<p>②将model locality放在encoder和encoder-decoder部分会更好（transformer有三个地方可以放）</p>
<p><img src="/images/15437141719564.jpg" width="50%" height="50%"><br>因为decoder本身就倾向关注临近的词，如果继续让其关注临近的词，那么就难以进行长程建模。</p>
<p>③越高层，window size（scope）越大。</p>
<p><img src="/images/15437142078121.jpg" width="70%" height="50%"></p>
<p>也即，在底层更倾向于捕获邻近词的语义；而高层倾向捕获长程依赖。但这不包括第一层，第一层是embedding，还没有上下文信息，因此倾向于捕获全局信息。</p>
<hr>
<h2 id="3️⃣-Effective-Approaches-to-Attention-based-Neural-Machine-Translation"><a href="#3️⃣-Effective-Approaches-to-Attention-based-Neural-Machine-Translation" class="headerlink" title="3️⃣[Effective Approaches to Attention-based Neural Machine Translation]"></a>3️⃣[Effective Approaches to Attention-based Neural Machine Translation]</h2><p>提出两种attention机制的翻译模型，global和local。</p>
<p>本文与原版的翻译模型略有不同：<br><img src="/images/15437143753188.jpg" width="40%" height="50%"><br><img src="/images/15437143893418.jpg" width="30%" height="50%"></p>
<p>c是context，h是decode的隐层。</p>
<h3 id="global-attention"><a href="#global-attention" class="headerlink" title="global attention"></a>global attention</h3><p><img src="/images/15437144396133.jpg" width="45%" height="50%"></p>
<p>计算attention分数：<br><img src="/images/15437145076271.jpg" width="40%" height="50%"></p>
<p>score有多种选择：<br><img src="/images/15437145588496.jpg" width="52%" height="50%"></p>
<p>注意到该模型与第一个提出attention based的模型不同之处：<br>$h_t -&gt; a_t -&gt; c_t -&gt; \tilde{h_t}$<br>原版是：<br>$h_{t-1} -&gt; a_t -&gt; c_t -&gt; h_t$</p>
<h3 id="local-attention"><a href="#local-attention" class="headerlink" title="local attention"></a>local attention</h3><p><img src="/images/15437147612512.jpg" width="45%" height="50%"></p>
<p>由于global attention计算代价高，且对于长句效果不好，我们可以选择一部分来做attention。<br>首先生成一个对齐位置$p_t$，再选择一个窗口$[p_t - D,p_t + D]$，其中D是超参。</p>
<p>如何获得$p_t$?<br>①直接假设$p_t=t$，也即source和target的位置大致一一对应。</p>
<p>②做预测：<br><img src="/images/15437150115321.jpg" width="43%" height="50%"><br>其中S是source的句子长度。</p>
<p>接着，以$p_t$为中心，添加一个高斯分布。最终attention计算公式：<br><img src="/images/15437150721538.jpg" width="50%" height="50%"></p>
<p>其中align和上面一致：<br><img src="/images/15437151043916.jpg" width="45%" height="50%"></p>
<p>也就是说，将位置信息也考虑进来。</p>
<h3 id="Input-feeding-Approach"><a href="#Input-feeding-Approach" class="headerlink" title="Input-feeding Approach"></a>Input-feeding Approach</h3><p>motivation：在下一次的alignment（也就是计算attention）之前，应当知道之前的alignment情况，所以应当作为输入信息传进下一层：<br><img src="/images/15437152269151.jpg" width="50%" height="50%"></p>
<p>注意这里和Bahdanau的不同。Bahdanau是直接用上下文去构造隐层。这里提出的模型相对更为通用，也可以被应用于非attention的模型中（也就是每次将encoder的最后一层作为输入在每个time step都输入）</p>
<hr>
<h2 id="4️⃣-Towards-Linear-Time-Neural-Machine-Translation-with-Capsule-Networks"><a href="#4️⃣-Towards-Linear-Time-Neural-Machine-Translation-with-Capsule-Networks" class="headerlink" title="4️⃣[Towards Linear Time Neural Machine Translation with Capsule Networks]"></a>4️⃣[Towards Linear Time Neural Machine Translation with Capsule Networks]</h2><p>思想：利用capsule提前生成source sentence的固定长度的表示，在decode的时候直接使用，而不需要attention，以达到线性时间NMT的目的。</p>
<p>Motivation：attention-based的NMT时间复杂度为$|S|\times |T|$，而本文希望能够将NMT减少到线性时间。而传统不加attention的NMT通常使用LSTM最后一层隐层作为源语言的encode信息传入decode，但这样的信息并不能很好地代表整个句子，因此本文使用capsule作为提取source sentence信息的方法，利用capsule生成固定长度表示，直接传入decode端，以达到线性时间的目的。</p>
<p><img src="/images/15437164176973.jpg" width="50%" height="50%"></p>
<h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><p>对于embedding：<br><img src="/images/15437164440500.jpg" width="37%" height="50%"><br>希望能够转换成固定长度的表示C：<br><img src="/images/15437164654931.jpg" width="37%" height="50%"></p>
<p>我们首先通过一个双向的LSTM：<br><img src="/images/15437165067165.jpg" width="28%" height="50%"></p>
<p>一种简单的获取C的方法：<br><img src="/images/15437165382025.jpg" width="30%" height="50%"><br>其中$h_1$和$h_L$有互补关系。</p>
<p>本文使用capsule提取更丰富的信息。</p>
<p>在decode阶段，由于拥有固定表示，那么就不需要attention：</p>
<p><img src="/images/15437166827481.jpg" width="35%" height="50%"><br><img src="/images/15437167374470.jpg" width="37%" height="50%"></p>
<p>总体架构：<br><img src="/images/15437167607085.jpg" width="60%" height="50%"></p>
<h3 id="Aggregation-layers-with-Capsule-Networks"><a href="#Aggregation-layers-with-Capsule-Networks" class="headerlink" title="Aggregation layers with Capsule Networks"></a>Aggregation layers with Capsule Networks</h3><p><img src="/images/15437168111687.jpg" width="65%" height="50%"><br>实际上就是dynamic routing那一套，对信息进行提取（论文公式有误就不贴图了）</p>
<p>算法：<br><img src="/images/15437168668191.jpg" width="55%" height="50%"></p>
<p>最终获得了：<br><img src="/images/15437168888967.jpg" width="27%" height="50%"></p>
<hr>
<h2 id="5️⃣-DropBlock-A-regularization-method-for-convolutional-networks"><a href="#5️⃣-DropBlock-A-regularization-method-for-convolutional-networks" class="headerlink" title="5️⃣[DropBlock: A regularization method for convolutional networks]"></a>5️⃣[DropBlock: A regularization method for convolutional networks]</h2><p>重读了一遍。<br>介绍一种新型的dropout，可用于卷积层提高表现。通过大量的实验得出许多有意义的结论。本文发表于NIPS2018。</p>
<h3 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h3><p>由于卷积层的feature相互之间有联系，即使使用了dropout，信息也能够根据周围的feature传到下一层。因此使用dropblock，一次将一个方块内的都drop掉。</p>
<p><img src="/images/15437170173072.jpg" width="50%" height="50%"></p>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p><img src="/images/15437170840909.jpg" width="80%" height="50%"></p>
<p>其中有两个超参：①block_size表示块的大小；γ表示有多少个unit要drop掉，等价传统的dropout的p。当block_size=1时等价dropout；当block size=整个feature map，等价于spatial dropout。</p>
<p>在实践中，通过以下公式计算γ：<br><img src="/images/15437172746112.jpg" width="55%" height="50%"></p>
<p>(why? 通过计算期望的方式将传统dropout的keep_prob与当前的γ联系起来，得到一个等式，整理即可获得上式）</p>
<p>在实验中，还可以逐渐减小keep_prob使得更加鲁棒性。</p>
<h3 id="实验-amp-结论"><a href="#实验-amp-结论" class="headerlink" title="实验&amp;结论"></a>实验&amp;结论</h3><p>①效果:dropout&lt; spatial dropout &lt; dropblock</p>
<p>②dropblock能有效去掉semantic information</p>
<p>③dropblock是一个更加强的regularization</p>
<p>④使用dropblock的模型，能够学习更多的区域，而不是只专注于一个区域<br><img src="/images/15437174940381.jpg" width="70%" height="50%"></p>
<p>对于resnet，直接将dropblock应用于添加完skip connection后的feature能够有更高的表现。</p>
<hr>
<h2 id="6️⃣-Contextual-String-Embeddings-for-Sequence-Labeling"><a href="#6️⃣-Contextual-String-Embeddings-for-Sequence-Labeling" class="headerlink" title="6️⃣[Contextual String Embeddings for Sequence Labeling]"></a>6️⃣[Contextual String Embeddings for Sequence Labeling]</h2><p>提出一种建立在character基础上的新型的上下文embedding(contextualized embedding）。用于sequence labeling。本文发表于coling2018。</p>
<h3 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h3><p>整体架构：<br><img src="/images/15437175991019.jpg" width="100%" height="50%"></p>
<p>首先将character作为基本单位，过一个双向LSTM，进行language model的建模。</p>
<p>如何提取一个词的词向量：<br><img src="/images/15437176650871.jpg" width="100%" height="50%"><br>提取前向LSTM中该词的最后一个character的后一个hidden state，以及后向LSTM中第一个词的前一个hidden state， 如上图所示。最终拼起来即可：<br><img src="/images/15437177090697.jpg" width="28%" height="50%"><br>因此该词不仅与词内部的character相关，还跟其周围的context有关。</p>
<p>sequence labeling我不感兴趣，该部分没看。</p>
<h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>相比word level的language model，character-level独立于tokenization和fixed vocabulary，模型更容易被训练，因为词表小且训练时间短。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/01/诗词&句/每周诗词16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/01/诗词&句/每周诗词16/" itemprop="url">每周诗词16</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-01T23:08:14+08:00">
                2018-12-01
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-01T23:08:49+08:00">
                2018-12-01
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣菩萨蛮"><a href="#1️⃣菩萨蛮" class="headerlink" title="1️⃣菩萨蛮"></a>1️⃣菩萨蛮</h3><p>[五代十国] 李煜<br>人生愁恨何能免，销魂独我情何限！故国梦重归，觉来双泪垂。<br>髙楼谁与上？长记秋晴望。<strong>往事已成空，还如一梦中</strong>。</p>
<p>觉(jue)来：醒来。</p>
<hr>
<h3 id="2️⃣南乡子-·-和杨元素，时移守密州"><a href="#2️⃣南乡子-·-和杨元素，时移守密州" class="headerlink" title="2️⃣南乡子 · 和杨元素，时移守密州"></a>2️⃣南乡子 · 和杨元素，时移守密州</h3><p>[宋] 苏轼<br>东武望馀杭，云海天涯两杳茫。<strong>何日功成名遂了，还乡，醉笑陪公三万场</strong>。<br><strong>不用诉离觞，痛饮从来别有肠</strong>。今夜送归灯火冷，河塘，堕泪羊公却姓杨。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/01/诗词&句/人不可能经历世界上所有热闹/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/01/诗词&句/人不可能经历世界上所有热闹/" itemprop="url">无题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-01T11:13:15+08:00">
                2018-12-01
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-02T11:33:38+08:00">
                2018-12-02
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>人不可能经历世界上所有热闹，但可以用眼睛看，用心感受，用胸怀扩张。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/11/19/碎片知识/每周碎片知识12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/19/碎片知识/每周碎片知识12/" itemprop="url">每周碎片知识12</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-19T15:32:14+08:00">
                2018-11-19
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-19T23:21:17+08:00">
                2018-11-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-Transformer"><a href="#1️⃣-Transformer" class="headerlink" title="1️⃣[Transformer]"></a>1️⃣[Transformer]</h3><p>对Transformer新理解：</p>
<ul>
<li>可以将Transformer理解成一张全连接图，其中每个节点与其他节点的关系通过attention权重表现。图关系是序列关系或者树关系的一般化。</li>
<li>为什么要有multi-head？不仅仅是论文的解释，或许还可以理解成，对一个向量的不同部分（如第1维到20维，第21维到40维等）施以不同的attention权重，如果不使用multi-head，那么对于一个query，就只会有一个权重，而不同的维度有不同的重要性。</li>
</ul>
<hr>
<h3 id="2️⃣-attention-amp-capsule"><a href="#2️⃣-attention-amp-capsule" class="headerlink" title="2️⃣[attention&amp;capsule]"></a>2️⃣[attention&amp;capsule]</h3><p>attention是收信息，query从value按权重获取信息，其中所有value的权重和是1。<br>capsule是发信息，对于$l-1$层的一个capsule来说，在传入到$l$层的k个capsule的信息，其权重和为1。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/11/19/论文/每周论文6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/19/论文/每周论文6/" itemprop="url">每周论文6</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-19T09:21:30+08:00">
                2018-11-19
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-01T15:47:04+08:00">
                2018-12-01
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1️⃣-A-STRUCTURED-SELF-ATTENTIVE-SENTENCE-EMBEDDING"><a href="#1️⃣-A-STRUCTURED-SELF-ATTENTIVE-SENTENCE-EMBEDDING" class="headerlink" title="1️⃣[A STRUCTURED SELF ATTENTIVE SENTENCE EMBEDDING]"></a>1️⃣[A STRUCTURED SELF ATTENTIVE SENTENCE EMBEDDING]</h2><p>介绍了一种生成sentence embedding的方法。与其他sentence embedding不同的地方在于，生成的是一个矩阵而不是一个向量。通过矩阵的形式，能够关注不同部分的语义表示，类似于Transformer的multi-head。</p>
<p>Contribution:</p>
<ul>
<li>将sentence embedding扩展为矩阵形式，能够获得更多的信息。</li>
<li>引入正则化，使得sentence matrix具有更丰富的多样性。</li>
</ul>
<p><img src="/images/15425908639518.jpg" width="70%" height="50%"></p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>双向LSTM+self-attention。</p>
<p>双向的LSTM获得上下文的表示：</p>
<p><img src="/images/15425911302081.jpg" width="27%" height="50%"></p>
<p><img src="/images/15425911849931.jpg" width="27%" height="50%"></p>
<p>因此可以获得attention权重向量：<br><img src="/images/15425912555350.jpg" width="50%" height="50%"></p>
<p>其中$H:n\times2u,W_{s1}:d_a\times2u ,w_{s2}:d_a$ ，$d_a$是超参。</p>
<p>现将向量$w_{s2}$扩展为矩阵，亦即有Multi-hop attention：<br><img src="/images/15425914364548.jpg" width="50%" height="50%"></p>
<p>$W_{s2}$维度为$r\times d_a$，$r$代表了head的个数。</p>
<p>因此最终的sentence embedding矩阵为：<br><img src="/images/15425915371381.jpg" width="15%" height="50%"></p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>为了让A尽可能有多样性（因为如果都是相似的，那么则会有冗余性），引入如下的正则化：<br><img src="/images/15425915930785.jpg" width="28%" height="50%"></p>
<p>原因：<br>对于不同的head $a^i$与$a^j$，$A A^T$有：<br><img src="/images/15425918790543.jpg" width="31%" height="50%"></p>
<p>如果$a^i$与$a^j$很相似那么就会接近于1，如果非常不相似(no overlay)则会接近于0。<br>因此整个式子就是:希望对角线部分接近于0（因为减了单位阵），这就相当于尽可能focus小部分的词；同时其他部分尽可能接近于0，也即不同的head之间没有overlap。</p>
<h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><p>文章提到，在做分类的时候可以直接将矩阵M展开，过全连接层即可。</p>
<hr>
<h2 id="2️⃣-Attention-over-Attention-Neural-Networks-for-Reading-Comprehension"><a href="#2️⃣-Attention-over-Attention-Neural-Networks-for-Reading-Comprehension" class="headerlink" title="2️⃣[Attention-over-Attention Neural Networks for Reading Comprehension]"></a>2️⃣[Attention-over-Attention Neural Networks for Reading Comprehension]</h2><p>在完形填空任务(Cloze-style Reading Comprehension)上提出一种新的attention，即nested-attention。</p>
<h3 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h3><p>三元组 $ D,Q,A $，document，question，answer。其中answer一般是document的一个词。</p>
<h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>本文提出的attention机制，是通过一个新的attention去指示另一个attention的重要程度。</p>
<p>首先通过一层共享的embedding层，将document和query都encode成word embedding，然后通过双向的GRU，将隐层拼接起来成为新的表示。</p>
<p>接着获得pair-wise matching matrix：<br><img src="/images/15425993645945.jpg" width="40%" height="50%"></p>
<p>其中$h$代表上述提到的拼接起来的表示，$M(i,j)$代表了document的词$i$和question的词$j$之间的匹配程度。</p>
<p>接着对<strong>column</strong>做softmax：<br><img src="/images/15425994692189.jpg" width="50%" height="50%"><br>其代表的意义即query-to-document attention，亦即<strong>对于一个query内的词，document的每个词与其匹配的权重</strong>。</p>
<p>接下来，对row进行softmax操作：<br><img src="/images/15425995482827.jpg" width="50%" height="50%"><br>代表的是<strong>给定一个document的词，query的哪个词更为重要</strong>。</p>
<p>接下来我们将β平均起来，获得一个向量：<br><img src="/images/15425996847558.jpg" width="20%" height="50%"><br>这个向量仍有attention的性质，即所有元素加和为1。代表的是<strong>从平均来看，query词的重要性</strong>。</p>
<p>最后，我们对α和β做点积以获得attended document-level attention：<br><img src="/images/15425997529193.jpg" width="13%" height="50%"></p>
<p>其中$s$的维度是$D\times 1$。s代表的意义即“a weighted sum of each individual document-level attention α(t) when looking at query word at time t”，也就是说，对α进行加权，代表query word的平均重要程度。</p>
<p>最终在做完型填空的预测时：<br><img src="/images/15425999965777.jpg" width="38%" height="50%"></p>
<p>个人觉得这种attention-over-attention的想法还是挺有创新的。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/11/13/机器学习与深度学习算法知识/网络优化与正则化总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/13/机器学习与深度学习算法知识/网络优化与正则化总结/" itemprop="url">网络优化与正则化总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-13T22:06:24+08:00">
                2018-11-13
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-01T15:43:16+08:00">
                2018-12-01
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>大量参考自<a href="https://nndl.github.io/" target="_blank" rel="noopener">《神经网络与深度学习》</a></p>
<h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><p>对于标准的SGD，常见的改进算法从两个方面进行：学习率衰减&amp;梯度方向优化。<br>记$g_t$为t时刻的导数：<br><img src="/images/2018-11-13-15421196736629.jpg" width="20%" height="50%"></p>
<h2 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h2><h3 id="AdaGrad算法"><a href="#AdaGrad算法" class="headerlink" title="AdaGrad算法"></a>AdaGrad算法</h3><p>通过计算历次的梯度平方累计值进行学习率衰减。<br>$G_t$是累计值：<br><img src="/images/2018-11-13-15421189802198.jpg" width="20%" height="50%"></p>
<p>更新值则为：<br><img src="/images/2018-11-13-15421190100615.jpg" width="30%" height="50%"></p>
<p>缺点：随着迭代次数的增加学习率递减。在经过一定次数的迭代依然没有找到最优点时，由于这时的学习率已经非常小，很难再继续找到最优点。</p>
<h3 id="RMSprop算法"><a href="#RMSprop算法" class="headerlink" title="RMSprop算法"></a>RMSprop算法</h3><p>对AdaGrad的改进，唯一的区别在于$G_t$的计算，将历史信息和当前信息进行线性加权，使得学习率可以动态改变而不是单调递减：<br><img src="/images/2018-11-13-15421192344025.jpg" width="40%" height="50%"></p>
<p>β为衰减率，通常取0.9。也即历史信息占主导。</p>
<h3 id="AdaDelta算法"><a href="#AdaDelta算法" class="headerlink" title="AdaDelta算法"></a>AdaDelta算法</h3><p>同样是对AdaGrad的改进。<br>每次计算：<br><img src="/images/2018-11-13-15421195264173.jpg" width="50%" height="50%"></p>
<p>也即历史更新差和上一时刻的更新差的加权（RMSprop是历史梯度和当前梯度）。</p>
<p>最终更新差值为：<br><img src="/images/2018-11-13-15421197355615.jpg" width="30%" height="50%"></p>
<p>其中$G_t$计算方法和RMSprop一致。</p>
<h2 id="梯度方向优化"><a href="#梯度方向优化" class="headerlink" title="梯度方向优化"></a>梯度方向优化</h2><p>利用历史的梯度（方向）调整当前时刻的梯度。</p>
<h3 id="动量（Momentum）法"><a href="#动量（Momentum）法" class="headerlink" title="动量（Momentum）法"></a>动量（Momentum）法</h3><p>动量法（Momentum Method）是用之前积累动量来替代真正的梯度。每次迭代的梯度可以看作是加速度。</p>
<p><img src="/images/2018-11-13-15421199473226.jpg" width="28%" height="50%"></p>
<p>也即上一时刻的更新差值和当前梯度共同决定当前的更新差值。$ρ$为动量因子，通常为0.9。也即动量占了主导。</p>
<p>当某个参数在最近一段时间内的梯度方向不一致时，其真实的参数更新幅度变小；相反，当在最近一段时间内的梯度方向都一致时，其真实的参数更新幅度变大，起到加速作用。一般而言，在迭代初期，梯度方法都比较一致，动量法会起到加速作用，可以更快地到达最优点。在迭代后期，梯度方法会取决不一致，在收敛值附近震荡，动量法会起到减速作用，增加稳定性。</p>
<h3 id="Nesterov加速梯度"><a href="#Nesterov加速梯度" class="headerlink" title="Nesterov加速梯度"></a>Nesterov加速梯度</h3><p>动量法的改进版本。</p>
<p>前面提到的动量法，是上一步的更新方向$\Delta \theta_{t-1}$与当前梯度$-g_t$的加和。因此可以理解成，先根据$∆θ_{t−1}$更新一次得到参数θ，再用$g_t$进行更新。亦即：<br><img src="/images/2018-11-13-15421202426163.jpg" width="27%" height="50%"><br>上式的第二步中，$g_t$是在$ \theta_{t-1}$上的梯度。我们将该步改为在$\theta_{t}$的梯度。<br>因此，有：<br><img src="/images/2018-11-13-15421203421465.jpg" width="50%" height="50%"></p>
<p>和动量法相比，相当于提前走了一步。<br><img src="/images/2018-11-13-15421203910771.jpg" width="70%" height="50%"></p>
<h3 id="Adam-amp-Nadam"><a href="#Adam-amp-Nadam" class="headerlink" title="Adam&amp;Nadam"></a>Adam&amp;Nadam</h3><p>Adam一方面计算梯度平方的加权，同时还计算梯度的加权：<br><img src="/images/2018-11-13-15421205162558.jpg" width="40%" height="50%"><br>通常$β_1=0.9$，$β_2=0.99$<br>也即历史信息占了主导。</p>
<p>在初期$M_t$与$G_t$会比真实均值和方差要小（想象$M_0=0$，$G_0=0$时）。因此对其进行修正，即：<br><img src="/images/2018-11-13-15421207635850.jpg" width="18%" height="50%"><br>因此最终有：<br><img src="/images/2018-11-13-15421207966341.jpg" width="26%" height="50%"></p>
<p>同理有Nadam。</p>
<p>Adam = Momentum + RMSprop<br>Nadam = Nesterov + RMSprop</p>
<h3 id="梯度截断-gradient-clipping"><a href="#梯度截断-gradient-clipping" class="headerlink" title="梯度截断 gradient clipping"></a>梯度截断 gradient clipping</h3><p>分为按值截断与按模截断。</p>
<h1 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h1><p>初始值选取很关键。假设全部初始化为0，则后续更新导致所有的激活值相同，也即对称权重现象。</p>
<p>原则：不能过大，否则激活值会变得饱和，如sigmoid；不能过小，否则经过多层信号会逐渐消失，并且导致sigmoid丢失非线性的能力（在0附近基本近似线性）。如果一个神经元的输入连接很多，它的每个输入连接上的权重就应该小一些，这是为了避免输出过大。</p>
<h2 id="Gaussian分布初始化"><a href="#Gaussian分布初始化" class="headerlink" title="Gaussian分布初始化"></a>Gaussian分布初始化</h2><p>同时考虑输入输出，可以按 $N(0,\sqrt{\frac{2}{n_{in} + n_{out}}})$ 高斯分布来初始化。</p>
<h2 id="均匀分布初始化"><a href="#均匀分布初始化" class="headerlink" title="均匀分布初始化"></a>均匀分布初始化</h2><p>在$[-r,r]$区间均匀分布初始化，其中r可以按照神经元数量自适应调整。</p>
<h3 id="Xavier初始化方法"><a href="#Xavier初始化方法" class="headerlink" title="Xavier初始化方法"></a>Xavier初始化方法</h3><p>自动计算超参r。r的公式为：<br><img src="/images/2018-11-14-15421648119504.jpg" width="22%" height="50%"><br>其中$n^l$代表第$l$层的神经元个数。</p>
<p>为什么是这个式子（推导见参考资料）：综合考虑了①输入输出的方差要一致；②反向传播中误差信号的方差不被放大或缩小。</p>
<h1 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h1><p>将数据分布归一化，使得分布保持稳定。<br><img src="/images/2018-11-14-15421656553319.jpg" width="100%" height="50%"><br>假设数据有四维(N,C,H,W)。N代表batch；C代表channel；H,W代表height和width。</p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>沿着通道进行归一化，亦即每个通道都有自己的均值和方差。<br><img src="/images/2018-11-14-15421657694248.jpg" width="70%" height="50%"><br>其中缩放平移变量是可学习的。</p>
<p>缺点：<br>①对batch size敏感，batch size太小则方差均值不足以代表数据分布<br>②对于不等长的输入如RNN来说，每一个timestep都需要保存不同的特征。</p>
<h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><p>对一个输入进行正则化，亦即每个输入都有自己的方差、均值。这样不依赖于batch大小和输入sequence的深度。</p>
<p>对RNN效果比较明显，但CNN中不如BN</p>
<h2 id="Instance-Normalization"><a href="#Instance-Normalization" class="headerlink" title="Instance Normalization"></a>Instance Normalization</h2><p>对HW进行归一化</p>
<h2 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h2><p>将channel分为多个group，每个group内做归一化</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://nndl.github.io/" target="_blank" rel="noopener">《神经网络与深度学习》</a><br><a href="https://blog.csdn.net/liuxiao214/article/details/81037416" target="_blank" rel="noopener">https://blog.csdn.net/liuxiao214/article/details/81037416</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/11/11/代码相关/代码记录10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/11/代码相关/代码记录10/" itemprop="url">代码片段记录10</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-11T11:16:30+08:00">
                2018-11-11
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-11T11:17:23+08:00">
                2018-11-11
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-get-sinusoid-encoding-table"><a href="#1️⃣-get-sinusoid-encoding-table" class="headerlink" title="1️⃣[get_sinusoid_encoding_table]"></a>1️⃣[get_sinusoid_encoding_table]</h3><p>Transformer绝对位置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sinusoid_encoding_table</span><span class="params">(n_position, d_hid, padding_idx=None)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_angle</span><span class="params">(position, hid_idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> position / np.power(<span class="number">10000</span>, <span class="number">2</span> * (hid_idx // <span class="number">2</span>) / d_hid)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_posi_angle_vec</span><span class="params">(position)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [cal_angle(position, hid_j) <span class="keyword">for</span> hid_j <span class="keyword">in</span> range(d_hid)]</span><br><span class="line"></span><br><span class="line">    sinusoid_table = np.array([get_posi_angle_vec(pos_i) <span class="keyword">for</span> pos_i <span class="keyword">in</span> range(n_position)])</span><br><span class="line"></span><br><span class="line">    sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line">    sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>] = np.cos(sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> padding_idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        sinusoid_table[padding_idx] = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.FloatTensor(sinusoid_table)  <span class="comment"># n_position,embed_dim</span></span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/11/11/碎片知识/每周碎片知识11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/11/碎片知识/每周碎片知识11/" itemprop="url">每周碎片知识11</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-11T10:56:14+08:00">
                2018-11-11
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-01T15:35:06+08:00">
                2018-12-01
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-Optimizer"><a href="#1️⃣-Optimizer" class="headerlink" title="1️⃣[Optimizer]"></a>1️⃣[Optimizer]</h3><p><a href="https://zhuanlan.zhihu.com/p/32262540" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32262540</a><br><a href="https://zhuanlan.zhihu.com/p/32338983" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32338983</a></p>
<p>Adam等自适应学习率算法对于稀疏数据具有优势，且收敛速度很快；但精调参数的SGD（+Momentum）往往能够取得更好的最终结果。</p>
<p>建议：<br>前期用Adam，享受Adam快速收敛的优势；后期切换到SGD，慢慢寻找最优解。<br>什么时候从Adam切换到SGD？当SGD的相应学习率的移动平均值基本不变的时候。</p>
<hr>
<h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>LongTensor除以浮点数，会对除数进行取整，再做除法。<br><img src="/images/2018-11-11-15419055399325.jpg" width="30%" height="50%"></p>
<hr>
<h3 id="3️⃣-Pytorch"><a href="#3️⃣-Pytorch" class="headerlink" title="3️⃣[Pytorch]"></a>3️⃣[Pytorch]</h3><p>使用Pytorch的DataParallel</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda:'</span> + str(</span><br><span class="line">    config.CUDA_VISIBLE_DEVICES[<span class="number">0</span>]) <span class="keyword">if</span> config.use_cuda <span class="keyword">else</span> <span class="string">'cpu'</span>)   <span class="comment"># 指定第一个设备</span></span><br><span class="line"></span><br><span class="line">model = ClassifyModel(</span><br><span class="line">    vocab_size=len(vocab), max_seq_len=config.max_sent_len,</span><br><span class="line">    embed_dim=config.embed_dim, n_layers=config.n_layers,</span><br><span class="line">    n_head=config.n_head, d_k=config.d_k,</span><br><span class="line">    d_v=config.d_v,</span><br><span class="line">    d_model=config.d_model, d_inner=config.d_inner_hid,</span><br><span class="line">    n_label=config.n_label,</span><br><span class="line">    dropout=config.dropout</span><br><span class="line">).to(device)</span><br><span class="line"></span><br><span class="line">model = DataParallel(model, device_ids=config.CUDA_VISIBLE_DEVICES)  <span class="comment"># 显式定义device_ids</span></span><br></pre></td></tr></table></figure>
<p>注意到：device_ids的起始编号要与之前定义的device中的“cuda:0”相一致，不然会报错。</p>
<p>如果不显式在代码中的DataParallel指定设备，那么需要在命令行内指定。如果是在命令行里面运行的，且device不是从0开始，应当显式设置GPU_id，否则会出错‘AssertionError: Invalid device id’，正确的命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=4,5  python -u classify_main.py --gpu_id 0,1</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg"
                alt="林泽辉" />
            
              <p class="site-author-name" itemprop="name">林泽辉</p>
              <p class="site-description motion-element" itemprop="description">人一己千</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">99</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">120</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">林泽辉</span>

  
</div>






  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
