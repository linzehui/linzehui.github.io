<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/favicon.ico?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="人一己千">
<meta property="og:type" content="website">
<meta property="og:title" content="Weekly Review">
<meta property="og:url" content="http://www.linzehui.me/index.html">
<meta property="og:site_name" content="Weekly Review">
<meta property="og:description" content="人一己千">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Weekly Review">
<meta name="twitter:description" content="人一己千">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.linzehui.me/"/>





  <title>Weekly Review</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Weekly Review</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2019/01/28/碎片知识/如何使用fairseq复现Transformer NMT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/28/碎片知识/如何使用fairseq复现Transformer NMT/" itemprop="url">如何使用fairseq复现Transformer NMT</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-28T22:05:05+08:00">
                2019-01-28
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-02-03T22:02:57+08:00">
                2019-02-03
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>基于Transformer的NMT虽然结果好，但超参非常难调，只要有一两个参数和论文不一样，就有可能得到和论文相去甚远的结果。fairseq是现有比较完善的seq2seq库，由于是大公司出品，因此也写得较为完善，不论是代码还是文档。</p>
<p>本文讨论如何使用fairseq复现基于Transformer的翻译任务，也即复现Vaswani, et al. 的论文结果。本文尽量不讨论实现细节，只讨论如何复现出结果。</p>
<p>fairseq项目地址：<a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">https://github.com/pytorch/fairseq</a></p>
<h2 id="使用教程"><a href="#使用教程" class="headerlink" title="使用教程"></a>使用教程</h2><p>在这里我们参考的是18年的文章<a href="https://arxiv.org/abs/1806.00187" target="_blank" rel="noopener">Scaling Neural Machine Translation</a>，同样是基于Transformer的NMT。同时，我们使用WMT16 EN-DE而不是Vaswani, et al.论文中的WMT14 EN-DE。二者只在一个文件（commoncrawl）上有区别，其他是一样的，由于WMT16 EN-DE有预处理好的数据，为了方便起见，我们就使用该份数据（下文也有预处理WMT14数据的方法）</p>
<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><ol>
<li>安装fairseq，在Readme内有</li>
<li>阅读Readme（optional）</li>
<li>阅读doc（optional）</li>
</ol>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><h4 id="Step1"><a href="#Step1" class="headerlink" title="Step1"></a>Step1</h4><p>数据预处理主要是下载多个文件并合并—&gt;清理/tokenize数据—&gt;将数据分为train、valid—&gt;bpe(bype pair encoding)。fairseq提供了一整套处理流程的脚本，在examples/translation/prepare-wmt14en2de.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># Adapted from https://github.com/facebookresearch/MIXER/blob/master/prepareData.sh</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'Cloning Moses github repository (for tokenization scripts)...'</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/moses-smt/mosesdecoder.git</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'Cloning Subword NMT repository (for BPE pre-processing)...'</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/rsennrich/subword-nmt.git</span><br><span class="line"></span><br><span class="line">SCRIPTS=mosesdecoder/scripts</span><br><span class="line">TOKENIZER=<span class="variable">$SCRIPTS</span>/tokenizer/tokenizer.perl</span><br><span class="line">CLEAN=<span class="variable">$SCRIPTS</span>/training/clean-corpus-n.perl</span><br><span class="line">NORM_PUNC=<span class="variable">$SCRIPTS</span>/tokenizer/normalize-punctuation.perl</span><br><span class="line">REM_NON_PRINT_CHAR=<span class="variable">$SCRIPTS</span>/tokenizer/remove-non-printing-char.perl</span><br><span class="line">BPEROOT=subword-nmt</span><br><span class="line">BPE_TOKENS=40000</span><br><span class="line"></span><br><span class="line">URLS=(</span><br><span class="line">    <span class="string">"http://statmt.org/wmt13/training-parallel-europarl-v7.tgz"</span></span><br><span class="line">    <span class="string">"http://statmt.org/wmt13/training-parallel-commoncrawl.tgz"</span></span><br><span class="line">    <span class="string">"http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz"</span></span><br><span class="line">    <span class="string">"http://data.statmt.org/wmt17/translation-task/dev.tgz"</span></span><br><span class="line">    <span class="string">"http://statmt.org/wmt14/test-full.tgz"</span></span><br><span class="line">)</span><br><span class="line">FILES=(</span><br><span class="line">    <span class="string">"training-parallel-europarl-v7.tgz"</span></span><br><span class="line">    <span class="string">"training-parallel-commoncrawl.tgz"</span></span><br><span class="line">    <span class="string">"training-parallel-nc-v12.tgz"</span></span><br><span class="line">    <span class="string">"dev.tgz"</span></span><br><span class="line">    <span class="string">"test-full.tgz"</span></span><br><span class="line">)</span><br><span class="line">CORPORA=(</span><br><span class="line">    <span class="string">"training/europarl-v7.de-en"</span></span><br><span class="line">    <span class="string">"commoncrawl.de-en"</span></span><br><span class="line">    <span class="string">"training/news-commentary-v12.de-en"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This will make the dataset compatible to the one used in "Convolutional Sequence to Sequence Learning"</span></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1705.03122</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$1</span>"</span> == <span class="string">"--icml17"</span> ]; <span class="keyword">then</span></span><br><span class="line">    URLS[2]=<span class="string">"http://statmt.org/wmt14/training-parallel-nc-v9.tgz"</span></span><br><span class="line">    FILES[2]=<span class="string">"training-parallel-nc-v9.tgz"</span></span><br><span class="line">    CORPORA[2]=<span class="string">"training/news-commentary-v9.de-en"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ ! -d <span class="string">"<span class="variable">$SCRIPTS</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"Please set SCRIPTS variable correctly to point to Moses scripts."</span></span><br><span class="line">    <span class="built_in">exit</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">src=en</span><br><span class="line">tgt=de</span><br><span class="line">lang=en-de</span><br><span class="line">prep=wmt14_en_de</span><br><span class="line">tmp=<span class="variable">$prep</span>/tmp</span><br><span class="line">orig=orig</span><br><span class="line">dev=dev/newstest2013</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$orig</span> <span class="variable">$tmp</span> <span class="variable">$prep</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$orig</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;<span class="variable">$&#123;#URLS[@]&#125;</span>;++i)); <span class="keyword">do</span></span><br><span class="line">    file=<span class="variable">$&#123;FILES[i]&#125;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="variable">$file</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$file</span> already exists, skipping download"</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        url=<span class="variable">$&#123;URLS[i]&#125;</span></span><br><span class="line">        wget <span class="string">"<span class="variable">$url</span>"</span></span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="variable">$file</span> ]; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"<span class="variable">$url</span> successfully downloaded."</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"<span class="variable">$url</span> not successfully downloaded."</span></span><br><span class="line">            <span class="built_in">exit</span> -1</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">if</span> [ <span class="variable">$&#123;file: -4&#125;</span> == <span class="string">".tgz"</span> ]; <span class="keyword">then</span></span><br><span class="line">            tar zxvf <span class="variable">$file</span></span><br><span class="line">        <span class="keyword">elif</span> [ <span class="variable">$&#123;file: -4&#125;</span> == <span class="string">".tar"</span> ]; <span class="keyword">then</span></span><br><span class="line">            tar xvf <span class="variable">$file</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"pre-processing train data..."</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    rm <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> <span class="string">"<span class="variable">$&#123;CORPORA[@]&#125;</span>"</span>; <span class="keyword">do</span></span><br><span class="line">        cat <span class="variable">$orig</span>/<span class="variable">$f</span>.<span class="variable">$l</span> | \</span><br><span class="line">            perl <span class="variable">$NORM_PUNC</span> <span class="variable">$l</span> | \</span><br><span class="line">            perl <span class="variable">$REM_NON_PRINT_CHAR</span> | \</span><br><span class="line">            perl <span class="variable">$TOKENIZER</span> -threads 8 -a -l <span class="variable">$l</span> &gt;&gt; <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"pre-processing test data..."</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$l</span>"</span> == <span class="string">"<span class="variable">$src</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">        t=<span class="string">"src"</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        t=<span class="string">"ref"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    grep <span class="string">'&lt;seg id'</span> <span class="variable">$orig</span>/<span class="built_in">test</span>-full/newstest2014-deen-<span class="variable">$t</span>.<span class="variable">$l</span>.sgm | \</span><br><span class="line">        sed -e <span class="string">'s/&lt;seg id="[0-9]*"&gt;\s*//g'</span> | \</span><br><span class="line">        sed -e <span class="string">'s/\s*&lt;\/seg&gt;\s*//g'</span> | \</span><br><span class="line">        sed -e <span class="string">"s/\’/\'/g"</span> | \</span><br><span class="line">    perl <span class="variable">$TOKENIZER</span> -threads 8 -a -l <span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/<span class="built_in">test</span>.<span class="variable">$l</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">""</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"splitting train and valid..."</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    awk <span class="string">'&#123;if (NR%100 == 0)  print $0; &#125;'</span> <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/valid.<span class="variable">$l</span></span><br><span class="line">    awk <span class="string">'&#123;if (NR%100 != 0)  print $0; &#125;'</span> <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/train.<span class="variable">$l</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">TRAIN=<span class="variable">$tmp</span>/train.de-en</span><br><span class="line">BPE_CODE=<span class="variable">$prep</span>/code</span><br><span class="line">rm -f <span class="variable">$TRAIN</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    cat <span class="variable">$tmp</span>/train.<span class="variable">$l</span> &gt;&gt; <span class="variable">$TRAIN</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"learn_bpe.py on <span class="variable">$&#123;TRAIN&#125;</span>..."</span></span><br><span class="line">python <span class="variable">$BPEROOT</span>/learn_bpe.py -s <span class="variable">$BPE_TOKENS</span> &lt; <span class="variable">$TRAIN</span> &gt; <span class="variable">$BPE_CODE</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> L <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> train.<span class="variable">$L</span> valid.<span class="variable">$L</span> <span class="built_in">test</span>.<span class="variable">$L</span>; <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"apply_bpe.py to <span class="variable">$&#123;f&#125;</span>..."</span></span><br><span class="line">        python <span class="variable">$BPEROOT</span>/apply_bpe.py -c <span class="variable">$BPE_CODE</span> &lt; <span class="variable">$tmp</span>/<span class="variable">$f</span> &gt; <span class="variable">$tmp</span>/bpe.<span class="variable">$f</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">perl <span class="variable">$CLEAN</span> -ratio 1.5 <span class="variable">$tmp</span>/bpe.train <span class="variable">$src</span> <span class="variable">$tgt</span> <span class="variable">$prep</span>/train 1 250</span><br><span class="line">perl <span class="variable">$CLEAN</span> -ratio 1.5 <span class="variable">$tmp</span>/bpe.valid <span class="variable">$src</span> <span class="variable">$tgt</span> <span class="variable">$prep</span>/valid 1 250</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> L <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    cp <span class="variable">$tmp</span>/bpe.test.<span class="variable">$L</span> <span class="variable">$prep</span>/<span class="built_in">test</span>.<span class="variable">$L</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>如果希望使用预处理好的数据，则可以使用WMT16 EN-DE，地址为：<a href="https://drive.google.com/uc?export=download&amp;id=0B_bZck-ksdkpM25jRUN2X2UxMm8" target="_blank" rel="noopener">https://drive.google.com/uc?export=download&amp;id=0B_bZck-ksdkpM25jRUN2X2UxMm8</a><br>并解压。</p>
<h4 id="Step2"><a href="#Step2" class="headerlink" title="Step2"></a>Step2</h4><p>接下来对数据进行二值化(binarize):</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">TEXT=wmt16_en_de_bpe32k</span><br><span class="line">mkdir <span class="variable">$TEXT</span></span><br><span class="line">tar -xzvf wmt16_en_de.tar.gz -C <span class="variable">$TEXT</span>  <span class="comment"># 解压文件</span></span><br><span class="line">python preprocess.py --<span class="built_in">source</span>-lang en --target-lang de \</span><br><span class="line">  --trainpref <span class="variable">$TEXT</span>/train.tok.clean.bpe.32000 \</span><br><span class="line">  --validpref <span class="variable">$TEXT</span>/newstest2013.tok.bpe.32000 \</span><br><span class="line">  --testpref <span class="variable">$TEXT</span>/newstest2014.tok.bpe.32000 \</span><br><span class="line">  --destdir data-bin/wmt16_en_de_bpe32k \</span><br><span class="line">  --nwordssrc 32768 --nwordstgt 32768 \</span><br><span class="line">  --joined-dictionary</span><br></pre></td></tr></table></figure>
<p>到这里，麻烦的预处理就结束了。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>cd到fairseq目录下，执行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7  python -m torch.distributed.launch --nproc_per_node 8 train.py data-bin/wmt16_en_de_bpe32k \        --arch transformer_wmt_en_de --share-all-embeddings \          --optimizer adam --adam-betas <span class="string">'(0.9, 0.98)'</span> --clip-norm 0.0 \            --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 \              --lr 0.0007 --min-lr 1e-09 \             --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --weight-decay 0.0\              --max-tokens  4096   --save-dir checkpoints/en-de-base\               --no-progress-bar --<span class="built_in">log</span>-format json --<span class="built_in">log</span>-interval 50\             --save-interval-updates  1000 --keep-interval-updates 20</span><br></pre></td></tr></table></figure>
<p>注意到该设置与原论文不大一致。但已证实该设置可以复现论文结果。</p>
<p>如果没有这么多卡，那么可以设置<code>update freq</code>以模拟8卡行为。如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=0,1,2,3  python -m torch.distributed.launch --nproc_per_node 4 \</span><br><span class="line">train.py data-bin/wmt16_en_de_bpe32k    \</span><br><span class="line"> --arch transformer_wmt_en_de --share-all-embeddings \</span><br><span class="line">--optimizer adam --adam-betas <span class="string">'(0.9, 0.98)'</span> \</span><br><span class="line">--clip-norm 0.0   --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000  \</span><br><span class="line">--lr 0.0007 --min-lr 1e-09 --criterion label_smoothed_cross_entropy \</span><br><span class="line">--label-smoothing 0.1 --weight-decay 0.0 --max-tokens  4096   \</span><br><span class="line">--save-dir checkpoints/en-de-16-base   \ </span><br><span class="line">--no-progress-bar --<span class="built_in">log</span>-format json --<span class="built_in">log</span>-interval 50 --save-interval-updates  1000 \</span><br><span class="line">--keep-interval-updates 20  --update-freq 2 |tee exp2.log</span><br></pre></td></tr></table></figure>
<p>4张卡则设<code>update freq=2</code>，2张卡则设<code>update freq=4</code>，以此类推。</p>
<p>大概在100个epoch内能够收敛，也即在475000个step。8张1080Ti在大概两天能够训练完成，4张1080Ti大概4天训练完成。</p>
<p>开始训练…<br><img src="/images/15491934129135.jpg" width="80%" height="50%"></p>
<p>最后则会获得checkpoint：<br><img src="/images/15491934935157.jpg" width="80%" height="50%"></p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>测试分为几个阶段：首先将几个checkpoint进行平均，实验表明，进行平均能够有一定的提升；其次，使用平均后的模型对test集的句子进行翻译；最终将生成的句子和正确的句子计算bleu值。</p>
<h4 id="average-checkpoint"><a href="#average-checkpoint" class="headerlink" title="average checkpoint"></a>average checkpoint</h4><p>在测试阶段，论文在Transformer-base中对最后五个checkpoint进行平均，也即对权值进行平均：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python scripts/average_checkpoints.py \</span><br><span class="line">--inputs checkpoints/en-de-base/ \</span><br><span class="line">--num-epoch-checkpoints  5 --output averaged_model.pt</span><br></pre></td></tr></table></figure>
<p>最终获得averaged_model.pt，我们将用该文件进行测试。</p>
<h4 id="generate"><a href="#generate" class="headerlink" title="generate"></a>generate</h4><p>我们采用和论文一致的超参：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 python generate.py \</span><br><span class="line">data-bin/wmt16_en_de_bpe32k/ --path /some_checkpoint \</span><br><span class="line">--remove-bpe --beam 4 --batch-size 64 --lenpen 0.6 \</span><br><span class="line">--max-len<span class="_">-a</span> 1 --max-len-b 50|tee generate.out</span><br></pre></td></tr></table></figure>
<p>其中lenpen是生成句子的长度惩罚系数；<code>max-len-a</code>和<code>max-len-b</code>指的是每个句子的最长长度限制，也即：假设源句子长度为x，则目标句子的长度应小于ax+b 。</p>
<p>最终我们翻译好的句子以及相对应的详细信息都在generate.out里面。我们需要提取源语言句子和目标语言句子，以方便后面的计算。因此：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">grep ^T generate.out | cut -f2- | perl -ple <span class="string">'s&#123;(\S)-(\S)&#125;&#123;$1 ##AT##-##AT## $2&#125;g'</span> &gt; generate.ref</span><br><span class="line"></span><br><span class="line">grep ^H generate.out |cut -f3- | perl -ple <span class="string">'s&#123;(\S)-(\S)&#125;&#123;$1 ##AT##-##AT## $2&#125;g'</span> &gt; generate.sys</span><br></pre></td></tr></table></figure>
<p>分别运行这两个bash命令，我们则获得了generate.ref和generate.sys，分别是目标和源语言的句子。</p>
<p>注意到这里有一个非常重要的小trick，也即<strong>split compound</strong>。因为一些历史原因（我也不知道为啥，tensor2tensor里面的脚本有提到），该trick已经在上面的脚本命令体现出来了。实践证明，使用该trick能够提高bleu值 0.5个点以上。</p>
<h4 id="score"><a href="#score" class="headerlink" title="score"></a>score</h4><p>我们此时就可以计算bleu值了，fairseq提供了该脚本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python score.py --sys generate.sys --ref generate.ref</span><br></pre></td></tr></table></figure>
<p>大功告成！我们终于复现出结果了。<br>作为参考：根据我的实验，只使用checkpoint中最好的一个checkpoint，在经过了上述的流程后，可以得到27.30的结果。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>根据我的需求，我还需要详细记录中间结果，并打印在tensorboard上方便可视化，如：<br><img src="/images/15491946401169.jpg" width="90%" height="50%"></p>
<p>fairseq并没有提供这种功能，因此需要自己修改部分源代码。<br>只需要修改train.py源文件即可。</p>
<p>①在开头加summary writer<br><img src="/images/15492019614168.jpg" width="70%" height="50%"></p>
<p>注意到每次实验都需要修改实验的名字。</p>
<p>②修改train函数<br>在训练过程中，添加记录的代码：<br><img src="/images/15492020396304.jpg" width="70%" height="50%"></p>
<p>在epoch结束，添加记录的代码：<br><img src="/images/15492021296611.jpg" width="70%" height="50%"></p>
<p>对validate的使用进行修改（添加了is_epoch）：<br><img src="/images/15492022604000.jpg" width="70%" height="50%"></p>
<p>train函数全部代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(args, trainer, task, epoch_itr)</span>:</span></span><br><span class="line">    <span class="string">"""Train the model for one epoch."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update parameters every N batches</span></span><br><span class="line">    <span class="keyword">if</span> epoch_itr.epoch &lt;= len(args.update_freq):</span><br><span class="line">        update_freq = args.update_freq[epoch_itr.epoch - <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        update_freq = args.update_freq[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize data iterator</span></span><br><span class="line">    itr = epoch_itr.next_epoch_itr(fix_batches_to_gpus=args.fix_batches_to_gpus)</span><br><span class="line">    itr = iterators.GroupedIterator(itr, update_freq)</span><br><span class="line">    progress = progress_bar.build_progress_bar(</span><br><span class="line">        args, itr, epoch_itr.epoch, no_progress_bar=<span class="string">'simple'</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    extra_meters = collections.defaultdict(<span class="keyword">lambda</span>: AverageMeter())</span><br><span class="line">    first_valid = args.valid_subset.split(<span class="string">','</span>)[<span class="number">0</span>]</span><br><span class="line">    max_update = args.max_update <span class="keyword">or</span> math.inf</span><br><span class="line">    <span class="keyword">for</span> i, samples <span class="keyword">in</span> enumerate(progress, start=epoch_itr.iterations_in_epoch):</span><br><span class="line">        log_output = trainer.train_step(samples)</span><br><span class="line">        <span class="keyword">if</span> log_output <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># log mid-epoch stats</span></span><br><span class="line">        stats = get_training_stats(trainer)</span><br><span class="line">        num_updates = stats[<span class="string">'num_updates'</span>]</span><br><span class="line">        <span class="comment"># print(type(num_updates))</span></span><br><span class="line">        <span class="comment"># print(type(stats['loss']))</span></span><br><span class="line">        summary_writer.add_scalar(<span class="string">'Training/training_loss_update'</span>, float(stats[<span class="string">'loss'</span>]), num_updates)</span><br><span class="line">        summary_writer.add_scalar(<span class="string">'Training/training_nll_loss_update'</span>, float(stats[<span class="string">'nll_loss'</span>]), num_updates)</span><br><span class="line">        summary_writer.add_scalar(<span class="string">'Training/training_ppl_update'</span>, float(stats[<span class="string">'ppl'</span>]), num_updates)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ------record training metrics --- #</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> log_output.items():</span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">in</span> [<span class="string">'loss'</span>, <span class="string">'nll_loss'</span>, <span class="string">'ntokens'</span>, <span class="string">'nsentences'</span>, <span class="string">'sample_size'</span>]:</span><br><span class="line">                <span class="keyword">continue</span>  <span class="comment"># these are already logged above</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">'loss'</span> <span class="keyword">in</span> k:</span><br><span class="line">                extra_meters[k].update(v, log_output[<span class="string">'sample_size'</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                extra_meters[k].update(v)</span><br><span class="line">            stats[k] = extra_meters[k].avg</span><br><span class="line">        progress.log(stats)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ignore the first mini-batch in words-per-second calculation</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            trainer.get_meter(<span class="string">'wps'</span>).reset()</span><br><span class="line"></span><br><span class="line">        num_updates = trainer.get_num_updates()</span><br><span class="line">        <span class="keyword">if</span> args.save_interval_updates &gt; <span class="number">0</span> <span class="keyword">and</span> num_updates % args.save_interval_updates == <span class="number">0</span> <span class="keyword">and</span> num_updates &gt; <span class="number">0</span>:</span><br><span class="line">            valid_losses = validate(args, trainer, task, epoch_itr, [first_valid], is_epoch=<span class="keyword">False</span>)</span><br><span class="line">            save_checkpoint(args, trainer, epoch_itr, valid_losses[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> num_updates &gt;= max_update:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># log end-of-epoch stats</span></span><br><span class="line">    stats = get_training_stats(trainer)</span><br><span class="line">    <span class="comment"># ------record training metrics --- #</span></span><br><span class="line">    summary_writer.add_scalar(<span class="string">'Training/training_loss_epoch'</span>, float(stats[<span class="string">'loss'</span>]), epoch_itr.epoch)</span><br><span class="line">    summary_writer.add_scalar(<span class="string">'Training/training_nll_loss_epoch'</span>, float(stats[<span class="string">'nll_loss'</span>]), epoch_itr.epoch)</span><br><span class="line">    summary_writer.add_scalar(<span class="string">'Training/training_ppl_epoch'</span>, float(stats[<span class="string">'ppl'</span>]), epoch_itr.epoch)</span><br><span class="line">    <span class="keyword">for</span> k, meter <span class="keyword">in</span> extra_meters.items():</span><br><span class="line">        stats[k] = meter.avg</span><br><span class="line">    progress.print(stats)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># reset training meters</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> [</span><br><span class="line">        <span class="string">'train_loss'</span>, <span class="string">'train_nll_loss'</span>, <span class="string">'wps'</span>, <span class="string">'ups'</span>, <span class="string">'wpb'</span>, <span class="string">'bsz'</span>, <span class="string">'gnorm'</span>, <span class="string">'clip'</span>,</span><br><span class="line">    ]:</span><br><span class="line">        meter = trainer.get_meter(k)</span><br><span class="line">        <span class="keyword">if</span> meter <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            meter.reset()</span><br></pre></td></tr></table></figure>
<p>③修改validate函数<br>添加了一个参数<code>is_epoch</code>：<br><img src="/images/15492023879130.jpg" width="50%" height="50%"></p>
<p>添加记录的代码：<br><img src="/images/15492024686078.jpg" width="90%" height="50%"></p>
<p>validate全部代码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">def validate(args, trainer, task, epoch_itr, subsets, is_epoch=True):</span><br><span class="line">    <span class="string">""</span><span class="string">"Evaluate the model on the validation set(s) and return the losses."</span><span class="string">""</span></span><br><span class="line">    valid_losses = []</span><br><span class="line">    <span class="keyword">for</span> subset <span class="keyword">in</span> subsets:</span><br><span class="line">        <span class="comment"># Initialize data iterator</span></span><br><span class="line">        itr = task.get_batch_iterator(</span><br><span class="line">            dataset=task.dataset(subset),</span><br><span class="line">            max_tokens=args.max_tokens,</span><br><span class="line">            max_sentences=args.max_sentences_valid,</span><br><span class="line">            max_positions=utils.resolve_max_positions(</span><br><span class="line">                task.max_positions(),</span><br><span class="line">                trainer.get_model().max_positions(),</span><br><span class="line">            ),</span><br><span class="line">            ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,</span><br><span class="line">            required_batch_size_multiple=8,</span><br><span class="line">            seed=args.seed,</span><br><span class="line">            num_shards=args.distributed_world_size,</span><br><span class="line">            shard_id=args.distributed_rank,</span><br><span class="line">            num_workers=args.num_workers,</span><br><span class="line">        ).next_epoch_itr(shuffle=False)</span><br><span class="line">        progress = progress_bar.build_progress_bar(</span><br><span class="line">            args, itr, epoch_itr.epoch,</span><br><span class="line">            prefix=<span class="string">'valid on \'</span>&#123;&#125;\<span class="string">' subset'</span>.format(subset),</span><br><span class="line">            no_progress_bar=<span class="string">'simple'</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reset validation loss meters</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> [<span class="string">'valid_loss'</span>, <span class="string">'valid_nll_loss'</span>]:</span><br><span class="line">            meter = trainer.get_meter(k)</span><br><span class="line">            <span class="keyword">if</span> meter is not None:</span><br><span class="line">                meter.reset()</span><br><span class="line">        extra_meters = collections.defaultdict(lambda: AverageMeter())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> sample <span class="keyword">in</span> progress:</span><br><span class="line">            log_output = trainer.valid_step(sample)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> k, v <span class="keyword">in</span> log_output.items():</span><br><span class="line">                <span class="keyword">if</span> k <span class="keyword">in</span> [<span class="string">'loss'</span>, <span class="string">'nll_loss'</span>, <span class="string">'ntokens'</span>, <span class="string">'nsentences'</span>, <span class="string">'sample_size'</span>]:</span><br><span class="line">                    <span class="built_in">continue</span></span><br><span class="line">                extra_meters[k].update(v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># log validation stats</span></span><br><span class="line">        stats = get_valid_stats(trainer)</span><br><span class="line">        <span class="comment"># ------record validate metrics --- #</span></span><br><span class="line">        <span class="keyword">if</span> is_epoch:  <span class="comment"># every epoch</span></span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_loss_epoch'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_loss'</span>]), epoch_itr.epoch)</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_nll_loss_epoch'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_nll_loss'</span>]), epoch_itr.epoch)</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_ppl_epoch'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_ppl'</span>]), epoch_itr.epoch)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># every n update</span></span><br><span class="line">            num_updates = stats[<span class="string">'num_updates'</span>]</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_loss_update'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_loss'</span>]),</span><br><span class="line">                                      num_updates / args.save_interval_updates)</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_nll_loss_update'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_nll_loss'</span>]),</span><br><span class="line">                                      num_updates / args.save_interval_updates)</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_ppl_update'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_ppl'</span>]),</span><br><span class="line">                                      num_updates / args.save_interval_updates)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, meter <span class="keyword">in</span> extra_meters.items():</span><br><span class="line">            stats[k] = meter.avg</span><br><span class="line">        progress.print(stats)</span><br><span class="line"></span><br><span class="line">        valid_losses.append(stats[<span class="string">'valid_loss'</span>])</span><br><span class="line">    <span class="built_in">return</span> valid_losses</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://github.com/pytorch/fairseq/tree/master/examples/translation#replicating-results-from-scaling-neural-machine-translation" target="_blank" rel="noopener">Replicating results from “Scaling Neural Machine Translation”
</a></p>
<p><a href="https://github.com/pytorch/fairseq/issues/346" target="_blank" rel="noopener">How to reproduce the result of WMT14 en-de on transformer BASE model?</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2019/01/06/代码相关/代码记录15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/06/代码相关/代码记录15/" itemprop="url">代码片段记录15</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-06T10:18:30+08:00">
                2019-01-06
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-01-06T10:24:16+08:00">
                2019-01-06
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-flatten-multi-dimentional-list"><a href="#1️⃣-flatten-multi-dimentional-list" class="headerlink" title="1️⃣[flatten multi-dimentional list]"></a>1️⃣[flatten multi-dimentional list]</h3><p>对多层嵌套的list进行展平。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 递归</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten</span><span class="params">(nestedList)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">aux</span><span class="params">(listOrItem)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(listOrItem, list):</span><br><span class="line">            <span class="keyword">for</span> elem <span class="keyword">in</span> listOrItem:</span><br><span class="line">                <span class="keyword">for</span> item <span class="keyword">in</span> aux(elem):</span><br><span class="line">                    <span class="keyword">yield</span> item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">yield</span> listOrItem</span><br><span class="line">    <span class="keyword">return</span> list(aux(nestedList))</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="2️⃣-sorted-index"><a href="#2️⃣-sorted-index" class="headerlink" title="2️⃣[sorted index]"></a>2️⃣[sorted index]</h3><p>使用内置方法获得排好序的index</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sorted_index=[i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> sorted(enumerate(sent_length),</span><br><span class="line">                                    key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>],</span><br><span class="line">                                    reverse=self.reverse)]</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2019/01/06/论文/每周论文11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/06/论文/每周论文11/" itemprop="url">每周论文11</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-06T09:58:30+08:00">
                2019-01-06
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-01-07T10:11:24+08:00">
                2019-01-07
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1️⃣-Multi-Head-Attention-with-Disagreement-Regularization"><a href="#1️⃣-Multi-Head-Attention-with-Disagreement-Regularization" class="headerlink" title="1️⃣[Multi-Head Attention with Disagreement Regularization]"></a>1️⃣[Multi-Head Attention with Disagreement Regularization]</h2><p>EMNLP的短文。</p>
<p>鼓励transformer中head与head之间的差异。</p>
<p>加了三种正则化方法：<br>①on subspace<br><img src="/images/15467399912055.jpg" width="40%" height="50%"></p>
<p>②on attention position<br><img src="/images/15467400218650.jpg" width="40%" height="50%"></p>
<p>③on output<br><img src="/images/15467400417247.jpg" width="40%" height="50%"></p>
<p>没什么亮点。</p>
<hr>
<h2 id="2️⃣-Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overﬁtting"><a href="#2️⃣-Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overﬁtting" class="headerlink" title="2️⃣[Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting]"></a>2️⃣[Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting]</h2><p>经典论文。<br>dropout方法很简单，但如何想到，其背后的intuition，以及一些现象很有启发意义。<br>仅罗列一些intuition/motivation以及现象：</p>
<ol>
<li>网络复杂关系学到很多噪声，导致overfitting</li>
<li>最好的regularization方法是对所有的parameter setting的结果进行average。这就是贝叶斯方法， dropout是对该方法进行近似，论文也提到了model combination</li>
<li>dropout能够减少unit之间复杂的co-adaptation，能够更鲁棒，也就是说，不需要依赖其他unit去纠正自己的错误。each hidden unit in a neural network trained with dropout must learn to work with a randomly chosen sample of other units. This should make each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes</li>
<li>dropout的特性：sparsity。标准的网络在训练过程中会固化其他unit的错误，导致复杂的co-adaptation，但这种复杂的adaptation会导致泛化性的降低，因为对于未见到的数据这种复杂的adaptation是没用的。因此dropout的网络中每个unit都要学会自己纠正自己的错误，因此每个unit能够独立学到数据的一部分特性。dropout会导致稀疏化，每次都只会有一小部分的activation高。使用dropout配合高的学习率比较好，因为dropout可能会导致gradient之间互相cancel，同时也可以使用高的momentum。</li>
</ol>
<p><img src="/images/15467404963033.jpg" width="80%" height="50%"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/31/碎片知识/关于Pytorch中index_copy_及其思考/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/31/碎片知识/关于Pytorch中index_copy_及其思考/" itemprop="url">关于Pytorch中index_copy_及其思考</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-31T17:39:14+08:00">
                2018-12-31
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-31T17:53:21+08:00">
                2018-12-31
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>前几日因为in-place操作的问题，debug了好几天，最终才发现问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">output,_=pad_packed_sequence(output,batch_first=<span class="keyword">True</span>)</span><br><span class="line">output=output.index_copy(<span class="number">0</span>,torch.tensor(sorted_index),output)</span><br></pre></td></tr></table></figure>
<p>因为Pytorch中pack_sequence需要将batch按长度排列，我在过完GRU后需要将其顺序还原，在这边sorted_index即是记录原来index映射。</p>
<p>然而我在写的时候，参考的是官方的example：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]], dtype=torch.float)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>index = torch.tensor([<span class="number">0</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.index_copy_(<span class="number">0</span>, index, t)</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure>
<p>因此我也不假思索地写：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">output,_=pad_packed_sequence(output,batch_first=<span class="keyword">True</span>)</span><br><span class="line">output=output.index_copy_(<span class="number">0</span>,torch.tensor(sorted_index),output)</span><br></pre></td></tr></table></figure></p>
<p>就因为多了一个_，导致逻辑和我想象中的不一样。</p>
<p>一个简单的例子展示为什么这么是错的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x=torch.Tensor([<span class="number">21</span>,<span class="number">42</span>,<span class="number">45</span>,<span class="number">59</span>])</span><br><span class="line"></span><br><span class="line">print(x)  <span class="comment"># tensor([21., 42., 45., 59.])</span></span><br><span class="line"></span><br><span class="line">index=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">x=x.index_copy_(<span class="number">0</span>,index,x)</span><br><span class="line"></span><br><span class="line">print(x)  <span class="comment"># tensor([21., 21., 21., 59.])</span></span><br></pre></td></tr></table></figure>
<p>由于是in-place操作，第一步，将index=0的数值（也即21）复制到index=1的地方，此时变成[21,21,45,59]；接着将index=1的数值复制到index=2的位置上，注意到之前已经是in-place操作，因此此时取的不是想象中的42，而是已经被替换的21。后面的也是如此。</p>
<p>正确的做法只需要去掉in-place即可。</p>
<hr>
<p>已经好几次遇到in-place的问题了，在每次做in-place操作时，都要警惕。应尽可能避免in-place操作。实际上Pytorch官方也不建议使用in-place操作。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/29/代码相关/代码记录14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/29/代码相关/代码记录14/" itemprop="url">代码片段记录14</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-29T19:29:30+08:00">
                2018-12-29
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-29T22:34:20+08:00">
                2018-12-29
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-shuffle-list"><a href="#1️⃣-shuffle-list" class="headerlink" title="1️⃣[shuffle list]"></a>1️⃣[shuffle list]</h3><p>shuffle list可以使用random的shuffle函数，亦即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">l=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">shuffle(l)  <span class="comment"># in place operation</span></span><br></pre></td></tr></table></figure>
<p>而想要shuffle两个对应list，也即等长且一一对应的list，则可以：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># borrow from stackoverflow</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shuffle</span><span class="params">(a,b)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(a) == len(b)</span><br><span class="line">    start_state = random.getstate()</span><br><span class="line">    random.shuffle(a)</span><br><span class="line">    random.setstate(start_state)</span><br><span class="line">    random.shuffle(b)</span><br><span class="line"></span><br><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]</span><br><span class="line">b = [<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>,<span class="number">19</span>]</span><br><span class="line">shuffle(a,b)</span><br><span class="line">print(a) <span class="comment"># [9, 7, 3, 1, 2, 5, 4, 8, 6]</span></span><br><span class="line">print(b) <span class="comment"># [19, 17, 13, 11, 12, 15, 14, 18, 16]</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="2️⃣-inverse-tensor"><a href="#2️⃣-inverse-tensor" class="headerlink" title="2️⃣[inverse tensor]"></a>2️⃣[inverse tensor]</h3><p>Pytorch目前还不支持步进为负的情况，因此不能使用类似Python的<code>l[::-1]</code>的方法reverse tensor。<br>一种解决方案：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">inv_idx = torch.arange(tensor.size(<span class="number">0</span>)<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>).long()</span><br><span class="line"><span class="comment"># or equivalently torch.range(tensor.size(0)-1, 0, -1).long()</span></span><br><span class="line">inv_tensor = tensor.index_select(<span class="number">0</span>, inv_idx)</span><br><span class="line"><span class="comment"># or equivalently</span></span><br><span class="line">inv_tensor = tensor[inv_idx]</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="3️⃣-GRU-initialization"><a href="#3️⃣-GRU-initialization" class="headerlink" title="3️⃣[GRU initialization]"></a>3️⃣[GRU initialization]</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_gru_init</span><span class="params">(self)</span>:</span>   <span class="comment"># use orthogonal seems better</span></span><br><span class="line">    nn.init.orthogonal_(self.word_RNN.weight_ih_l0.data)  <span class="comment">#没有data不行，会报leaf variable in-place错误，可能weight_ih_l0不是parameter</span></span><br><span class="line">    nn.init.orthogonal_(self.word_RNN.weight_hh_l0.data)</span><br><span class="line">    self.word_RNN.bias_ih_l0.data.zero_()</span><br><span class="line">    self.word_RNN.bias_hh_l0.data.zero_()</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="4️⃣-sort-counter"><a href="#4️⃣-sort-counter" class="headerlink" title="4️⃣[sort counter]"></a>4️⃣[sort counter]</h3><p>需求：统计document的句子个数的分布，并按照长度顺序排列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n_sents=[len(sentences) <span class="keyword">for</span> sentences <span class="keyword">in</span> documents]</span><br><span class="line">n_lengths=Counter(n_sents)</span><br><span class="line">n_lengths=sorted(n_lengths.items())</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/29/碎片知识/每周碎片知识17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/29/碎片知识/每周碎片知识17/" itemprop="url">每周碎片知识17</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-29T19:28:14+08:00">
                2018-12-29
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-29T19:28:42+08:00">
                2018-12-29
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>在有RNN的代码中，如果出现</p>
<blockquote>
<p>Cuda Error : RuntimeError: CUDNN_STATUS_EXECUTION_FAILED</p>
</blockquote>
<p>那么可能的出错原因是没有将init state放入cuda中。</p>
<p>Reference: <a href="https://discuss.pytorch.org/t/cuda-error-runtimeerror-cudnn-status-execution-failed/17625" target="_blank" rel="noopener">https://discuss.pytorch.org/t/cuda-error-runtimeerror-cudnn-status-execution-failed/17625</a></p>
<hr>
<h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>clone() → Tensor<br>Returns a copy of the self tensor. The copy has the same size and data type as self.<br><strong>Unlike copy_(), this function is recorded in the computation graph. Gradients propagating to the cloned tensor will propagate to the original tensor.</strong></p>
<p>如果需要另一个相同的tensor做其他计算，则使用clone()而不是copy_()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">forward_vec=sent_vec</span><br><span class="line"><span class="comment"># backward_vec=sent_vec   wrong</span></span><br><span class="line">backward_vec=sent_vec.clone()</span><br></pre></td></tr></table></figure>
<p>当然也不能直接赋值，因为赋的只是指针，改变backward_vec也会改变原来的值。</p>
<hr>
<h3 id="3️⃣-Python"><a href="#3️⃣-Python" class="headerlink" title="3️⃣[Python]"></a>3️⃣[Python]</h3><p>Python中<code>==</code>和<code>is</code>的区别：<br>is表示是否是同一个object；而==表示是否是同一个值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">str=<span class="string">'GRU'</span></span><br><span class="line">str == <span class="string">'GRU'</span>  <span class="comment"># True</span></span><br><span class="line">str <span class="keyword">is</span> <span class="string">'GRU'</span>  <span class="comment"># True</span></span><br><span class="line">str=str.upper()</span><br><span class="line">str == <span class="string">'GRU'</span>  <span class="comment"># False</span></span><br><span class="line">str <span class="keyword">is</span> <span class="string">'GRU'</span>  <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<hr>
<h3 id="4️⃣-RNN"><a href="#4️⃣-RNN" class="headerlink" title="4️⃣[RNN]"></a>4️⃣[RNN]</h3><p>在RNN的初始化中，使用正交初始化会比其他方法好一些（待对比实验测验）。<br>Reference: <a href="https://smerity.com/articles/2016/orthogonal_init.html" target="_blank" rel="noopener">https://smerity.com/articles/2016/orthogonal_init.html</a></p>
<hr>
<h3 id="5️⃣-Pytorch"><a href="#5️⃣-Pytorch" class="headerlink" title="5️⃣[Pytorch]"></a>5️⃣[Pytorch]</h3><p>在提供预训练embedding作为初始化时，正确做法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> pretrained_matrix <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">    pretrained_matrix=torch.from_numpy(pretrained_matrix).type(torch.FloatTensor)</span><br><span class="line">    self.embedding.weight= nn.Parameter(pretrained_matrix,</span><br><span class="line">                                                requires_grad=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>必须要有<code>.type(torch.FloatTensor)</code>，否则会出错：CuDNN error: CUDNN_STATUS_EXECUTION_FAILED</p>
<hr>
<h3 id="6️⃣-Pytorch"><a href="#6️⃣-Pytorch" class="headerlink" title="6️⃣[Pytorch]"></a>6️⃣[Pytorch]</h3><p>Pytorch中，将初始hidden state作为可学习参数实践：<br><a href="https://discuss.pytorch.org/t/solved-train-initial-hidden-state-of-rnns/2589/9" target="_blank" rel="noopener">https://discuss.pytorch.org/t/solved-train-initial-hidden-state-of-rnns/2589/9</a><br><a href="https://discuss.pytorch.org/t/learn-initial-hidden-state-h0-for-rnn/10013/7" target="_blank" rel="noopener">https://discuss.pytorch.org/t/learn-initial-hidden-state-h0-for-rnn/10013/7</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/23/见闻&想法/A Day with Google/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/23/见闻&想法/A Day with Google/" itemprop="url">A Day with Google</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-23T10:06:14+08:00">
                2018-12-23
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-23T10:27:28+08:00">
                2018-12-23
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本周一乡下人终于再次进城了🙈<br><img src="/images/IMG_2243.jpg" width="70%" height="50%"></p>
<p><img src="/images/IMG_8631.jpg" width="70%" height="50%"></p>
<p>本次的目的是来参观Google。</p>
<p>高楼林立：<br><img src="/images/IMG_2273.jpg" width="70%" height="50%"></p>
<p>Here We are:<br><img src="/images/IMG_9209-1.jpg" width="70%" height="50%"></p>
<p>咕果是什么鬼？<br><img src="/images/IMG_3389.jpg" width="70%" height="50%"></p>
<p>宣讲：<br><img src="/images/IMG_1782.jpg" width="70%" height="50%"></p>
<p><img src="/images/IMG_1075.jpg" width="70%" height="50%"></p>
<p>不得不感慨食堂真好🦆，还有专门吃面的食堂。而且还都不用钱🙉，对比张江的食堂🙉：</p>
<p><img src="/images/IMG_0546.jpg" width="70%" height="50%"></p>
<p>溜了溜了：<br><img src="/images/IMG_1255.jpg" width="70%" height="50%"></p>
<p><img src="/images/IMG_1256.jpg" width="70%" height="50%"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/23/论文/每周论文10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/23/论文/每周论文10/" itemprop="url">每周论文10</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-23T09:46:30+08:00">
                2018-12-23
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-01-06T10:19:35+08:00">
                2019-01-06
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1️⃣-Regularization-of-Neural-Networks-using-DropConnect"><a href="#1️⃣-Regularization-of-Neural-Networks-using-DropConnect" class="headerlink" title="1️⃣[Regularization of Neural Networks using DropConnect]"></a>1️⃣[Regularization of Neural Networks using DropConnect]</h2><p>在dropout的基础上提出dropconnect。与dropout不同的是，dropconnect对weight进行drop而不是对layer进行drop。</p>
<p>创新之处在于inference的时候和dropout不同。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p><img src="/images/15455297378934.jpg" width="50%" height="50%"></p>
<h3 id="inference"><a href="#inference" class="headerlink" title="inference"></a>inference</h3><p><img src="/images/15455297645976.jpg" width="50%" height="50%"></p>
<p>在inference的时候通过高斯采样的方法去模拟训练时的伯努利分布。<br><strong>intuition</strong>：<br>本文对dropout在inference简单对unit进行缩放进行反思，认为这在数学上并不合理，因此提出用高斯分布去采样。<br><img src="/images/15455299241433.jpg" width="50%" height="50%"></p>
<p><img src="/images/15455299403032.jpg" width="50%" height="50%"></p>
<p><img src="/images/15455299548705.jpg" width="50%" height="50%"></p>
<hr>
<h2 id="2️⃣-Attentive-Pooling-Networks"><a href="#2️⃣-Attentive-Pooling-Networks" class="headerlink" title="2️⃣[Attentive Pooling Networks]"></a>2️⃣[Attentive Pooling Networks]</h2><p>提出attentive pooling机制，用以answer selection。<br>（什么是answer selection：给定一个问题，给定多个答案候选，要从答案选项中选择正确的答案。）</p>
<p>传统answer selection：<br><img src="/images/15455301265939.jpg" width="35%" height="50%"><br>首先将词转化成词向量，接着通过bi-LSTM或CNN获得一个矩阵表示，接下来对Q和A分别进行max-pooling获得固定表示，最后通过cos距离判断答案是否是正确答案，从答案候选中选择分数最高的。</p>
<p>但这样的问题在于Q和A之间没有交互。</p>
<p>本文利用attention作为Q和A的交互。<br><img src="/images/15455301891043.jpg" width="39%" height="50%"></p>
<p>获得Q和A矩阵的方式是一致的。<br>接下来，首先计算一个G矩阵，通过双线性attention公式获得：<br><img src="/images/15455302279543.jpg" width="20%" height="50%"></p>
<p>G所代表的意义是Q和A的每个词之间的对齐：对于第i行来说，代表Q的第i个词和A中所有词的一个分数；对于第j列来说，代表第j个词和Q中所有词的分数。</p>
<p>接下来对G的行和列分别进行max-pooling操作：<br><img src="/images/15455303089243.jpg" width="25%" height="50%"></p>
<p>此步代表选择与某词关系最重要的词。</p>
<p>接下来对g分别进行softmax，再分别进行点积以获得最终向量表示：<br><img src="/images/15455303516483.jpg" width="13%" height="50%"></p>
<p>同样，最终使用cos距离计算相似度。</p>
<hr>
<h2 id="3️⃣-Improved-Regularization-of-Convolutional-Neural-Networks-with-Cutout"><a href="#3️⃣-Improved-Regularization-of-Convolutional-Neural-Networks-with-Cutout" class="headerlink" title="3️⃣[Improved Regularization of Convolutional Neural Networks with Cutout]"></a>3️⃣[Improved Regularization of Convolutional Neural Networks with Cutout]</h2><p>是从数据增强和dropout的角度：</p>
<blockquote>
<p>dropout in convolutional layers simply acts to increase robustness to noisy inputs, rather than having the same model averaging effect that is observed in fully-connected layers</p>
</blockquote>
<p>某个输入被移去，所有后面相关的的feature map都被移去：</p>
<blockquote>
<p>In this sense, cutout is much closer to data augmentation than dropout, as it is not creating noise, but instead generating images that appear novel to the network</p>
</blockquote>
<p>其实只是将输入随机drop掉一块。<br><img src="/images/15455304317998.jpg" width="50%" height="50%"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/23/代码相关/代码记录13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/23/代码相关/代码记录13/" itemprop="url">代码片段记录13</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-23T09:42:30+08:00">
                2018-12-23
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-23T09:44:04+08:00">
                2018-12-23
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-flatten-list"><a href="#1️⃣-flatten-list" class="headerlink" title="1️⃣[flatten list]"></a>1️⃣[flatten list]</h3><p>对二维list进行展开。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">list2d = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>], [<span class="number">8</span>,<span class="number">9</span>]]</span><br><span class="line"><span class="comment"># ①</span></span><br><span class="line">flatten = [l <span class="keyword">for</span> list <span class="keyword">in</span> list2d <span class="keyword">for</span> l <span class="keyword">in</span> list]</span><br><span class="line"><span class="comment"># ②</span></span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line">merged = list(itertools.chain(*list2d))</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">merged = list(itertools.chain.from_iterable(list2d))</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/23/碎片知识/每周碎片知识16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/23/碎片知识/每周碎片知识16/" itemprop="url">每周碎片知识16</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-23T09:09:14+08:00">
                2018-12-23
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-29T18:48:42+08:00">
                2018-12-29
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-Softmax"><a href="#1️⃣-Softmax" class="headerlink" title="1️⃣[Softmax]"></a>1️⃣[Softmax]</h3><p>在使用softmax的时候，要非常注意softmax的行为。应尽量控制softmax前元素的规模，否则容易出现one-hot的情况，导致训练困难。<br><img src="/images/15455275366030.jpg" width="70%" height="50%"></p>
<p>同时，对全-inf做softmax是未定义的，因此也会出现问题：<br><img src="/images/15455278529550.jpg" width="40%" height="50%"></p>
<hr>
<h3 id="2️⃣-slice"><a href="#2️⃣-slice" class="headerlink" title="2️⃣[slice]"></a>2️⃣[slice]</h3><p>在对tensor或array操作时，如果需要取某维的slice：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.rand(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">a[:,<span class="number">1</span>:<span class="number">3</span>]  <span class="comment"># 取第1列到第2列的slice</span></span><br><span class="line">a[:][<span class="number">1</span>:<span class="number">3</span>] <span class="comment"># wrong，获得的是第1行到第2行的slice</span></span><br></pre></td></tr></table></figure>
<p>原因是，<code>a[:][1:3]</code>是先做<code>a[:]</code>操作，获得了全部元素，然后再做<code>[1:3]</code>操作，也即获得第1行到第2行的元素。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg"
                alt="林泽辉" />
            
              <p class="site-author-name" itemprop="name">林泽辉</p>
              <p class="site-description motion-element" itemprop="description">人一己千</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">116</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">137</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">林泽辉</span>

  
</div>






  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
