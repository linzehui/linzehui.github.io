<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/favicon.ico?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="人一己千">
<meta property="og:type" content="website">
<meta property="og:title" content="Weekly Review">
<meta property="og:url" content="http://www.linzehui.me/index.html">
<meta property="og:site_name" content="Weekly Review">
<meta property="og:description" content="人一己千">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Weekly Review">
<meta name="twitter:description" content="人一己千">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.linzehui.me/"/>





  <title>Weekly Review</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Weekly Review</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2019/03/31/论文/每周论文15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/31/论文/每周论文15/" itemprop="url">每周论文15</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-31T22:04:30+08:00">
                2019-03-31
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-04-01T11:21:55+08:00">
                2019-04-01
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本周论文:</p>
<ol>
<li>Selective Kernel Networks</li>
<li>Attentional pooling for action recognition</li>
</ol>
<h2 id="1️⃣-Selective-Kernel-Networks"><a href="#1️⃣-Selective-Kernel-Networks" class="headerlink" title="1️⃣[Selective Kernel Networks]"></a>1️⃣[Selective Kernel Networks]</h2><p>通过对不同kernel size的feature map之间进行信息筛选获得更为鲁棒的表示，能够对不同的感受野进行整合，实现动态调整感受野。其思路还挺有意思的。</p>
<p>Introduction将该模型与视觉神经的理论结合在一起，也即，对于人类而言，在看不同尺寸不同远近的物体时，视觉皮层神经元<strong>感受野大小</strong>是会根据刺激来进行调节的，但一般而言在CNN中卷积核的大小是固定的。该模型正是从这一现象中获得灵感。</p>
<p>整个模型一共分为三个步骤：split，fuse，select</p>
<p>split生成多个不同kernel size的feature map，也即对应不同的感受野大小；fuse将不同feature map结合起来，获得一个全局的综合的向量表示；select根据不同的weight选择不同感受野的feature map。</p>
<p><img src="/images/15540416698404.jpg" width="80%" height="50%"></p>
<p>以上图为例。</p>
<h3 id="SK-Net"><a href="#SK-Net" class="headerlink" title="SK-Net"></a>SK-Net</h3><h4 id="第一步split"><a href="#第一步split" class="headerlink" title="第一步split"></a>第一步split</h4><p>给定输入$\mathbf{X} \in \mathbb{R}^{H^{\prime} \times W^{\prime} \times C^{\prime}}$，通过不同的kernel size的CNN的卷积获得不同的feature map，上图是$3\times 3$与$5\times 5$的卷积核。卷积可以是传统的convolution卷积，也可以是空洞卷积（dilated convolution），或者深度卷积（depthwise convolution）。则有：<br>$\widetilde{\mathcal{F}} : \mathbf{X} \rightarrow \widetilde{\mathbf{U}} \in \mathbb{R}^{H \times W \times C}$ 与 $\widehat{\mathcal{F}} : \mathbf{X} \rightarrow \widehat{\mathbf{U}} \in \mathbb{R}^{H \times W \times C}$，其中$\widetilde{\mathcal{F}},\widehat{\mathcal{F}}$是卷积变换。</p>
<h4 id="第二步fuse"><a href="#第二步fuse" class="headerlink" title="第二步fuse"></a>第二步fuse</h4><p>直接将不同的feature map结合起来以获得全局信息，用以之后的动态调整。这里采用简单的求和以及global average pooling以获得channel-wise的信息$\mathbf{s} \in \mathbb{R}^{C}$：</p>
<script type="math/tex; mode=display">\mathbf{U}=\widetilde{\mathbf{U}}+\widehat{\mathbf{U}} \\ s_{c}=\mathcal{F}_{g p}\left(\mathbf{U}_{c}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} \mathbf{U}_{c}(i, j)</script><p>在获得$\mathbf{s}$后再通过MLP获得$\mathbf{z}$：</p>
<script type="math/tex; mode=display">\mathbf{z}=\mathcal{F}_{f c}(\mathbf{s})=\delta(\mathcal{B}(\mathbf{W} \mathbf{s}))</script><p>其中$\mathcal{B}$是batch normalization；$\delta$是Relu。</p>
<h4 id="第三步select"><a href="#第三步select" class="headerlink" title="第三步select"></a>第三步select</h4><p>使用soft attention去选择不同kernel size的feature map并结合在一起。也即：</p>
<script type="math/tex; mode=display">a_{c}=\frac{e^{\mathbf{A}_{c} \mathbf{z}}}{e^{\mathbf{A}_{c} \mathbf{z}}+e^{\mathbf{B}_{c} \mathbf{z}}}, b_{c}=\frac{e^{\mathbf{B}_{c} \mathbf{z}}}{e^{\mathbf{A}_{c} \mathbf{z}}+e^{\mathbf{B}_{c} \mathbf{z}}}</script><p>其中$\mathbf{A}_{c}$是对应$\widetilde{\mathbf{U}}$第$c$个channel的参数，$\mathbf{B}_{c}$是对应$\widehat{\mathbf{U}}$第$c$个channel的参数。$\mathbf{A}, \mathbf{B} \in \mathbb{R}^{C \times d}$，那么$a_{c},b_{c}$就对应不同feature map的weight。</p>
<p>因此，最终的feature map $\mathbf{V}$：</p>
<script type="math/tex; mode=display">\mathbf{V}_{c}=a_{c} \cdot \tilde{\mathbf{U}}_{c}+b_{c} \cdot \widehat{\mathbf{U}}_{c}, \quad a_{c}+b_{c}=1 \\ \mathbf{V}=\left[\mathbf{V}_{1}, \mathbf{V}_{2}, \dots, \mathbf{V}_{C}\right], \mathbf{V}_{c} \in \mathbb{R}^{H \times W}</script><h3 id="对比-amp-思考"><a href="#对比-amp-思考" class="headerlink" title="对比&amp;思考"></a>对比&amp;思考</h3><h4 id="与SE-Net"><a href="#与SE-Net" class="headerlink" title="与SE-Net"></a>与SE-Net</h4><p>SE-Net是通过不同channel之间的交互，使得channel获得全局的感受野，使用的是对channel的放缩（详见上一篇论文笔记）；而SK-Net是不同的感受野之间的同一channel在通过全局信息的指导下以soft-attention的形式加权平均，这就和论文中提到的人类视觉对不同物体进行动态调整感受野的思路一致。</p>
<h4 id="与dynamic-convolution"><a href="#与dynamic-convolution" class="headerlink" title="与dynamic convolution"></a>与dynamic convolution</h4><p>在论文[PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS]中，研究人员提出动态感受野的convolution，通过利用当前词预测一个卷积窗口，增加了模型的灵活性，并在机器翻译上取得了很好的结果。</p>
<p>虽然目的与本篇论文一致，但思路是完全不同的。一个是通过预测；另一个是在全局信息的指导下进行加权。在我的理解看来，或许本篇论文的思路更加合理一些，第一，在有了全局信息的指导下能够更好的进行加权，而通过预测，似乎有些盲目，可能需要更多的数据才能学得更好；第二，dynamic convolution论文中也提到了，如果不使用如深度可分离卷积等轻量级卷积方法，dynamic convolution不大现实（A dynamic version of standard convolutions would be impractical for current GPUs due to their large memory requirements），而SK-Net则不会有这个问题。</p>
<h4 id="其他思考"><a href="#其他思考" class="headerlink" title="其他思考"></a>其他思考</h4><p>从另一个角度去思考，SK-Net通过人工定义好的几种不同大小的卷积，相当于在模型中引入更强的先验（inductive bias），也即假设了数据不会超过这几种大小的卷积的处理范围，这或许比不引入先验，完全靠数据去学某种特定pattern的dynamic convolution对小数据集更友好，因此可以不需要更多的数据来使得模型表现良好。类似的理解可以在CNN/RNN与Transformer的对比中看见，因为CNN/RNN引入了较强的local bias，因此对于小数据集更友好，但同时其上限或许不如Transformer高；而Transformer一开始就是全局感受野，使得需要更多数据来帮助模型学到某种特定pattern（如某种local bias），但当数据充足时，Transformer的上限更高，近期非常火的pretrained model GPT/GPT-2.0/Bert似乎也印证了这点。</p>
<hr>
<h2 id="2️⃣-Attentional-pooling-for-action-recognition"><a href="#2️⃣-Attentional-pooling-for-action-recognition" class="headerlink" title="2️⃣[Attentional pooling for action recognition]"></a>2️⃣[Attentional pooling for action recognition]</h2><p>提出一种基于attention的pooling策略，采用低秩近似的方法，使得模型能够在计算量不增加很多的情况下达到更好的效果。可以将该方法理解成对二阶pooling的低秩近似。</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="一阶pooling"><a href="#一阶pooling" class="headerlink" title="一阶pooling"></a>一阶pooling</h4><p>记$X \in R^{n \times f}$为被pooling的层，其中n为空间位置的个数，如$16\times 16$，$f$为channel个数。标准的sum/max pooling将该矩阵缩减为$R^{f \times 1}$，然后使用全连接的权重$\mathbf{w} \in R^{f \times 1}$获得一个分类的分数。这里假设的是二分类，但可以很容易推广为多分类。</p>
<p>上述操作形式化可以写成：</p>
<script type="math/tex; mode=display">\operatorname{score}_{p o o l}(X)=\mathbf{1}^{T} X \mathbf{w}, \quad \text { where } \quad X \in R^{n \times f}, \mathbf{1} \in R^{n \times 1}, \mathbf{w} \in R^{f \times 1}</script><p>其中$\mathbf{1}$为全1向量，$\mathbf{x}=\mathbf{1}^{T} X \in R^{1 \times f}$就是通过sum pooling后的feature。</p>
<h4 id="二阶pooling"><a href="#二阶pooling" class="headerlink" title="二阶pooling"></a>二阶pooling</h4><p>构建二阶feature $X^{T} X \in R^{f \times f}$，在获得二阶feature后，通常或向量化该矩阵，再送入全连接以做分类。也即我们会学习一个$f\times f$的全连接权重向量。若保持二阶feature与对应的全连接权重向量的形式为矩阵，矩阵相乘，其中的迹实际上就是这两个向量化后的矩阵所做内积获得的分数。形式化可以写成：</p>
<script type="math/tex; mode=display">\text {score}_{order2}(X)=\operatorname{Tr}\left(X^{T} X W^{T}\right), \quad \text { where } \quad X \in R^{n \times f}, W \in R^{f \times f}</script><p>这可以用迹的定义去证明：示意图<br><img src="/images/15540862875594.jpg" width="100%" height="50%"></p>
<h4 id="低秩二阶pooling"><a href="#低秩二阶pooling" class="headerlink" title="低秩二阶pooling"></a>低秩二阶pooling</h4><p>现尝试使用低秩去近似该二阶pooling，也即对$W$近似，将$W$写成两个向量的乘积，也即：</p>
<script type="math/tex; mode=display">W=\mathbf{a b}^{T} \text { where } \mathbf{a}, \mathbf{b} \in R^{f \times 1}</script><p>将上式代入二阶pooling，可获得：</p>
<script type="math/tex; mode=display">\begin{aligned} \text {score}_{\text {attention}}(X) &=\operatorname{Tr}\left(X^{T} X \mathbf{b a}^{T}\right), \quad \text { where } \quad X \in R^{n \times f}, \mathbf{a}, \mathbf{b} \in R^{f \times 1} \\ &=\operatorname{Tr}\left(\mathbf{a}^{T} X^{T} X \mathbf{b}\right) \\ &=\mathbf{a}^{T} X^{T} X \mathbf{b} \\ &=\mathbf{a}^{T}\left(X^{T}(X \mathbf{b})\right) \end{aligned}</script><p>第二行使用的是迹的定理：$\operatorname{Tr}(A B C)=\operatorname{Tr}(C A B)$<br>第三行使用的是标量的迹等于标量本身。<br>最后一行表明整个流程：给定一个feature map $X$，首先计算一个对所有空间位置的attentional map：$\mathbf{h}= {X \mathbf{b} \in R^{n \times 1}}$；然后根据该attentional map计算加权平均的feature：$\mathbf{x}=X^{T} \mathbf{h} \in R^{f \times 1}$。该feature再通过线性层获得最终的分数。</p>
<p>实际上上式还有其他理解的角度：</p>
<script type="math/tex; mode=display">\begin{aligned} \text {score}_{\text {attention}}(X) &=\left((X \mathbf{a})^{T} X\right) \mathbf{b} \\ &=(X \mathbf{a})^{T}(X \mathbf{b}) \end{aligned}</script><p>第一行表明attentional map也可以通过$X \mathbf{a} \in R^{n \times 1}$来计算，$\mathbf{b}$来做classifier。<br>第二行表明，该式子本质上是对称的，可以看成<strong>两个attentional heapmap的内积</strong>。</p>
<p>下图是整个流程：<br><img src="/images/15540869385196.jpg" width="80%" height="50%"></p>
<h4 id="Top-down-attention"><a href="#Top-down-attention" class="headerlink" title="Top-down attention"></a>Top-down attention</h4><p>现将二分类推广为多分类：</p>
<script type="math/tex; mode=display">\text {score}_{order2}(X, k)=\operatorname{Tr}\left(X^{T} X W_{k}^{T}\right), \quad \text { where } \quad X \in R^{n \times f}, W_{k} \in R^{f \times f}</script><p>也即将$W$替换成类相关的参数，仿照上面的推导，可以推出每个类都有特定的$\boldsymbol{a}_{k}$与$\boldsymbol{b}_{k}$。</p>
<p>但在这里通过固定其中一个参数为与类无关的参数，也即$\boldsymbol{b}_{k}=\boldsymbol{b}$。实际上就等价于一个是类相关的top-down attention；另一个是类无关的bottom-up attention。一个获得类特定的特征；另一个获得全局通用的特征。</p>
<p>因此最终低秩attention model为：</p>
<script type="math/tex; mode=display">\text {score}_{attention}(X, k)=\mathbf{t}_{k}^{T} \mathbf{h}, \quad \text { where } \quad \mathbf{t}_{k}=X \mathbf{a}_{k}, \mathbf{h}=X \mathbf{b}</script><h4 id="Average-pooling-Revisited"><a href="#Average-pooling-Revisited" class="headerlink" title="Average-pooling Revisited"></a>Average-pooling Revisited</h4><p>当然在给定了上述一系列的推导，我们对average-pooling重新进行形式化：</p>
<script type="math/tex; mode=display">\text {score}_{top-down}(X, k)=\mathbf{1}^{T} X \mathbf{w}_{k}=\mathbf{1}^{T} \mathbf{t}_{k} \quad \text { where } \quad \mathbf{t}_{k}=X \mathbf{w}_{k}</script><p>将$\mathbf{w}$替换成类相关的$\mathbf{w}_{k}$，实际上就是将二分类推广为多分类。但该形式赋予了average-pooling新的理解。</p>
<p>当然，我们还可以将rank-1推广为rank-k，实验证明对于大数据集使用大的秩会更好。</p>
<h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><h4 id="与Self-attentive的联系"><a href="#与Self-attentive的联系" class="headerlink" title="与Self-attentive的联系"></a>与Self-attentive的联系</h4><p>论文[A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING]就提出了利用可学习的head对feature进行attention加权平均的方法，并且将一个head推广到多个head。<br>实际上在$\mathbf{h}= {X \mathbf{b} \in R^{n \times 1}}$我们就可以看出，$\mathbf{b}$在这里扮演的角色就是self-attentive的head的角色。对于秩为1的近似，就是head为1的情况，若将秩为1推广为秩为k，也即等价于在Self-attentive中多个head的情况。</p>
<p>本文巧妙的地方在于head有两个作用，一种是top-down的head，获得的是类相关的feature；另一个是bottom-up的feature，获得的是通用的feature。并且本文通过巧妙的数学推导来获得新的解释，本来仅仅是二阶feature过一个全连接，但通过公式推导赋予了attention的解释，这点让人眼前一亮。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2019/03/31/诗词&句/每周诗词21/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/31/诗词&句/每周诗词21/" itemprop="url">每周诗词21</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-31T11:00:14+08:00">
                2019-03-31
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-03-31T11:02:39+08:00">
                2019-03-31
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣醉落魄-·-席上呈元素"><a href="#1️⃣醉落魄-·-席上呈元素" class="headerlink" title="1️⃣醉落魄 · 席上呈元素"></a>1️⃣醉落魄 · 席上呈元素</h3><p>[宋] 苏轼<br>分携如昨，人生到处萍飘泊。偶然相聚还离索。多病多愁，须信从来错。<br><strong>尊前一笑休辞却，天涯同是伤沦落</strong>。故山犹负平生约。西望峨嵋，长羡归飞鹤。</p>
<p><a href="http://lib.xcz.im/work/57c467a86be3ff0058452840" target="_blank" rel="noopener">http://lib.xcz.im/work/57c467a86be3ff0058452840</a></p>
<hr>
<h3 id="2️⃣戏为六绝句"><a href="#2️⃣戏为六绝句" class="headerlink" title="2️⃣戏为六绝句"></a>2️⃣戏为六绝句</h3><p>[唐] 杜甫<br>【其一】<br>庾信文章老更成，凌云健笔意纵横。<br>今人嗤点流传赋，不觉前贤畏后生。</p>
<p>【其三】<br>纵使卢王操翰墨，劣于汉魏近风骚。<br>龙文虎脊皆君驭，历块过都见尔曹。</p>
<p>过都历块 (guò dōu lì kuài)<br>解释：越过都市，经过山阜。意指纵横驰骋，施展才能。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2019/03/24/论文/每周论文14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/24/论文/每周论文14/" itemprop="url">每周论文14</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-24T09:57:30+08:00">
                2019-03-24
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-03-24T10:54:54+08:00">
                2019-03-24
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本周论文:</p>
<ol>
<li>Is Second-order Information Helpful for Large-scale Visual Recognition?</li>
<li>The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classification</li>
</ol>
<h2 id="1️⃣-Is-Second-order-Information-Helpful-for-Large-scale-Visual-Recognition"><a href="#1️⃣-Is-Second-order-Information-Helpful-for-Large-scale-Visual-Recognition" class="headerlink" title="1️⃣[Is Second-order Information Helpful for Large-scale Visual Recognition?]"></a>1️⃣[Is Second-order Information Helpful for Large-scale Visual Recognition?]</h2><p>通过协方差的方法获得图像的二阶信息。<br>参考了<a href="https://zhuanlan.zhihu.com/p/46864160" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46864160</a></p>
<p>深度分类网络主要分为两个部分：特征提取和分类器。无论是VGG还是GoogleNet，后来的Resnet、Densenet，在连接分类器之前，一般都连接了一个Pooling层。<br>但pooling只获得了feature的一阶信息，对于细分类问题中类间差异不显著，一阶信息可能有一些不适用，因此我们可以通过一阶信息获得二阶信息，从而获取更有价值的信息。</p>
<p>本文通过获取<strong>特征协方差</strong>的方法，以达到该目的。</p>
<p>输入:$\mathbf{X} \in \mathbb{R}^{d \times N}$</p>
<p>则协方差矩阵为$\mathbf{X} \mapsto \mathbf{P}, \quad \mathbf{P}=\mathbf{X} \overline{\mathbf{I}} \mathbf{X}^{T}$，其中$\overline{\mathbf{I}}=\frac{1}{N}\left(\mathbf{I}-\frac{1}{N} \mathbf{1} \mathbf{1}^{T}\right)$, $\mathbf{I}$是单位阵，$\mathbf{1}$是全1的向量。</p>
<p>协方差矩阵是半正定矩阵，因此可写成$\mathbf{P} \mapsto(\mathbf{U}, \mathbf{\Lambda}), \quad \mathbf{P}=\mathbf{U} \mathbf{\Lambda} \mathbf{U}^{T}$，其中$\boldsymbol{\Lambda}=\operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{d}\right)$，$\mathbf{U}=\left[\mathbf{u}_{1}, \dots, \mathbf{u}_{d}\right]$，$\mathbf{U}$是对应的特征向量。</p>
<p>最终可获得$\mathbf{Q}$矩阵：$(\mathbf{U}, \boldsymbol{\Lambda}) \mapsto \mathbf{Q}, \mathbf{Q} \triangleq \mathbf{P}^{\alpha}=\mathbf{U F}(\mathbf{\Lambda}) \mathbf{U}^{T}$，其中$\alpha$是一个正实数，$\mathbf{F}(\boldsymbol{\Lambda})=\operatorname{diag}\left(f\left(\lambda_{1}\right), \ldots, f\left(\lambda_{d}\right)\right)$，其中$f\left(\lambda_{i}\right)=\lambda_{i}^{\alpha}$，是特征值的幂，如果要做归一化，那么可以有：</p>
<script type="math/tex; mode=display">f\left(\lambda_{i}\right)=\left\{\begin{array}{cc}{\lambda_{i}^{\alpha} / \lambda_{1}^{\alpha}} & {\text { for MPN+M }-\ell_{2}} \\ {\lambda_{i}^{\alpha} /\left(\sum_{k} \lambda_{k}^{2 \alpha}\right)^{\frac{1}{2}}} & {\text { for MPN+M-Fro }}\end{array}\right.</script><p>之所以取幂，是为了解决在协方差估计中小样本高维度的问题，以resnet为例，最后得到的feature为7X7X512，也就是49个512维的feature，这样估计出来的协方差矩阵是不靠谱的，而通过幂这个操作，可以解决这一问题。通过实验可以发现，当幂次为0.5也就是平方根操作时，效果最优。（似乎类似的有word2vec的平滑）</p>
<p>（虽然这篇有些看不大懂，但一个启发就是，可以通过协方差的方式进行特征之间的交互）</p>
<hr>
<h2 id="2️⃣-The-Treasure-beneath-Convolutional-Layers-Cross-convolutional-layer-Pooling-for-Image-Classification"><a href="#2️⃣-The-Treasure-beneath-Convolutional-Layers-Cross-convolutional-layer-Pooling-for-Image-Classification" class="headerlink" title="2️⃣[The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classification]"></a>2️⃣[The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classification]</h2><p>提出使用卷积出来后的feature经过pooling作为最后的图像特征表示而不是全连接后的特征表示。</p>
<p>Motivation：只使用最后一层fc的特征有一个缺点，就是丢失位置信息，而convolution layer包含了丰富的空间信息。在pooling完后每个local区域都能获得一个特征，并拼接起来作为最后的表示。</p>
<p><img src="/images/15533936911589.jpg" width="60%" height="50%"></p>
<p>prerequisite:<br>①首先有一个预训练好的模型<br>②有两层一样$H\times W$的convolution。论文以AlexNet作为例子</p>
<p>假设卷积后的feature map是$H × W × D$，那么可以理解成，我们将图片分为$H × W$的区域，每个区域的特征用$D$维表示。我们称每个$D$维特征一个spatial unit。当使用全连接时，这部分的空间信息就丢失了，并且无法还原。</p>
<p>本文提出，将每个区域提取出一个特征，然后拼起来组成一整张图的特征，如下图，每个长条（也即$1\times 1\times channel$）作为一个特征：</p>
<p><img src="/images/15533939765641.jpg" width="50%" height="50%"></p>
<p>如何判断区域？一种方法是首先检测出多个区域，每个区域对应一种object part，然后对于落入该区域的特征进行pooling，给定D种human-specified object parts，那么可以获得D个feature且拼在一起。</p>
<script type="math/tex; mode=display">\mathbf{P}_{k}^{t}=\sum_{i=1} \mathbf{x}_{i} I_{i, k}</script><p>具体而言，$\mathbf{x}_{i}$是特征，$I_{i, k}$是二元的indicator，表明$\mathbf{x}_{i}$是否落入该区域，每个$I$实际上定义了一个池化通道。当然，这里可以进一步将indicator从二元扩展为权重。</p>
<p>但在实现的过程中，并没有human-specified的区域。这里我们就借助下一层的卷积作为indicator。</p>
<blockquote>
<p>By doing so, D t+1 pooling channels are created for the local features extracted from the tth convolutional layer</p>
</blockquote>
<p>这也就被称为cross-convolutional-layer pooling。</p>
<p>如何做？</p>
<blockquote>
<p>the filter of a convolutional layer works as a part detector and its feature map serves a similar role as the part region indicator map.</p>
</blockquote>
<p>具体而言，有：</p>
<script type="math/tex; mode=display">\begin{array}{l}{\mathbf{P}^{t}=\left[\mathbf{P}_{1}^{t}, \mathbf{P}_{2}^{t}, \cdots, \mathbf{P}_{k}^{t}, \cdots, \mathbf{P}_{D_{t+1}}^{t}\right]} \\ {\text { where, } \mathbf{P}_{k}^{t}=\sum_{i=1}^{N_{t}} \mathbf{x}_{i}^{t} a_{i, k}^{t+1}}\end{array}</script><p>$\mathbf{P}^{t}$表示第t层convolution在卷积过后做cross-pooling后的特征集合，也即我们要获得的表示，该表示通过$D_{t+1}$次pooling后的结果拼接而成。$D_{t+1}$具体来说，就是第t+1层的卷积的channel维数。假设$\mathbf{a}_{i}^{t+1} \in \mathbb{R}^{D_{t+1}}$是第t+1层convolution的第i个空间单位（spatial unit）的feature vector，其中$a_{i, k}^{t+1}$是该向量的一个值，该值就作为pooling的权重。</p>
<p>上述有些绕口且难懂，直接看例子：<br><img src="/images/15533957004853.jpg" width="80%" height="50%"><br><img src="/images/15533957336100.jpg" width="80%" height="50%"></p>
<p>即，第t+1层convolution的channel维度为多少，则pooling后的特征个数即为多少。因为第t层与第t+1层的$H\times W$是一致的，那么可以用t+1层的每个slice去对第t层的convolution进行加权。</p>
<p>为什么这样是合理的？<br>因为第t+1层的convolution提取了$D_{t+1}$个特征，使用的是$m\times n$的kernel size，如果$x$是被$m\times n$的某个kernel提取了，那么很自然的，$x$就是对应该kernel提取出来的feature的一个spatial unit。说白了就是第t层与第t+1层的空间对应。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2019/03/23/诗词&句/每周诗词20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/23/诗词&句/每周诗词20/" itemprop="url">每周诗词20</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-23T17:32:14+08:00">
                2019-03-23
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-03-23T17:35:11+08:00">
                2019-03-23
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣杂诗七首（其四）"><a href="#1️⃣杂诗七首（其四）" class="headerlink" title="1️⃣杂诗七首（其四）"></a>1️⃣杂诗七首（其四）</h3><p>[三国] 曹植<br>南国有佳人，容华若桃李。<br>朝游江北岸，夕宿潇湘沚。<br>时俗薄朱颜，谁为发皓齿？<br>俯仰岁将暮，荣耀难久恃。</p>
<p><a href="http://lib.xcz.im/work/58ad4c78ac502e007e9f6f9f" target="_blank" rel="noopener">http://lib.xcz.im/work/58ad4c78ac502e007e9f6f9f</a></p>
<hr>
<h3 id="2️⃣梦江南"><a href="#2️⃣梦江南" class="headerlink" title="2️⃣梦江南"></a>2️⃣梦江南</h3><p>[唐] 温庭筠<br>千万恨，恨极在天涯。山月不知心里事，水风空落眼前花，摇曳碧云斜。</p>
<p><a href="http://lib.xcz.im/work/57b8d0c77db2a2005425c856" target="_blank" rel="noopener">http://lib.xcz.im/work/57b8d0c77db2a2005425c856</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2019/03/17/论文/每周论文13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/17/论文/每周论文13/" itemprop="url">每周论文13</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-17T22:32:30+08:00">
                2019-03-17
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-03-19T15:41:14+08:00">
                2019-03-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1️⃣-Depthwise-Separable-Convolutions-for-Neural-Machine-Translation"><a href="#1️⃣-Depthwise-Separable-Convolutions-for-Neural-Machine-Translation" class="headerlink" title="1️⃣[Depthwise Separable Convolutions for Neural Machine Translation]"></a>1️⃣[Depthwise Separable Convolutions for Neural Machine Translation]</h2><p>将depthwise separable convolution 深度可分离卷积 用于翻译任务，并在此基础上对depthwise separable进行更进一步的参数量优化，也即super-separable。（其实我觉得并没有啥创新性的感觉）</p>
<p><img src="/images/15528281403428.jpg" width="90%" height="50%"></p>
<p>首先介绍什么是depthwise separable convolution，实际上就是一个depthwise+pointwise。</p>
<script type="math/tex; mode=display">\operatorname{Conv}(W, y)_{(i, j)}=\sum_{k, l, m}^{K, L, M} W_{(k, l, m)} \cdot y_{(i+k, j+l, m)}</script><script type="math/tex; mode=display">\operatorname{PointwiseConv}(W, y)_{(i, j)}=\sum_{m}^{M} W_{m} \cdot y_{(i, j, m)}</script><script type="math/tex; mode=display">\text {DepthwiseConv}(W, y)_{(i, j)}=\sum_{k, l}^{K, L} W_{(k, l)} \odot y_{(i+k, j+l)}</script><script type="math/tex; mode=display">\operatorname{SepConv}\left(W_{p}, W_{d}, y\right)_{(i, j)}=\text {PointwiseConv}_{(i, j)}\left(W_{p}, \text { DepthwiseConv }_{(i, j)}\left(W_{d}, y\right)\right)</script><p>几种convolution的参数量对比：<br><img src="/images/15528283555357.jpg" width="80%" height="50%"><br>其中k是kernel size，c是channel，g是group。</p>
<p>g-Sub-separable是指将channel分为几个group，每个group进行常规的convolution操作；g-Super-separable，也即本文中提出的convolution，同样是将channel分为几个group，然后对每个group进行depthwise-separable的卷积。</p>
<hr>
<h2 id="2️⃣-Squeeze-and-Excitation-Networks"><a href="#2️⃣-Squeeze-and-Excitation-Networks" class="headerlink" title="2️⃣[Squeeze-and-Excitation Networks]"></a>2️⃣[Squeeze-and-Excitation Networks]</h2><p>提出一种新型的网络，能够通过建模channel之间的关系，使得每个channel能够获得全局的信息，进而提高模型的能力。<br><img src="/images/15528285519609.jpg" width="90%" height="50%"></p>
<p>分为两步：第一步是获得一个全局的表示，第二步是根据全局信息更新每个channel的信息。</p>
<h3 id="符号"><a href="#符号" class="headerlink" title="符号"></a>符号</h3><p>输入：$ \mathbf{X} \in \mathbb{R}^{H^{\prime} \times W^{\prime} \times C^{\prime}} $<br>经过特征提取后（如Convolution)：$\mathbf{U} \in \mathbb{R}^{H \times W \times C}$，也即：$\mathbf{U}=\mathbf{F}_{t r}(\mathbf{X})$<br>将$\mathbf{U}$写成：$\mathbf{U}=\left[\mathbf{u}_{1}, \mathbf{u}_{2}, \dots, \mathbf{u}_{C}\right]$<br>$\mathbf{V}$ 是可学习的卷积核参数： $\mathbf{V}=\left[\mathbf{v}_{1}, \mathbf{v}_{2}, \ldots, \mathbf{v}_{C}\right]$</p>
<p>则上述卷积变换可写成：$\mathbf{u}_{c}=\mathbf{v}_{c} \ast \mathbf{X}=\sum_{s=1}^{C^{\prime}} \mathbf{v}_{c}^{s} \ast \mathbf{x}^{s}$</p>
<h3 id="Squeeze-Global-Information-Embedding"><a href="#Squeeze-Global-Information-Embedding" class="headerlink" title="Squeeze: Global Information Embedding"></a>Squeeze: Global Information Embedding</h3><p>第一步，将所有的特征进行整合得到全局的特征：</p>
<script type="math/tex; mode=display">z_{c}=\mathbf{F}_{s q}\left(\mathbf{u}_{c}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} u_{c}(i, j)</script><p>论文提取全局特征的方法直接用简单的global average pooling。那么$\mathbf{z} \in \mathbb{R}^{C}$的每一维就代表每一维的channel。</p>
<h3 id="Excitation-Adaptive-Recalibration"><a href="#Excitation-Adaptive-Recalibration" class="headerlink" title="Excitation: Adaptive Recalibration"></a>Excitation: Adaptive Recalibration</h3><p>与attention不同的是，论文希望能够同时强调不同多个channel的重要（而不是one-hot的形式），因此使用一个简单的门控制机制，采用sigmoid激活函数：（这里的想法挺有意思，相对attention的softmax似乎确实会更好的样子）</p>
<script type="math/tex; mode=display">\mathbf{s}=\mathbf{F}_{ex}(\mathbf{z}, \mathbf{W})=\sigma(g(\mathbf{z}, \mathbf{W}))=\sigma\left(\mathbf{W}_{2} \delta\left(\mathbf{W}_{1} \mathbf{z}\right)\right)</script><p>为了减少参数这里的MLP采用了bottleneck的形式。亦即：<br>${\mathbf{W}_{1} \in \mathbb{R}^{\frac{C}{r} \times C}}$ $ {\mathbf{W}_{2} \in \mathbb{R}^{C \times \frac{C}{r}}}$<br>$r$是reduction ratio。</p>
<p>贴上作者的思路：</p>
<blockquote>
<p>To make use of the information aggregated in the squeeze operation, we follow it with a second operation which aims to fully capture channel-wise dependencies. To fulfill this objective, the function must meet two criteria: first, it must be ﬂexible (in particular, it must be capable of learning a nonlinear interaction between channels) and second, it must learn a non-mutually-exclusive relationship since we would like to ensure that multiple channels are allowed to be emphasised (rather than enforcing a one-hot activation). To meet these criteria, we opt to employ a simple gating mechanism with a sigmoid activation.</p>
</blockquote>
<p>最后对每个channel进行<strong>放缩</strong>，获得新的表示：</p>
<script type="math/tex; mode=display">\widetilde{\mathbf{x}}_{c}=\mathbf{F}_{\text {scale}}\left(\mathbf{u}_{c}, s_{c}\right)=s_{c} \cdot \mathbf{u}_{c}</script><hr>
<h2 id="3️⃣-Non-local-Neural-Networks"><a href="#3️⃣-Non-local-Neural-Networks" class="headerlink" title="3️⃣[Non-local Neural Networks]"></a>3️⃣[Non-local Neural Networks]</h2><p>提出一种新的结构，与上一篇类似，希望模型的每个位置都能感知到其他位置，从而捕获长程依赖，拥有全局信息。</p>
<p><img src="/images/15528297540165.jpg" width="60%" height="50%"></p>
<h3 id="Non-local-Network"><a href="#Non-local-Network" class="headerlink" title="Non-local Network"></a>Non-local Network</h3><p>定义non-local网络：</p>
<script type="math/tex; mode=display">\mathbf{y}_{i}=\frac{1}{\mathcal{C}(\mathbf{x})} \sum_{\forall j} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) g\left(\mathbf{x}_{j}\right)</script><p>其中$\mathcal{C}$是归一化函数；$f$是第$i$个位置与第$j$个位置的交互函数；$g$计算第$j$个位置的表示。</p>
<h4 id="g-的具体形式"><a href="#g-的具体形式" class="headerlink" title="$g$的具体形式"></a>$g$的具体形式</h4><p>一个线性函数：$g\left(\mathbf{x}_{j}\right)=W_{g} \mathbf{x}_{j}$<br>在实现的时候是一个$1\times1$或 $1\times1\times1$的convolution。</p>
<h4 id="f-的具体形式"><a href="#f-的具体形式" class="headerlink" title="$f$的具体形式"></a>$f$的具体形式</h4><p>①Gaussian<br>$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=e^{\mathbf{x}_{i}^{T} \mathbf{x}_{j}}$<br>则归一化定义为$\mathcal{C}(\mathbf{x})=\sum_{\forall j} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)$ 。</p>
<p>②Embedded Gaussian<br>$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=e^{\theta\left(\mathbf{x}_{i}\right)^{T} \phi\left(\mathbf{x}_{j}\right)}$<br>其中：$\theta\left(\mathbf{x}_{i}\right)=W_{\theta} \mathbf{x}_{i} $, $ \phi\left(\mathbf{x}_{j}\right)=W_{\phi} \mathbf{x}_{j}$<br>归一化：$\mathcal{C}(\mathbf{x})=\sum_{\forall j} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)$</p>
<p>可以看到self-attention是Embedded Gaussian的一种形式。虽然有这样的关系，但作者在实验中发现softmax并不是必要的。</p>
<p>③Dot product<br>$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\theta\left(\mathbf{x}_{i}\right)^{T} \phi\left(\mathbf{x}_{j}\right)$<br>归一化：$\mathcal{C}(\mathbf{x})=N$</p>
<p>④Concatenation<br>$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\operatorname{ReLU}\left(\mathbf{w}_{f}^{T}\left[\theta\left(\mathbf{x}_{i}\right), \phi\left(\mathbf{x}_{j}\right)\right]\right)$<br>$\mathcal{C}(\mathbf{x})=N$</p>
<p>有了上面的non-local的介绍，可以直接将其用于residual network。<br>$\mathbf{z}_{i}=W_{z} \mathbf{y}_{i}+\mathbf{x}_{i}$<br>$y$则是non-local network的输出。</p>
<h3 id="Non-local-block的策略-tricks"><a href="#Non-local-block的策略-tricks" class="headerlink" title="Non-local block的策略/tricks"></a>Non-local block的策略/tricks</h3><p>①设置$W_g$,$W_θ$,$W_ϕ$的channel的数目为x的channel数目的一半，这样就形成了一个bottleneck，能够减少一半的计算量。Wz再重新放大到x的channel数目，保证输入输出维度一致。</p>
<p>②在$\frac{1}{\mathcal{C}(\hat{\mathbf{x}})} \sum_{\forall j} f\left(\mathbf{x}_{i}, \hat{\mathbf{x}}_{j}\right) g\left(\hat{\mathbf{x}}_{j}\right)$使用下采样，如max-pooling，减少计算量。</p>
<hr>
<h2 id="4️⃣-Bilinear-CNN-Models-for-Fine-grained-Visual-Recognition"><a href="#4️⃣-Bilinear-CNN-Models-for-Fine-grained-Visual-Recognition" class="headerlink" title="4️⃣[Bilinear CNN Models for Fine-grained Visual Recognition]"></a>4️⃣[Bilinear CNN Models for Fine-grained Visual Recognition]</h2><p>提出一种双线性模型，由两个特征提取器组成，他们的输出做<strong>外积</strong>，最终获得图像描述特征。</p>
<p>Motivation(?不确定是不是这样)：对于细粒度物体的分类，先对局部定位，再提取特征。两个特征提取器一个是提取location，另一个提取特征。</p>
<p><img src="/images/15528310057673.jpg" width="60%" height="50%"></p>
<p>为什么用<strong>外积</strong>？</p>
<blockquote>
<p>outer product captures pairwise correlations between the feature channels</p>
</blockquote>
<p>有意思的是作者将该模型和人脑视觉处理的两个假设联系在一起(stream hypothesis)：<br>here are two main pathways, or “streams”. The ventral stream (or, “what pathway”) is involved with object identiﬁcation and recognition. The dorsal stream (or, “where pathway”) is involved with processing the object’s spatial location relative to the viewer.<br>不过看看就好，并没有什么道理。</p>
<p>对于一个分类的双线性模型而言，其一般形式是一个四元组：$\mathcal{B}=\left(f_{A}, f_{B}, \mathcal{P}, \mathcal{C}\right)$。其中$f$是特征函数，$\mathcal{P}$是pooling函数，$\mathcal{C}$是分类函数。具体而言，$f$是一个映射，${f : \mathcal{L} \times \mathcal{I} \rightarrow} {R^ {c\times D}} $。也即将一个image和一个location L 映射成feature。（We refer to locations generally which can include position and scale 其实这里不是很懂location的意思）</p>
<p>将feature a和feature b结合在一起：<br>$\text { bilinear }\left(l, \mathcal{I}, f_{A}, f_{B}\right)=f_{A}(l, \mathcal{I})^{T} f_{B}(l, \mathcal{I})$</p>
<p>pooling有好几种，可以直接加起来，或者使用max-pooling。这里使用直接加起来的方式，可以理解为，这些特征是无序(orderless)的叠加。</p>
<p>在获得输出后再做一些操作/trick能够提升表现：<br>$\begin{array}{l}{\mathbf{y} \leftarrow \operatorname{sign}(\mathbf{x}) \sqrt{|\mathbf{x}|}} \\ {\mathbf{z} \leftarrow \mathbf{y} /|\mathbf{y}|_{2}}\end{array}$</p>
<p>讨论：<br>①But do the networks specialize into roles of localization (“where”) and appearance modeling (“what”) when initialized asymmetrically and ﬁne-tuned?<br>通过可视化发现，并没有明确的功能分开。<br>Both these networks tend to activate strongly on highly speciﬁc semantic parts</p>
<p>②bilinear的好处还可以扩展成trilinear，添加更多的信息。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2019/03/17/诗词&句/每周诗词19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/17/诗词&句/每周诗词19/" itemprop="url">每周诗词19</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-17T09:26:14+08:00">
                2019-03-17
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-03-17T22:32:17+08:00">
                2019-03-17
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣送灵澈上人"><a href="#1️⃣送灵澈上人" class="headerlink" title="1️⃣送灵澈上人"></a>1️⃣送灵澈上人</h3><p>[唐] 刘长卿<br>苍苍竹林寺，杳杳钟声晚。<br>荷笠带斜阳，青山独归远。</p>
<p>荷（hè）笠：背着斗笠。</p>
<p><a href="http://lib.xcz.im/work/57b90887128fe10054c9c750" target="_blank" rel="noopener">http://lib.xcz.im/work/57b90887128fe10054c9c750</a></p>
<hr>
<h3 id="2️⃣苏幕遮-·-怀旧"><a href="#2️⃣苏幕遮-·-怀旧" class="headerlink" title="2️⃣苏幕遮 · 怀旧"></a>2️⃣苏幕遮 · 怀旧</h3><p>[宋] 范仲淹<br>碧云天，黄叶地，秋色连波，波上寒烟翠。山映斜阳天接水，芳草无情，更在斜阳外。<br>黯乡魂，追旅思。夜夜除非，好梦留人睡。明月楼高休独倚，酒入愁肠，化作相思泪。</p>
<p><a href="http://lib.xcz.im/work/57b8ee4a128fe10054c91757" target="_blank" rel="noopener">http://lib.xcz.im/work/57b8ee4a128fe10054c91757</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2019/03/10/论文/每周论文12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/10/论文/每周论文12/" itemprop="url">每周论文12</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-10T10:52:30+08:00">
                2019-03-10
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-03-12T21:44:42+08:00">
                2019-03-12
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1️⃣-PAY-LESS-ATTENTION-WITH-LIGHTWEIGHT-AND-DYNAMIC-CONVOLUTIONS"><a href="#1️⃣-PAY-LESS-ATTENTION-WITH-LIGHTWEIGHT-AND-DYNAMIC-CONVOLUTIONS" class="headerlink" title="1️⃣[PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS]"></a>1️⃣[PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS]</h2><p>Facebook研究人员提出的两种基于卷积的方法尝试替代self-attention在transformer中的作用，拥有更少的参数以及更快的速度，并且能够达到很好的效果。</p>
<p><img src="/images/15521843619624.jpg" width="80%" height="80%"></p>
<h3 id="Lightweight-convolution"><a href="#Lightweight-convolution" class="headerlink" title="Lightweight convolution"></a>Lightweight convolution</h3><p>背景：depthwise convolution<br>每个channel独立进行卷积，注意到放到NLP任务上channel是指embedding的每一维。</p>
<script type="math/tex; mode=display">O_{i, c}=\text{DepthwiseConv}\left(X, W_{c, :}, i, c\right)=\sum_{j=1}^{k} W_{c, j} \cdot X_{\left(i+j-\left\lceil\frac{k+1}{2}\right]\right), c}</script><p>因此Lightweight convolution的计算方法为：</p>
<script type="math/tex; mode=display">\operatorname{LightConv}\left(X, W_{\left\lceil\frac{c H}{d}\right\rceil,:}, i, c\right)=\text { DepthwiseConv}\left(X, \text{softmax}(W_{\left\lceil\frac{c H}{d}\right\rceil,:}), i, c\right)</script><p>每一层都有固定的window size，这和self-attention不同，self-attention是所有的context都进行交互。</p>
<ul>
<li>Weight sharing 注意到这里讲每d/H个channel的参数进行绑定，进一步减少参数。</li>
<li>Softmax-normalization 对channel一维进行softmax，相当于归一化每个词的每一维的的重要性（比self-attention更精细）。实验证明，如果没有softmax没办法收敛。</li>
</ul>
<p>因此总体的架构为：<br>input—&gt;linear —&gt; GLU(gated linear unit) —&gt; lightconv/dynamicConv —&gt; linear</p>
<h3 id="Dynamic-convolution"><a href="#Dynamic-convolution" class="headerlink" title="Dynamic convolution"></a>Dynamic convolution</h3><p>与lightweight convolution相似，但加了一个动态的kernel size。</p>
<script type="math/tex; mode=display">\text { DynamicConv}( X , i , c ) = \operatorname{LightConv}\left(X, f\left(X_{i}\right)_{h,:}, i, c\right)</script><p>这里的kernel size简单使用线性映射：$f : \mathbb { R } ^ { d } \rightarrow \mathbb { R } ^ { H \times k }$<br>如：$f\left(X_{i}\right)=\sum_{c=1}^{d} W_{h, j, c}^{Q} X_{i, c}$</p>
<hr>
<h2 id="2️⃣-Joint-Embedding-of-Words-and-Labels-for-Text-Classiﬁcation"><a href="#2️⃣-Joint-Embedding-of-Words-and-Labels-for-Text-Classiﬁcation" class="headerlink" title="2️⃣[Joint Embedding of Words and Labels for Text Classiﬁcation]"></a>2️⃣[Joint Embedding of Words and Labels for Text Classiﬁcation]</h2><p>提出一种机制将label作为embedding与词一同训练，同时引入label和word的attention机制，在分类上获得效果。</p>
<p><img src="/images/15521854844061.jpg" width="40%" height="50%"></p>
<p>上图中，C是label embedding，维度为$P\times K$ ; V是句子所有词的embedding矩阵，维度为$P\times L$。<br>$\mathbf{G}$的计算公式为：</p>
<script type="math/tex; mode=display">\mathbf{G}=\left(\mathbf{C}^{\top} \mathbf{V}\right) \oslash \hat{\mathbf{G}}</script><p>$\oslash$表示element-wise相除。$\hat{\mathbf{G}}$表示l2 norm，也即：</p>
<script type="math/tex; mode=display">\hat{g}_{k l}=\left\|\boldsymbol{c}_{k}\right\|\left\|\boldsymbol{v}_{l}\right\|</script><p>因此公式的本质即在计算label与每个词的cos距离。</p>
<p>在获得了$\mathbf{G}$后，为了获得更高的的表示，如phrase，将一个一个block取出，并过线性层：</p>
<script type="math/tex; mode=display">\boldsymbol{u}_{l}=\operatorname{ReLU}\left(\mathbf{G}_{l-r : l+r} \mathbf{W}_{1}+\boldsymbol{b}_{1}\right)</script><p>接着对每个$\boldsymbol{u}_{l}$取最大值：</p>
<script type="math/tex; mode=display">m_{l}=\textbf{max-pooling}\left(\boldsymbol{u}_{l}\right)</script><p>此时的$\mathbf{m}$是一个长度为L的向量。最终对m做softmax获得一个分数的分布：</p>
<script type="math/tex; mode=display">\boldsymbol{\beta}=\operatorname{SoftMax}(\boldsymbol{m})</script><p>将该分数和每个词做加权求和，获得最终的向量表示：</p>
<script type="math/tex; mode=display">\boldsymbol{z}=\sum_{l} \beta_{l} \boldsymbol{v}_{l}</script><p>思考：将label与embedding放在一起训练这个思路不错。但整合的方式是否过于简单粗暴了<br>？特别是phrase的提取和随后的max-pooling的可解释性并不强的样子。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2019/03/10/碎片知识/每周碎片知识18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/10/碎片知识/每周碎片知识18/" itemprop="url">每周碎片知识18</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-10T09:46:14+08:00">
                2019-03-10
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-03-10T09:59:03+08:00">
                2019-03-10
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-Depthwise-seperable-convolution"><a href="#1️⃣-Depthwise-seperable-convolution" class="headerlink" title="1️⃣[Depthwise seperable convolution]"></a>1️⃣[Depthwise seperable convolution]</h3><p>Depthwise seperable convolution = depthwise + pointwise<br>先每个卷积核独立对一个feature map进行卷积，再通过一个$1\times 1 \times n$的卷积核对feature map进行整合。</p>
<p><a href="https://blog.csdn.net/tintinetmilou/article/details/81607721" target="_blank" rel="noopener">https://blog.csdn.net/tintinetmilou/article/details/81607721</a></p>
<hr>
<h3 id="2️⃣-如何寻找较好的lr"><a href="#2️⃣-如何寻找较好的lr" class="headerlink" title="2️⃣[如何寻找较好的lr]"></a>2️⃣[如何寻找较好的lr]</h3><p>一种启发式的方法：</p>
<blockquote>
<p>Over an epoch begin your SGD with a very low learning rate (like 10−8) but change it (by multiplying it by a certain factor for instance) at each mini-batch until it reaches a very high value (like 1 or 10). Record the loss each time at each iteration and once you’re finished, plot those losses against the learning rate.</p>
</blockquote>
<p><a href="https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html" target="_blank" rel="noopener">https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2019/03/09/诗词&句/每周诗词18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/09/诗词&句/每周诗词18/" itemprop="url">每周诗词18</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-09T21:04:14+08:00">
                2019-03-09
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-03-17T09:29:39+08:00">
                2019-03-17
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣西江月-·-遣兴"><a href="#1️⃣西江月-·-遣兴" class="headerlink" title="1️⃣西江月 · 遣兴"></a>1️⃣西江月 · 遣兴</h3><p>[宋] 辛弃疾<br>醉里且贪欢笑，要愁那得工夫。近来始觉古人书，信著全无是处。<br>昨夜松边醉倒，问松「我醉何如」。只疑松动要来扶，以手推松曰「去」！</p>
<p><a href="http://lib.xcz.im/work/57b935bcd342d3005ac8e63f" target="_blank" rel="noopener">http://lib.xcz.im/work/57b935bcd342d3005ac8e63f</a></p>
<hr>
<h3 id="2️⃣蝶恋花"><a href="#2️⃣蝶恋花" class="headerlink" title="2️⃣蝶恋花"></a>2️⃣蝶恋花</h3><p>[宋] 晏殊<br>槛菊愁烟兰泣露，罗幕轻寒，燕子双飞去。明月不谙离恨苦，斜光到晓穿朱户。<br>昨夜西风凋碧树，独上高楼，望尽天涯路。欲寄彩笺兼尺素，山长水阔知何处？</p>
<p><a href="http://lib.xcz.im/work/57b318dd1532bc00618ffaff" target="_blank" rel="noopener">http://lib.xcz.im/work/57b318dd1532bc00618ffaff</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2019/01/28/碎片知识/如何使用fairseq复现Transformer NMT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/28/碎片知识/如何使用fairseq复现Transformer NMT/" itemprop="url">如何使用fairseq复现Transformer NMT</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-28T22:05:05+08:00">
                2019-01-28
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-02-03T22:02:57+08:00">
                2019-02-03
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>基于Transformer的NMT虽然结果好，但超参非常难调，只要有一两个参数和论文不一样，就有可能得到和论文相去甚远的结果。fairseq是现有比较完善的seq2seq库，由于是大公司出品，因此也写得较为完善，不论是代码还是文档。</p>
<p>本文讨论如何使用fairseq复现基于Transformer的翻译任务，也即复现Vaswani, et al. 的论文结果。本文尽量不讨论实现细节，只讨论如何复现出结果。</p>
<p>fairseq项目地址：<a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">https://github.com/pytorch/fairseq</a></p>
<h2 id="使用教程"><a href="#使用教程" class="headerlink" title="使用教程"></a>使用教程</h2><p>在这里我们参考的是18年的文章<a href="https://arxiv.org/abs/1806.00187" target="_blank" rel="noopener">Scaling Neural Machine Translation</a>，同样是基于Transformer的NMT。同时，我们使用WMT16 EN-DE而不是Vaswani, et al.论文中的WMT14 EN-DE。二者只在一个文件（commoncrawl）上有区别，其他是一样的，由于WMT16 EN-DE有预处理好的数据，为了方便起见，我们就使用该份数据（下文也有预处理WMT14数据的方法）</p>
<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><ol>
<li>安装fairseq，在Readme内有</li>
<li>阅读Readme（optional）</li>
<li>阅读doc（optional）</li>
</ol>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><h4 id="Step1"><a href="#Step1" class="headerlink" title="Step1"></a>Step1</h4><p>数据预处理主要是下载多个文件并合并—&gt;清理/tokenize数据—&gt;将数据分为train、valid—&gt;bpe(bype pair encoding)。fairseq提供了一整套处理流程的脚本，在examples/translation/prepare-wmt14en2de.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># Adapted from https://github.com/facebookresearch/MIXER/blob/master/prepareData.sh</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'Cloning Moses github repository (for tokenization scripts)...'</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/moses-smt/mosesdecoder.git</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'Cloning Subword NMT repository (for BPE pre-processing)...'</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/rsennrich/subword-nmt.git</span><br><span class="line"></span><br><span class="line">SCRIPTS=mosesdecoder/scripts</span><br><span class="line">TOKENIZER=<span class="variable">$SCRIPTS</span>/tokenizer/tokenizer.perl</span><br><span class="line">CLEAN=<span class="variable">$SCRIPTS</span>/training/clean-corpus-n.perl</span><br><span class="line">NORM_PUNC=<span class="variable">$SCRIPTS</span>/tokenizer/normalize-punctuation.perl</span><br><span class="line">REM_NON_PRINT_CHAR=<span class="variable">$SCRIPTS</span>/tokenizer/remove-non-printing-char.perl</span><br><span class="line">BPEROOT=subword-nmt</span><br><span class="line">BPE_TOKENS=40000</span><br><span class="line"></span><br><span class="line">URLS=(</span><br><span class="line">    <span class="string">"http://statmt.org/wmt13/training-parallel-europarl-v7.tgz"</span></span><br><span class="line">    <span class="string">"http://statmt.org/wmt13/training-parallel-commoncrawl.tgz"</span></span><br><span class="line">    <span class="string">"http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz"</span></span><br><span class="line">    <span class="string">"http://data.statmt.org/wmt17/translation-task/dev.tgz"</span></span><br><span class="line">    <span class="string">"http://statmt.org/wmt14/test-full.tgz"</span></span><br><span class="line">)</span><br><span class="line">FILES=(</span><br><span class="line">    <span class="string">"training-parallel-europarl-v7.tgz"</span></span><br><span class="line">    <span class="string">"training-parallel-commoncrawl.tgz"</span></span><br><span class="line">    <span class="string">"training-parallel-nc-v12.tgz"</span></span><br><span class="line">    <span class="string">"dev.tgz"</span></span><br><span class="line">    <span class="string">"test-full.tgz"</span></span><br><span class="line">)</span><br><span class="line">CORPORA=(</span><br><span class="line">    <span class="string">"training/europarl-v7.de-en"</span></span><br><span class="line">    <span class="string">"commoncrawl.de-en"</span></span><br><span class="line">    <span class="string">"training/news-commentary-v12.de-en"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This will make the dataset compatible to the one used in "Convolutional Sequence to Sequence Learning"</span></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1705.03122</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$1</span>"</span> == <span class="string">"--icml17"</span> ]; <span class="keyword">then</span></span><br><span class="line">    URLS[2]=<span class="string">"http://statmt.org/wmt14/training-parallel-nc-v9.tgz"</span></span><br><span class="line">    FILES[2]=<span class="string">"training-parallel-nc-v9.tgz"</span></span><br><span class="line">    CORPORA[2]=<span class="string">"training/news-commentary-v9.de-en"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ ! -d <span class="string">"<span class="variable">$SCRIPTS</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"Please set SCRIPTS variable correctly to point to Moses scripts."</span></span><br><span class="line">    <span class="built_in">exit</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">src=en</span><br><span class="line">tgt=de</span><br><span class="line">lang=en-de</span><br><span class="line">prep=wmt14_en_de</span><br><span class="line">tmp=<span class="variable">$prep</span>/tmp</span><br><span class="line">orig=orig</span><br><span class="line">dev=dev/newstest2013</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$orig</span> <span class="variable">$tmp</span> <span class="variable">$prep</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$orig</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;<span class="variable">$&#123;#URLS[@]&#125;</span>;++i)); <span class="keyword">do</span></span><br><span class="line">    file=<span class="variable">$&#123;FILES[i]&#125;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="variable">$file</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$file</span> already exists, skipping download"</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        url=<span class="variable">$&#123;URLS[i]&#125;</span></span><br><span class="line">        wget <span class="string">"<span class="variable">$url</span>"</span></span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="variable">$file</span> ]; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"<span class="variable">$url</span> successfully downloaded."</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"<span class="variable">$url</span> not successfully downloaded."</span></span><br><span class="line">            <span class="built_in">exit</span> -1</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">if</span> [ <span class="variable">$&#123;file: -4&#125;</span> == <span class="string">".tgz"</span> ]; <span class="keyword">then</span></span><br><span class="line">            tar zxvf <span class="variable">$file</span></span><br><span class="line">        <span class="keyword">elif</span> [ <span class="variable">$&#123;file: -4&#125;</span> == <span class="string">".tar"</span> ]; <span class="keyword">then</span></span><br><span class="line">            tar xvf <span class="variable">$file</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"pre-processing train data..."</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    rm <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> <span class="string">"<span class="variable">$&#123;CORPORA[@]&#125;</span>"</span>; <span class="keyword">do</span></span><br><span class="line">        cat <span class="variable">$orig</span>/<span class="variable">$f</span>.<span class="variable">$l</span> | \</span><br><span class="line">            perl <span class="variable">$NORM_PUNC</span> <span class="variable">$l</span> | \</span><br><span class="line">            perl <span class="variable">$REM_NON_PRINT_CHAR</span> | \</span><br><span class="line">            perl <span class="variable">$TOKENIZER</span> -threads 8 -a -l <span class="variable">$l</span> &gt;&gt; <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"pre-processing test data..."</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$l</span>"</span> == <span class="string">"<span class="variable">$src</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">        t=<span class="string">"src"</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        t=<span class="string">"ref"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    grep <span class="string">'&lt;seg id'</span> <span class="variable">$orig</span>/<span class="built_in">test</span>-full/newstest2014-deen-<span class="variable">$t</span>.<span class="variable">$l</span>.sgm | \</span><br><span class="line">        sed -e <span class="string">'s/&lt;seg id="[0-9]*"&gt;\s*//g'</span> | \</span><br><span class="line">        sed -e <span class="string">'s/\s*&lt;\/seg&gt;\s*//g'</span> | \</span><br><span class="line">        sed -e <span class="string">"s/\’/\'/g"</span> | \</span><br><span class="line">    perl <span class="variable">$TOKENIZER</span> -threads 8 -a -l <span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/<span class="built_in">test</span>.<span class="variable">$l</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">""</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"splitting train and valid..."</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    awk <span class="string">'&#123;if (NR%100 == 0)  print $0; &#125;'</span> <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/valid.<span class="variable">$l</span></span><br><span class="line">    awk <span class="string">'&#123;if (NR%100 != 0)  print $0; &#125;'</span> <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/train.<span class="variable">$l</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">TRAIN=<span class="variable">$tmp</span>/train.de-en</span><br><span class="line">BPE_CODE=<span class="variable">$prep</span>/code</span><br><span class="line">rm -f <span class="variable">$TRAIN</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    cat <span class="variable">$tmp</span>/train.<span class="variable">$l</span> &gt;&gt; <span class="variable">$TRAIN</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"learn_bpe.py on <span class="variable">$&#123;TRAIN&#125;</span>..."</span></span><br><span class="line">python <span class="variable">$BPEROOT</span>/learn_bpe.py -s <span class="variable">$BPE_TOKENS</span> &lt; <span class="variable">$TRAIN</span> &gt; <span class="variable">$BPE_CODE</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> L <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> train.<span class="variable">$L</span> valid.<span class="variable">$L</span> <span class="built_in">test</span>.<span class="variable">$L</span>; <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"apply_bpe.py to <span class="variable">$&#123;f&#125;</span>..."</span></span><br><span class="line">        python <span class="variable">$BPEROOT</span>/apply_bpe.py -c <span class="variable">$BPE_CODE</span> &lt; <span class="variable">$tmp</span>/<span class="variable">$f</span> &gt; <span class="variable">$tmp</span>/bpe.<span class="variable">$f</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">perl <span class="variable">$CLEAN</span> -ratio 1.5 <span class="variable">$tmp</span>/bpe.train <span class="variable">$src</span> <span class="variable">$tgt</span> <span class="variable">$prep</span>/train 1 250</span><br><span class="line">perl <span class="variable">$CLEAN</span> -ratio 1.5 <span class="variable">$tmp</span>/bpe.valid <span class="variable">$src</span> <span class="variable">$tgt</span> <span class="variable">$prep</span>/valid 1 250</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> L <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    cp <span class="variable">$tmp</span>/bpe.test.<span class="variable">$L</span> <span class="variable">$prep</span>/<span class="built_in">test</span>.<span class="variable">$L</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>如果希望使用预处理好的数据，则可以使用WMT16 EN-DE，地址为：<a href="https://drive.google.com/uc?export=download&amp;id=0B_bZck-ksdkpM25jRUN2X2UxMm8" target="_blank" rel="noopener">https://drive.google.com/uc?export=download&amp;id=0B_bZck-ksdkpM25jRUN2X2UxMm8</a><br>并解压。</p>
<h4 id="Step2"><a href="#Step2" class="headerlink" title="Step2"></a>Step2</h4><p>接下来对数据进行二值化(binarize):</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">TEXT=wmt16_en_de_bpe32k</span><br><span class="line">mkdir <span class="variable">$TEXT</span></span><br><span class="line">tar -xzvf wmt16_en_de.tar.gz -C <span class="variable">$TEXT</span>  <span class="comment"># 解压文件</span></span><br><span class="line">python preprocess.py --<span class="built_in">source</span>-lang en --target-lang de \</span><br><span class="line">  --trainpref <span class="variable">$TEXT</span>/train.tok.clean.bpe.32000 \</span><br><span class="line">  --validpref <span class="variable">$TEXT</span>/newstest2013.tok.bpe.32000 \</span><br><span class="line">  --testpref <span class="variable">$TEXT</span>/newstest2014.tok.bpe.32000 \</span><br><span class="line">  --destdir data-bin/wmt16_en_de_bpe32k \</span><br><span class="line">  --nwordssrc 32768 --nwordstgt 32768 \</span><br><span class="line">  --joined-dictionary</span><br></pre></td></tr></table></figure>
<p>到这里，麻烦的预处理就结束了。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>cd到fairseq目录下，执行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7  python -m torch.distributed.launch --nproc_per_node 8 train.py data-bin/wmt16_en_de_bpe32k \        --arch transformer_wmt_en_de --share-all-embeddings \          --optimizer adam --adam-betas <span class="string">'(0.9, 0.98)'</span> --clip-norm 0.0 \            --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 \              --lr 0.0007 --min-lr 1e-09 \             --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --weight-decay 0.0\              --max-tokens  4096   --save-dir checkpoints/en-de-base\               --no-progress-bar --<span class="built_in">log</span>-format json --<span class="built_in">log</span>-interval 50\             --save-interval-updates  1000 --keep-interval-updates 20</span><br></pre></td></tr></table></figure>
<p>注意到该设置与原论文不大一致。但已证实该设置可以复现论文结果。</p>
<p>如果没有这么多卡，那么可以设置<code>update freq</code>以模拟8卡行为。如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=0,1,2,3  python -m torch.distributed.launch --nproc_per_node 4 \</span><br><span class="line">train.py data-bin/wmt16_en_de_bpe32k    \</span><br><span class="line"> --arch transformer_wmt_en_de --share-all-embeddings \</span><br><span class="line">--optimizer adam --adam-betas <span class="string">'(0.9, 0.98)'</span> \</span><br><span class="line">--clip-norm 0.0   --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000  \</span><br><span class="line">--lr 0.0007 --min-lr 1e-09 --criterion label_smoothed_cross_entropy \</span><br><span class="line">--label-smoothing 0.1 --weight-decay 0.0 --max-tokens  4096   \</span><br><span class="line">--save-dir checkpoints/en-de-16-base   \ </span><br><span class="line">--no-progress-bar --<span class="built_in">log</span>-format json --<span class="built_in">log</span>-interval 50 --save-interval-updates  1000 \</span><br><span class="line">--keep-interval-updates 20  --update-freq 2 |tee exp2.log</span><br></pre></td></tr></table></figure>
<p>4张卡则设<code>update freq=2</code>，2张卡则设<code>update freq=4</code>，以此类推。</p>
<p>大概在100个epoch内能够收敛，也即在475000个step。8张1080Ti在大概两天能够训练完成，4张1080Ti大概4天训练完成。</p>
<p>开始训练…<br><img src="/images/15491934129135.jpg" width="80%" height="50%"></p>
<p>最后则会获得checkpoint：<br><img src="/images/15491934935157.jpg" width="80%" height="50%"></p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>测试分为几个阶段：首先将几个checkpoint进行平均，实验表明，进行平均能够有一定的提升；其次，使用平均后的模型对test集的句子进行翻译；最终将生成的句子和正确的句子计算bleu值。</p>
<h4 id="average-checkpoint"><a href="#average-checkpoint" class="headerlink" title="average checkpoint"></a>average checkpoint</h4><p>在测试阶段，论文在Transformer-base中对最后五个checkpoint进行平均，也即对权值进行平均：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python scripts/average_checkpoints.py \</span><br><span class="line">--inputs checkpoints/en-de-base/ \</span><br><span class="line">--num-epoch-checkpoints  5 --output averaged_model.pt</span><br></pre></td></tr></table></figure>
<p>最终获得averaged_model.pt，我们将用该文件进行测试。</p>
<h4 id="generate"><a href="#generate" class="headerlink" title="generate"></a>generate</h4><p>我们采用和论文一致的超参：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 python generate.py \</span><br><span class="line">data-bin/wmt16_en_de_bpe32k/ --path /some_checkpoint \</span><br><span class="line">--remove-bpe --beam 4 --batch-size 64 --lenpen 0.6 \</span><br><span class="line">--max-len<span class="_">-a</span> 1 --max-len-b 50|tee generate.out</span><br></pre></td></tr></table></figure>
<p>其中lenpen是生成句子的长度惩罚系数；<code>max-len-a</code>和<code>max-len-b</code>指的是每个句子的最长长度限制，也即：假设源句子长度为x，则目标句子的长度应小于ax+b 。</p>
<p>最终我们翻译好的句子以及相对应的详细信息都在generate.out里面。我们需要提取源语言句子和目标语言句子，以方便后面的计算。因此：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">grep ^T generate.out | cut -f2- | perl -ple <span class="string">'s&#123;(\S)-(\S)&#125;&#123;$1 ##AT##-##AT## $2&#125;g'</span> &gt; generate.ref</span><br><span class="line"></span><br><span class="line">grep ^H generate.out |cut -f3- | perl -ple <span class="string">'s&#123;(\S)-(\S)&#125;&#123;$1 ##AT##-##AT## $2&#125;g'</span> &gt; generate.sys</span><br></pre></td></tr></table></figure>
<p>分别运行这两个bash命令，我们则获得了generate.ref和generate.sys，分别是目标和源语言的句子。</p>
<p>注意到这里有一个非常重要的小trick，也即<strong>split compound</strong>。因为一些历史原因（我也不知道为啥，tensor2tensor里面的脚本有提到），该trick已经在上面的脚本命令体现出来了。实践证明，使用该trick能够提高bleu值 0.5个点以上。</p>
<h4 id="score"><a href="#score" class="headerlink" title="score"></a>score</h4><p>我们此时就可以计算bleu值了，fairseq提供了该脚本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python score.py --sys generate.sys --ref generate.ref</span><br></pre></td></tr></table></figure>
<p>大功告成！我们终于复现出结果了。<br>作为参考：根据我的实验，只使用checkpoint中最好的一个checkpoint，在经过了上述的流程后，可以得到27.30的结果。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>根据我的需求，我还需要详细记录中间结果，并打印在tensorboard上方便可视化，如：<br><img src="/images/15491946401169.jpg" width="90%" height="50%"></p>
<p>fairseq并没有提供这种功能，因此需要自己修改部分源代码。<br>只需要修改train.py源文件即可。</p>
<p>①在开头加summary writer<br><img src="/images/15492019614168.jpg" width="70%" height="50%"></p>
<p>注意到每次实验都需要修改实验的名字。</p>
<p>②修改train函数<br>在训练过程中，添加记录的代码：<br><img src="/images/15492020396304.jpg" width="70%" height="50%"></p>
<p>在epoch结束，添加记录的代码：<br><img src="/images/15492021296611.jpg" width="70%" height="50%"></p>
<p>对validate的使用进行修改（添加了is_epoch）：<br><img src="/images/15492022604000.jpg" width="70%" height="50%"></p>
<p>train函数全部代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(args, trainer, task, epoch_itr)</span>:</span></span><br><span class="line">    <span class="string">"""Train the model for one epoch."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update parameters every N batches</span></span><br><span class="line">    <span class="keyword">if</span> epoch_itr.epoch &lt;= len(args.update_freq):</span><br><span class="line">        update_freq = args.update_freq[epoch_itr.epoch - <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        update_freq = args.update_freq[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize data iterator</span></span><br><span class="line">    itr = epoch_itr.next_epoch_itr(fix_batches_to_gpus=args.fix_batches_to_gpus)</span><br><span class="line">    itr = iterators.GroupedIterator(itr, update_freq)</span><br><span class="line">    progress = progress_bar.build_progress_bar(</span><br><span class="line">        args, itr, epoch_itr.epoch, no_progress_bar=<span class="string">'simple'</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    extra_meters = collections.defaultdict(<span class="keyword">lambda</span>: AverageMeter())</span><br><span class="line">    first_valid = args.valid_subset.split(<span class="string">','</span>)[<span class="number">0</span>]</span><br><span class="line">    max_update = args.max_update <span class="keyword">or</span> math.inf</span><br><span class="line">    <span class="keyword">for</span> i, samples <span class="keyword">in</span> enumerate(progress, start=epoch_itr.iterations_in_epoch):</span><br><span class="line">        log_output = trainer.train_step(samples)</span><br><span class="line">        <span class="keyword">if</span> log_output <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># log mid-epoch stats</span></span><br><span class="line">        stats = get_training_stats(trainer)</span><br><span class="line">        num_updates = stats[<span class="string">'num_updates'</span>]</span><br><span class="line">        <span class="comment"># print(type(num_updates))</span></span><br><span class="line">        <span class="comment"># print(type(stats['loss']))</span></span><br><span class="line">        summary_writer.add_scalar(<span class="string">'Training/training_loss_update'</span>, float(stats[<span class="string">'loss'</span>]), num_updates)</span><br><span class="line">        summary_writer.add_scalar(<span class="string">'Training/training_nll_loss_update'</span>, float(stats[<span class="string">'nll_loss'</span>]), num_updates)</span><br><span class="line">        summary_writer.add_scalar(<span class="string">'Training/training_ppl_update'</span>, float(stats[<span class="string">'ppl'</span>]), num_updates)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ------record training metrics --- #</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> log_output.items():</span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">in</span> [<span class="string">'loss'</span>, <span class="string">'nll_loss'</span>, <span class="string">'ntokens'</span>, <span class="string">'nsentences'</span>, <span class="string">'sample_size'</span>]:</span><br><span class="line">                <span class="keyword">continue</span>  <span class="comment"># these are already logged above</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">'loss'</span> <span class="keyword">in</span> k:</span><br><span class="line">                extra_meters[k].update(v, log_output[<span class="string">'sample_size'</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                extra_meters[k].update(v)</span><br><span class="line">            stats[k] = extra_meters[k].avg</span><br><span class="line">        progress.log(stats)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ignore the first mini-batch in words-per-second calculation</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            trainer.get_meter(<span class="string">'wps'</span>).reset()</span><br><span class="line"></span><br><span class="line">        num_updates = trainer.get_num_updates()</span><br><span class="line">        <span class="keyword">if</span> args.save_interval_updates &gt; <span class="number">0</span> <span class="keyword">and</span> num_updates % args.save_interval_updates == <span class="number">0</span> <span class="keyword">and</span> num_updates &gt; <span class="number">0</span>:</span><br><span class="line">            valid_losses = validate(args, trainer, task, epoch_itr, [first_valid], is_epoch=<span class="keyword">False</span>)</span><br><span class="line">            save_checkpoint(args, trainer, epoch_itr, valid_losses[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> num_updates &gt;= max_update:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># log end-of-epoch stats</span></span><br><span class="line">    stats = get_training_stats(trainer)</span><br><span class="line">    <span class="comment"># ------record training metrics --- #</span></span><br><span class="line">    summary_writer.add_scalar(<span class="string">'Training/training_loss_epoch'</span>, float(stats[<span class="string">'loss'</span>]), epoch_itr.epoch)</span><br><span class="line">    summary_writer.add_scalar(<span class="string">'Training/training_nll_loss_epoch'</span>, float(stats[<span class="string">'nll_loss'</span>]), epoch_itr.epoch)</span><br><span class="line">    summary_writer.add_scalar(<span class="string">'Training/training_ppl_epoch'</span>, float(stats[<span class="string">'ppl'</span>]), epoch_itr.epoch)</span><br><span class="line">    <span class="keyword">for</span> k, meter <span class="keyword">in</span> extra_meters.items():</span><br><span class="line">        stats[k] = meter.avg</span><br><span class="line">    progress.print(stats)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># reset training meters</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> [</span><br><span class="line">        <span class="string">'train_loss'</span>, <span class="string">'train_nll_loss'</span>, <span class="string">'wps'</span>, <span class="string">'ups'</span>, <span class="string">'wpb'</span>, <span class="string">'bsz'</span>, <span class="string">'gnorm'</span>, <span class="string">'clip'</span>,</span><br><span class="line">    ]:</span><br><span class="line">        meter = trainer.get_meter(k)</span><br><span class="line">        <span class="keyword">if</span> meter <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            meter.reset()</span><br></pre></td></tr></table></figure>
<p>③修改validate函数<br>添加了一个参数<code>is_epoch</code>：<br><img src="/images/15492023879130.jpg" width="50%" height="50%"></p>
<p>添加记录的代码：<br><img src="/images/15492024686078.jpg" width="90%" height="50%"></p>
<p>validate全部代码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">def validate(args, trainer, task, epoch_itr, subsets, is_epoch=True):</span><br><span class="line">    <span class="string">""</span><span class="string">"Evaluate the model on the validation set(s) and return the losses."</span><span class="string">""</span></span><br><span class="line">    valid_losses = []</span><br><span class="line">    <span class="keyword">for</span> subset <span class="keyword">in</span> subsets:</span><br><span class="line">        <span class="comment"># Initialize data iterator</span></span><br><span class="line">        itr = task.get_batch_iterator(</span><br><span class="line">            dataset=task.dataset(subset),</span><br><span class="line">            max_tokens=args.max_tokens,</span><br><span class="line">            max_sentences=args.max_sentences_valid,</span><br><span class="line">            max_positions=utils.resolve_max_positions(</span><br><span class="line">                task.max_positions(),</span><br><span class="line">                trainer.get_model().max_positions(),</span><br><span class="line">            ),</span><br><span class="line">            ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,</span><br><span class="line">            required_batch_size_multiple=8,</span><br><span class="line">            seed=args.seed,</span><br><span class="line">            num_shards=args.distributed_world_size,</span><br><span class="line">            shard_id=args.distributed_rank,</span><br><span class="line">            num_workers=args.num_workers,</span><br><span class="line">        ).next_epoch_itr(shuffle=False)</span><br><span class="line">        progress = progress_bar.build_progress_bar(</span><br><span class="line">            args, itr, epoch_itr.epoch,</span><br><span class="line">            prefix=<span class="string">'valid on \'</span>&#123;&#125;\<span class="string">' subset'</span>.format(subset),</span><br><span class="line">            no_progress_bar=<span class="string">'simple'</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reset validation loss meters</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> [<span class="string">'valid_loss'</span>, <span class="string">'valid_nll_loss'</span>]:</span><br><span class="line">            meter = trainer.get_meter(k)</span><br><span class="line">            <span class="keyword">if</span> meter is not None:</span><br><span class="line">                meter.reset()</span><br><span class="line">        extra_meters = collections.defaultdict(lambda: AverageMeter())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> sample <span class="keyword">in</span> progress:</span><br><span class="line">            log_output = trainer.valid_step(sample)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> k, v <span class="keyword">in</span> log_output.items():</span><br><span class="line">                <span class="keyword">if</span> k <span class="keyword">in</span> [<span class="string">'loss'</span>, <span class="string">'nll_loss'</span>, <span class="string">'ntokens'</span>, <span class="string">'nsentences'</span>, <span class="string">'sample_size'</span>]:</span><br><span class="line">                    <span class="built_in">continue</span></span><br><span class="line">                extra_meters[k].update(v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># log validation stats</span></span><br><span class="line">        stats = get_valid_stats(trainer)</span><br><span class="line">        <span class="comment"># ------record validate metrics --- #</span></span><br><span class="line">        <span class="keyword">if</span> is_epoch:  <span class="comment"># every epoch</span></span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_loss_epoch'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_loss'</span>]), epoch_itr.epoch)</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_nll_loss_epoch'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_nll_loss'</span>]), epoch_itr.epoch)</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_ppl_epoch'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_ppl'</span>]), epoch_itr.epoch)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># every n update</span></span><br><span class="line">            num_updates = stats[<span class="string">'num_updates'</span>]</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_loss_update'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_loss'</span>]),</span><br><span class="line">                                      num_updates / args.save_interval_updates)</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_nll_loss_update'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_nll_loss'</span>]),</span><br><span class="line">                                      num_updates / args.save_interval_updates)</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_ppl_update'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_ppl'</span>]),</span><br><span class="line">                                      num_updates / args.save_interval_updates)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, meter <span class="keyword">in</span> extra_meters.items():</span><br><span class="line">            stats[k] = meter.avg</span><br><span class="line">        progress.print(stats)</span><br><span class="line"></span><br><span class="line">        valid_losses.append(stats[<span class="string">'valid_loss'</span>])</span><br><span class="line">    <span class="built_in">return</span> valid_losses</span><br></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://github.com/pytorch/fairseq/tree/master/examples/translation#replicating-results-from-scaling-neural-machine-translation" target="_blank" rel="noopener">Replicating results from “Scaling Neural Machine Translation”
</a></p>
<p><a href="https://github.com/pytorch/fairseq/issues/346" target="_blank" rel="noopener">How to reproduce the result of WMT14 en-de on transformer BASE model?</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg"
                alt="林泽辉" />
            
              <p class="site-author-name" itemprop="name">林泽辉</p>
              <p class="site-description motion-element" itemprop="description">人一己千</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">125</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">148</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">林泽辉</span>

  
</div>






  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
