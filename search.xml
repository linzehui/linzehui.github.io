<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>每周碎片知识13</title>
      <link href="/2018/12/02/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8613/"/>
      <url>/2018/12/02/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8613/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-attention"><a href="#1️⃣-attention" class="headerlink" title="1️⃣[attention]"></a>1️⃣[attention]</h3><p>所有attention的总结：<br><img src="/images/15437180657954.jpg" width="70%" height="50%"><br><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank" rel="noopener">Attention? Attention!</a></p><hr><h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>①torch.no_grad能够显著减少内存使用，model.eval不能。因为eval不会关闭历史追踪。</p><blockquote><p>model.eval() will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval model instead of training mode.<br>torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script).</p></blockquote><p>Reference:<br><a href="https://discuss.pytorch.org/t/does-model-eval-with-torch-set-grad-enabled-is-train-have-the-same-effect-for-grad-history/17183/3" target="_blank" rel="noopener">Does model.eval() &amp; with torch.set_grad_enabled(is_train) have the same effect for grad history?</a></p><p><a href="https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615" target="_blank" rel="noopener">‘model.eval()’ vs ‘with torch.no_grad()’</a></p><p>②torch.full(…) returns a tensor filled with value.</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> attention </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录11</title>
      <link href="/2018/12/02/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9511/"/>
      <url>/2018/12/02/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9511/</url>
      
        <content type="html"><![CDATA[<h2 id="①"><a href="#①" class="headerlink" title="①"></a>①</h2><p>需求：对于两个向量$a$、$b$，$a,b \in R^d$，定义一种减法，有：</p><script type="math/tex; mode=display">a-b=M</script><p>其中$M \in R^{d\times d}$，$M_{ij}=a_i-b_j$</p><p>在代码中实际的维度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=torch.rand(batch_size,sequence_len,dim)</span><br><span class="line">b=torch.rand(batch_size,sequence_len,dim)</span><br></pre></td></tr></table></figure><p>方法①：for循环</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">M=torch.zeros(bz,seq_len,seq_len)</span><br><span class="line"><span class="keyword">for</span> b_i <span class="keyword">in</span> range(bz):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(seq_len):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(seq_len):</span><br><span class="line">            M_ij=torch.norm(a[b_i][i]-b[b_i][j])</span><br><span class="line">            M[b][i][j]=M_ij</span><br></pre></td></tr></table></figure><p>方法②：矩阵运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=a.unsqueeze(<span class="number">2</span>)  <span class="comment"># bz,seq_len,1,dim</span></span><br><span class="line">b=b.unsqueeze(<span class="number">1</span>)  <span class="comment"># bz,1,seq_lens,dim</span></span><br><span class="line">M=torch.norm(a-b,dim=<span class="number">-1</span>)   <span class="comment"># will broadcast</span></span><br></pre></td></tr></table></figure><hr><h2 id="②"><a href="#②" class="headerlink" title="②"></a>②</h2><p>需求，生成一个mask矩阵，每一行有一段连续的位置填充1，其中每一行填充1的开始位置和结束位置都不同。具体来说，先生成一个中心位置center，则开始位置为center-window；结束位置为center+window。其中开始位置和结束位置不能越界，也即不小于0和大于行的总长度。<br>如：<br><img src="/images/15437208061953.jpg" width="25%" height="50%"></p><p>思路：<br>①先生成n行每行对应的随机中心位置，然后再获得左和右边界</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">centers=torch.randint(low=<span class="number">0</span>,high=query_len,size=(query_len,),dtype=torch.long)</span><br><span class="line"></span><br><span class="line">left=centers-self.window</span><br><span class="line">left=torch.max(left,torch.LongTensor([<span class="number">0</span>])).unsqueeze(<span class="number">1</span>)   <span class="comment"># query_len,1</span></span><br><span class="line"></span><br><span class="line">right=centers+self.window</span><br><span class="line">right=torch.min(right,torch.LongTensor([query_len<span class="number">-1</span>])).unsqueeze(<span class="number">1</span>)  <span class="comment"># query_len,1</span></span><br></pre></td></tr></table></figure><p>②生成一个每行都用[0,n-1]填充的矩阵，[0,n-1]表示的是该元素的index，亦即：<br><img src="/images/15437212363142.jpg" width="25%" height="50%"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">range_matrix=torch.range(<span class="number">0</span>,query_len<span class="number">-1</span>,dtype=torch.long).unsqueeze(<span class="number">0</span>).expand(query_len,<span class="number">-1</span>)  <span class="comment"># query_len,query_len</span></span><br></pre></td></tr></table></figure><p>③利用&lt;=和&gt;=获得一个左边界和右边界矩阵，左边界矩阵表示在该左边界的左边都是填充的1；右边界矩阵表示在该右边界右边都是填充的1。再进行异或操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">range_matrix=torch.range(<span class="number">0</span>,query_len<span class="number">-1</span>,dtype=torch.long).unsqueeze(<span class="number">0</span>).expand(query_len,<span class="number">-1</span>)  <span class="comment"># query_len,query_len</span></span><br><span class="line">left_matrix=range_matrix&lt;=left</span><br><span class="line">right_matrix=range_matrix&lt;=right</span><br><span class="line">final_matrix=left_matrix^right_matrix</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文7</title>
      <link href="/2018/12/02/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%877/"/>
      <url>/2018/12/02/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%877/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-Convolutional-Self-Attention-Network"><a href="#1️⃣-Convolutional-Self-Attention-Network" class="headerlink" title="1️⃣[Convolutional Self-Attention Network]"></a>1️⃣[Convolutional Self-Attention Network]</h2><p>对self-attention进行改进，引入CNN的local-bias，也即对query的邻近词进行attention而不是所有词；将self-attention扩展到2D，也即让不同的head之间也有attention交互。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>1️⃣the normalization in Softmax may inhibits the attention to neighboring information 也即邻居的信息更重要，要加强邻居的重要性</p><p>2️⃣features can be better captured by modeling dependencies across different channels 对于不同的channel/head也增加他们之间的交互。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="/images/15437126353758.jpg" width="80%" height="50%"></p><p>对于1D的convolution：选取中心词周围一个window：<br><img src="/images/15437128149700.jpg" width="28%" height="50%"></p><p>对于2D的convolution，则有：<br><img src="/images/15437128476725.jpg" width="45%" height="50%"></p><p>在具体实践中，只对前三层添加local bias，这是因为modeling locality在底层更有效，对于高层应该捕获更远的信息。</p><hr><h2 id="2️⃣-Modeling-Localness-for-Self-Attention-Networks"><a href="#2️⃣-Modeling-Localness-for-Self-Attention-Networks" class="headerlink" title="2️⃣[Modeling Localness for Self-Attention Networks]"></a>2️⃣[Modeling Localness for Self-Attention Networks]</h2><p>和上文一样，引入local bias对self-attention进行改进，从而提升了翻译表现。和上文是同一作者，发在EMNLP上。</p><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>1️⃣self-attention存在的问题：虽然能够增加长程关注，但因此会导致注意力的分散，对邻居的信号会忽略。实践证明，对local bias建模在self-attention有提升。</p><p>2️⃣从直觉上来说，在翻译模型中，当目标词i与源语言词j有对齐关系时，我们希望词i能同时对词j周围的词进行对齐，使得能够捕获上下文信息，如phrase的信息。</p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>在原来的公式上添加G：<br><img src="/images/15437133932791.jpg" width="45%" height="50%"><br>也即：<br><img src="/images/15437134105761.jpg" width="70%" height="50%"></p><p>G是一个alignment position matrix（对齐位置矩阵），元素ij代表目标词i与源语言词j之间的紧密程度。<br>我们每次根据目标词i预测一个源语言的中心词，则$G_{ij}$则为：</p><p><img src="/images/15437135769000.jpg" width="23%" height="50%"></p><p>$P_i$就是对于目标词j而言源语言的中心词。 $\sigma$ 手动设定，通常是$\frac{D}{2}$，D代表窗口大小。</p><p>也即最终我们需要计算的是，中心词$P_i$和窗口$D$。</p><h4 id="计算-P-i"><a href="#计算-P-i" class="headerlink" title="计算$P_i$"></a>计算$P_i$</h4><p>利用对应的目标词i的query即可：<br><img src="/images/15437138514005.jpg" width="28%" height="50%"><br>$p_i$是一个实数。</p><h4 id="计算window-size"><a href="#计算window-size" class="headerlink" title="计算window size"></a>计算window size</h4><p>①固定窗口，将其作为一个超参。</p><p>②Layer-Speciﬁc Window<br>将该层所有的key平均，计算出一个共享的window size：<br><img src="/images/15437139914993.jpg" width="28%" height="50%"></p><p>③Query-Speciﬁc Window<br>每个query都有自己的window size<br><img src="/images/15437140367683.jpg" width="30%" height="50%"></p><h3 id="实验分析与结论"><a href="#实验分析与结论" class="headerlink" title="实验分析与结论"></a>实验分析与结论</h3><p>①将model locality用于低层效果会更好，这是因为低层对相邻建模，而越高层越关注更远的词。</p><p><img src="/images/15437141387365.jpg" width="50%" height="50%"></p><p>②将model locality放在encoder和encoder-decoder部分会更好（transformer有三个地方可以放）</p><p><img src="/images/15437141719564.jpg" width="50%" height="50%"><br>因为decoder本身就倾向关注临近的词，如果继续让其关注临近的词，那么就难以进行长程建模。</p><p>③越高层，window size（scope）越大。</p><p><img src="/images/15437142078121.jpg" width="70%" height="50%"></p><p>也即，在底层更倾向于捕获邻近词的语义；而高层倾向捕获长程依赖。但这不包括第一层，第一层是embedding，还没有上下文信息，因此倾向于捕获全局信息。</p><hr><h2 id="3️⃣-Effective-Approaches-to-Attention-based-Neural-Machine-Translation"><a href="#3️⃣-Effective-Approaches-to-Attention-based-Neural-Machine-Translation" class="headerlink" title="3️⃣[Effective Approaches to Attention-based Neural Machine Translation]"></a>3️⃣[Effective Approaches to Attention-based Neural Machine Translation]</h2><p>提出两种attention机制的翻译模型，global和local。</p><p>本文与原版的翻译模型略有不同：<br><img src="/images/15437143753188.jpg" width="40%" height="50%"><br><img src="/images/15437143893418.jpg" width="30%" height="50%"></p><p>c是context，h是decode的隐层。</p><h3 id="global-attention"><a href="#global-attention" class="headerlink" title="global attention"></a>global attention</h3><p><img src="/images/15437144396133.jpg" width="45%" height="50%"></p><p>计算attention分数：<br><img src="/images/15437145076271.jpg" width="40%" height="50%"></p><p>score有多种选择：<br><img src="/images/15437145588496.jpg" width="52%" height="50%"></p><p>注意到该模型与第一个提出attention based的模型不同之处：<br>$h_t -&gt; a_t -&gt; c_t -&gt; \tilde{h_t}$<br>原版是：<br>$h_{t-1} -&gt; a_t -&gt; c_t -&gt; h_t$</p><h3 id="local-attention"><a href="#local-attention" class="headerlink" title="local attention"></a>local attention</h3><p><img src="/images/15437147612512.jpg" width="45%" height="50%"></p><p>由于global attention计算代价高，且对于长句效果不好，我们可以选择一部分来做attention。<br>首先生成一个对齐位置$p_t$，再选择一个窗口$[p_t - D,p_t + D]$，其中D是超参。</p><p>如何获得$p_t$?<br>①直接假设$p_t=t$，也即source和target的位置大致一一对应。</p><p>②做预测：<br><img src="/images/15437150115321.jpg" width="43%" height="50%"><br>其中S是source的句子长度。</p><p>接着，以$p_t$为中心，添加一个高斯分布。最终attention计算公式：<br><img src="/images/15437150721538.jpg" width="50%" height="50%"></p><p>其中align和上面一致：<br><img src="/images/15437151043916.jpg" width="45%" height="50%"></p><p>也就是说，将位置信息也考虑进来。</p><h3 id="Input-feeding-Approach"><a href="#Input-feeding-Approach" class="headerlink" title="Input-feeding Approach"></a>Input-feeding Approach</h3><p>motivation：在下一次的alignment（也就是计算attention）之前，应当知道之前的alignment情况，所以应当作为输入信息传进下一层：<br><img src="/images/15437152269151.jpg" width="50%" height="50%"></p><p>注意这里和Bahdanau的不同。Bahdanau是直接用上下文去构造隐层。这里提出的模型相对更为通用，也可以被应用于非attention的模型中（也就是每次将encoder的最后一层作为输入在每个time step都输入）</p><hr><h2 id="4️⃣-Towards-Linear-Time-Neural-Machine-Translation-with-Capsule-Networks"><a href="#4️⃣-Towards-Linear-Time-Neural-Machine-Translation-with-Capsule-Networks" class="headerlink" title="4️⃣[Towards Linear Time Neural Machine Translation with Capsule Networks]"></a>4️⃣[Towards Linear Time Neural Machine Translation with Capsule Networks]</h2><p>思想：利用capsule提前生成source sentence的固定长度的表示，在decode的时候直接使用，而不需要attention，以达到线性时间NMT的目的。</p><p>Motivation：attention-based的NMT时间复杂度为$|S|\times |T|$，而本文希望能够将NMT减少到线性时间。而传统不加attention的NMT通常使用LSTM最后一层隐层作为源语言的encode信息传入decode，但这样的信息并不能很好地代表整个句子，因此本文使用capsule作为提取source sentence信息的方法，利用capsule生成固定长度表示，直接传入decode端，以达到线性时间的目的。</p><p><img src="/images/15437164176973.jpg" width="50%" height="50%"></p><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><p>对于embedding：<br><img src="/images/15437164440500.jpg" width="37%" height="50%"><br>希望能够转换成固定长度的表示C：<br><img src="/images/15437164654931.jpg" width="37%" height="50%"></p><p>我们首先通过一个双向的LSTM：<br><img src="/images/15437165067165.jpg" width="28%" height="50%"></p><p>一种简单的获取C的方法：<br><img src="/images/15437165382025.jpg" width="30%" height="50%"><br>其中$h_1$和$h_L$有互补关系。</p><p>本文使用capsule提取更丰富的信息。</p><p>在decode阶段，由于拥有固定表示，那么就不需要attention：</p><p><img src="/images/15437166827481.jpg" width="35%" height="50%"><br><img src="/images/15437167374470.jpg" width="37%" height="50%"></p><p>总体架构：<br><img src="/images/15437167607085.jpg" width="60%" height="50%"></p><h3 id="Aggregation-layers-with-Capsule-Networks"><a href="#Aggregation-layers-with-Capsule-Networks" class="headerlink" title="Aggregation layers with Capsule Networks"></a>Aggregation layers with Capsule Networks</h3><p><img src="/images/15437168111687.jpg" width="65%" height="50%"><br>实际上就是dynamic routing那一套，对信息进行提取（论文公式有误就不贴图了）</p><p>算法：<br><img src="/images/15437168668191.jpg" width="55%" height="50%"></p><p>最终获得了：<br><img src="/images/15437168888967.jpg" width="27%" height="50%"></p><hr><h2 id="5️⃣-DropBlock-A-regularization-method-for-convolutional-networks"><a href="#5️⃣-DropBlock-A-regularization-method-for-convolutional-networks" class="headerlink" title="5️⃣[DropBlock: A regularization method for convolutional networks]"></a>5️⃣[DropBlock: A regularization method for convolutional networks]</h2><p>重读了一遍。<br>介绍一种新型的dropout，可用于卷积层提高表现。通过大量的实验得出许多有意义的结论。本文发表于NIPS2018。</p><h3 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h3><p>由于卷积层的feature相互之间有联系，即使使用了dropout，信息也能够根据周围的feature传到下一层。因此使用dropblock，一次将一个方块内的都drop掉。</p><p><img src="/images/15437170173072.jpg" width="50%" height="50%"></p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p><img src="/images/15437170840909.jpg" width="80%" height="50%"></p><p>其中有两个超参：①block_size表示块的大小；γ表示有多少个unit要drop掉，等价传统的dropout的p。当block_size=1时等价dropout；当block size=整个feature map，等价于spatial dropout。</p><p>在实践中，通过以下公式计算γ：<br><img src="/images/15437172746112.jpg" width="55%" height="50%"></p><p>(why? 通过计算期望的方式将传统dropout的keep_prob与当前的γ联系起来，得到一个等式，整理即可获得上式）</p><p>在实验中，还可以逐渐减小keep_prob使得更加鲁棒性。</p><h3 id="实验-amp-结论"><a href="#实验-amp-结论" class="headerlink" title="实验&amp;结论"></a>实验&amp;结论</h3><p>①效果:dropout&lt; spatial dropout &lt; dropblock</p><p>②dropblock能有效去掉semantic information</p><p>③dropblock是一个更加强的regularization</p><p>④使用dropblock的模型，能够学习更多的区域，而不是只专注于一个区域<br><img src="/images/15437174940381.jpg" width="70%" height="50%"></p><p>对于resnet，直接将dropblock应用于添加完skip connection后的feature能够有更高的表现。</p><hr><h2 id="6️⃣-Contextual-String-Embeddings-for-Sequence-Labeling"><a href="#6️⃣-Contextual-String-Embeddings-for-Sequence-Labeling" class="headerlink" title="6️⃣[Contextual String Embeddings for Sequence Labeling]"></a>6️⃣[Contextual String Embeddings for Sequence Labeling]</h2><p>提出一种建立在character基础上的新型的上下文embedding(contextualized embedding）。用于sequence labeling。本文发表于coling2018。</p><h3 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h3><p>整体架构：<br><img src="/images/15437175991019.jpg" width="100%" height="50%"></p><p>首先将character作为基本单位，过一个双向LSTM，进行language model的建模。</p><p>如何提取一个词的词向量：<br><img src="/images/15437176650871.jpg" width="100%" height="50%"><br>提取前向LSTM中该词的最后一个character的后一个hidden state，以及后向LSTM中第一个词的前一个hidden state， 如上图所示。最终拼起来即可：<br><img src="/images/15437177090697.jpg" width="28%" height="50%"><br>因此该词不仅与词内部的character相关，还跟其周围的context有关。</p><p>sequence labeling我不感兴趣，该部分没看。</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>相比word level的language model，character-level独立于tokenization和fixed vocabulary，模型更容易被训练，因为词表小且训练时间短。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> attention </tag>
            
            <tag> capsule </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> self-attention </tag>
            
            <tag> NMT </tag>
            
            <tag> locality modeling </tag>
            
            <tag> dropblock </tag>
            
            <tag> contextualized embedding </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词16</title>
      <link href="/2018/12/01/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D16/"/>
      <url>/2018/12/01/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D16/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣菩萨蛮"><a href="#1️⃣菩萨蛮" class="headerlink" title="1️⃣菩萨蛮"></a>1️⃣菩萨蛮</h3><p>[五代十国] 李煜<br>人生愁恨何能免，销魂独我情何限！故国梦重归，觉来双泪垂。<br>髙楼谁与上？长记秋晴望。<strong>往事已成空，还如一梦中</strong>。</p><p>觉(jue)来：醒来。</p><hr><h3 id="2️⃣南乡子-·-和杨元素，时移守密州"><a href="#2️⃣南乡子-·-和杨元素，时移守密州" class="headerlink" title="2️⃣南乡子 · 和杨元素，时移守密州"></a>2️⃣南乡子 · 和杨元素，时移守密州</h3><p>[宋] 苏轼<br>东武望馀杭，云海天涯两杳茫。<strong>何日功成名遂了，还乡，醉笑陪公三万场</strong>。<br><strong>不用诉离觞，痛饮从来别有肠</strong>。今夜送归灯火冷，河塘，堕泪羊公却姓杨。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>无题</title>
      <link href="/2018/12/01/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BA%BA%E4%B8%8D%E5%8F%AF%E8%83%BD%E7%BB%8F%E5%8E%86%E4%B8%96%E7%95%8C%E4%B8%8A%E6%89%80%E6%9C%89%E7%83%AD%E9%97%B9/"/>
      <url>/2018/12/01/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BA%BA%E4%B8%8D%E5%8F%AF%E8%83%BD%E7%BB%8F%E5%8E%86%E4%B8%96%E7%95%8C%E4%B8%8A%E6%89%80%E6%9C%89%E7%83%AD%E9%97%B9/</url>
      
        <content type="html"><![CDATA[<p>人不可能经历世界上所有热闹，但可以用眼睛看，用心感受，用胸怀扩张。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识12</title>
      <link href="/2018/11/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8612/"/>
      <url>/2018/11/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8612/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Transformer"><a href="#1️⃣-Transformer" class="headerlink" title="1️⃣[Transformer]"></a>1️⃣[Transformer]</h3><p>对Transformer新理解：</p><ul><li>可以将Transformer理解成一张全连接图，其中每个节点与其他节点的关系通过attention权重表现。图关系是序列关系或者树关系的一般化。</li><li>为什么要有multi-head？不仅仅是论文的解释，或许还可以理解成，对一个向量的不同部分（如第1维到20维，第21维到40维等）施以不同的attention权重，如果不使用multi-head，那么对于一个query，就只会有一个权重，而不同的维度有不同的重要性。</li></ul><hr><h3 id="2️⃣-attention-amp-capsule"><a href="#2️⃣-attention-amp-capsule" class="headerlink" title="2️⃣[attention&amp;capsule]"></a>2️⃣[attention&amp;capsule]</h3><p>attention是收信息，query从value按权重获取信息，其中所有value的权重和是1。<br>capsule是发信息，对于$l-1$层的一个capsule来说，在传入到$l$层的k个capsule的信息，其权重和为1。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Transformer </tag>
            
            <tag> attention </tag>
            
            <tag> capsule </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文6</title>
      <link href="/2018/11/19/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%876/"/>
      <url>/2018/11/19/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%876/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-A-STRUCTURED-SELF-ATTENTIVE-SENTENCE-EMBEDDING"><a href="#1️⃣-A-STRUCTURED-SELF-ATTENTIVE-SENTENCE-EMBEDDING" class="headerlink" title="1️⃣[A STRUCTURED SELF ATTENTIVE SENTENCE EMBEDDING]"></a>1️⃣[A STRUCTURED SELF ATTENTIVE SENTENCE EMBEDDING]</h2><p>介绍了一种生成sentence embedding的方法。与其他sentence embedding不同的地方在于，生成的是一个矩阵而不是一个向量。通过矩阵的形式，能够关注不同部分的语义表示，类似于Transformer的multi-head。</p><p>Contribution:</p><ul><li>将sentence embedding扩展为矩阵形式，能够获得更多的信息。</li><li>引入正则化，使得sentence matrix具有更丰富的多样性。</li></ul><p><img src="/images/15425908639518.jpg" width="70%" height="50%"></p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>双向LSTM+self-attention。</p><p>双向的LSTM获得上下文的表示：</p><p><img src="/images/15425911302081.jpg" width="27%" height="50%"></p><p><img src="/images/15425911849931.jpg" width="27%" height="50%"></p><p>因此可以获得attention权重向量：<br><img src="/images/15425912555350.jpg" width="50%" height="50%"></p><p>其中$H:n\times2u,W_{s1}:d_a\times2u ,w_{s2}:d_a$ ，$d_a$是超参。</p><p>现将向量$w_{s2}$扩展为矩阵，亦即有Multi-hop attention：<br><img src="/images/15425914364548.jpg" width="50%" height="50%"></p><p>$W_{s2}$维度为$r\times d_a$，$r$代表了head的个数。</p><p>因此最终的sentence embedding矩阵为：<br><img src="/images/15425915371381.jpg" width="15%" height="50%"></p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>为了让A尽可能有多样性（因为如果都是相似的，那么则会有冗余性），引入如下的正则化：<br><img src="/images/15425915930785.jpg" width="28%" height="50%"></p><p>原因：<br>对于不同的head $a^i$与$a^j$，$A A^T$有：<br><img src="/images/15425918790543.jpg" width="31%" height="50%"></p><p>如果$a^i$与$a^j$很相似那么就会接近于1，如果非常不相似(no overlay)则会接近于0。<br>因此整个式子就是:希望对角线部分接近于0（因为减了单位阵），这就相当于尽可能focus小部分的词；同时其他部分尽可能接近于0，也即不同的head之间没有overlap。</p><h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><p>文章提到，在做分类的时候可以直接将矩阵M展开，过全连接层即可。</p><hr><h2 id="2️⃣-Attention-over-Attention-Neural-Networks-for-Reading-Comprehension"><a href="#2️⃣-Attention-over-Attention-Neural-Networks-for-Reading-Comprehension" class="headerlink" title="2️⃣[Attention-over-Attention Neural Networks for Reading Comprehension]"></a>2️⃣[Attention-over-Attention Neural Networks for Reading Comprehension]</h2><p>在完形填空任务(Cloze-style Reading Comprehension)上提出一种新的attention，即nested-attention。</p><h3 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h3><p>三元组 $ D,Q,A $，document，question，answer。其中answer一般是document的一个词。</p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>本文提出的attention机制，是通过一个新的attention去指示另一个attention的重要程度。</p><p>首先通过一层共享的embedding层，将document和query都encode成word embedding，然后通过双向的GRU，将隐层拼接起来成为新的表示。</p><p>接着获得pair-wise matching matrix：<br><img src="/images/15425993645945.jpg" width="40%" height="50%"></p><p>其中$h$代表上述提到的拼接起来的表示，$M(i,j)$代表了document的词$i$和question的词$j$之间的匹配程度。</p><p>接着对<strong>column</strong>做softmax：<br><img src="/images/15425994692189.jpg" width="50%" height="50%"><br>其代表的意义即query-to-document attention，亦即<strong>对于一个query内的词，document的每个词与其匹配的权重</strong>。</p><p>接下来，对row进行softmax操作：<br><img src="/images/15425995482827.jpg" width="50%" height="50%"><br>代表的是<strong>给定一个document的词，query的哪个词更为重要</strong>。</p><p>接下来我们将β平均起来，获得一个向量：<br><img src="/images/15425996847558.jpg" width="20%" height="50%"><br>这个向量仍有attention的性质，即所有元素加和为1。代表的是<strong>从平均来看，query词的重要性</strong>。</p><p>最后，我们对α和β做点积以获得attended document-level attention：<br><img src="/images/15425997529193.jpg" width="13%" height="50%"></p><p>其中$s$的维度是$D\times 1$。s代表的意义即“a weighted sum of each individual document-level attention α(t) when looking at query word at time t”，也就是说，对α进行加权，代表query word的平均重要程度。</p><p>最终在做完型填空的预测时：<br><img src="/images/15425999965777.jpg" width="38%" height="50%"></p><p>个人觉得这种attention-over-attention的想法还是挺有创新的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> attention </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> sentence embedding </tag>
            
            <tag> nested attention </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>网络优化与正则化总结</title>
      <link href="/2018/11/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96%E6%80%BB%E7%BB%93/"/>
      <url>/2018/11/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>大量参考自<a href="https://nndl.github.io/" target="_blank" rel="noopener">《神经网络与深度学习》</a></p><h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><p>对于标准的SGD，常见的改进算法从两个方面进行：学习率衰减&amp;梯度方向优化。<br>记$g_t$为t时刻的导数：<br><img src="/images/2018-11-13-15421196736629.jpg" width="20%" height="50%"></p><h2 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h2><h3 id="AdaGrad算法"><a href="#AdaGrad算法" class="headerlink" title="AdaGrad算法"></a>AdaGrad算法</h3><p>通过计算历次的梯度平方累计值进行学习率衰减。<br>$G_t$是累计值：<br><img src="/images/2018-11-13-15421189802198.jpg" width="20%" height="50%"></p><p>更新值则为：<br><img src="/images/2018-11-13-15421190100615.jpg" width="30%" height="50%"></p><p>缺点：随着迭代次数的增加学习率递减。在经过一定次数的迭代依然没有找到最优点时，由于这时的学习率已经非常小，很难再继续找到最优点。</p><h3 id="RMSprop算法"><a href="#RMSprop算法" class="headerlink" title="RMSprop算法"></a>RMSprop算法</h3><p>对AdaGrad的改进，唯一的区别在于$G_t$的计算，将历史信息和当前信息进行线性加权，使得学习率可以动态改变而不是单调递减：<br><img src="/images/2018-11-13-15421192344025.jpg" width="40%" height="50%"></p><p>β为衰减率，通常取0.9。也即历史信息占主导。</p><h3 id="AdaDelta算法"><a href="#AdaDelta算法" class="headerlink" title="AdaDelta算法"></a>AdaDelta算法</h3><p>同样是对AdaGrad的改进。<br>每次计算：<br><img src="/images/2018-11-13-15421195264173.jpg" width="50%" height="50%"></p><p>也即历史更新差和上一时刻的更新差的加权（RMSprop是历史梯度和当前梯度）。</p><p>最终更新差值为：<br><img src="/images/2018-11-13-15421197355615.jpg" width="30%" height="50%"></p><p>其中$G_t$计算方法和RMSprop一致。</p><h2 id="梯度方向优化"><a href="#梯度方向优化" class="headerlink" title="梯度方向优化"></a>梯度方向优化</h2><p>利用历史的梯度（方向）调整当前时刻的梯度。</p><h3 id="动量（Momentum）法"><a href="#动量（Momentum）法" class="headerlink" title="动量（Momentum）法"></a>动量（Momentum）法</h3><p>动量法（Momentum Method）是用之前积累动量来替代真正的梯度。每次迭代的梯度可以看作是加速度。</p><p><img src="/images/2018-11-13-15421199473226.jpg" width="28%" height="50%"></p><p>也即上一时刻的更新差值和当前梯度共同决定当前的更新差值。$ρ$为动量因子，通常为0.9。也即动量占了主导。</p><p>当某个参数在最近一段时间内的梯度方向不一致时，其真实的参数更新幅度变小；相反，当在最近一段时间内的梯度方向都一致时，其真实的参数更新幅度变大，起到加速作用。一般而言，在迭代初期，梯度方法都比较一致，动量法会起到加速作用，可以更快地到达最优点。在迭代后期，梯度方法会取决不一致，在收敛值附近震荡，动量法会起到减速作用，增加稳定性。</p><h3 id="Nesterov加速梯度"><a href="#Nesterov加速梯度" class="headerlink" title="Nesterov加速梯度"></a>Nesterov加速梯度</h3><p>动量法的改进版本。</p><p>前面提到的动量法，是上一步的更新方向$\Delta \theta_{t-1}$与当前梯度$-g_t$的加和。因此可以理解成，先根据$∆θ_{t−1}$更新一次得到参数θ，再用$g_t$进行更新。亦即：<br><img src="/images/2018-11-13-15421202426163.jpg" width="27%" height="50%"><br>上式的第二步中，$g_t$是在$ \theta_{t-1}$上的梯度。我们将该步改为在$\theta_{t}$的梯度。<br>因此，有：<br><img src="/images/2018-11-13-15421203421465.jpg" width="50%" height="50%"></p><p>和动量法相比，相当于提前走了一步。<br><img src="/images/2018-11-13-15421203910771.jpg" width="70%" height="50%"></p><h3 id="Adam-amp-Nadam"><a href="#Adam-amp-Nadam" class="headerlink" title="Adam&amp;Nadam"></a>Adam&amp;Nadam</h3><p>Adam一方面计算梯度平方的加权，同时还计算梯度的加权：<br><img src="/images/2018-11-13-15421205162558.jpg" width="40%" height="50%"><br>通常$β_1=0.9$，$β_2=0.99$<br>也即历史信息占了主导。</p><p>在初期$M_t$与$G_t$会比真实均值和方差要小（想象$M_0=0$，$G_0=0$时）。因此对其进行修正，即：<br><img src="/images/2018-11-13-15421207635850.jpg" width="18%" height="50%"><br>因此最终有：<br><img src="/images/2018-11-13-15421207966341.jpg" width="26%" height="50%"></p><p>同理有Nadam。</p><p>Adam = Momentum + RMSprop<br>Nadam = Nesterov + RMSprop</p><h3 id="梯度截断-gradient-clipping"><a href="#梯度截断-gradient-clipping" class="headerlink" title="梯度截断 gradient clipping"></a>梯度截断 gradient clipping</h3><p>分为按值截断与按模截断。</p><h1 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h1><p>初始值选取很关键。假设全部初始化为0，则后续更新导致所有的激活值相同，也即对称权重现象。</p><p>原则：不能过大，否则激活值会变得饱和，如sigmoid；不能过小，否则经过多层信号会逐渐消失，并且导致sigmoid丢失非线性的能力（在0附近基本近似线性）。如果一个神经元的输入连接很多，它的每个输入连接上的权重就应该小一些，这是为了避免输出过大。</p><h2 id="Gaussian分布初始化"><a href="#Gaussian分布初始化" class="headerlink" title="Gaussian分布初始化"></a>Gaussian分布初始化</h2><p>同时考虑输入输出，可以按 $N(0,\sqrt{\frac{2}{n_{in} + n_{out}}})$ 高斯分布来初始化。</p><h2 id="均匀分布初始化"><a href="#均匀分布初始化" class="headerlink" title="均匀分布初始化"></a>均匀分布初始化</h2><p>在$[-r,r]$区间均匀分布初始化，其中r可以按照神经元数量自适应调整。</p><h3 id="Xavier初始化方法"><a href="#Xavier初始化方法" class="headerlink" title="Xavier初始化方法"></a>Xavier初始化方法</h3><p>自动计算超参r。r的公式为：<br><img src="/images/2018-11-14-15421648119504.jpg" width="22%" height="50%"><br>其中$n^l$代表第$l$层的神经元个数。</p><p>为什么是这个式子（推导见参考资料）：综合考虑了①输入输出的方差要一致；②反向传播中误差信号的方差不被放大或缩小。</p><h1 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h1><p>将数据分布归一化，使得分布保持稳定。<br><img src="/images/2018-11-14-15421656553319.jpg" width="100%" height="50%"><br>假设数据有四维(N,C,H,W)。N代表batch；C代表channel；H,W代表height和width。</p><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>沿着通道进行归一化，亦即每个通道都有自己的均值和方差。<br><img src="/images/2018-11-14-15421657694248.jpg" width="70%" height="50%"><br>其中缩放平移变量是可学习的。</p><p>缺点：<br>①对batch size敏感，batch size太小则方差均值不足以代表数据分布<br>②对于不等长的输入如RNN来说，每一个timestep都需要保存不同的特征。</p><h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><p>对一个输入进行正则化，亦即每个输入都有自己的方差、均值。这样不依赖于batch大小和输入sequence的深度。</p><p>对RNN效果比较明显，但CNN中不如BN</p><h2 id="Instance-Normalization"><a href="#Instance-Normalization" class="headerlink" title="Instance Normalization"></a>Instance Normalization</h2><p>对HW进行归一化</p><h2 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h2><p>将channel分为多个group，每个group内做归一化</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://nndl.github.io/" target="_blank" rel="noopener">《神经网络与深度学习》</a><br><a href="https://blog.csdn.net/liuxiao214/article/details/81037416" target="_blank" rel="noopener">https://blog.csdn.net/liuxiao214/article/details/81037416</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习🤖 </tag>
            
            <tag> 优化算法 </tag>
            
            <tag> 参数初始化 </tag>
            
            <tag> Normalization </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录10</title>
      <link href="/2018/11/11/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9510/"/>
      <url>/2018/11/11/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9510/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-get-sinusoid-encoding-table"><a href="#1️⃣-get-sinusoid-encoding-table" class="headerlink" title="1️⃣[get_sinusoid_encoding_table]"></a>1️⃣[get_sinusoid_encoding_table]</h3><p>Transformer绝对位置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sinusoid_encoding_table</span><span class="params">(n_position, d_hid, padding_idx=None)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_angle</span><span class="params">(position, hid_idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> position / np.power(<span class="number">10000</span>, <span class="number">2</span> * (hid_idx // <span class="number">2</span>) / d_hid)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_posi_angle_vec</span><span class="params">(position)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [cal_angle(position, hid_j) <span class="keyword">for</span> hid_j <span class="keyword">in</span> range(d_hid)]</span><br><span class="line"></span><br><span class="line">    sinusoid_table = np.array([get_posi_angle_vec(pos_i) <span class="keyword">for</span> pos_i <span class="keyword">in</span> range(n_position)])</span><br><span class="line"></span><br><span class="line">    sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line">    sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>] = np.cos(sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> padding_idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        sinusoid_table[padding_idx] = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.FloatTensor(sinusoid_table)  <span class="comment"># n_position,embed_dim</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识11</title>
      <link href="/2018/11/11/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8611/"/>
      <url>/2018/11/11/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8611/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Optimizer"><a href="#1️⃣-Optimizer" class="headerlink" title="1️⃣[Optimizer]"></a>1️⃣[Optimizer]</h3><p><a href="https://zhuanlan.zhihu.com/p/32262540" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32262540</a><br><a href="https://zhuanlan.zhihu.com/p/32338983" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32338983</a></p><p>Adam等自适应学习率算法对于稀疏数据具有优势，且收敛速度很快；但精调参数的SGD（+Momentum）往往能够取得更好的最终结果。</p><p>建议：<br>前期用Adam，享受Adam快速收敛的优势；后期切换到SGD，慢慢寻找最优解。<br>什么时候从Adam切换到SGD？当SGD的相应学习率的移动平均值基本不变的时候。</p><hr><h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>LongTensor除以浮点数，会对除数进行取整，再做除法。<br><img src="/images/2018-11-11-15419055399325.jpg" width="30%" height="50%"></p><hr><h3 id="3️⃣-Pytorch"><a href="#3️⃣-Pytorch" class="headerlink" title="3️⃣[Pytorch]"></a>3️⃣[Pytorch]</h3><p>使用Pytorch的DataParallel</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda:'</span> + str(</span><br><span class="line">    config.CUDA_VISIBLE_DEVICES[<span class="number">0</span>]) <span class="keyword">if</span> config.use_cuda <span class="keyword">else</span> <span class="string">'cpu'</span>)   <span class="comment"># 指定第一个设备</span></span><br><span class="line"></span><br><span class="line">model = ClassifyModel(</span><br><span class="line">    vocab_size=len(vocab), max_seq_len=config.max_sent_len,</span><br><span class="line">    embed_dim=config.embed_dim, n_layers=config.n_layers,</span><br><span class="line">    n_head=config.n_head, d_k=config.d_k,</span><br><span class="line">    d_v=config.d_v,</span><br><span class="line">    d_model=config.d_model, d_inner=config.d_inner_hid,</span><br><span class="line">    n_label=config.n_label,</span><br><span class="line">    dropout=config.dropout</span><br><span class="line">).to(device)</span><br><span class="line"></span><br><span class="line">model = DataParallel(model, device_ids=config.CUDA_VISIBLE_DEVICES)  <span class="comment"># 显式定义device_ids</span></span><br></pre></td></tr></table></figure><p>注意到：device_ids的起始编号要与之前定义的device中的“cuda:0”相一致，不然会报错。</p><p>如果不显式在代码中的DataParallel指定设备，那么需要在命令行内指定。如果是在命令行里面运行的，且device不是从0开始，应当显式设置GPU_id，否则会出错‘AssertionError: Invalid device id’，正确的命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=4,5  python -u classify_main.py --gpu_id 0,1</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Optimizer </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于sparse gradient</title>
      <link href="/2018/11/11/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8Esparse%20gradient/"/>
      <url>/2018/11/11/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8Esparse%20gradient/</url>
      
        <content type="html"><![CDATA[<p>前几天在看AllenAI在EMNLP的ppt时，有一页写道：<br><img src="/images/2018-11-11-15419037448379.jpg" width="70%" height="50%"></p><p>为什么会出现这种情况？</p><p>Embedding是一个很大的矩阵，每一次其实都只有一个小部分进行了更新，对于一些词来说，出现的频率不高，或者说，其实大部分的词在一个loop/epoch中，被更新的次数是较少的。但是，注意到一般的optimizer算法，是以matrix为单位进行更新的，也就是每一次都是$W^{t+1}=W^{t}-\eta \frac{\partial L}{\partial{W}}$</p><p>而Adam算法：<br><img src="/images/2018-11-11-15419038346958.jpg" width="70%" height="50%"></p><p>动量占了主导。但这样，每次batch更新，那些没被更新的词（也即gradient=0）的动量仍然会被衰减，所以这样当到这个词更新的时候，他的动量已经被衰减完了，所以更新的gradient就很小。</p><p>解决方案：</p><p>①在PyTorch中，Embedding的API：<br><code>torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False, _weight=None)</code></p><p>其中sparse (bool, optional) – if True, gradient w.r.t. weight matrix will be a sparse tensor.</p><p>将sparse设为True即可。</p><p>②针对sparse矩阵，使用不同的optimizer，如torch.optim.SparseAdam：</p><blockquote><p>Implements lazy version of Adam algorithm suitable for sparse tensors.<br>In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> sparse gradient </tag>
            
            <tag> 代码实践 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文5</title>
      <link href="/2018/11/10/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%875/"/>
      <url>/2018/11/10/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%875/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-Neural-Turing-Machine"><a href="#1️⃣-Neural-Turing-Machine" class="headerlink" title="1️⃣[Neural Turing Machine]"></a>1️⃣[Neural Turing Machine]</h2><p>通过模仿冯诺依曼机，引入外部内存(externel memory)。<br><img src="/images/2018-11-10-15418626837026.jpg" width="70%" height="50%"></p><p>和普通神经网络一样，与外界交互，获得一个输入，产生一个输出。但不同的是，内部还有一个memory进行读写。<br>假设memory是一个N × M的矩阵，N是内存的位置数量。</p><h3 id="读写memory"><a href="#读写memory" class="headerlink" title="读写memory"></a>读写memory</h3><p>①读<br><img src="/images/2018-11-10-15418627403268.jpg" width="25%" height="50%"><br>其中读的时候对各内存位置线性加权。w是归一化权重。</p><p>②写<br>$e_t$是擦除向量（erase vector）<br><img src="/images/2018-11-10-15418627941358.jpg" width="35%" height="50%"></p><p>$a_t$是加和向量(add vector)<br><img src="/images/2018-11-10-15418628323343.jpg" width="30%" height="50%"></p><p>具体如何获得权重就不说了。</p><h3 id="Controller-network"><a href="#Controller-network" class="headerlink" title="Controller network"></a>Controller network</h3><p>中间的controller network可以是一个普通的feed forward或者RNN。</p><p>在实际中NTM用得并不多。</p><hr><h2 id="2️⃣-Efficient-Contextualized-Representation-Language-Model-Pruning-for-Sequence-Labeling"><a href="#2️⃣-Efficient-Contextualized-Representation-Language-Model-Pruning-for-Sequence-Labeling" class="headerlink" title="2️⃣[Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling]"></a>2️⃣[Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling]</h2><p>ELMo的精简版，通过即插即用的方法来压缩语言模型，对特定任务剪枝不同的层，使得能够减少inference的时间。<br>这篇的idea挺有创新的，但似乎有些trivial的感觉。</p><p><img src="/images/2018-11-11-15418977712883.jpg" width="70%" height="50%"></p><h3 id="RNN-and-Dense-Connectivity"><a href="#RNN-and-Dense-Connectivity" class="headerlink" title="RNN and Dense Connectivity"></a>RNN and Dense Connectivity</h3><p>每一层的输出都会传到所有层作为输入，因此对于L层的输入：<br><img src="/images/2018-11-11-15418979328482.jpg" width="35%" height="50%"></p><p>这样我们就能够随意地去掉任意中间层了。同时一些语言信息也分散到各个层，即使去掉某些层也没有关系。</p><p>则最终的output为：<br><img src="/images/2018-11-11-15418991345288.jpg" width="33%" height="50%"></p><p>最终作projection到正常维度（在每层都会这么做，将输入降维到正常维度再输入）：<br><img src="/images/2018-11-11-15418992517849.jpg" width="37%" height="50%"></p><p>再做一个softmax：<br><img src="/images/2018-11-11-15418993146246.jpg" width="32%" height="50%"></p><p>由于 $h^{※}$ 用于softmax，所以可能和target word，也即下一个词比较相似，<strong>因此可能没有很多的上下文信息</strong>。</p><p>所以最终我们使用$h_t$，以及反向的$h_t^r$，再过一层线性层获得最终的embedding（和ELMo有些不同，ELMo是直接拼起来）：<br><img src="/images/2018-11-11-15418994541442.jpg" width="40%" height="50%"></p><h3 id="Layer-Selection"><a href="#Layer-Selection" class="headerlink" title="Layer Selection"></a>Layer Selection</h3><p>我们在每层的output都加一个权重系数。<br><img src="/images/2018-11-11-15418996081852.jpg" width="30%" height="50%"></p><p>我们希望在target task上用的时候，部分z能够变成0，达到layer selection的效果，加快inference的速度。</p><p>亦即：<br><img src="/images/2018-11-11-15418996697208.jpg" width="20%" height="50%"></p><p>一种理想的方法是L0正则化：<br><img src="/images/2018-11-11-15418997294756.jpg" width="17%" height="50%"></p><p>但由于没办法求导，因此，采用L1正则化：<br><img src="/images/2018-11-11-15418997747882.jpg" width="15%" height="50%"><br>但使用L1正则化有一定的风险，因为如果让所有z都远离1，那么会影响performance。</p><p>引入新的正则化方法$R_2 =\delta(|z|_0&gt;\lambda_1) |z|_1$<br>亦即，只有在非零z的个数大于某个阈值时，才能有正则化效果，保证非零的个数。’it can be “turned-off” after achieving a satisfying sparsity’.</p><p>进一步引入$R_3=\delta(|z|_0&gt;\lambda_1) |z|_1 + |z(1-z)|_1$<br>其中第二项为了鼓励z向0或1走。</p><h3 id="Layer-wise-Dropout"><a href="#Layer-wise-Dropout" class="headerlink" title="Layer-wise Dropout"></a>Layer-wise Dropout</h3><p>随机删除部分layer，这些layer的输出不会传入之后的层，但仍然会参与最后的representation计算。<br><img src="/images/2018-11-11-15419000928057.jpg" width="70%" height="50%"></p><p>这种dropout会让perplexity更高，但对生成更好的representation有帮助。</p><hr><h2 id="3️⃣-Constituency-Parsing-with-a-Self-Attentive-Encoder"><a href="#3️⃣-Constituency-Parsing-with-a-Self-Attentive-Encoder" class="headerlink" title="3️⃣[Constituency Parsing with a Self-Attentive Encoder]"></a>3️⃣[Constituency Parsing with a Self-Attentive Encoder]</h2><p>其中的positional encoding我比较感兴趣。<br>原版的positional encoding是直接和embedding相加的。<br>亦即：<br><img src="/images/2018-11-11-15419002563338.jpg" width="22%" height="50%"><br>那么在selt-attention时，有：<br><img src="/images/2018-11-11-15419002855901.jpg" width="45%" height="50%"><br>这样会有交叉项：<br><img src="/images/2018-11-11-15419003111684.jpg" width="13%" height="50%"><br>该项没有什么意义，且可能会带来过拟合。</p><p>因此在这边将positional encoding和embedding拼起来，亦即：<br><img src="/images/2018-11-11-15419003740409.jpg" width="23%" height="50%"></p><p>并且，在进入multi-head时的线性层也做改变：<br><img src="/images/2018-11-11-15419004269693.jpg" width="24%" height="50%"></p><p>这样在相乘的时候就不会有交叉项了。</p><p>实验证明，该方法有一定的提升。</p><hr><h2 id="4️⃣-DropBlock-A-regularization-method-for-convolutional-networks"><a href="#4️⃣-DropBlock-A-regularization-method-for-convolutional-networks" class="headerlink" title="4️⃣[DropBlock: A regularization method for convolutional networks]"></a>4️⃣[DropBlock: A regularization method for convolutional networks]</h2><p>大致翻了一下。<br>Motivation:在CNN中，dropout对convolutional layer的作用不大，一般都只用在全连接层。作者推测，因为每个feature map都有一个感受野范围，仅仅对单个像素进行dropout并不能降低feature map学习的特征范围，亦即网络仍可以通过该位置的相邻位置元素去学习对应的语义信息，也就不会促使网络去学习更加鲁棒的特征。</p><p>因此作者的做法是，dropout一整块位置。<br><img src="/images/2018-11-11-15419007355875.jpg" width="80%" height="50%"></p><hr><h2 id="5️⃣-Accelerating-Neural-Transformer-via-an-Average-Attention-Network"><a href="#5️⃣-Accelerating-Neural-Transformer-via-an-Average-Attention-Network" class="headerlink" title="5️⃣[Accelerating Neural Transformer via an Average Attention Network]"></a>5️⃣[Accelerating Neural Transformer via an Average Attention Network]</h2><p>提出了AAN(average attention network)，对transformer翻译模型的decode部分进行改进，加速了过程。</p><p>由于Transformer在decode阶段需要用到前面所有的y，也即自回归(auto-regressive)的性质，所以无法并行：</p><p><img src="/images/2018-11-11-15419009098650.jpg" width="50%" height="50%"></p><h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><p>给定y：<br><img src="/images/2018-11-11-15419010325049.jpg" width="27%" height="50%"></p><p>首先将他们加起来，过一层全连接：<br><img src="/images/2018-11-11-15419010603010.jpg" width="27%" height="50%"><br>这也相当于就是让所有的y有相同的权重，此时g就是上下文相关的表示。</p><p>接下来添加一个gating：<br><img src="/images/2018-11-11-15419011154221.jpg" width="27%" height="50%"><br>控制了从过去保存多少信息和获取多少新的信息。</p><p>和Transformer原版论文一样，添加一个residual connection：<br><img src="/images/2018-11-11-15419011595237.jpg" width="30%" height="50%"></p><p>如图整个过程：<br><img src="/images/2018-11-11-15419011840751.jpg" width="55%" height="50%"></p><p>总结：AAN=average layer+gating layer</p><h3 id="加速"><a href="#加速" class="headerlink" title="加速"></a>加速</h3><p>①考虑到加和操作是序列化的，只能一个一个来，不能并行，在这里使用一个mask的trick，使得在训练时也能够并行：<br><img src="/images/2018-11-11-15419013219526.jpg" width="60%" height="50%"></p><p>②在inference时的加速：<br><img src="/images/2018-11-11-15419019335926.jpg" width="20%" height="50%"></p><p>这样Transformer就能够类似RNN，只考虑前一个的state，而不是前面所有的state。</p><p>最终的模型：<br><img src="/images/2018-11-11-15419023032628.jpg" width="60%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Transformer </tag>
            
            <tag> dropout </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> self-attention </tag>
            
            <tag> NTM </tag>
            
            <tag> ELMo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词15</title>
      <link href="/2018/11/10/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D15/"/>
      <url>/2018/11/10/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D15/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣蜀相"><a href="#1️⃣蜀相" class="headerlink" title="1️⃣蜀相"></a>1️⃣蜀相</h3><p>[唐] 杜甫<br>丞相祠堂何处寻，锦官城外柏森森。<br>映阶碧草自春色，隔叶黄鹂空好音。<br>三顾频烦天下计，两朝开济老臣心。<br><strong>出师未捷身先死，长使英雄泪满襟</strong>。</p><p><a href="http://m.xichuangzhu.com/work/57b93410a633bd00665efd4a" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b93410a633bd00665efd4a</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文4</title>
      <link href="/2018/11/04/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%874/"/>
      <url>/2018/11/04/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%874/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-Character-Level-Language-Modeling-with-Deeper-Self-Attention"><a href="#1️⃣-Character-Level-Language-Modeling-with-Deeper-Self-Attention" class="headerlink" title="1️⃣[Character-Level Language Modeling with Deeper Self-Attention]"></a>1️⃣[Character-Level Language Modeling with Deeper Self-Attention]</h2><p>将transformer用于character-level的语言模型中，通过添加多个loss来提高其表现以及加快拟合速度，同时加深transformer的层数，极大提升表现，12层的transformer layer能达到SOTA，而64层则有更多的提升。</p><p>普通RNN用于character-level language model：<br>将句子按character为单位组成多个batch，每个batch预测最后一个词，然后将该batch的隐状态传入下一个batch。也即“truncated backpropagation through time” (TBTT)。</p><p>如果用在Transformer，如下图，我们只预测$t_4$。<br><img src="/images/2018-11-04-15412915431327.jpg" width="90%" height="50%"></p><p>本文的一大贡献是多加了三种loss，并且有些loss的权值会随着训练的过程而逐渐减小，每个loss都会自己的schedule。这些loss加快了拟合速度，同时也提升了表现。</p><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><h4 id="Multiple-Positions"><a href="#Multiple-Positions" class="headerlink" title="Multiple Positions"></a>Multiple Positions</h4><p>对于batch内而言，每个时间步t都要预测下一个词。<br><img src="/images/2018-11-04-15412916429104.jpg" width="90%" height="50%"></p><h4 id="Intermediate-Layer-Losses"><a href="#Intermediate-Layer-Losses" class="headerlink" title="Intermediate Layer Losses"></a>Intermediate Layer Losses</h4><p>要求中间层也做出预测：<br><img src="/images/2018-11-04-15412916704097.jpg" width="95%" height="50%"></p><p>在这里，越底层的layer其loss权值越低。</p><h4 id="Multiple-Targets"><a href="#Multiple-Targets" class="headerlink" title="Multiple Targets"></a>Multiple Targets</h4><p>每一个position，不仅仅要预测下一个词，还要预测下几个词，预测下一个词和预测下几个词的分类器是独立的。</p><p><img src="/images/2018-11-04-15412917374689.jpg" width="70%" height="50%"></p><h3 id="Positional-embedding"><a href="#Positional-embedding" class="headerlink" title="Positional embedding"></a>Positional embedding</h3><p>每一层的都添加一个不共享的可学习的positional embedding。</p><hr><h2 id="2️⃣-Self-Attention-with-Relative-Position-Representations"><a href="#2️⃣-Self-Attention-with-Relative-Position-Representations" class="headerlink" title="2️⃣[Self-Attention with Relative Position Representations]"></a>2️⃣[Self-Attention with Relative Position Representations]</h2><p>提出使用相对位置替代Transformer的绝对位置信息，并在NMT上有一定的提升。</p><p>分解：<br>在原先的self-attention中，输出为：<br><img src="/images/2018-11-04-15412923510664.jpg" width="25%" height="50%"></p><p>其中：<br><img src="/images/2018-11-04-15412923744647.jpg" width="25%" height="50%"><br><img src="/images/2018-11-04-15412923773686.jpg" width="25%" height="50%"></p><p>现在我们考虑添加相对位置，其中相对位置信息在各层都是共享的：<br><img src="/images/2018-11-04-15412924279426.jpg" width="30%" height="50%"><br><img src="/images/2018-11-04-15412924396468.jpg" width="30%" height="50%"></p><p>$a_{ij}^K$的具体形式：<br><img src="/images/2018-11-04-15412925792994.jpg" width="40%" height="50%"><br><img src="/images/2018-11-04-15412925910424.jpg" width="55%" height="50%"><br>上式为了降低复杂度，不考虑长于k的相对位置信息。</p><p>考虑到transformer的并行性，为了并行性，我们考虑如下式子：<br><img src="/images/2018-11-04-15412926687951.jpg" width="50%" height="50%"><br>其中，第一项和原来的Transformer一致；第二项，通过reshape可以达到并行的效果，然后两项直接加起来。</p><p>实验证明，使用相对位置效果是有一定的提升的，而同时使用绝对位置和相对位置并没有提升。<br><img src="/images/2018-11-04-15412930642978.jpg" width="90%" height="50%"></p><hr><h2 id="3️⃣-WEIGHTED-TRANSFORMER-NETWORK-FOR-MACHINE-TRANSLATION"><a href="#3️⃣-WEIGHTED-TRANSFORMER-NETWORK-FOR-MACHINE-TRANSLATION" class="headerlink" title="3️⃣[WEIGHTED TRANSFORMER NETWORK FOR MACHINE TRANSLATION]"></a>3️⃣[WEIGHTED TRANSFORMER NETWORK FOR MACHINE TRANSLATION]</h2><p>这篇被ICLR拒了，但有审稿人打了9分的高分。</p><p>对Transformer进行改进，拥有更好的效果和更小的计算代价。</p><p>传统的Transformer：</p><script type="math/tex; mode=display">Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><script type="math/tex; mode=display">head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)</script><script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat_i (head_i)W^O</script><script type="math/tex; mode=display">FFN(x)=max(0,xW_1+b_1)W_2 + b_2</script><p>在本文中，先对head进行升维并乘以权重，过了FNN后，再乘以另一个权重。其中权重$\alpha$ $ \kappa$为可学习参数：</p><script type="math/tex; mode=display">head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)</script><script type="math/tex; mode=display">\overline{head_i}=head_i W^{O_i} \times \kappa_i</script><script type="math/tex; mode=display">BranchedAttention(Q,K,V)=\sum_{i=1}^{M} \alpha_i FFN(\overline{head}_i)</script><p>其中要求权重之和为1。即$\sum_{i=1}^{M}\alpha_i=1$,$\sum_{i=1}^{M}\kappa_i=1$。</p><p><img src="/images/2018-11-04-15412939412047.jpg" width="90%" height="50%"></p><p>文中对$\kappa$和$\alpha$作了解释。</p><blockquote><p>κ can be interpreted as a learned concatenation weight and α as the learned addition weight</p></blockquote><p>通过实验，发现该模型会有更好的正则化特性。同时效果也有一定提升，收敛速度更快：<br><img src="/images/2018-11-04-15412940966579.jpg" width="80%" height="50%"></p><hr><h2 id="4️⃣-You-May-Not-Need-Attention"><a href="#4️⃣-You-May-Not-Need-Attention" class="headerlink" title="4️⃣[You May Not Need Attention]"></a>4️⃣[You May Not Need Attention]</h2><p>粗略地过了一遍，一些细节没有弄明白。</p><p>提出一种将encoder-decoder融合起来的模型，也即eager translation model，不需要attention，能够实现即时的翻译，也即读入一个词就能翻译一个词，同时不需要记录encoder的所有输出，因此需要很少的内存。</p><p><img src="/images/2018-11-04-15412942175720.jpg" width="50%" height="50%"></p><p>分为三步：<br>①pre-processing<br>进行预处理，使得源句子和目标句子满足<strong>eager feasible</strong> for every aligned pair of words $(s_i , t_j ), i ≤ j$。</p><p>首先通过现成的工具进行对齐操作(alignment)，然后对于那些不符合eager feasible的有具体算法（没认真看）进行补padding。如图<br><img src="/images/2018-11-04-15412945231042.jpg" width="60%" height="50%"></p><p>我们还可以在target sentence的开头添加b个padding，使得模型能够在开始预测之前获取更多的source sentence的词。</p><p>②模型<br>两层的LSTM，输入是上一次的y和当前的x拼接起来直接传进去。</p><p>③post processing<br>在最终结果之前，将padding去掉。</p><p>在inference（也即beam search）时，还有几个操作/trick：</p><ul><li>Padding limit</li><li>Source padding injection SPI</li></ul><p>实验表明，eager model在长的句子表现超过传统带attention的NMT，而长句子的建模正是attention-based 的模型的一大挑战；而在短句子上就不如attention-based的NMT。<br><img src="/images/2018-11-04-15412946442983.jpg" width="50%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Transformer </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> Language Modeling </tag>
            
            <tag> self-attention </tag>
            
            <tag> relative position </tag>
            
            <tag> positional encoding </tag>
            
            <tag> NMT </tag>
            
            <tag> eager translation model </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词14</title>
      <link href="/2018/11/04/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D14/"/>
      <url>/2018/11/04/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D14/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣鹤冲天"><a href="#1️⃣鹤冲天" class="headerlink" title="1️⃣鹤冲天"></a>1️⃣鹤冲天</h3><p>[宋] 柳永<br>黄金榜上，偶失龙头望。明代暂遗贤，如何向？未遂风云便，争不恣游狂荡。何须论得丧？才子词人，自是白衣卿相。<br>烟花巷陌，依约丹靑屛障。幸有意中人，堪寻访。且恁偎红倚翠，风流事，平生畅。靑春都一饷。<strong>忍把浮名，换了浅斟低唱</strong>！</p><p>恣（zì）：放纵，随心所欲。<br>恁（nèn）：如此。</p><p><a href="http://m.xichuangzhu.com/work/57aeff68a633bd0057f7d406" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57aeff68a633bd0057f7d406</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录9</title>
      <link href="/2018/11/04/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%959/"/>
      <url>/2018/11/04/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%959/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-collate-fn"><a href="#1️⃣-collate-fn" class="headerlink" title="1️⃣[collate_fn]"></a>1️⃣[collate_fn]</h3><p>将不等长句子组合成batch。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(insts)</span>:</span></span><br><span class="line">    <span class="string">''' Pad the instance to the max seq length in batch '''</span></span><br><span class="line"></span><br><span class="line">    max_len = max(len(inst) <span class="keyword">for</span> inst <span class="keyword">in</span> insts)</span><br><span class="line"></span><br><span class="line">    batch_seq = np.array([</span><br><span class="line">        inst + [Constants.PAD] * (max_len - len(inst))</span><br><span class="line">        <span class="keyword">for</span> inst <span class="keyword">in</span> insts])</span><br><span class="line"></span><br><span class="line">    batch_pos = np.array([</span><br><span class="line">        [pos_i + <span class="number">1</span> <span class="keyword">if</span> w_i != Constants.PAD <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">         <span class="keyword">for</span> pos_i, w_i <span class="keyword">in</span> enumerate(inst)] <span class="keyword">for</span> inst <span class="keyword">in</span> batch_seq]) <span class="comment"># 位置信息</span></span><br><span class="line"></span><br><span class="line">    batch_seq = torch.LongTensor(batch_seq)</span><br><span class="line">    batch_pos = torch.LongTensor(batch_pos)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> batch_seq, batch_pos</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>“莱斯杯”挑战赛有感</title>
      <link href="/2018/10/30/%E8%A7%81%E9%97%BB&amp;%E6%83%B3%E6%B3%95/%E2%80%9C%E8%8E%B1%E6%96%AF%E6%9D%AF%E2%80%9D%E6%8C%91%E6%88%98%E8%B5%9B%E6%9C%89%E6%84%9F/"/>
      <url>/2018/10/30/%E8%A7%81%E9%97%BB&amp;%E6%83%B3%E6%B3%95/%E2%80%9C%E8%8E%B1%E6%96%AF%E6%9D%AF%E2%80%9D%E6%8C%91%E6%88%98%E8%B5%9B%E6%9C%89%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>历时三个月的“莱斯杯”全国第一届“军事智能·机器阅读”挑战赛终于落下帷幕，前几日（10.26-10.28）有幸在南京青旅宾馆参与决赛，体验多多，收获满满，心中亦有一些感想。</p><p>一个是南京总带给我一种回家的感觉，对南京的事物总有亲切感。第一次来南京是一年半前，也是来参加比赛。周五晚上的夜游秦淮，让我感受到许久未曾感受到的烟火气息。</p><p><img src="/images/2018-10-30-511540860855_.pic_hd.jpg" width="90%" height="50%"></p><p>第二个是此次主办方提供的食宿令人惊喜。一开始听到青旅宾馆，我已经做好了艰苦奋战的准备了，然而酒店是星级酒店的，吃方面直接到楼下的自助。可以看出主办方此次确实用心在举办这次比赛。</p><p><img src="/images/2018-10-30-15408644190676.jpg" width="100%" height="50%"></p><p>第三点是关于比赛的，关于比赛的整个历程我还是颇有感触。<br>我们是以第9名的成绩挺进决赛，其实在后期比赛中，我们都有所懈怠了，几乎没有花时间在这上面，10月初发布决赛的数据集，而我们在10月20日才得知这一事情，此时离决赛只剩一周时间。因此我们确实准备不足。当然我们也没有预料到我们的决赛成绩会这么靠前，否则我们肯定会更加充分去准备。这确实是我们的失误。</p><p>我们在比赛过程中，一直尝试在使用ELMo，这正是我负责的部分。一开始使用官方TensorFlow的代码，费了九牛二虎之力我才跑通代码，但因为队长使用的是pytorch，而二者在cuda版本上不兼容，因此在初赛我们没有使用ELMo。而在最后几天，我尝试使用哈工大的pytorch训练代码，但因为inference速度实在太慢，我们最终还是弃用了这个方案。而在决赛现场，我们发现也确实是因为速度和资源的原因，大家都没有使用ELMo，除了一组。该组正是凭借了ELMo弯道超车从第7升到了第一，拿走了20万大奖。这也是我们非常遗憾的一个地方，我们在遇到困难时没有尝试解决，而是直接弃用，最终没有取得更好的成绩。</p><p>此次我们的成绩排名第4(三等奖)，是有一定的进步的，但有一点遗憾的是，我们仅差0.18百分点，就能超过第三名拿到5万的奖金了。后面我们分析了一下，还是因为我们对比赛懈怠的态度，其他组都对数据进行了分析并有针对性的改进，而我们并没有做这一步。</p><p>Anyway，第一次组队参加比赛就有收获，增长了见识，从交流中也获得了许多。这个比赛之后，就得好好看paper了。 __(:з」∠)_</p><p><img src="/images/2018-10-30-521540861008_.pic_hd.jpg" width="80%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 有感 </tag>
            
            <tag> 比赛 </tag>
            
            <tag> 莱斯杯 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词13</title>
      <link href="/2018/10/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D13/"/>
      <url>/2018/10/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D13/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣行路难三首"><a href="#1️⃣行路难三首" class="headerlink" title="1️⃣行路难三首"></a>1️⃣行路难三首</h3><p>[唐] 李白<br>【其一】<br>金樽清酒斗十千，玉盘珍羞直万钱。<br><strong>停杯投箸不能食，拔剑四顾心茫然</strong>。<br>欲渡黄河冰塞川，将登太行雪满山。<br>闲来垂钓碧溪上，忽复乘舟梦日边。<br>行路难，行路难，多歧路，今安在？<br><strong>长风破浪会有时，直挂云帆济沧海</strong>！</p><p><strong>注释</strong>：<br>「闲来垂钓碧溪上，忽复乘舟梦日边。」句：暗用典故：姜太公吕尚曾在渭水的磻溪上钓鱼，得遇周文王，助周灭商；伊尹曾梦见自己乘船从日月旁边经过，后被商汤聘请，助商灭夏。这两句表示诗人自己对从政仍有所期待。碧，一作「坐」。</p><hr><h3 id="2️⃣登科后"><a href="#2️⃣登科后" class="headerlink" title="2️⃣登科后"></a>2️⃣登科后</h3><p>[唐] 孟郊<br>昔日龌龊不足夸，今朝放荡思无涯。<br><strong>春风得意马蹄疾，一日看尽长安花</strong>。</p><p><strong>注释</strong>：<br>龌龊（wò chuò）：原意是肮脏，这里指不如意的处境。不足夸：不值得提起。<br>放荡（dàng）：自由自在，不受约束。<br>思无涯：兴致高涨。</p><p><a href="http://m.xichuangzhu.com/work/57add198a633bd0057eefa8a" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57add198a633bd0057eefa8a</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文3</title>
      <link href="/2018/10/29/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%873/"/>
      <url>/2018/10/29/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%873/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-A-Neural-Probabilistic-Language-Model"><a href="#1️⃣-A-Neural-Probabilistic-Language-Model" class="headerlink" title="1️⃣[A Neural Probabilistic Language Model]"></a>1️⃣[A Neural Probabilistic Language Model]</h2><p>第一篇使用神经网络获得词向量的paper。</p><p>通过对language model建模，将词映射到低维表示，在训练过程中同时训练语言模型以及每个词的词向量。</p><p><img src="/images/2018-10-29-15407808716787.jpg" width="50%" height="50%"></p><p>将中心词的前n个拼接起来 $x=(C(w_{t-1},C(w_{t-2}),…,C(w_{t-n+1}))$<br>将$x$送入神经网络中获得$y=b+Wx+Utanh(d+Hx)$，最后做一个softmax即可。</p><hr><h2 id="2️⃣-Adaptive-Computation-Time-for-Recurrent-Neural-Networks"><a href="#2️⃣-Adaptive-Computation-Time-for-Recurrent-Neural-Networks" class="headerlink" title="2️⃣[Adaptive Computation Time for Recurrent Neural Networks]"></a>2️⃣[Adaptive Computation Time for Recurrent Neural Networks]</h2><p>一种允许RNN动态堆叠层数的算法。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>证据证明，RNN的堆叠层数多，效果会有提升。但是，对于不同的任务，要求不同的计算复杂度。我们需要先验来决定特定任务的计算复杂度。当然我们可以粗暴地直接堆叠深层的网络。ACT(Adaptive Computation Time)能够动态决定每个输入t所需的计算次数。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>将RNN每一步的输出过一个网络+sigmoid层，获得一个概率分布，也即什么时候应当停止不再继续往上堆叠，直到概率加和为1。同时为了尽可能抑制层数的无限增长，在loss添加一项惩罚。</p><h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>对于普通的RNN：<br><img src="/images/2018-10-29-15408103221289.jpg" width="30%" height="50%"></p><p>s是隐藏层；y是输出。</p><p>对于ACT的RNN，有：<br><img src="/images/2018-10-29-15408103823681.jpg" width="40%" height="50%"></p><p>上标n是指的t时刻的层数；其中：<br><img src="/images/2018-10-29-15408104201718.jpg" width="20%" height="50%"></p><p>$δ$是flat，指示x是第几次输入。</p><p>引入新的网络，输入时隐状态，输出是一个概率分布：<br><img src="/images/2018-10-29-15408105451770.jpg" width="30%" height="50%"></p><p>那么每一层的概率是：<br><img src="/images/2018-10-29-15408105687677.jpg" width="35%" height="50%"></p><p>其中$R(t)$是在每一层概率求和超过1时的剩余概率（为了保证概率和为1，可以试着举一个例子来证明）<br><img src="/images/2018-10-29-15408106099743.jpg" width="45%" height="50%"></p><p><img src="/images/2018-10-29-15408106125837.jpg" width="25%" height="50%"></p><p>ε是为了解决第一次输出时就超过1-ε的情况，ε一般取很小。</p><p>最终，加权求和，作为最终的结果，传入下一个时间步：<br><img src="/images/2018-10-29-15408106649319.jpg" width="45%" height="50%"></p><p>普通RNN与ACT的RNN对比：<br><img src="/images/2018-10-29-15408106950342.jpg" width="90%" height="50%"></p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>为了防止模型层数无限增长，添加一项惩罚项以抑制。</p><p>记每一步的惩罚项为：<br><img src="/images/2018-10-29-15408107184035.jpg" width="23%" height="50%"></p><p>总的惩罚项则为：<br><img src="/images/2018-10-29-15408107351871.jpg" width="19%" height="50%"></p><p>Loss function则为：<br><img src="/images/2018-10-29-15408108024183.jpg" width="35%" height="50%"></p><p>因为N(t)是不可导的，我们在实际过程中只去最小化R(t)  （<del>我觉得不甚合理</del>，一种解读是如果我们不断最小化R(t)直到变成0，那么相当于N(t)少了一层，接着R(t)就会变得很大，然后又继续最小化R(t)…）</p><hr><h2 id="3️⃣-Universal-Transformers"><a href="#3️⃣-Universal-Transformers" class="headerlink" title="3️⃣[Universal Transformers]"></a>3️⃣[Universal Transformers]</h2><p>提出一种新型通用的transformer。</p><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>Transformer的问题：RNN的归纳偏置(inductive bias)在一些任务上很重要，也即RNN的循环学习的过程；Transformer在一些问题上表现不好，可能是归纳偏置的原因。</p><blockquote><p>Notably, however, the Transformer foregoes the RNN’s inductive bias towards learning iterative or recursive transformations.Our experiments indicate that this inductive bias may be crucial for several algorithmic and language understanding tasks of varying complexity: in contrast to models such as the Neural Turing Machine [13], the Neural GPU [17] or Stack RNNs [16], the Transformer does not generalize well to input lengths not encountered during training.</p></blockquote><p>因此在Transformer内引入归纳偏置</p><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li>每一层的权重是共享的，也即multi-head上的权重以及transition function在每一层是一致的。这一点和RNN、CNN一致。</li><li>动态层数（ACT mechanism ）：对于每个词都会有不同的循环次数；也即有些词需要更多的refine；而有些词不需要。和固定层数的transformer相比，会有更好的通用性。</li></ul><h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><h4 id="总体架构"><a href="#总体架构" class="headerlink" title="总体架构"></a>总体架构</h4><p><img src="/images/2018-10-29-15408125098915.jpg" width="90%" height="50%"></p><p>过程：<br><img src="/images/2018-10-29-15408125469899.jpg" width="45%" height="50%"></p><p><img src="/images/2018-10-29-15408125685038.jpg" width="70%" height="50%"></p><p><img src="/images/2018-10-29-15408125974795.jpg" width="70%" height="50%"></p><p><img src="/images/2018-10-29-15408126143943.jpg" width="60%" height="50%"></p><p>和普通Transformer不同的地方在于：</p><ul><li>加了一层Transition层，Transition可以是depth-wise separable convolution（<a href="https://www.cnblogs.com/adong7639/p/7918527.html" target="_blank" rel="noopener">是什么？</a>）或者全连接层。</li><li>每层都添加了position embedding；以及timestep embedding，用以指示层数。</li></ul><h4 id="ACT"><a href="#ACT" class="headerlink" title="ACT"></a>ACT</h4><p>由于一个句子中间，有些词比其他词更难学会，需要更多计算量，但堆叠太多层会大大增加计算量，为了节省计算量，我们可以引入ACT来动态分配计算量。</p><p>ACT原来用于RNN，在Transformer中，当halting unit指示词t应当停止时，直接讲该词的状态复制到下一个time step，直到所有的词都停止。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Embedding </tag>
            
            <tag> Paper </tag>
            
            <tag> Transformer </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> Language Modeling </tag>
            
            <tag> ACT </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词12</title>
      <link href="/2018/10/21/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D12/"/>
      <url>/2018/10/21/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D12/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣望海潮"><a href="#1️⃣望海潮" class="headerlink" title="1️⃣望海潮"></a>1️⃣望海潮</h3><p>[宋] 柳永<br>东南形胜，三吴都会，钱塘自古繁华。烟柳画桥，风帘翠幕，参差十万人家。云树绕堤沙，怒涛卷霜雪，天堑无涯。市列珠玑，户盈罗绮，竞豪奢。<br>重湖叠巘清嘉，有三秋桂子，十里荷花。羌管弄晴，菱歌泛夜，嬉嬉钓叟莲娃。千骑拥高牙，乘醉听箫鼓，吟赏烟霞。异日图将好景，归去凤池夸。</p><p>叠巘（yǎn）：层层叠叠的山峦。</p><p><a href="http://m.xichuangzhu.com/work/57b318228ac247005f2223db" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b318228ac247005f2223db</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录8</title>
      <link href="/2018/10/21/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%958/"/>
      <url>/2018/10/21/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%958/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-batchify"><a href="#1️⃣-batchify" class="headerlink" title="1️⃣[batchify]"></a>1️⃣[batchify]</h3><p>快速将数据分成batch。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchify</span><span class="params">(data, bsz)</span>:</span></span><br><span class="line">    <span class="comment"># Work out how cleanly we can divide the dataset into bsz parts.</span></span><br><span class="line">    nbatch = data.size(<span class="number">0</span>) // bsz</span><br><span class="line">    <span class="comment"># Trim off any extra elements that wouldn't cleanly fit (remainders).</span></span><br><span class="line">    data = data.narrow(<span class="number">0</span>, <span class="number">0</span>, nbatch * bsz)</span><br><span class="line">    <span class="comment"># Evenly divide the data across the bsz batches.</span></span><br><span class="line">    data = data.view(bsz, <span class="number">-1</span>).t().contiguous()</span><br><span class="line">    <span class="keyword">return</span> data.to(device)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PRML第四章 分类的线性模型</title>
      <link href="/2018/10/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%88%86%E7%B1%BB%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
      <url>/2018/10/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%88%86%E7%B1%BB%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="判别函数"><a href="#判别函数" class="headerlink" title="判别函数"></a>判别函数</h1><p><img src="/images/2018-10-21-Xnip2018-10-21_09-26-42.jpg" alt="0"></p><hr><p><img src="/images/2018-10-21-Xnip2018-10-21_09-27-57.jpg" alt="1"></p><p>—-未完—-</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> PRML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文2</title>
      <link href="/2018/10/20/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%872/"/>
      <url>/2018/10/20/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%872/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-An-Empirical-Evaluation-of-Generic-Convolutional-and-Recurrent-Networks-for-Sequence-Modeling"><a href="#1️⃣-An-Empirical-Evaluation-of-Generic-Convolutional-and-Recurrent-Networks-for-Sequence-Modeling" class="headerlink" title="1️⃣[An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling]"></a>1️⃣[An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling]</h3><p>本文贡献：提出一种新的模型<strong>TCN（Temporal Convolutional Networks）</strong>进行language model建模。</p><h4 id="Dilated-convolution"><a href="#Dilated-convolution" class="headerlink" title="Dilated convolution"></a>Dilated convolution</h4><p>每一层的感受野都可以是不同的，也即，同样的kernel size，高层的可以跳着看。<br><img src="/images/2018-10-20-15400016170606.jpg" width="60%" height="50%"></p><p>每层的d逐渐增大（也即跳的步数），一般按指数增大。（我觉得这样很有道理，如果每一层的d都是一样的，那capture到的信息就会有重复，能看到的视野也不如逐渐增大的多）</p><h4 id="Residual-block"><a href="#Residual-block" class="headerlink" title="Residual block"></a>Residual block</h4><p><img src="/images/2018-10-20-15400017320092.jpg" width="70%" height="50%"></p><p>这边的residual block比较复杂；一个值得主意的细节是，因为感受野的不同，上层的感受野总是比下层的大很多，因此不应该直接将下层的加到上层，而是可以使用一个1*1的convolution对下层的x进行卷积，这就类似scale对输入进行放缩。</p><hr><h3 id="2️⃣-Dissecting-Contextual-Word-Embeddings：-Architecture-and-Representation"><a href="#2️⃣-Dissecting-Contextual-Word-Embeddings：-Architecture-and-Representation" class="headerlink" title="2️⃣[Dissecting Contextual Word Embeddings： Architecture and Representation]"></a>2️⃣[Dissecting Contextual Word Embeddings： Architecture and Representation]</h3><p>一篇分析的文章。ELMo作者的又一篇文章。</p><p>对比三种不同的建模方式（LSTM/GCNN/Transformer）获得的词向量，以及在不同任务上的表现；以及不同层获得的不同信息…获得了不同的结论。</p><p>①biLM 专注于word morphology词的形态；底层的LM关注local syntax；而高层的LM关注semantic content；</p><p>②不同的任务会有不同的正则化s的倾向。</p><hr><h3 id="3️⃣-Transformer-XL-Language-modeling-with-longer-term-dependency"><a href="#3️⃣-Transformer-XL-Language-modeling-with-longer-term-dependency" class="headerlink" title="3️⃣[Transformer-XL: Language modeling with longer-term dependency]"></a>3️⃣[Transformer-XL: Language modeling with longer-term dependency]</h3><p>利用Transformer进行language model，与普通的Transformer建模不同的是，Transformer-XL添加了历史信息，能够显著提升表现。这篇还在ICLR2019审稿中。</p><p>贡献：本文提出了能够进行长程依赖的基于Transformer的语言模型 Transformer-XL；引入相对位置的positional encoding。</p><h4 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h4><p>原先的transformer language model是将句子分为一个一个segment。segment之间是没有联系的。（为什么不直接按原版的Transformer那样所有的词都相互做self-attention？因为考虑到效率问题，句子长度可能会很长）</p><p>训练阶段：<br><img src="/images/2018-10-20-15400023645268.jpg" width="35%" height="50%"></p><p>而在测试阶段，每次向右滑动一格：<br><img src="/images/2018-10-20-15400024115822.jpg" width="80%" height="50%"><br>这样每一个时间步都要重新计算一遍，历史信息没有利用到。显然速度很慢。</p><p>在Transformer引入recurrence，也即引入历史信息。基于这样的想法，提出的新模型Transformer-XL。在结构上同样分为每个segment，但在每个阶段都接收上一个（甚至上L个）历史信息。</p><p>训练阶段：<br><img src="/images/2018-10-20-15400026059113.jpg" width="80%" height="50%"></p><p>而在测试阶段，同样分为segment，但因为接收了历史信息，不需要每次滑动一格也能获得大量信息。<br><img src="/images/2018-10-20-15400027040526.jpg" width="45%" height="50%"></p><p>具体来说：<br><img src="/images/2018-10-20-15400027302545.jpg" width="120%" height="50%"><br>SG代表stop gradient，和该阶段的hidden state进行拼接。</p><h4 id="RELATIVE-POSITIONAL-ENCODINGS"><a href="#RELATIVE-POSITIONAL-ENCODINGS" class="headerlink" title="RELATIVE POSITIONAL ENCODINGS"></a>RELATIVE POSITIONAL ENCODINGS</h4><p>如果我们使用了absolute positional encodings（也即原版的positional encodings）那么会出现这种情况</p><p><img src="/images/2018-10-20-15400027991211.jpg" width="70%" height="50%"></p><p>在同一层之间的前一个segment和后一个segment使用了同样的绝对位置信息，对于当前segment的高层，对于同一个位置i，无法区分该位置信息是来自当前segment的还是上一个segment的（因为都是同样的绝对位置）。</p><p>因此我们引入相对位置信息R，其中第i行代表相对距离i的encoding。</p><p>具体来说：</p><p>首先我们在传统的计算$query_i$和$key_j$的attention分数时，可以拆解成：</p><p><img src="/images/2018-10-20-15400030310583.jpg" width="80%" height="50%"><br>（因为query=(embedding E +positional embedding U），key也一样，将式子拆开就能获得上述式子)</p><p>我们将该式子进行修改：</p><p><img src="/images/2018-10-20-15400031662378.jpg" width="80%" height="50%"></p><p>第一，将出现了absolute positional embedding $U$的地方，统统改成$R_{i-j}$，也即在b和d项。其中这里的R和原版的Transformer的位置计算公式相同。</p><p>第二，在c项中，使用一个$u$替代了$U_i W_q$，这一项原本的意义在于，$query_i$的positional encoding对$key_j$的embedding进行attention，也就是说，该项表现了$query_i$位置对哪些$key_j$的内容有兴趣，作者认为query不管在哪个位置上都是一样的，也就是说query的位置信息应当没影响，所以统统替换成一个可学习的$u$。基于类似的理由d项换成了$v$。</p><p>第三，将$W_k$细分成了两个$W_{k,E}$和$W_{k,R}$。这是根据query是Embedding还是positional encoding来区分的。for producing the content-based key vectors and location-based key vectors respectively</p><p>每一项现在都有了不同的意义：</p><blockquote><p>Under the new parameterization, each term has an intuitive meaning: term (a) represents contentbased addressing, term (b) captures a content-dependent positional bias, term (c) governs a global content bias, and (d) encodes a global positional bias.</p></blockquote><p>最后总结一下整个结构：</p><p><img src="/images/2018-10-20-15400040342520.jpg" width="120%" height="50%"></p><p>与原版Transformer不同的是，Transformer-XL在每一层都添加了位置信息。</p><hr><h3 id="4️⃣-Trellis-Networks-for-Sequence-Modeling"><a href="#4️⃣-Trellis-Networks-for-Sequence-Modeling" class="headerlink" title="4️⃣[Trellis Networks for Sequence Modeling]"></a>4️⃣[Trellis Networks for Sequence Modeling]</h3><p>一种结合RNN和CNN的语言建模方式。</p><p>最小的单元结构：</p><p><img src="/images/2018-10-21-15400858162232.jpg" width="40%" height="50%"></p><p>也即：<br><img src="/images/2018-10-21-15400860605560.jpg" width="40%" height="50%"></p><p>接下来再处理非线性：<br><img src="/images/2018-10-21-15400861618655.jpg" width="30%" height="50%"></p><p>因为每层都要输入x，且W是共享的，所以我们可以提前计算好这一项，后面直接用即可。<br><img src="/images/2018-10-21-15400861870898.jpg" width="35%" height="50%"></p><p>最终在实现的时候是：<br><img src="/images/2018-10-21-15400862184335.jpg" width="40%" height="50%"></p><p><img src="/images/2018-10-21-15400862303741.jpg" width="40%" height="50%"></p><p>总体框架：<br><img src="/images/2018-10-21-15400874987498.jpg" width="70%" height="50%"></p><p>与TCN（temporal convolution network）不同之处：①filter weight不仅在time step之间共享，在不同层之间也共享；②在每一层都添加了输入</p><p>优点：共享了W，显著减少了参数；‘Weight tying can be viewed as a form of regularization that can stabilize training’</p><p>我们还可以扩展该网络，引入gate：<br><img src="/images/2018-10-21-15400875805208.jpg" width="40%" height="50%"></p><hr><h3 id="5️⃣-Towards-Decoding-as-Continuous-Optimisation-in-Neural-Machine-Translation"><a href="#5️⃣-Towards-Decoding-as-Continuous-Optimisation-in-Neural-Machine-Translation" class="headerlink" title="5️⃣[Towards Decoding as Continuous Optimisation in Neural Machine Translation]"></a>5️⃣[Towards Decoding as Continuous Optimisation in Neural Machine Translation]</h3><p>一篇很有意思的paper。用于NMT decode的inference阶段。这篇有一定的难度，以下只是我的理解。</p><h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><p>Motivation：<br>NMT中的decode inference阶段，通常都是从左到右的，这样有个缺点，就是整体的target之间的依赖是没有被充分利用到的，比如说生成的词的右边是没有用到的。那么我们为什么不直接全部生成呢？然后不断更新。也就是说我们将离散（discrete）的decode过程变成一个连续的过程（continuous optimization）。</p><p>假设我们已经训练好模型，给定一个句子，我们要翻译成目标句子，且假设我们已知要生成的句子长度是l，那么我们有：<br><img src="/images/2018-10-21-15400876953609.jpg" width="45%" height="50%"><br>我们要找到一个最优的序列$y$，使得$-log$最小。</p><p>等价于：<br><img src="/images/2018-10-21-15400877226851.jpg" width="55%" height="50%"><br>其中$\widetilde{y}_i$是one-hot。其实这里就是假设有这么一个ground truth，但实际上是没有的。</p><p>我们将$\widetilde{y}_i$是one-hot这个条件放宽一些，变成是一个概率单纯型（其实就是所有元素加起来是1，且都大于等于0）。</p><p>那么就变成了：<br><img src="/images/2018-10-21-15400879019592.jpg" width="50%" height="50%"></p><p>这个改变的本质是：<br><img src="/images/2018-10-21-15400879379023.jpg" width="50%" height="50%"></p><p>就是说原来one-hot的$\widetilde{y}_i$生成后丢到下一个时间步，取了一个词向量，接着计算。现在是一个概率分布$\hat{y}_i$丢进来，就相当于取了多个词向量的加权求和。</p><p>在利用下述的更新算法更新完$\hat{y}_i$之后，对于每个时间步t，我们找$\hat{y}_i$中元素最大的值对应的词作为生成的词。</p><p>有两种方法Exponentiated Gradient 和 SGD。实际上方法倒在其次了，主要是前面所述的continuous optimization这种思想。</p><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><h5 id="Exponentiated-Gradient"><a href="#Exponentiated-Gradient" class="headerlink" title="Exponentiated Gradient"></a>Exponentiated Gradient</h5><p><img src="/images/2018-10-21-15400881918713.jpg" width="80%" height="50%"><br>具体见论文</p><h5 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h5><p>因为我们要保证单纯形的约束不变，因此我们引入一个r，然后做一个softmax<br><img src="/images/2018-10-21-15400882306948.jpg" width="80%" height="50%"></p><h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>这种连续decode可以用在哪？</p><h5 id="Bidirectional-Ensemble"><a href="#Bidirectional-Ensemble" class="headerlink" title="Bidirectional Ensemble"></a>Bidirectional Ensemble</h5><p>可以很方便地进行双向的生成：</p><p><img src="/images/2018-10-21-15400883321474.jpg" width="45%" height="50%"><br>而在传统的方法中没办法（很难）做到</p><h5 id="Bilingual-Ensemble"><a href="#Bilingual-Ensemble" class="headerlink" title="Bilingual Ensemble"></a>Bilingual Ensemble</h5><p>我们希望源语言到目标语言和目标到源语言都生成得好</p><p><img src="/images/2018-10-21-15400883583228.jpg" width="50%" height="50%"></p><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>$\hat{y}_i$的初始化很重要，一不小心就会陷入local minima；生成的速度慢</p><hr><h3 id="6️⃣-Universal-Language-Model-Fine-tuning-for-Text-Classiﬁcation"><a href="#6️⃣-Universal-Language-Model-Fine-tuning-for-Text-Classiﬁcation" class="headerlink" title="6️⃣[Universal Language Model Fine-tuning for Text Classiﬁcation]"></a>6️⃣[Universal Language Model Fine-tuning for Text Classiﬁcation]</h3><p>和ELMo、OpenAI GPT一样，都是预训练语言模型，迁移到其他任务上（这里是分类任务）。可以在非常小的数据集上有很好的效果。</p><p>贡献：</p><ol><li>迁移学习模型ULMFiT</li><li>提出几种trick：discriminative ﬁne-tuning, slanted triangular learning rates,gradual unfreezing ，最大保证知识的保留。</li></ol><h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p><img src="/images/2018-10-21-15401043145373.jpg" width="90%" height="50%"></p><p>三部曲：</p><ol><li>通用语言模型预训练</li><li>目标任务的语言模型fine-tuning</li><li>目标任务的分类fine-tuning</li></ol><h4 id="trick"><a href="#trick" class="headerlink" title="trick"></a>trick</h4><h5 id="Discriminative-ﬁne-tuning"><a href="#Discriminative-ﬁne-tuning" class="headerlink" title="Discriminative ﬁne-tuning"></a>Discriminative ﬁne-tuning</h5><p>Motivation：不同层有不同的信息；应当fine-tune 不同程度，也即使用不同的learning rate。</p><p><img src="/images/2018-10-21-15401044160803.jpg" width="35%" height="50%"></p><p>作者发现上一层的学习率是下一层的2.6倍时效果比较好。</p><h5 id="Slanted-triangular-learning-rates-STLR"><a href="#Slanted-triangular-learning-rates-STLR" class="headerlink" title="Slanted triangular learning rates (STLR)"></a>Slanted triangular learning rates (STLR)</h5><p><img src="/images/2018-10-21-15401045153164.jpg" width="60%" height="50%"></p><p>具体公式：<br><img src="/images/2018-10-21-15401045316305.jpg" width="50%" height="50%"></p><h5 id="Gradual-unfreezing"><a href="#Gradual-unfreezing" class="headerlink" title="Gradual unfreezing"></a>Gradual unfreezing</h5><p>从顶层到底层，一步一步unfreeze，也即从上到下fine-tune。这是因为最上一层有最少的general knowledge。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> TCN </tag>
            
            <tag> Transformer-XL </tag>
            
            <tag> Trellis Networks </tag>
            
            <tag> continuous decoding </tag>
            
            <tag> ULMFiT </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文1</title>
      <link href="/2018/10/14/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%871/"/>
      <url>/2018/10/14/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%871/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Learned-in-Translation-Contextualized-Word-Vectors"><a href="#1️⃣-Learned-in-Translation-Contextualized-Word-Vectors" class="headerlink" title="1️⃣[Learned in Translation: Contextualized Word Vectors]"></a>1️⃣[Learned in Translation: Contextualized Word Vectors]</h3><p>CoVe是第一个引入动态词向量的模型。<br>Motivation：翻译模型能够保存最多的信息，因为如果保存信息不够多，decoder接收到的信息不足，翻译效果就不会好。（但实际上，我个人认为，decoder的表现还和language model有关，如果decoder是一个好的language model，也有可能翻译出不错的结果）</p><p>做法：使用传统NMT的encoder-decoder的做法翻译模型，只是将(bi)LSTM所得到的隐层状态表示取出来和embedding拼接起来，作为一个词的表示：</p><script type="math/tex; mode=display">w=[GloVe(w); CoVe(w)]</script><hr><h3 id="2️⃣-Language-Modeling-with-Gated-Convolutional-Networks"><a href="#2️⃣-Language-Modeling-with-Gated-Convolutional-Networks" class="headerlink" title="2️⃣[Language Modeling with Gated Convolutional Networks]"></a>2️⃣[Language Modeling with Gated Convolutional Networks]</h3><p>使用CNN对语言模型进行建模，提高并行性。</p><p>贡献：使用了CNN进行language model建模；提出了简化版的gate机制应用在CNN中。</p><p>做法：<br><img src="/images/2018-10-14-15394870930257.jpg" width="50%" height="50%"></p><p>实际上就是一个输入两个filter，卷积出来的做一个gate的操作$H_0 = A⊗σ(B)$，控制流向下一层的数据。</p><p>一个小细节是，为了不让language model看到下一个词，每一层在开始卷积的时候会在左边添加kernel_size-1个padding。</p><p>扩展：因为CNN的并行性高，可以使用CNN来对language model建模替代ELMo，同样可以获得动态词向量。这个想法已经由提出ELMo的团队做出来并进行对比了。论文：Dissecting Contextual Word Embeddings: Architecture and Representation</p><p>目前正在<a href="https://github.com/linzehui/Gated-Convolutional-Networks" target="_blank" rel="noopener">复现</a>该论文 。</p><hr><h3 id="3️⃣-Attention-is-All-you-need"><a href="#3️⃣-Attention-is-All-you-need" class="headerlink" title="3️⃣[Attention is All you need]"></a>3️⃣[Attention is All you need]</h3><p>非常经典的论文。提出了Transformer。为了读BERT重温了一遍。<br><img src="/images/2018-10-14-15394876881322.jpg" width="70%" height="50%"></p><p><img src="/images/2018-10-14-15394877200390.jpg" width="70%" height="50%"></p><p><img src="/images/2018-10-14-15394877478814.jpg" width="70%" height="50%"></p><hr><h3 id="4️⃣-Improving-Language-Understanding-by-Generative-Pre-Training"><a href="#4️⃣-Improving-Language-Understanding-by-Generative-Pre-Training" class="headerlink" title="4️⃣[Improving Language Understanding by Generative Pre-Training]"></a>4️⃣[Improving Language Understanding by Generative Pre-Training]</h3><p>BERT就是follow这篇文章的工作。<br>使用Transformer预训练一个language model进行迁移学习。</p><p>训练过程分为两步：①使用未标记数据训练language model；②使用有标记数据进行fine-tune</p><p>Motivation：ELMo是训练好language model，然后获得动态词向量再用到其他任务上，这样就会多了很多参数。和ELMo不同的是，这里使用一个Transformer模型解决多种任务（利用迁移学习）。</p><p>贡献：使用Transformer进行language model建模；尝试利用language model进行迁移学习而不是另一种思路（ELMo）只提取词向量。</p><p>①无监督学习language model<br><img src="/images/2018-10-14-15395044176746.jpg" width="40%" height="50%"></p><p>具体到Transformer就是：<br><img src="/images/2018-10-14-15395044608239.jpg" width="50%" height="50%"></p><p>②监督学习（fine-tune）<br>根据输入预测标签<br><img src="/images/2018-10-14-15395045508824.jpg" width="35%" height="50%"></p><p>具体就是：<br><img src="/images/2018-10-14-15395045734859.jpg" width="40%" height="50%"></p><p>将两个任务一起训练，则有：<br><img src="/images/2018-10-14-15395045932795.jpg" width="30%" height="50%"></p><p>对于不同任务，对输入进行一定的改动以适应Transformer结构：<br><img src="/images/2018-10-14-15395046364928.jpg" width="90%" height="50%"></p><hr><h3 id="5️⃣-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding"><a href="#5️⃣-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding" class="headerlink" title="5️⃣[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]"></a>5️⃣[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]</h3><p>刷爆各榜单的一篇神文。使用Transformer预训练一个language model进行迁移学习。</p><p>Motivation：之前的language model只能根据前面的词来预测下一个（即使ELMo是双向的LSTM，也是分别训练一个前向和一个后向的），限制了双向的context；因此提出了双向的language model。</p><h4 id="做法："><a href="#做法：" class="headerlink" title="做法："></a>做法：</h4><p>模型分为两个部分：<br>①masked LM：因为使用了两边的context，而language model的目的是预测下一个词，这样模型会提前看到下一个词，为了解决该问题，训练的时候讲部分词mask掉，最终只预测被mask掉的词。</p><p>②Next Sentence Prediction：随机50%生成两个句子是有上下句关系的，50%两个句子是没有关系的，然后做分类；具体来说是拿第一个词[CLS]（这是手动添加的）的表示，过一个softmax层得到。<br><img src="/images/2018-10-14-15394891973653.jpg" width="50%" height="50%"></p><p>联合训练这两个任务。</p><p>接下来是通过具体的任务进行fine-tune。一个模型解决多种问题：<br><img src="/images/2018-10-14-15395038593955.jpg" width="80%" height="50%"></p><p>本文贡献：使用Transformer进行双向的language model建模。论文提到的一些细节/tricks非常值得讨论，比如对token embedding添加了许多信息，非常简单粗暴。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Transformer </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> CoVe </tag>
            
            <tag> GCNN </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 18:Deep Reinforcement Learning</title>
      <link href="/2018/10/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2018:%20Deep%20Reinforcement%20Learning/"/>
      <url>/2018/10/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2018:%20Deep%20Reinforcement%20Learning/</url>
      
        <content type="html"><![CDATA[<p>记号： $a$是action，$s$即外部状态state，$\pi_{\theta}(s)$也即从$s$映射到$a$的函数；$r$是reward，每采取一个动作，会有一个reward，则总的reward为</p><script type="math/tex; mode=display">R_\theta = \sum_{t=1}^{T} r_t</script><p>我们使用神经网络来拟合$\pi$，一个eposide $\tau$是一个流程下来的的所有state、action和reward的集合。</p><script type="math/tex; mode=display">\tau = \{s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T \}</script><p>如果我们使用相同的actor运行n次，则每个$\tau$会有一定的概率被采样到，采样概率记为$P(\tau|\theta)$，则我们可以通过采样的方式来对期望reward进行估计：</p><script type="math/tex; mode=display">\overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta) ≈ \frac{1}{N} \sum_{n=1}^{N} R(\tau^n)</script><p>那么我们接下来的<strong>目标</strong>就是最大化期望reward，其中期望reward是：</p><script type="math/tex; mode=display">\overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta)</script><p>我们同样使用梯度上升：其中与$θ$相关的是$P$，则可以写成：</p><script type="math/tex; mode=display">\nabla \overline{R}_\theta = \sum_\tau R(\tau) \nabla P(\tau|\theta)= \sum_\tau R(\tau) P(\tau|\theta) \frac{\nabla P(\tau|\theta)}{P(\tau|\theta)}</script><p>由于$\dfrac {d\log \left( f\left( x\right) \right) }{dx}=\dfrac {1}{f\left( x\right) }\dfrac {df(x)}{dx}$，则前式可写成：</p><script type="math/tex; mode=display">\nabla \overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta) \nabla log P(\tau | \theta) ≈ \frac{1}{N} \sum_{n=1}^{N} R(\tau^n) log P(\tau ^n| \theta)</script><p>如何求梯度？<br>由于：</p><script type="math/tex; mode=display">P(\tau | \theta)=p(s_1)p(a_1|s_1,\theta)p(r_1,s_2|s_1,a_1)p(a_2|s_2,\theta)p(r_2,s_3|s_2,a_2)...\\=p(s_1)\prod_{t=1}^{T}p(a_t|s_t,\theta)p(r_t , s_{t+1}| s_t,a_t)</script><p>实际上，其中与梯度相关的只有中间项$p(a_t|s_t,\theta)$，该项也即$π$函数，从state到action的映射。<br>取log并求导，有：</p><script type="math/tex; mode=display">\nabla log P(\tau | \theta)= \sum_{t=1}^{T} \nabla log p(a_t|s_t,\theta)</script><p>代回，因此最终$\overline{R}_\theta$的梯度为：</p><script type="math/tex; mode=display">\nabla \overline{R}_\theta = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^n) \nabla log p(a_{t}^n | s_t^n,\theta)</script><p>注意到该式子告诉我们，应考虑整体的reward而不应该只考虑每一步的reward；并且取log的原因可以理解成是对action取归一化，因为：</p><script type="math/tex; mode=display">\frac{\nabla p(a_t^n | s_t^n,\theta)}{p(a_t^n | s_t^n,\theta)}</script><p>也就是说对于那些出现次数较多的action，要衡量他们对reward的真正影响，应当对他们归一化。</p><p>为了让那些出现可能性较低的action不会因为没被sample到而在更新后被降低他们的概率，可以添加一个baseline，只有超过$b$的reward才会增加他们出现的概率。</p><script type="math/tex; mode=display">\nabla \overline{R}_\theta = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} (R(\tau^n)-b) \nabla log p(a_{t}^n | s_t^n,\theta)</script>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Deep Reinforcement Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 17:Ensemble</title>
      <link href="/2018/10/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2017:%20Ensemble/"/>
      <url>/2018/10/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2017:%20Ensemble/</url>
      
        <content type="html"><![CDATA[<h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>对于复杂模型，往往variance会大，通过对多个模型的平均，能够减小variance：<br><img src="/images/2018-10-14-15394832736339.jpg" width="50%" height="50%"></p><p>bagging的思想是多次有放回地采样N’个点（通常N’=N），然后对采样的几个数据集分别训练一个模型<br><img src="/images/2018-10-14-15394833007835.jpg" width="50%" height="50%"></p><p>测试的时候再对几个模型进行平均或投票<br><img src="/images/2018-10-14-15394833255306.jpg" width="50%" height="50%"></p><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>基本思想是对几个弱分类器线性加权，得到强分类器。分类器按先后顺序训练，每次训练完，对新模型分类错误的数据进行调高权重，而正确的数据则降低权重。</p><p>可以保证：只要分类器的错误率小于50%，在boosting后能够有100%的正确率（在训练集）。</p><p>证明过程略。</p><h2 id="Ensemble-Stacking"><a href="#Ensemble-Stacking" class="headerlink" title="Ensemble: Stacking"></a>Ensemble: Stacking</h2><p>基本思想：使用训练数据训练多个初级分类器，将初级分类器的输出作为次级分类器的输入，获得最终的输出。我们应当使用不同的训练数据来训练次级分类器<br><img src="/images/2018-10-14-15394833971028.jpg" width="50%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Ensemble </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>浅谈mask矩阵</title>
      <link href="/2018/10/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%B5%85%E8%B0%88mask%E7%9F%A9%E9%98%B5/"/>
      <url>/2018/10/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%B5%85%E8%B0%88mask%E7%9F%A9%E9%98%B5/</url>
      
        <content type="html"><![CDATA[<p>个人目前对mask矩阵的一点理解。</p><hr><h2 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h2><p>mask矩阵是什么？是一个由0和1组成的矩阵。一个例子是，在自然语言处理(NLP)中，句子的长度是不等长的，但因为我们经常将句子组成mini-batch用以训练，因此那些长度较短的句子都会在句尾进行填充0，也即padding的操作。一个mask矩阵即用以指示哪些是真正的数据，哪些是padding。如：<br><img src="/images/2018-10-12-15393574958961.jpg" width="50%" height="50%"><br>图片来源：<a href="https://www.cnblogs.com/neopenx/p/4806006.html" target="_blank" rel="noopener">Theano：LSTM源码解析</a></p><p>其中mask矩阵中1代表真实数据；0代表padding数据。</p><h2 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h2><p>为什么要使用mask矩阵？使用mask矩阵是为了让那些被mask掉的tensor不会被更新。考虑一个tensor T的size(a,b)，同样大小的mask矩阵M，相乘后，在反向回传的时候在T对应mask为0的地方，0的梯度仍为0。因此不会被更新。</p><h2 id="怎么做"><a href="#怎么做" class="headerlink" title="怎么做"></a>怎么做</h2><p>接下来介绍几种（可能不全）使用mask的场景。</p><h3 id="对输入进行mask"><a href="#对输入进行mask" class="headerlink" title="对输入进行mask"></a>对输入进行mask</h3><p>考虑NLP中常见的句子不等长的情况。设我们的输入的batch I:(batch_size,max_seqlen)，我们在过一层Embedding层之前，<br>在过了一层Embedding层，则有 E:(batch_size,max_seqlen,embed_dim)，如果我们希望Embedding是更新的(比如我们的Embedding是随机初始化的，那当然Embedding需要更新)，但我们又不希望padding更新。<br>一种方法即令E与M相乘。其中M是mask矩阵(batch_size,max_seqlen,1) (1是因为要broadcast），这样在Embedding更新梯度时，因为mask矩阵的关系，padding位置上的梯度就是0。<br>当然在Pytorch中还可以直接显式地写：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.embedding = nn.Embedding(vocab_size, embed_dim,padding_idx=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><p>而此时应当将padding显式添加到词典的第一个。</p><h3 id="对模型中间进行mask"><a href="#对模型中间进行mask" class="headerlink" title="对模型中间进行mask"></a>对模型中间进行mask</h3><p>一个很经典的场景就是dropout。<br>对于参数矩阵W:(h,w)，同样大小的mask矩阵M，在前向传播时令W’=W*M，则在反向传播时，M中为0的部分不被更新。<br>当然，我们可以直接调用PyTorch中的包<code>nn.Dropout()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Dropout(p=<span class="number">0.2</span>)</span><br><span class="line">input = torch.randn(<span class="number">20</span>, <span class="number">16</span>)</span><br><span class="line">output = m(input)</span><br></pre></td></tr></table></figure><h3 id="对loss进行mask"><a href="#对loss进行mask" class="headerlink" title="对loss进行mask"></a>对loss进行mask</h3><p>考虑NLP中的language model，每个词都需要预测下一个词，在一个batch中句子总是有长有短，对于一个短句，此时在计算loss的时候，会出现这样的场景：<code>&lt;pad&gt;</code>词要预测下一个<code>&lt;pad&gt;</code>词。举个例子：三个句子[a,b,c,d],[e,f,g],[h,i]，在组成batch后，会变成<br>X：</p><div class="table-container"><table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>a</td><td>b</td><td>c</td><td>d</td></tr><tr><td>e</td><td>f</td><td>g</td><td><code>&lt;pad&gt;</code></td></tr><tr><td>h</td><td>i</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr></tbody></table></div><p>Y：</p><div class="table-container"><table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>b</td><td>c</td><td>d</td><td><code>&lt;pad&gt;</code></td></tr><tr><td>f</td><td>g</td><td><code>&lt;eos&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr><tr><td>i</td><td><code>&lt;eos&gt;</code></td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr></tbody></table></div><p>X是输入，Y是预测。那么从第三行可以看出，<code>&lt;pad&gt;</code>在预测下一个<code>&lt;pad&gt;</code>。这显然是有问题的。<br>一种解决方案就是使用mask矩阵，在loss的计算时，将那些本不应该计算的mask掉，使得其loss为0，这样就不会反向回传了。<br>具体实践：在PyTorch中，以CrossEntropy为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">nn</span>.<span class="title">CrossEntropyLoss</span><span class="params">(weight=None, size_average=None, ignore_index=<span class="number">-100</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">reduce=None, reduction=’elementwise_mean’</span></span></span><br></pre></td></tr></table></figure><p>如果<code>reduction=None</code>则会返回一个与输入同样大小的矩阵。在与mask矩阵相乘后，再对新矩阵进行mean操作。<br>在PyTorch实践上还可以可以这么写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">masked_outputs = torch.masked_select(dec_outputs, mask)</span><br><span class="line">masked_targets = torch.masked_select(targets, mask)</span><br><span class="line">loss = my_criterion(masked_outputs, masked_targets)</span><br></pre></td></tr></table></figure><p>另一种更为简单的解决方案是，直接在CrossEntropy中设<code>ignore_index=0</code>，这样，在计算loss的时候，发现target=0时，会自动不对其进行loss的计算。其本质和mask矩阵是一致的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>mask矩阵可以用在任何地方，只要希望与之相乘的tensor相对应的地方不更新就可以进行mask操作。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 代码实践 </tag>
            
            <tag> mask矩阵 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>深度炼丹tricks合集</title>
      <link href="/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%B7%B1%E5%BA%A6%E7%82%BC%E4%B8%B9tricks%E5%90%88%E9%9B%86/"/>
      <url>/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%B7%B1%E5%BA%A6%E7%82%BC%E4%B8%B9tricks%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p>长期更新。受知乎深度炼丹的启发，以及个人在实践过程和阅读中会接触到一些tricks，认为有必要做一个合集，将一些可能有用的tricks做记录。有实践过的会特别标注。</p><h2 id="调参技巧"><a href="#调参技巧" class="headerlink" title="调参技巧"></a>调参技巧</h2><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><h4 id="1️⃣zero-center"><a href="#1️⃣zero-center" class="headerlink" title="1️⃣zero-center"></a>1️⃣zero-center</h4><p>[9]将数据中心化</p><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><h4 id="1️⃣Xavier-initialization-7-方法"><a href="#1️⃣Xavier-initialization-7-方法" class="headerlink" title="1️⃣Xavier initialization[7]方法"></a>1️⃣Xavier initialization[7]方法</h4><p>适用[9]于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)</p><h4 id="2️⃣He-initialization-8-方法"><a href="#2️⃣He-initialization-8-方法" class="headerlink" title="2️⃣He initialization[8]方法"></a>2️⃣He initialization[8]方法</h4><p>适用[9]于ReLU：scale = np.sqrt(6/n)</p><h4 id="3️⃣Batch-normalization-10"><a href="#3️⃣Batch-normalization-10" class="headerlink" title="3️⃣Batch normalization[10]"></a>3️⃣Batch normalization[10]</h4><h4 id="4️⃣RNN-LSTM-init-hidden-state"><a href="#4️⃣RNN-LSTM-init-hidden-state" class="headerlink" title="4️⃣RNN/LSTM init hidden state"></a>4️⃣RNN/LSTM init hidden state</h4><p>Hinton[3]提到将RNN/LSTM的初始hidden state设置为可学习的weight</p><h3 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h3><h4 id="1️⃣Gradient-Clipping-5-6"><a href="#1️⃣Gradient-Clipping-5-6" class="headerlink" title="1️⃣Gradient Clipping[5,6]"></a>1️⃣Gradient Clipping[5,6]</h4><h4 id="2️⃣learning-rate"><a href="#2️⃣learning-rate" class="headerlink" title="2️⃣learning rate"></a>2️⃣learning rate</h4><p>原则：当validation loss开始上升时，减少学习率。<br>[1]Time/Drop-based/Cyclical Learning Rate</p><h4 id="3️⃣batch-size"><a href="#3️⃣batch-size" class="headerlink" title="3️⃣batch size"></a>3️⃣batch size</h4><p>[2]中详细论述了增加batch size而不是减小learning rate能够提升模型表现。保持学习率不变，提高batch size，直到batch size~训练集/10，接下来再采用学习率下降的策略。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]<a href="https://blog.goodaudience.com/how-to-make-your-model-happy-again-part-1-40d94a9ffb41" target="_blank" rel="noopener">How to make your model happy again — part 1</a></p><p>[2]<a href="https://arxiv.org/abs/1711.00489" target="_blank" rel="noopener">Don’t Decay the Learning Rate, Increase the Batch Size</a></p><p>[3]<a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf" target="_blank" rel="noopener">CSC2535 2013: Advanced Machine Learning Lecture 10 Recurrent neural networks</a></p><p>[4]<a href="https://zhuanlan.zhihu.com/p/25110150" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25110150</a></p><p>[5]<a href="https://arxiv.org/abs/1211.5063" target="_blank" rel="noopener">On the difficulty of training Recurrent Neural Networks</a></p><p>[6]<a href="https://arxiv.org/abs/1612.08083" target="_blank" rel="noopener">Language Modeling with Gated Convolutional Networks</a></p><p>[7]<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Understanding the difficulty of training deep feedforward neural networks</a></p><p>[8]<a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></p><p>[9]<a href="https://www.zhihu.com/question/41631631" target="_blank" rel="noopener">知乎：你有哪些deep learning（rnn、cnn）调参的经验？</a></p><p>[10]<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 调参 </tag>
            
            <tag> tricks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词11</title>
      <link href="/2018/10/07/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D11/"/>
      <url>/2018/10/07/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D11/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣赋得古原草送别"><a href="#1️⃣赋得古原草送别" class="headerlink" title="1️⃣赋得古原草送别"></a>1️⃣赋得古原草送别</h3><p>[唐] 白居易<br>离离原上草，一岁一枯荣。<br><strong>野火烧不尽，春风吹又生</strong>。<br>远芳侵古道，晴翠接荒城。<br>又送王孙去，萋萋满别情。</p><p>萋萋（qī）：形容草木长得茂盛的样子。</p><p><a href="http://m.xichuangzhu.com/work/57b8a5371532bc005b99da51" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8a5371532bc005b99da51</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PRML第三章 回归的线性模型</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%9B%9E%E5%BD%92%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%9B%9E%E5%BD%92%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="线性基函数模型"><a href="#线性基函数模型" class="headerlink" title="线性基函数模型"></a>线性基函数模型</h1><p><img src="/images/2018-10-07-Xnip2018-10-07_15-51-50.jpg" alt="0"></p><p><img src="/images/2018-10-07-Xnip2018-10-07_15-53-54.jpg" alt="1"></p><h1 id="偏置-⽅差分解"><a href="#偏置-⽅差分解" class="headerlink" title="偏置-⽅差分解"></a>偏置-⽅差分解</h1><p><img src="/images/2018-10-07-Xnip2018-10-07_15-56-30.jpg" alt="0"></p><h1 id="贝叶斯线性回归"><a href="#贝叶斯线性回归" class="headerlink" title="贝叶斯线性回归"></a>贝叶斯线性回归</h1><p><img src="/images/2018-10-07-Xnip2018-10-07_16-13-57.jpg" alt="1"></p><h1 id="贝叶斯模型⽐较"><a href="#贝叶斯模型⽐较" class="headerlink" title="贝叶斯模型⽐较"></a>贝叶斯模型⽐较</h1><p><img src="/images/2018-10-07-Xnip2018-10-07_23-30-33.jpg" alt="1"></p><h1 id="证据近似"><a href="#证据近似" class="headerlink" title="证据近似"></a>证据近似</h1><p><img src="/images/2018-10-09-Xnip2018-10-09_22-06-22.jpg" alt="1"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> PRML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 16:SVM</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2016:%20SVM/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2016:%20SVM/</url>
      
        <content type="html"><![CDATA[<p><strong>Hinge Loss+kernel method = SVM</strong></p><h3 id="Hinge-Loss"><a href="#Hinge-Loss" class="headerlink" title="Hinge Loss"></a>Hinge Loss</h3><p>SVM与logistic regression的区别即在于loss function的不同，logistic是cross entropy，而SVM是hinge loss<br><img src="/images/2018-10-07-15388878731499.jpg" width="50%" height="50%"></p><p>也即如果分类间隔大于1，则 $L(m_i)=max(0,1−m_i(w))$，则损失为0。因此SVM更具鲁棒性，因为对离群点不敏感。</p><p>对于linear SVM：</p><ul><li>定义函数 $f(x)=\sum_i w_i x_i +b=w^T x$</li><li>定义损失函数  $L(f)=\sum_n l(f(x^n),\hat{y}^n)+\lambda ||w||_2$，其中$l(f(x^n),\hat{y}^n)=max(0,1-\hat{y}^n f(x))$</li><li><p>梯度下降求解（省略了正则化）</p><script type="math/tex; mode=display">\frac{\partial{l(f(x^n),\hat{y}^n})}{\partial{w_i}}=  \frac{\partial{l(f(x^n),\hat{y}^n})}{\partial{f(x^n)}}  \frac{\partial{f(x^n)}}{\partial{w_i}} x_i^n</script><p>  而</p><script type="math/tex; mode=display">f(x^n)=w^T \cdot x^n</script></li></ul><script type="math/tex; mode=display">\frac{\partial{max(0,1-\hat{y}^n f(x^n)})}{\partial{f(x^n)}}=\left\{               \begin{array}{**lr**}                -\hat{y}^n & if  \hat{y}^n f(x^n)<1 \\                 0  & otherwise &                 \end{array}  \right.</script><p>因此最终有：<br><img src="/images/2018-10-07-15388891611785.jpg" width="55%" height="50%"><br>我们接下来用$c^n(w)$替代$-\delta(\hat{y}^n f(x^n)&lt;1) \hat{y}^n$</p><h3 id="Kernel-Method"><a href="#Kernel-Method" class="headerlink" title="Kernel Method"></a>Kernel Method</h3><p>一个事实：$w$是$x$的线性加和，其中$α$不等于0对应的$x$就是support vectors</p><p>证明：<br>我们前面说过，更新过程：<br><img src="/images/2018-10-07-15388894194698.jpg" width="30%" height="50%"></p><p>将其组织成向量形式：<br><img src="/images/2018-10-07-15388894627632.jpg" width="25%" height="50%"></p><p><strong>如果我们将$w$初始化成0向量</strong>，那么$w$最终就是$x$的线性组合。证毕</p><p>因为$c(w)$是hinge loss，因此大多数的值是0，会造成$α$稀疏。<br>如果我们将训练数据$x$组织成一个矩阵，那么有：<br><img src="/images/2018-10-07-15388895570090.jpg" width="25%" height="50%"><br>也即：<br><img src="/images/2018-10-07-15388895870336.jpg" width="40%" height="50%"></p><p>所以对于$f(x)$，有：<br><img src="/images/2018-10-07-15388896378645.jpg" width="50%" height="50%"></p><p>实际上$X^Tx$就是每个训练数据和$x$进行点积的结果，但实际上线性函数往往表达能力不强，我们希望$x$能够变成非线性的。如果我们引入kernel，将点积换成kernel，则会有：</p><script type="math/tex; mode=display">f(x)=\sum_n \alpha_n (x_n\cdot x)=\sum_n \alpha_n K(x_n,x)</script><p>所以我们的问题就变成了：</p><ul><li>定义函数 $f(x)=\sum_n \alpha_n K(x_n,x)$</li><li>找到最佳的α，最小化loss function：$L(f)=\sum_n l(f(x^n),\hat{y}^n)=\sum_n l(\sum_{n’} \alpha_{n’} K(x^{n^{‘}},x^n),\hat{y}^n)$</li></ul><p>实际上我们不需要真的知道$x$的非线性的具体形式，我们只需要会算$K$就行，这种绕过$x$的具体形式的方法就是<strong>kernel trick</strong>。直接计算$K$，比先将$x$非线性转化再做点积来得高效。甚至有时候，我们对$x$做的非线性是无穷多维的，是无法直接做非线性化的。比如RBF核:</p><script type="math/tex; mode=display">K(x,z)=exp(-\frac{1}{2}||x-z||_2)</script><p>通过泰勒展开可以知道，RBF核是无穷维的。</p><p>另一个kernel的例子是sigmoid kernel：</p><script type="math/tex; mode=display">K(x,z)=tanh(x\cdot z)</script><p>当我们使用sigmoid kernel时，就相当于一层hidden layer的神经网络，如图：<br><img src="/images/2018-10-07-15388901736757.jpg" width="40%" height="50%"></p><p>给定一个输入，共有n个neuron，其中的weight就是每个训练数据的向量值，然后再将这些neuron加和得到输出。当然大部分的α的值是0，因此实质上神经元的个数和support vector的个数一致。</p><p>我们可以直接设计kernel，而不需要考虑x的非线性变换的形式，只要kernel符合mercer’s theory即可。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 15:Transfer Learning</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2015:%20Transfer%20Learning/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2015:%20Transfer%20Learning/</url>
      
        <content type="html"><![CDATA[<h3 id="Model-Fine-tuning"><a href="#Model-Fine-tuning" class="headerlink" title="Model Fine-tuning"></a>Model Fine-tuning</h3><p>假设我们有很多的source data $(x^s,y^s )$，与任务相关的target data $(x^t,y^t )$  很少。<br>我们利用source data训练一个模型，然后用target data来fine tune模型。</p><h4 id="conservative-training"><a href="#conservative-training" class="headerlink" title="conservative training"></a>conservative training</h4><p><img src="/images/2018-10-07-15388871902739.jpg" width="50%" height="50%"></p><p>我们可以用source data训练好的模型的weight作为新的模型的weight，然后设定一些限制，比如source data作为输入的output应和target data作为输入的output尽量相似，或者参数尽量相似等。</p><h4 id="layer-transfer"><a href="#layer-transfer" class="headerlink" title="layer transfer"></a>layer transfer</h4><p>也就是新模型有几层是直接copy旧模型的，只训练其它层。注意到不同任务所应copy的层是不同的，语音任务最后几层效果好，图像识别前面几层效果好</p><h3 id="Multitask-Learning"><a href="#Multitask-Learning" class="headerlink" title="Multitask Learning"></a>Multitask Learning</h3><p>不同任务之间共享相同的中间层，如：<br><img src="/images/2018-10-07-15388872452545.jpg" width="30%" height="50%"><br><img src="/images/2018-10-07-15388872627707.jpg" width="30%" height="50%"></p><p>还有一种progressive neural networks：<br><img src="/images/2018-10-07-15388872920224.jpg" width="50%" height="50%"><br>首先训练好第一个任务的模型，然后在训练第二个模型的时候将第一个模型的隐层加入到第二个模型的隐层中；训练第三个模型则将第二个和第一个模型的隐层加入到第三个模型的隐层中，以此类推</p><h3 id="Domain-adversarial-training"><a href="#Domain-adversarial-training" class="headerlink" title="Domain-adversarial training"></a>Domain-adversarial training</h3><p>source data是有标签的，而target data是无标签的，都属于同一个任务，但数据是mismatch的，如：<br><img src="/images/2018-10-07-15388873325090.jpg" width="50%" height="50%"></p><p>因为NN的隐层可以理解成是在抽取图像的特征，我们希望能够在训练NN的过程中去掉source data的一些domain specific的特性，这样就可以用在target data上了。因此我们在feature exactor后面连接两个模块：<br><img src="/images/2018-10-07-15388873772888.jpg" width="50%" height="50%"></p><p>一方面我们希望抽取的特征能够使得分类器正确地分类，另一方面我们希望这些特征能够让domain classifier能够无法识别特征是从哪些data抽取得到的，这样得到的特征就是被去掉domain specific特征的。</p><p>具体训练：<br><img src="/images/2018-10-07-15388874447304.jpg" width="50%" height="50%"></p><h3 id="Zero-shot-Learning"><a href="#Zero-shot-Learning" class="headerlink" title="Zero-shot Learning"></a>Zero-shot Learning</h3><p>source data有标签，target data无标签，但任务不同，如：<br><img src="/images/2018-10-07-15388874838165.jpg" width="50%" height="50%"></p><h4 id="Representing-each-class-by-its-attributes"><a href="#Representing-each-class-by-its-attributes" class="headerlink" title="Representing each class by its attributes"></a>Representing each class by its attributes</h4><p>一种方法是将每一个类都用特征表示，但特征要足够丰富：<br><img src="/images/2018-10-07-15388875088114.jpg" width="50%" height="50%"></p><p>在训练的时候，输入是图片，输出则是这些特征：<br><img src="/images/2018-10-07-15388875558853.jpg" width="40%" height="50%"><br>这样在将target data放入训练好的NN后也会得到一个这样的attribute，查表即可找到最相似的特征对应的类。</p><h4 id="Attribute-embedding"><a href="#Attribute-embedding" class="headerlink" title="Attribute embedding"></a>Attribute embedding</h4><p>如果特征维度太高，也可以将特征压缩成一个向量表示，这样在训练的时候，输出则是这样的向量特征，输入target data，输出向量特征，找到最近的特征对应的类即可<br><img src="/images/2018-10-07-15388875888699.jpg" width="50%" height="50%"></p><h4 id="Attribute-embedding-word-embedding"><a href="#Attribute-embedding-word-embedding" class="headerlink" title="Attribute embedding + word embedding"></a>Attribute embedding + word embedding</h4><p>如果没有attribute数据，利用word embedding也可以达到不错的效果。<br>在zero-shot learning中，光是让相同类的f和g相似是不够的，还应该让不同的f和g尽量远。</p><script type="math/tex; mode=display">f^∗,g^∗=arg min_{(f,g)}⁡∑_nmax(0,k−f(x^n )\cdot g(y^n )+max_{(m≠n)} ⁡f(x^m )\cdot g(x^m ) )</script>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Transfer Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 14:Unsupervised Learning:Generation</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2014:%20Unsupervised%20Learning:%20Generation/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2014:%20Unsupervised%20Learning:%20Generation/</url>
      
        <content type="html"><![CDATA[<h3 id="Component-by-component"><a href="#Component-by-component" class="headerlink" title="Component-by-component"></a>Component-by-component</h3><p>对于图像来说，每次生成一个pixel：PixelRNN</p><h3 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h3><p>架构：<br><img src="/images/2018-10-07-15388837191574.jpg" width="50%" height="50%"></p><p>其中e是噪声，σ是方差，目标是最小化reconstruction error，以及一个限制。该限制的目的即防止σ=0，m是正则化项。</p><p><del>中间的推导以及为什么是这样的架构我还不是很懂，之后再更新。</del><br>实际上可以这么理解，有几个要点：</p><ul><li>首先我们是基于这么一个假设：中间的code应当是服从正态分布的，而encoder的作用即在于拟合该正态分布的均值与方差的对数（因为方差应当恒为正，但神经网络的输出可能有正有负）</li><li>如果生成出来的code不符合正态分布，会有一个惩罚项，也就是上图的constraint（可以通过KL散度推导获得）</li><li>按理说，应当是在生成了均值和方差后，定义好该正态分布，然后再从中采样，但是这样没办法回传更新梯度，因此这里使用重参数技巧(Reparameterization Trick)，也即从$N(\mu,\sigma^2)$中采样$Z$，相当于从$N(0,I)$中采样$\varepsilon$，然后让$Z=\mu + \varepsilon \times \mu$</li></ul><p><img src="/images/2018-10-08-15389638077301.jpg" width="70%" height="50%"></p><p><strong>Reference</strong>:<br><a href="https://www.sohu.com/a/226209674_500659" target="_blank" rel="noopener">https://www.sohu.com/a/226209674_500659</a></p><p>VAE的主要问题在于，网络只试图去记住见过的图像，但没法真正去生成没见过的图像。</p><h3 id="Generative-Adversarial-Network-GAN"><a href="#Generative-Adversarial-Network-GAN" class="headerlink" title="Generative Adversarial Network (GAN)"></a>Generative Adversarial Network (GAN)</h3><p>GAN包含一个discriminator和一个generator，generator试图生成能够骗过discriminator的样本，而generator试图能够将generator生成的样本和真实的样本区分。</p><p>之后会有详细的介绍。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Unsupervised Learning </tag>
            
            <tag> Generation </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 13:Unsupervised Learning:Auto-encoder</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2013:%20Unsupervised%20Learning:%20Auto-encoder/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2013:%20Unsupervised%20Learning:%20Auto-encoder/</url>
      
        <content type="html"><![CDATA[<h3 id="Auto-encoder"><a href="#Auto-encoder" class="headerlink" title="Auto-encoder"></a>Auto-encoder</h3><p>由一个encoder和一个decoder组成，encoder负责将输入转成一个向量表示（维度通常小于输入），decoder负责将这段向量表示恢复成原来的输入。那么中间的code就可以作为输入的一个低维表示：<br><img src="/images/2018-10-07-15388832782913.jpg" width="50%" height="50%"></p><h3 id="Auto-encoder-for-CNN"><a href="#Auto-encoder-for-CNN" class="headerlink" title="Auto-encoder for CNN"></a>Auto-encoder for CNN</h3><p><img src="/images/2018-10-07-15388833149617.jpg" width="50%" height="50%"></p><h4 id="Unpooling"><a href="#Unpooling" class="headerlink" title="Unpooling"></a>Unpooling</h4><p>有两种方法，一种在pooling的时候记录最大值的位置，在unpooling时在相对位置填充最大值，其他位置填充0；另一种不记录最大值位置，直接在pooling区域全部填充最大值。<br><img src="/images/2018-10-07-15388833530548.jpg" width="50%" height="50%"></p><h4 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h4><p>其实本质就是convolution。</p><p>这是convolution:</p><p><img src="/images/2018-10-07-15388834044149.jpg" width="10%" height="50%"></p><p>我们期待的convolution：<br><img src="/images/2018-10-07-15388834434741.jpg" width="15%" height="50%"></p><p>实际上就等价在两边做padding，然后直接convolution：<br><img src="/images/2018-10-07-15388834751493.jpg" width="15%" height="50%"></p><h3 id="Auto-encoder的用处"><a href="#Auto-encoder的用处" class="headerlink" title="Auto-encoder的用处"></a>Auto-encoder的用处</h3><p>可以预训练每一层的DNN：<br><img src="/images/2018-10-07-15388835335550.jpg" width="50%" height="50%"></p><p>同理其它层也是一样，每次fix住其他层然后做Auto-encoder。那么在bp的时候只需要fine-tune就行。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Unsupervised Learning </tag>
            
            <tag> Auto-encoder </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 12:Unsupervised Learning:Neighbor Embedding</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2012:%20Unsupervised%20Learning:%20Neighbor%20Embedding/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2012:%20Unsupervised%20Learning:%20Neighbor%20Embedding/</url>
      
        <content type="html"><![CDATA[<h3 id="Locally-Linear-Embedding-LLE"><a href="#Locally-Linear-Embedding-LLE" class="headerlink" title="Locally Linear Embedding (LLE)"></a>Locally Linear Embedding (LLE)</h3><p>一种降维方法<br>思想：假设每个点可以由其周围的点来表示<br><img src="/images/2018-10-07-15388822769215.jpg" width="25%" height="50%"></p><p>我们需要找到这样的$w_{ij}$，使得：</p><script type="math/tex; mode=display">∑_i‖x^i−∑_j w_{ij} x^j ‖_2</script><p>这样在降维的时候，我们仍然保持x之间的这样的关系:<br><img src="/images/2018-10-07-15388823792351.jpg" width="50%" height="50%"></p><h3 id="Laplacian-Eigenmaps"><a href="#Laplacian-Eigenmaps" class="headerlink" title="Laplacian Eigenmaps"></a>Laplacian Eigenmaps</h3><p>一种降维方法<br>基本思想：如果$x^1$与$x^2$在高维空间中相近，则降维后也应该接近：</p><script type="math/tex; mode=display">S=1/2 ∑_{i,j} w_{i,j} (z^i−z^j )^2</script><p>其中：<br><img src="/images/2018-10-07-15388824984809.jpg" width="30%" height="50%"></p><p>如果将z全设为0，显然S最小，因此我们需要给z一个限制：z应当充满空间，也即假如z是M维，那么$\{z^1,z^2…,z^N\}$的秩应该等于M</p><h3 id="T-distributed-Stochastic-Neighbor-Embedding-t-SNE"><a href="#T-distributed-Stochastic-Neighbor-Embedding-t-SNE" class="headerlink" title="T-distributed Stochastic Neighbor Embedding (t-SNE)"></a>T-distributed Stochastic Neighbor Embedding (t-SNE)</h3><p>也是一种降维方法<br>前面提到的方法有一个问题：同一类的点确实聚在一起，但不同类的点并没有尽量分开<br><img src="/images/2018-10-07-15388826477983.jpg" width="50%" height="50%"></p><p>t-SNE的主要思想：将数据点映射到概率分布，我们希望降维前和降维后，数据分布的概率应当尽可能一致。<br>t-SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。t-SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。</p><p>如何做？<br>在高维空间中，我们定义：</p><script type="math/tex; mode=display">P(x^j |x^i )=\frac{S(x^i,x^j )}{∑_{k≠i}S(x^i,x^k )}</script><p>其中S表示i与j之间的相似度。</p><p>在低维空间中，同样有：</p><script type="math/tex; mode=display">Q(z^j |z^i )=\frac{S′(z^i,z^j )}{∑_{k≠i}S′(z^i,z^k )}</script><p>使用KL散度去计算两个分布之间的差异：</p><script type="math/tex; mode=display">L=∑_i KL(P(∗|x^i )||Q(∗|z^i )) =∑_i∑_j P(x^j |x^i )\frac{log P(x^j |x^i )}{Q(z^j |z^i )}</script><p>t-SNE中，高维空间和低维空间计算相似度的公式不大一样：</p><script type="math/tex; mode=display">S(x^i,x^j )=exp(−‖x^i−x^j ‖_2 )</script><script type="math/tex; mode=display">S′(z^i,z^j )=\frac{1}{(1+‖z^i−z^j ‖_2)}</script><p>两个公式的图示：<br><img src="/images/2018-10-07-15388830652023.jpg" width="70%" height="50%"></p><p>也即<strong>低维空间会拉长距离，使得距离远的点尽可能被拉开</strong>。</p><p>t-SNE的问题在于：t-SNE无法对新的数据点进行降维。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Unsupervised Learning </tag>
            
            <tag> Neighbor Embedding </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 11:Unsupervised Learning:Linear Dimension Reduction</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2011:%20Unsupervised%20Learning:%20Linear%20Dimension%20Reduction/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2011:%20Unsupervised%20Learning:%20Linear%20Dimension%20Reduction/</url>
      
        <content type="html"><![CDATA[<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><h3 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h3><p>算法步骤：<br><img src="/images/2018-10-07-15388800377875.jpg" width="70%" height="50%"></p><p>迭代更新使得最后聚类中心收敛。但事先需要定好有多少类。</p><h3 id="Hierarchical-Agglomerative-Clustering-HAC"><a href="#Hierarchical-Agglomerative-Clustering-HAC" class="headerlink" title="Hierarchical Agglomerative Clustering (HAC)"></a>Hierarchical Agglomerative Clustering (HAC)</h3><p>自下而上，每次选两个最近的聚为一类，直到所有的都分成一类<br>最后选择一个阈值划分，如蓝色绿色和红色的线<br><img src="/images/2018-10-07-15388801021791.jpg" width="50%" height="50%"></p><h2 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h2><p>找到一个映射，使得x能够映射到低维z</p><h3 id="Principle-Component-Analysis-PCA"><a href="#Principle-Component-Analysis-PCA" class="headerlink" title="Principle Component Analysis (PCA)"></a>Principle Component Analysis (PCA)</h3><p>目的是找到一个维度，使得投影得到的variance最大，也即最大程度保留数据的差异性。<br><img src="/images/2018-10-07-15388801830659.jpg" width="50%" height="50%"></p><p>形式化可以写成（一维情形）：</p><script type="math/tex; mode=display">Var(z_1 )=\frac{1}{N} ∑_{z_1}(z_1−\overline{z_1} )^2</script><p>其中：</p><script type="math/tex; mode=display">‖w^1 ‖_2=1</script><script type="math/tex; mode=display">z_1=w^1 \cdot x</script><p>$\overline{z_1}$表示z的均值</p><p>假如我们要投影到多维，其他维度也有同样的目标。其中每个维度之间都应该是相互正交的。<br><img src="/images/2018-10-07-15388804752506.jpg" width="20%" height="50%"></p><h4 id="如何做？"><a href="#如何做？" class="headerlink" title="如何做？"></a>如何做？</h4><p>找到$ \frac{1}{N}∑(x−\overline{x} ) (x−\overline{x})^T$的前k个最大的特征值对应的特征向量，组合起来即是我们要找的$W$</p><h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><p>—-Warning of Math—-<br>目的：$Var(z_1 )=\frac{1}{N} ∑_{z_1}(z_1−\overline{z_1} )^2 $<br>其中 $\overline{z_1} =\frac{1}{N} ∑{z_1} = \frac{1}{N} ∑ w^1 \cdot x=w^1\cdot \overline{x}$</p><p>推导：<br><img src="/images/2018-10-07-15388811276042.jpg" width="35%" height="50%"><br>改变符号 $S=Cov(x)$</p><p>利用拉格朗日乘数法，有：<br>$Sw^1=αw^1$<br>等式两边各左乘$(w^1)^T$，有：<br>$(w^1 )^T Sw^1=α(w^1 )^T w^1=α$</p><p>也即，$α$是$S$的特征值，选择最大的特征值，就能够最大化我们的目标。</p><p>同理，我们要找$w^2$，最大化$(w^2 )^T Sw^2$，其中有：<br>$(w^2 )^T w^2=1$<br>$(w^2 )^T w^1=0$ （与第一维正交）</p><p>因此利用拉格朗日乘数法：</p><script type="math/tex; mode=display">g(w^2 )= (w^2 )^T Sw^2−α((w^2 )^T w^2−1)−β((w^2 )^T w^1−0)</script><p>最终得到，w2对应第二大的特征值的特征向量。</p><p>以此类推，其他维也同理。<br>—-End of Math—-</p><h4 id="PCA的其他"><a href="#PCA的其他" class="headerlink" title="PCA的其他"></a>PCA的其他</h4><p>实际上最终得到的z，每一维之间的协方差都为0<br><img src="/images/2018-10-07-15388815546680.jpg" width="50%" height="50%"></p><p>证明如下：<br><img src="/images/2018-10-07-15388815837458.jpg" width="50%" height="50%"></p><p>PCA也可以用SVD来做：<br><img src="/images/2018-10-07-15388816250075.jpg" width="60%" height="50%"></p><p>U中保存了K个特征向量。</p><p>从另一种角度理解PCA，也可以认为PCA是一种autoencoder：<br><img src="/images/2018-10-07-15388816896369.jpg" width="50%" height="50%"></p><h4 id="PCA的问题"><a href="#PCA的问题" class="headerlink" title="PCA的问题"></a>PCA的问题</h4><p>PCA是无监督学习，如果有标签，则无法按照类别来进行正确降维，如：<br><img src="/images/2018-10-07-15388817393283.jpg" width="30%" height="50%"></p><p>第二就是PCA是线性变换，对于一些需要非线性变换的无能为力<br><img src="/images/2018-10-07-15388817566149.jpg" width="28%" height="50%"></p><h3 id="Matrix-Factorization"><a href="#Matrix-Factorization" class="headerlink" title="Matrix Factorization"></a>Matrix Factorization</h3><p>定义：矩阵分解，就是将一个矩阵D分解为U和V的乘积，即对于一个特定的规模为m*n的矩阵D，估计出规模分别为m*k和n*k的矩阵U和V，使得$UV^T$的值尽可能逼近矩阵D。常用于推荐系统。</p><p>思想：<br>假如有一个矩阵：<br><img src="/images/2018-10-07-15388819053983.jpg" width="60%" height="50%"></p><p>假设横轴和纵轴每一维都有一个向量代表该维，矩阵的每个元素就是横轴和纵轴对应维的点积。我们的目的是尽可能减小：</p><script type="math/tex; mode=display">L=\sum_{(i,j)} (r^i \cdot r^j -n_{ij})^2</script><p>其中$r_i$ $r_j$就是向量表示，$n_{ij}$就是矩阵的内容。</p><p>可以使用SVD求解上式：<br><img src="/images/2018-10-07-15388820642382.jpg" width="50%" height="50%"></p><p>实际上，考虑每一行或列本身的特性，我们对Loss进行扩展：</p><script type="math/tex; mode=display">Minimizing \ \ L=\sum_{(i,j)} (r^i \cdot r^j +b_i+b_j-n_{ij})^2</script><p>使用SGD可以求解。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Unsupervised Learning </tag>
            
            <tag> Linear Dimension Reduction </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>梯度消失与梯度爆炸的推导</title>
      <link href="/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E6%8E%A8%E5%AF%BC/"/>
      <url>/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E6%8E%A8%E5%AF%BC/</url>
      
        <content type="html"><![CDATA[<p> 记RNN中每一步的损失为$E_t$，则损失对$h_{t-1}$的权重$W$的导数有：</p><script type="math/tex; mode=display">\frac{\partial{E_t}}{\partial{W}}=\sum_{k=1}^{t}    \frac{\partial{E_t}}{\partial{y_t}} \frac{\partial{y_t}}{\partial{h_t}} \frac{\partial{h_t}}{\partial{h_k}} \frac{\partial{h_k}}{\partial{W}}</script><p>其中$\frac{\partial{h_t}}{\partial{h_k}}$使用链式法则有：</p><script type="math/tex; mode=display">\frac{\partial{h_t}}{\partial{h_k}} =     \prod_{j=k+1}^{t} \frac{\partial{h_j}}{\partial{h_{j-1}}} =    \prod_{j=k+1}^{t} W^T \times diag[f^{\prime}(h_{j-1})]</script><p>其中$\frac{\partial{h_j}}{\partial{h_{j-1}}}$ 是雅克比矩阵。对其取模(norm)，有：</p><script type="math/tex; mode=display">\rVert \frac{\partial{h_j}}{\partial{h_{j-1}}}\rVert ≤ \rVert W^T \rVert \rVert diag[f^{\prime}(h_{j-1})] \rVert ≤ \beta_W \beta_h</script><p>当$f$为sigmoid时，$f^{\prime}(h_{j-1})$最大值为1。</p><p>最终我们有：</p><script type="math/tex; mode=display">\rVert \frac{\partial{h_t}}{\partial{h_{k}}}\rVert ≤ \rVert \prod_{j=k+1}^{t} \frac{\partial{h_j}}{\partial{h_{j-1}}} \rVert ≤ (\beta_W \beta_h)^{t-k}</script><p>从上式可以看出，当t-k足够大时，如果$(\beta_W \beta_h)$小于1则$(\beta_W \beta_h)^{t-k}$则会变得非常小，相反，若$(\beta_W \beta_h)$大于1则$(\beta_W \beta_h)^{t-k}$则会变得非常大。</p><p>在计算机中，当梯度值很大时，会造成上溢(NaN)，也即梯度爆炸问题，当梯度值很小时，会变成0，也即梯度消失。注意到，t-k的损失实际上评估的是一个较远的词对当前t的贡献，梯度消失也即意味着对当前的贡献消失。</p><p>Reference:<br>CS224d: Deep Learning for NLP Lecture4</p>]]></content>
      
      
      
        <tags>
            
            <tag> 梯度消失 </tag>
            
            <tag> 梯度爆炸 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识10</title>
      <link href="/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8610/"/>
      <url>/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8610/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-正态分布"><a href="#1️⃣-正态分布" class="headerlink" title="1️⃣[正态分布]"></a>1️⃣[正态分布]</h3><p>高维正态分布是从一维发展而来的：<br><img src="/images/2018-10-07-15388761009977.jpg" width="70%" height="50%"></p><p><a href="https://www.zhihu.com/question/36339816" target="_blank" rel="noopener">https://www.zhihu.com/question/36339816</a></p><hr><h3 id="2️⃣-RNN"><a href="#2️⃣-RNN" class="headerlink" title="2️⃣[RNN]"></a>2️⃣[RNN]</h3><p>from <a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf" target="_blank" rel="noopener">https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf</a></p><p>通常而言，我们都会将RNN的initial state设为全0，但在Hinton的slide中提到，我们可以将初始状态作为可学习的变量，和我们在学习权重矩阵一样。</p><p><img src="/images/2018-10-07-15388770544817.jpg" width="80%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 正态分布 </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PRML第二章 概率分布</title>
      <link href="/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83/"/>
      <url>/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83/</url>
      
        <content type="html"><![CDATA[<h1 id="二元变量"><a href="#二元变量" class="headerlink" title="二元变量"></a>二元变量</h1><p><img src="/images/2018-09-30-Xnip2018-09-30_14-25-21.jpg" alt="二元变量1"></p><p><img src="/images/2018-09-30-Xnip2018-09-30_14-26-46.jpg" alt="贝塔分布"></p><h1 id="多项式分布"><a href="#多项式分布" class="headerlink" title="多项式分布"></a>多项式分布</h1><p><img src="/images/2018-09-30-Xnip2018-09-30_14-31-55.jpg" alt="多项式分布"></p><h1 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h1><p><img src="/images/2018-09-30-Xnip2018-09-30_14-40-32.jpg" alt="高斯分布"></p><p><img src="/images/2018-09-30-Xnip2018-09-30_14-43-09.jpg" alt="2"></p><p><img src="/images/2018-09-30-Xnip2018-09-30_14-45-15.jpg" alt="3"></p><p><img src="/images/2018-09-30-Xnip2018-09-30_14-48-58.jpg" alt="4"></p><h1 id="指数族分布"><a href="#指数族分布" class="headerlink" title="指数族分布"></a>指数族分布</h1><p><img src="/images/2018-09-30-Xnip2018-09-30_14-51-25.jpg" alt="1"></p><p><img src="/images/2018-10-03-Xnip2018-10-03_10-03-54.jpg" alt="2"></p><h1 id="非参数优化"><a href="#非参数优化" class="headerlink" title="非参数优化"></a>非参数优化</h1><p><img src="/images/2018-10-03-Xnip2018-10-03_10-05-24.jpg" alt="1"></p><p><img src="/images/2018-10-03-Xnip2018-10-03_10-06-55.jpg" alt="2"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> PRML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 10:Semi-supervised learning</title>
      <link href="/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2010:%20Semi-supervised/"/>
      <url>/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2010:%20Semi-supervised/</url>
      
        <content type="html"><![CDATA[<p>什么是semi-supervised learning</p><p>给定数据${(x^r,\hat{y}^r)}_{r=1}^{R},{(x_u)}_{u=R}^{R+U}$，其中未标记数据远远多于标记数据 $U&gt;&gt;R$</p><p>为什么半监督学习有用？<br>因为未标记数据的分布可能能够给我们一些信息。</p><h3 id="生成模型的半监督学习"><a href="#生成模型的半监督学习" class="headerlink" title="生成模型的半监督学习"></a>生成模型的半监督学习</h3><p>给定两类$C_1$、$C_2$，要求得到后验概率分布</p><script type="math/tex; mode=display">P(C_1 |x)=\frac{P(x|C_1 )P(C_1 )}{(P(x|C_1 )P(C_1 )+P(x|C_2 )P(C_2 ) )}</script><p>其中联合概率分布服从高斯分布。未标记数据此时的作用即帮我们重新估计$P(C_1),P(C_2),\mu,\Sigma$</p><p><img src="/images/2018-09-30-15382829251218.jpg" width="50%" height="50%"></p><p>如何做?<br>先初始化$P(C_1),P(C_2),\mu,\Sigma$，通常可以先用有标记数据进行估计</p><ol><li>计算每个未标记数据的后验概率分布</li><li>以该概率分布更新模型<br>不断重复直至拟合</li></ol><p><img src="/images/2018-09-30-15382829987091.jpg" width="70%" height="50%"></p><p>原因：<br>当我们在做监督学习时，使用最大似然求解：</p><script type="math/tex; mode=display">logL(θ)=∑_{x^r,\hat{y}^r} logP_θ (x^r |\hat{y}^r )</script><p>加上了未标记数据后，同样也要做最大似然：</p><script type="math/tex; mode=display">logL(θ)=∑_{(x^r,\hat{y}^r)} logP_θ (x^r |\hat{y}^r )+∑_{x^u} logP_θ (x^u)</script><h3 id="Low-density-Separation"><a href="#Low-density-Separation" class="headerlink" title="Low-density Separation"></a>Low-density Separation</h3><p>假设不同类别之间有一条明显的分界线，也即存在一个区域，其密度比其他区域小</p><h4 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h4><p>如何做?</p><ol><li>先用有标签数据训练一个模型$f$；</li><li>利用模型对未标记数据进行标记，这些标签称为伪标签（pseudo-label）</li><li>将部分有伪标签的数据放入有标签数据中，重新训练<br>重复直到拟合</li></ol><p>这种方式和生成模型的区别：该方法使用的是hard label而生成模型使用的是soft label</p><h4 id="Entropy-based-Regularization"><a href="#Entropy-based-Regularization" class="headerlink" title="Entropy-based Regularization"></a>Entropy-based Regularization</h4><p>将未标记数据充当正则化的效果，我们希望模型预测标签的概率较为集中，也即熵应该尽可能小。也就是说，未标记数据使得分类边界尽可能划在低密度区域。<br><img src="/images/2018-09-30-15382837957204.jpg" width="30%" height="50%"></p><h3 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h3><p>假设：位于稠密数据区域的两个距离很近的样例的类标签相似，通过high density path连接。</p><p><img src="/images/2018-09-30-15382840889274.jpg" width="40%" height="50%"><br>x1与x2之间较为稠密，因此x2与x1比x2与x3更为接近。</p><p><strong>如何知道x1与x2通过high density path连接？</strong><br><img src="/images/2018-09-30-15382841851160.jpg" width="50%" height="50%"></p><p>基于图的方法：</p><ol><li>定义xi与xj之间的相似度$s(x^i,x^j)$</li><li>添加边，有两种选择<ol><li>k nearest neighbor</li><li>e-neighborhood<br><img src="/images/2018-09-30-15382842669412.jpg" width="50%" height="50%"></li></ol></li><li>边之间的权重通过相似度来衡量。如： $s(x^i,x^j )=exp(−γ‖x^i−x^j‖^2)$</li></ol><p>该方法本质即利用有标签数据去影响未标记数据，通过图的传播。但一个问题是如果数据不够多，就可能没办法传播。如：<br><img src="/images/2018-09-30-15382844101208.jpg" width="30%" height="50%"></p><p>在建立好图后，如何使用?</p><ul><li>定义图的平滑程度，$y$表示标签。$S$越小表示越平滑。<script type="math/tex; mode=display">S=1/2∑_{i,j} w_{i,j} (y^i−y^j )^2=y^T Ly</script><script type="math/tex; mode=display">y=[⋯y^i⋯y^j⋯]^T</script><script type="math/tex; mode=display">L=D−W</script></li></ul><p>D是邻接矩阵，第ij个元素即xi与xj之间的weight，W是对角矩阵，ii个元素是D的第i行的加和；L称为Graph Laplacian<br><img src="/images/2018-09-30-15382847006975.jpg" width="50%" height="50%"></p><ul><li>我们最终在计算Loss的时候要加上这项正则项<script type="math/tex; mode=display">L=∑_{x^r}C(y^r,\hat{y}^r ) +λS</script></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> Semi-supervised learning </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 7:Tips for DL</title>
      <link href="/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%207:%20Tips%20for%20DL/"/>
      <url>/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%207:%20Tips%20for%20DL/</url>
      
        <content type="html"><![CDATA[<p>大纲<br><img src="/images/2018-09-30-15382757690955.jpg" width="50%" height="50%"></p><h2 id="new-activation-function"><a href="#new-activation-function" class="headerlink" title="new activation function"></a>new activation function</h2><p>梯度消失问题：由于sigmoid会将值压缩，所以在反向传播时，越到后面值越小。</p><p><img src="/images/2018-09-30-15382758841507.jpg" width="30%" height="50%"><br>所以后层的更新会比前层的更新更快，导致前层还没converge，后层就根据前层的数据（random）达到converge了</p><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p><img src="/images/2018-09-30-15382759503401.jpg" width="30%" height="50%"><br>能够快速计算，且能够解决梯度消失问题。</p><p>因为会有部分neuron的值是0，所以相当于每次训练一个瘦长的神经网络。<br><img src="/images/2018-09-30-15382760030965.jpg" width="50%" height="50%"></p><h4 id="ReLU的变体"><a href="#ReLU的变体" class="headerlink" title="ReLU的变体"></a>ReLU的变体</h4><p><img src="/images/2018-09-30-15382928024258.jpg" width="50%" height="50%"></p><h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><p>首先将几个neuron归为一组，然后每次前向传播时取最大的作为输出。<br><img src="/images/2018-09-30-15382761367509.jpg" width="50%" height="50%"></p><p>实际上ReLU是maxout的一种特殊形式：<br><img src="/images/2018-09-30-15382761741870.jpg" width="40%" height="50%"></p><p>更一般的，有：<br><img src="/images/2018-09-30-15382762237369.jpg" width="40%" height="50%"></p><p>因为w和b的变化，所以该activation function实际上就是一个learnable activation function</p><p>这样一个learnable activation function有这样的特点：</p><blockquote><p>Activation function in maxout network can be any piecewise linear convex function<br>How many pieces depending on how many elements in a group</p></blockquote><p>如：<br><img src="/images/2018-09-30-15382763537888.jpg" width="60%" height="50%"></p><p>maxout应如何训练？</p><p><img src="/images/2018-09-30-15382764343880.jpg" width="50%" height="50%"></p><p>实际上就是一个普通的瘦长network，常规训练即可。<br><img src="/images/2018-09-30-15382764564859.jpg" width="50%" height="50%"></p><h2 id="Adaptive-learning-rate"><a href="#Adaptive-learning-rate" class="headerlink" title="Adaptive learning rate"></a>Adaptive learning rate</h2><p>在adagrad中:<br><img src="/images/2018-09-30-15382765516383.jpg" width="30%" height="50%"></p><p>越到后面learning rate越来越小，但实际上在dl里面，error surface是非常复杂的，越来越小的learning rate可能不适用于dl。如：<br><img src="/images/2018-09-30-15382765821828.jpg" width="50%" height="50%"></p><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p><img src="/images/2018-09-30-15382766372355.jpg" width="60%" height="50%"><br>$σ^t$是历史信息，也就是说$σ^t$参考了过去的梯度和当前的梯度获得一个新的放缩大小</p><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>引入惯性作为参考，也即参考了上一次梯度的方向。引入惯性后，可能有机会越过local minimum。<br>普通的gradient descent:<br><img src="/images/2018-09-30-15382769712428.jpg" width="40%" height="50%"><br>每次朝着梯度的反方向走。</p><p>Momentum:<br><img src="/images/2018-09-30-15382770120104.jpg" width="40%" height="50%"></p><p>考虑了上一步走的方向。</p><p>具体算法：<br><img src="/images/2018-09-30-15382771516587.jpg" width="30%" height="50%"></p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>结合了RMSprop和Momentum，也即综合考虑了历史信息决定当前步长；考虑了上一步的方向决定当前走的方向。<br>具体算法：<br><img src="/images/2018-09-30-15382772986250.jpg" width="60%" height="50%"></p><h2 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h2><p>就是在validation set的loss不再减小时停止<br><img src="/images/2018-09-30-15382814784406.jpg" width="50%" height="50%"></p><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p><img src="/images/2018-09-30-15382815175056.jpg" width="50%" height="50%"><br>其中<br><img src="/images/2018-09-30-15382815336088.jpg" width="30%" height="50%"><br>因此更新公式为：<br>    <img src="/images/2018-09-30-15382815641437.jpg" width="50%" height="50%"></p><p>也即每次以$1-\eta \lambda$对w进行放缩，使w更接近0<br>正则化在DL中也称为weight decay</p><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p><img src="/images/2018-09-30-15382816962676.jpg" width="25%" height="50%"></p><p><img src="/images/2018-09-30-15382817122102.jpg" width="25%" height="50%"><br><img src="/images/2018-09-30-15382817409633.jpg" width="25%" height="50%"></p><p>则更新公式为：<br><img src="/images/2018-09-30-15382817897319.jpg" width="50%" height="50%"></p><p>也即每次以$ηλsgn(w)$ 使w往0靠（sgn表示符号函数）</p><p>可以看出，L1每次都加减相同的值，而L2按比例进行缩放。因此L1更为稀疏(sparse)。</p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>训练的时候每一层采样p%的神经元设为0，让其不工作<br><img src="/images/2018-09-30-15382819376579.jpg" width="50%" height="50%"></p><p>实际上就是每个batch改变了网络结构，使得网络更细长<br><img src="/images/2018-09-30-15382819772611.jpg" width="50%" height="50%"></p><p>测试的时候所有的weight都乘以1-p%</p><p>从ensemble的角度看待dropout：<br>在训练的时候训练一堆不同结构的network，最多有$2^N$种组合，N为neuron个数，可以称为终极的ensemble方法了。而在测试的时候对这些不同的网络进行平均。</p><p><img src="/images/2018-09-30-15382821089494.jpg" width="50%" height="50%"></p><p><img src="/images/2018-09-30-15382821379688.jpg" width="50%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Tips for DL </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>采样浅析</title>
      <link href="/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/%E9%87%87%E6%A0%B7%E6%B5%85%E6%9E%90/"/>
      <url>/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/%E9%87%87%E6%A0%B7%E6%B5%85%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>总结在NLP中的采样方法（持续更新）。</p><h2 id="采样方法"><a href="#采样方法" class="headerlink" title="采样方法"></a>采样方法</h2><h3 id="1️⃣逆变换采样-Inverse-Sampling"><a href="#1️⃣逆变换采样-Inverse-Sampling" class="headerlink" title="1️⃣逆变换采样(Inverse Sampling)"></a>1️⃣逆变换采样(Inverse Sampling)</h3><p>目的：已知任意概率分布的<strong>累积分布函数</strong>时，用于从该分布中生成随机样本。</p><p>—-什么是累积分布函数(CDF)—-<br>是概率密度函数(PDF)的积分，定义：</p><script type="math/tex; mode=display">F_X(x)=P(X≤x)=\int_{-∞}^{x}f_X(t)dt</script><p>—-END—-</p><p>想象我们知道高斯分布的概率密度函数，我们应该如何采样？本质上我们只能对均匀分布进行直接采样（高斯分布有<a href="https://www.zhihu.com/question/29971598" target="_blank" rel="noopener">算法</a>可以生成采样，但无法一般化）。对于这种连续的随机变量，我们只能通过间接的方法进行采样。</p><p>逆变换采样即是通过累积分布函数的反函数来采样。因为累积分布函数的值域为$[0,1]$，因此我们通过在$[0,1]$上进行采样，再映射到原分布。<br>例子:<br><img src="/images/2018-09-30-15382714567064.jpg" width="80%" height="50%"><br>映射关系如图：<br><img src="/images/2018-09-30-15382715821631.jpg" width="50%" height="50%"></p><h3 id="2️⃣重要性采样-Importance-Sampling"><a href="#2️⃣重要性采样-Importance-Sampling" class="headerlink" title="2️⃣重要性采样(Importance Sampling)"></a>2️⃣重要性采样(Importance Sampling)</h3><p>目的：已知某个分布$P$，希望能估计$f(x)$的期望。亦即：</p><script type="math/tex; mode=display">E[f(x)]=\int_{x}f(x)p(x)dx≈\frac{1}{n}\sum_{i=1}^{n}f(x_i)</script><p>其中$x\sim p$。<br>假设$p(x)$的分布复杂或样本不好生成，另一分布$q(x)$方便生成样本。因此我们引入$q(x)$对原先分布进行估计。</p><script type="math/tex; mode=display">E[f(x)]=\int_{x}f(x)p(x)dx=\int_{x}f(x)\frac{p(x)}{q(x)}q(x)dx≈\frac{1}{n}\sum_{i=1}^{n}f(x_i)\frac{p(x_i)}{q(x_i)}</script><p>其中，$x \sim q$。$w(x)=\frac{p(x)}{q(x)}$称为Importance Weight</p><p>根据上式，实际上就是每次采样的加权求和。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>逆变换采样<br><a href="https://zh.wikipedia.org/wiki/%E9%80%86%E5%8F%98%E6%8D%A2%E9%87%87%E6%A0%B7" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E9%80%86%E5%8F%98%E6%8D%A2%E9%87%87%E6%A0%B7</a></p><p>重要性采样<br><a href="https://www.youtube.com/watch?v=S3LAOZxGcnk" target="_blank" rel="noopener">https://www.youtube.com/watch?v=S3LAOZxGcnk</a></p><p>——持续更新——</p>]]></content>
      
      
      
        <tags>
            
            <tag> 采样 </tag>
            
            <tag> sampling </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识9</title>
      <link href="/2018/09/30/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%869/"/>
      <url>/2018/09/30/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%869/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>Pytorch中保存checkpoint是一个dict形式，可以保存任意多个模型到一个checkpoint中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#save</span></span><br><span class="line">torch.save(&#123;            <span class="string">'epoch'</span>: epoch,            <span class="string">'model_state_dict'</span>: model.state_dict(),            <span class="string">'optimizer_state_dict'</span>: optimizer.state_dict(),            <span class="string">'loss'</span>: loss,            ...            &#125;, PATH)</span><br><span class="line"><span class="comment">#load</span></span><br><span class="line">model = TheModelClass(*args, **kwargs)optimizer = TheOptimizerClass(*args, **kwargs)</span><br><span class="line">checkpoint = torch.load(PATH)model.load_state_dict(checkpoint[<span class="string">'model_state_dict'</span>])optimizer.load_state_dict(checkpoint[<span class="string">'optimizer_state_dict'</span>])epoch = checkpoint[<span class="string">'epoch'</span>]loss = checkpoint[<span class="string">'loss'</span>]</span><br><span class="line">model.eval()<span class="comment"># - or -</span>model.train()</span><br></pre></td></tr></table></figure></p><hr><h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>Pytorch可以load部分模型，也就是只load进来部分我们需要的层，这在transfer learning中用到。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">torch.save(modelA.state_dict(), PATH)</span><br><span class="line">modelB = TheModelBClass(*args, **kwargs)modelB.load_state_dict(torch.load(PATH), strict=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>no title</title>
      <link href="/2018/09/30/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%9C%89%E5%8A%9B%E9%87%8F%E7%9A%84%E6%96%87%E5%AD%97/"/>
      <url>/2018/09/30/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%9C%89%E5%8A%9B%E9%87%8F%E7%9A%84%E6%96%87%E5%AD%97/</url>
      
        <content type="html"><![CDATA[<p>每当我遇到自己不敢直视的困难时，我就会闭上双眼，想象自己是一个80岁的老人，为人生中曾放弃和逃避过的无数困难而懊悔不已，我会对自己说，能再年轻一次该有多好，然后我睁开眼睛：砰！我又年轻一次了！</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词10</title>
      <link href="/2018/09/30/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D10/"/>
      <url>/2018/09/30/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D10/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣次北固山下"><a href="#1️⃣次北固山下" class="headerlink" title="1️⃣次北固山下"></a>1️⃣次北固山下</h3><p>[唐] 王湾<br>客路青山外，行舟绿水前。<br>潮平两岸阔，风正一帆悬。<br><strong>海日生残夜，江春入旧年。</strong><br>乡书何处达，归雁洛阳边。</p><p>次：旅途中暂时停宿，这里是停泊的意思。</p><p><a href="http://m.xichuangzhu.com/work/57b95de92e958a005fa8919e" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b95de92e958a005fa8919e</a></p><hr><h3 id="2️⃣将赴吴兴登乐游原"><a href="#2️⃣将赴吴兴登乐游原" class="headerlink" title="2️⃣将赴吴兴登乐游原"></a>2️⃣将赴吴兴登乐游原</h3><p>[唐] 杜牧<br>清时有味是无能，闲爱孤云静爱僧。<br><strong>欲把一麾江海去，乐游原上望昭陵。</strong></p><p>无能：无所作为。</p><p><a href="http://m.xichuangzhu.com/work/57b99db9165abd005a6da742" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b99db9165abd005a6da742</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PRML第一章 绪论</title>
      <link href="/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E7%BB%AA%E8%AE%BA/"/>
      <url>/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E7%BB%AA%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<p>记录PRML学习过程。<br>笔记共享链接：<a href="https://1drv.ms/u/s!Apsp2510NHF6rIRjMclFB16v7B0FWg" target="_blank" rel="noopener">https://1drv.ms/u/s!Apsp2510NHF6rIRjMclFB16v7B0FWg</a></p><hr><h1 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h1><p><img src="/images/2018-09-23-Xnip2018-09-23_10-27-21.jpg" alt="概率论"></p><h1 id="决策论"><a href="#决策论" class="headerlink" title="决策论"></a>决策论</h1><p><img src="/images/2018-09-23-Xnip2018-09-23_10-36-52.jpg" alt="决策论"></p><h1 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h1><p><img src="/images/2018-09-23-Xnip2018-09-23_10-38-46.jpg" alt="信息论"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> PRML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 6:Backpropagation</title>
      <link href="/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%206:%20Backpropagation/"/>
      <url>/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%206:%20Backpropagation/</url>
      
        <content type="html"><![CDATA[<h3 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h3><p>基本公式<br><img src="/images/2018-09-23-15376684598125.jpg" width="50%" height="50%"><br><img src="/images/2018-09-23-15376684674095.jpg" width="50%" height="50%"></p><h3 id="forward-pass和backward-pass"><a href="#forward-pass和backward-pass" class="headerlink" title="forward pass和backward pass"></a>forward pass和backward pass</h3><p>可以将backpropagation分为两步</p><h4 id="forward-pass"><a href="#forward-pass" class="headerlink" title="forward pass"></a>forward pass</h4><p>在前向传播的时候提前计算/保存好，因为该梯度很简单<br><img src="/images/2018-09-23-15376686358832.jpg" width="50%" height="50%"></p><p>比如z对w1的梯度就是x1，就是和w1相连的项<br><img src="/images/2018-09-23-15376687025289.jpg" width="20%" height="50%"></p><h4 id="backward-pass"><a href="#backward-pass" class="headerlink" title="backward pass"></a>backward pass</h4><p>回传的时候逐层相乘下去，类似动态规划，获得了后一层的梯度才能求出前一层的梯度。<br><img src="/images/2018-09-23-15376687488493.jpg" width="50%" height="50%"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/images/2018-09-23-15376687824826.jpg" width="50%" height="50%"></p><p>先前向，提前算出最邻近的梯度，直到output layer，计算完该梯度，再不断回传逐层相乘获得output对各层的梯度。</p><h3 id="代码实现例子"><a href="#代码实现例子" class="headerlink" title="代码实现例子"></a>代码实现例子</h3><p>relu实现forward pass和backward pass<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return</span></span><br><span class="line"><span class="string">        a Tensor containing the output. ctx is a context object that can be used</span></span><br><span class="line"><span class="string">        to stash information for backward computation. You can cache arbitrary</span></span><br><span class="line"><span class="string">        objects for use in the backward pass using the ctx.save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ctx.save_for_backward(input)  <span class="comment">#为了之后的backward计算</span></span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Backpropagation </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 5:Classification:Logistic Regression</title>
      <link href="/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%205%20Classification:%20Logistic%20Regression/"/>
      <url>/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%205%20Classification:%20Logistic%20Regression/</url>
      
        <content type="html"><![CDATA[<h3 id="logistic-regression如何做？"><a href="#logistic-regression如何做？" class="headerlink" title="logistic regression如何做？"></a>logistic regression如何做？</h3><p>step1: 定义function set<br><img src="/images/2018-09-23-15376666391912.jpg" width="30%" height="50%"></p><p>step2: 更新<br>使用最大似然更新</p><script type="math/tex; mode=display">L(w,b)=f_{w,b}(x^1 )f_{w,b}(x^2 )(1−f_{w,b} (x^3 ))⋯f_{w,b} (x^N )</script><p>找到w，b使得L最大</p><p>对似然函数取负对数，则有：<br><img src="/images/2018-09-23-15376667791380.jpg" width="60%" height="50%"></p><p>将式子的每个元素写成伯努利分布形式：<br><img src="/images/2018-09-23-15376669013511.jpg" width="60%" height="50%"></p><p>上式就是cross-entropy损失函数。</p><p>求导该式子可得：<br><img src="/images/2018-09-23-15376669743224.jpg" width="30%" height="50%"><br>更新公式：<br><img src="/images/2018-09-23-15376669980138.jpg" width="40%" height="50%"><br>可以看出上式很直观：和答案差距越大，更新步伐越大。</p><p>同时发现上式和linear regression的更新公式是一致的。</p><h3 id="为什么不像linear-regression那样设loss为square？"><a href="#为什么不像linear-regression那样设loss为square？" class="headerlink" title="为什么不像linear regression那样设loss为square？"></a>为什么不像linear regression那样设loss为square？</h3><p>假设我们使用square loss，则求导得到的梯度：<br><img src="/images/2018-09-23-15376671202521.jpg" width="50%" height="50%"><br>上式可以看出，当接近target时，梯度小；远离target时，梯度也小。难以达到全局最小<br><img src="/images/2018-09-23-15376672527230.jpg" width="60%" height="50%"></p><p>下图是cross entropy和square error的图像示意：<br><img src="/images/2018-09-23-15376672892502.jpg" width="60%" height="50%"></p><p>如图，square loss难以到达全局最小。</p><h3 id="生成式模型与判别式模型的区别"><a href="#生成式模型与判别式模型的区别" class="headerlink" title="生成式模型与判别式模型的区别"></a>生成式模型与判别式模型的区别</h3><p>生成式对联合概率分布进行建模，再通过贝叶斯定理获得后验概率；而判别式模型直接对后验概率建模。<br><img src="/images/2018-09-23-15376674213503.jpg" width="60%" height="50%"><br>二者所定义的function set是一致的，但同一组数据可能会得到不同的w和b。</p><p>二者优劣对比：</p><ul><li>数据量多时，一般来说判别式模型会更好。因为判别式模型没有先验假设，完全依赖于数据。但如果数据有噪声，容易受影响。</li><li>生成式模型是有一定的假设的，当假设错误，会影响分类效果。</li><li>正因为有一定的先验假设，当数据量很少时，可能效果会不错；对于噪声更具有鲁棒性。</li><li>先验可以从其他数据源获得来帮助特定任务，如语音识别问题。</li></ul><h3 id="logistic的局限"><a href="#logistic的局限" class="headerlink" title="logistic的局限"></a>logistic的局限</h3><p>本质仍是一个线性分类器，没办法分类非线性的数据。<br>如何解决该问题?<br><strong>将logistic regression model拼接起来</strong>，前面的model对数据进行feature transformation，然后再对新的feature进行分类。<br><img src="/images/2018-09-23-15376677559470.jpg" width="70%" height="50%"></p><p>logistic与deep learning的联系：<br>如果将logistic regression的一个单元称为neuron，拼起来就是neural network了！！！</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Classification </tag>
            
            <tag> Logistic Regression </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识8</title>
      <link href="/2018/09/23/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%868/"/>
      <url>/2018/09/23/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%868/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>torch.max()有两种不同写法。<br>torch.max(input) → Tensor 返回其中最大的元素<br>torch.max(input, dim, keepdim=False, out=None) → (Tensor, LongTensor) 返回该维度上最大值，以及对应的index</p><hr><h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>将模型同时部署到多张卡上训练，本质就是将一个batch的数据split，送到各个model，然后合并结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model)</span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> rand_loader:</span><br><span class="line">    input = data.to(device)</span><br><span class="line">    output = model(input)</span><br></pre></td></tr></table></figure><hr><h3 id="3️⃣-求导"><a href="#3️⃣-求导" class="headerlink" title="3️⃣[求导]"></a>3️⃣[求导]</h3><p>标量、向量、矩阵之间的求导有两种布局，即分子布局和分母布局。分子布局和分母布局只差一个转置。<br>我的记法：在求导过程中，假设分母为m*n，分子为 k*n，则导数矩阵应该为 k*m 。一些特殊的如标量对矩阵求导等除外。<br>具体直接查表：<a href="https://en.m.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">https://en.m.wikipedia.org/wiki/Matrix_calculus</a></p><p>按位计算求导：<br>假设一个函数$f(x)$的输入是标量$x$。对于一组K个标量$x_1,··· ,x_K$，我们<br>可以通过$f(x)$得到另外一组K个标量$z_1,··· ,z_K$，<br>$z_k = f(x_k),∀k = 1,··· ,K$<br>其中，$f(x)$是按位运算的，即$[f(x)]_i = f(x_i)$<br>其导数是一个对角矩阵：<br><img src="/images/2018-09-23-15376727095200.jpg" width="50%" height="50%"></p><p><strong>Reference</strong>：<br><a href="https://en.m.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">https://en.m.wikipedia.org/wiki/Matrix_calculus</a><br><a href="https://blog.csdn.net/uncle_gy/article/details/78879131" target="_blank" rel="noopener">https://blog.csdn.net/uncle_gy/article/details/78879131</a><br><a href="https://nndl.github.io/chap-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.pdf" target="_blank" rel="noopener">https://nndl.github.io/chap-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.pdf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> 求导 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录7</title>
      <link href="/2018/09/23/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%957/"/>
      <url>/2018/09/23/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%957/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣softmax的numpy实现"><a href="#1️⃣softmax的numpy实现" class="headerlink" title="1️⃣softmax的numpy实现"></a>1️⃣softmax的numpy实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x,axis=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Compute softmax values for each sets of scores in x."""</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(x) / np.sum(np.exp(x), axis=axis)</span><br></pre></td></tr></table></figure><hr><h3 id="2️⃣numpy-手动求导relu"><a href="#2️⃣numpy-手动求导relu" class="headerlink" title="2️⃣numpy 手动求导relu"></a>2️⃣numpy 手动求导relu</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.dot(w1)</span><br><span class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.dot(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = np.square(y_pred - y).sum()</span><br><span class="line">    print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><hr><h3 id="3️⃣Pytorch实现relu"><a href="#3️⃣Pytorch实现relu" class="headerlink" title="3️⃣Pytorch实现relu"></a>3️⃣Pytorch实现relu</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return</span></span><br><span class="line"><span class="string">        a Tensor containing the output. ctx is a context object that can be used</span></span><br><span class="line"><span class="string">        to stash information for backward computation. You can cache arbitrary</span></span><br><span class="line"><span class="string">        objects for use in the backward pass using the ctx.save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ctx.save_for_backward(input)  <span class="comment">#为了之后的backward计算</span></span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br></pre></td></tr></table></figure><hr><h3 id="4️⃣Pytorch在多张卡上部署"><a href="#4️⃣Pytorch在多张卡上部署" class="headerlink" title="4️⃣Pytorch在多张卡上部署"></a>4️⃣Pytorch在多张卡上部署</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model)</span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> rand_loader:</span><br><span class="line">    input = data.to(device)</span><br><span class="line">    output = model(input)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>听达观杯现场答辩有感</title>
      <link href="/2018/09/19/%E8%A7%81%E9%97%BB&amp;%E6%83%B3%E6%B3%95/%E5%90%AC%E8%BE%BE%E8%A7%82%E6%9D%AF%E7%8E%B0%E5%9C%BA%E7%AD%94%E8%BE%A9%E6%9C%89%E6%84%9F/"/>
      <url>/2018/09/19/%E8%A7%81%E9%97%BB&amp;%E6%83%B3%E6%B3%95/%E5%90%AC%E8%BE%BE%E8%A7%82%E6%9D%AF%E7%8E%B0%E5%9C%BA%E7%AD%94%E8%BE%A9%E6%9C%89%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>前几日（周日）去了达观杯答辩现场听了前10名做了报告，有了一些感想，但一直没有抽出时间写一下自己的感想（懒）。</p><p>自己大概花了十来天做了一下比赛，实际上也就是一个文本分类的<a href="http://www.dcjingsai.com/common/cmpt/%E2%80%9C%E8%BE%BE%E8%A7%82%E6%9D%AF%E2%80%9D%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98%E8%B5%9B_%E8%B5%9B%E4%BD%93%E4%B8%8E%E6%95%B0%E6%8D%AE.html" target="_blank" rel="noopener">比赛</a>，因为没有比赛经验的缘故，走了很多弯路。不过也学到了一些东西。</p><p>现记录前十名的一些idea/trick：</p><ul><li>数据增强<ul><li>因为给的句子长度很长，因此在做截断的时候后面的就没法训练到了，可以将文本倒序作为新的数据训练模型。可以充分利用到数据</li><li>将数据打乱、随机删除，实际上就是对一个句子的词进行sample再组合</li><li>打乱词序以增加数据量</li><li>使用pseudo labeling，但有的队伍使用这个做出效果了，但有的没有</li></ul></li><li>特征工程<ul><li>假设开头中间结尾的信息对分类有帮助，因此截取该部分信息做训练</li><li>改进baseline的tfidf的特征工程方法，使用基于熵的词权重计算</li><li>降维，留下最重要的特征。先用卡方分布降到20万，再用SVD降到8000</li><li>将word2vec和GloVe拼接起来作为deep learning模型的输入</li><li>将文章分段，每段取前20后20拼起来</li></ul></li><li>模型融合<br>  所有队伍都无一例外使用了模型融合，stacking或者简单的投票<ul><li>DL+ML —&gt; lgbm model —&gt; voting</li><li>深度模型+传统模型，在深度模型最后一层加入传统模型的信息/feature</li><li>后向选择剔除冗余模型</li></ul></li><li>DL&amp;其他<ul><li>HAN，选择10个attention vector</li><li>对易错类增加权重，通过改变损失函数来增加权重</li><li>CNN, [1,2,3,4,5,6]*600</li><li>提出新的模型（第一名）</li></ul></li></ul><p>其实除了一些trick，我还是有些失望的，因为都是用模型融合堆出来的，这也让我对比赛失去了一些兴趣。虽然能理解现在的比赛都是这样的，但感觉实在太暴力了。<br>当然，其中还是有一些亮点的，有一支队伍立意很高，从理解业务的角度出发而不是堆模型，也取得了很好的效果；还有一个使用了最新论文中的特征工程改进方法，令我耳目一新；以及第一名在比赛过程中提出来三个新的模型。</p><p>Anyway，我目前还是太菜了，还是安心搞科研吧。_(:з」∠)</p>]]></content>
      
      
      
        <tags>
            
            <tag> 有感 </tag>
            
            <tag> 达观杯 </tag>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识7</title>
      <link href="/2018/09/16/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%867/"/>
      <url>/2018/09/16/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%867/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>只有一个元素的tensor，可用.item()来获取元素</p><p>tensor &lt;—&gt; numpy 相互转化会共享内部数据，因此改变其中一个会改变另一个</p><p>可用使用 .to 来移动到设备<br><img src="/images/2018-09-16-15370671350548.jpg" alt=""></p><p>.detech()  detach it from the computation history, and to prevent future computation from being tracked. 将其从计算图中分离，变为叶子节点，并且requires_grad=False</p><p>Function 记录了这个tensor是怎么来的，所有的tensor都有，除非是用户自定义的：<br><img src="/images/2018-09-16-15370672806253.jpg" width="65%" height="50%"></p><hr><h3 id="2️⃣-协方差"><a href="#2️⃣-协方差" class="headerlink" title="2️⃣[协方差]"></a>2️⃣[协方差]</h3><p>关于协方差的理解，x与y关于某个自变量的变化程度，即度量了x与y之间的联系。<br><a href="https://www.zhihu.com/question/20852004" target="_blank" rel="noopener">https://www.zhihu.com/question/20852004</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> 协方差 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 4:Classification:Probabilistic Generative Model</title>
      <link href="/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%204%20Classification%20%20Probabilistic%20Generative%20Model/"/>
      <url>/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%204%20Classification%20%20Probabilistic%20Generative%20Model/</url>
      
        <content type="html"><![CDATA[<h3 id="为什么不使用regression来分类？"><a href="#为什么不使用regression来分类？" class="headerlink" title="为什么不使用regression来分类？"></a>为什么不使用regression来分类？</h3><p>1️⃣如果使用regression的思想来分类，会对离边界较远的点进行惩罚：<br><img src="/images/2018-09-16-15370662150910.jpg" width="70%" height="50%"></p><p>2️⃣如果多分类使用regression，如class 1, class 2, class 3；则隐式地假设了class 1 和 class 2较为接近，如果没有这种接近关系，则分类会不正确。</p><h3 id="问题描述与定义"><a href="#问题描述与定义" class="headerlink" title="问题描述与定义"></a>问题描述与定义</h3><p><img src="/images/2018-09-16-15370662762802.jpg" width="70%" height="50%"></p><p>当P大于0.5则是C1类，反之是C2类<br>先验P(C1)和P(C2)都好计算，计算C1占总的比例即可<br>因此，我们需要计算的就是p(x|C)</p><p>这一想法，本质是得到了生成式模型：<br><img src="/images/2018-09-16-15370663099515.jpg" width="70%" height="50%"></p><h3 id="原理概述"><a href="#原理概述" class="headerlink" title="原理概述"></a>原理概述</h3><p>现<strong>假设训练数据点的分布服从高斯分布</strong>：（显然可以自己设任何分布）<br>即数据从高斯分布采样得到：<br><img src="/images/2018-09-16-15370663870205.jpg" width="55%" height="50%"></p><p>根据最大似然估计，可以获得每个类别的μ和Σ：<br><img src="/images/2018-09-16-15370664041246.jpg" width="70%" height="50%"></p><p>得到了参数后，即可代入得到P(C|x) ：<br><img src="/images/2018-09-16-15370664355955.jpg" width="80%" height="50%"></p><p>刚刚假设$Σ$对于不同类别不同，现我们<strong>令不同类别共享相同$Σ$</strong>：<br>（因为协方差代表的是不同feature之间的联系，可以认为是和类别无关的）</p><p>$Σ$的计算公式是加权求和：<br><img src="/images/2018-09-16-15370665362081.jpg" width="24%" height="50%"></p><p>在使用了相同的协方差矩阵后，边界就是线性的（后面会提到为什么是这样）：<br><img src="/images/2018-09-16-15370665553962.jpg" alt=""></p><p> 总结：<br> 三步走，定义function set，计算μ和协方差矩阵，得到best function：<br><img src="/images/2018-09-16-15370665888683.jpg" width="60%" height="50%"></p><p>注意到，如果我们认为，不同feature之间没有关系，每个feature符合特定的高斯分布，则该分类器则是朴素贝叶斯分类器：<br><img src="/images/2018-09-16-15370666092459.jpg" width="60%" height="50%"></p><h3 id="分类与logistics-regression"><a href="#分类与logistics-regression" class="headerlink" title="分类与logistics regression"></a>分类与logistics regression</h3><p>现推导，该分类问题与logistics regression之间的联系：<br>即：<br><img src="/images/2018-09-16-15370666567324.jpg" width="60%" height="50%"></p><h4 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h4><p>数据服从高斯分布，共享$Σ$</p><h4 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h4><p>①总框架：<br><img src="/images/2018-09-16-15370667000974.jpg" width="50%" height="50%"></p><p>令<br><img src="/images/2018-09-16-15370667135868.jpg" width="24%" height="50%"></p><p>则有：<br><img src="/images/2018-09-16-15370667376312.jpg" width="50%" height="50%"></p><p>②z的进一步推导与简化：<br><img src="/images/2018-09-16-15370667582564.jpg" width="30%" height="50%"></p><p>将z展开：<br><img src="/images/2018-09-16-15370667763267.jpg" width="50%" height="50%"></p><p>而第一部分有：<br><img src="/images/2018-09-16-15370667880942.jpg" width="50%" height="50%"></p><p>第一部分相除，有：<br><img src="/images/2018-09-16-15370668274551.jpg" width="50%" height="50%"></p><p>再进行展开，有：<br><img src="/images/2018-09-16-15370668394919.jpg" width="50%" height="50%"></p><p>最终z的公式为：<br><img src="/images/2018-09-16-15370668522531.jpg" width="50%" height="50%"></p><p>由于共享协方差矩阵，则可以消去部分，得到：<br><img src="/images/2018-09-16-15370668957564.jpg" width="50%" height="50%"></p><p>替换成w和b：<br><img src="/images/2018-09-16-15370669103562.jpg" width="50%" height="50%"></p><p>③最终，将z带回到原式：<br><img src="/images/2018-09-16-15370669221472.jpg" width="25%" height="50%"></p><p>所以我们不需要再估计N1,N2,μ和Σ，直接计算w和b即可。也因此，分界线是线性的。</p><p>全过程：<br><img src="/images/2018-09-16-15370669594744.jpg" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Classification </tag>
            
            <tag> Probabilistic Generative Model </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 3:Gradient Descent</title>
      <link href="/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%203%20Gradient%20Descent/"/>
      <url>/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%203%20Gradient%20Descent/</url>
      
        <content type="html"><![CDATA[<h2 id="Gradient-Descent-tips"><a href="#Gradient-Descent-tips" class="headerlink" title="Gradient Descent tips"></a>Gradient Descent tips</h2><h3 id="tip-1：Adaptive-Learning-Rates"><a href="#tip-1：Adaptive-Learning-Rates" class="headerlink" title="tip 1：Adaptive Learning Rates"></a>tip 1：Adaptive Learning Rates</h3><h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><h5 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h5><p><img src="/images/2018-09-16-15370654467771.jpg" width="30%" height="50%"></p><p><img src="/images/2018-09-16-15370654502774.jpg" width="20%" height="50%"></p><p>其中σ是之前所有的梯度的平方根<br><img src="/images/2018-09-16-15370654728457.jpg" width="30%" height="50%"></p><p>化简形式：<br><img src="/images/2018-09-16-15370654853172.jpg" width="50%" height="50%"></p><h5 id="为什么要怎么做？"><a href="#为什么要怎么做？" class="headerlink" title="为什么要怎么做？"></a>为什么要怎么做？</h5><p>考虑一个开口向上的二次函数<br><img src="/images/2018-09-16-15370655091732.jpg" width="50%" height="50%"></p><p>也即，最好的步长是一次导除以二次导，但二次导计算量大，因此使用近似的方式：<br><strong>对一次导作多次的sample</strong>。<br>下图显示，如果二次导小，那么多次sample获得的一次导也小，反之则大，也就是说，一次导在某种程度上可以反映二次导的大小，所以直接用一次导近似，可以减少计算量。</p><p><img src="/images/2018-09-16-15370655850876.jpg" width="50%" height="50%"></p><h3 id="tip-2：feature-scaling"><a href="#tip-2：feature-scaling" class="headerlink" title="tip 2：feature scaling"></a>tip 2：feature scaling</h3><p><img src="/images/2018-09-16-15370656836724.jpg" width="50%" height="50%"></p><p>能够改变loss的分布，上图1中w2对loss的影响较大，则较陡峭，参数更新就较困难，需要adaptive learning rate；如果进行feature scaling，能够更好达到local optimal</p><h2 id="Gradient-Descent-Theory"><a href="#Gradient-Descent-Theory" class="headerlink" title="Gradient Descent Theory"></a>Gradient Descent Theory</h2><p>另一种角度看gradient descent：</p><p>基本思想：<br>我们希望每一次都在当前点附近找到一个最小的点，即在一个范围内：<br><img src="/images/2018-09-16-15370657785697.jpg" width="40%" height="50%"></p><p>应该如何找到该最小点？</p><p>我们知道，泰勒级数的形式：<br><img src="/images/2018-09-16-15370658066669.jpg" width="50%" height="50%"></p><p>当x接近x0时，会有如下近似：<br><img src="/images/2018-09-16-15370658167935.jpg" width="30%" height="50%"></p><p>推广到多元泰勒级数则有：<br><img src="/images/2018-09-16-15370658315314.jpg" width="60%" height="50%"></p><p>那么，如前所述，x接近x0，对于图中，即圆圈足够小时：<br><img src="/images/2018-09-16-15370658494091.jpg" width="50%" height="50%"></p><p>简化符号：<br><img src="/images/2018-09-16-15370658736683.jpg" width="12%" height="50%"></p><p><img src="/images/2018-09-16-15370658623258.jpg" width="30%" height="50%"></p><p>所以可以简写成：<br><img src="/images/2018-09-16-15370658855882.jpg" width="30%" height="50%"></p><p>由于s,u,v都是常数，在圆圈范围内寻找最小值对应的参数可以简化成：<br><img src="/images/2018-09-16-15370658981519.jpg" width="40%" height="50%"></p><p><img src="/images/2018-09-16-15370659061025.jpg" width="30%" height="50%"></p><p>再度简化，可以表达成：<br><img src="/images/2018-09-16-15370659680601.jpg" width="40%" height="50%"></p><p><img src="/images/2018-09-16-15370659747339.jpg" width="30%" height="50%"></p><p>在图中可以画为两个向量的点积<br><img src="/images/2018-09-16-15370660126195.jpg" width="40%" height="50%"></p><p>显然，当反方向时，最小：<br><img src="/images/2018-09-16-15370660243469.jpg" width="40%" height="50%"></p><p>也即：<br><img src="/images/2018-09-16-15370660628961.jpg" width="50%" height="50%"></p><p>最终完整的式子：<br><img src="/images/2018-09-16-15370660794436.jpg" width="55%" height="50%"></p><p>因此，当learning rate不够小时，是不满足泰勒级数近似的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Gradient Descent </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 2:Bias and Variance</title>
      <link href="/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%202%20Bias%20and%20Variance/"/>
      <url>/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%202%20Bias%20and%20Variance/</url>
      
        <content type="html"><![CDATA[<h3 id="如何理解bias-amp-variance"><a href="#如何理解bias-amp-variance" class="headerlink" title="如何理解bias&amp;variance"></a>如何理解bias&amp;variance</h3><p><img src="/images/2018-09-16-15370650140053.jpg" width="40%" height="50%"><br>bias是function space中心离optimal model的差距，variance是某次实验所得模型离function space中心的距离。</p><p>比如说，简单地模型的function space小，随机性小，因此variance小，但也因为function space小，表示能力有限，因此bias大。</p><p>如图：<br><img src="/images/2018-09-16-15370651353167.jpg" width="70%" height="50%"><br>该图中蓝色圈代表模型所能表达的范围。</p><h3 id="如何解决variance大的问题"><a href="#如何解决variance大的问题" class="headerlink" title="如何解决variance大的问题"></a>如何解决variance大的问题</h3><p>①更多的data<br>②regularization：强迫function更平滑，因此减小variance，但因为调整了function space，可能会增加bias。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> bias&amp;variance </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词9</title>
      <link href="/2018/09/16/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D9/"/>
      <url>/2018/09/16/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D9/</url>
      
        <content type="html"><![CDATA[<h3 id="白雪歌送武判官归京"><a href="#白雪歌送武判官归京" class="headerlink" title="白雪歌送武判官归京"></a>白雪歌送武判官归京</h3><p>[唐] 岑参<br>北风卷地白草折，胡天八月即飞雪。<br><strong>忽如一夜春风来，千树万树梨花开</strong>。<br>散入珠帘湿罗幕，狐裘不暖锦衾薄。<br>将军角弓不得控，都护铁衣冷难着。<br>瀚海阑干百丈冰，愁云惨淡万里凝。<br>中军置酒饮归客，胡琴琵琶与羌笛。<br>纷纷暮雪下辕门，风掣红旗冻不翻。<br><strong>轮台东门送君去，去时雪满天山路。<br>山回路转不见君，雪上空留马行处。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290</a></p><hr><h3 id="绝命诗"><a href="#绝命诗" class="headerlink" title="绝命诗"></a>绝命诗</h3><p>谭嗣同<br>望门投止思张俭，<br>忍死须臾待杜根。<br><strong>我自横刀向天笑，<br>去留肝胆两昆仑！</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pytorch backward()浅析</title>
      <link href="/2018/09/16/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Pytorch%20backward()%E6%B5%85%E6%9E%90/"/>
      <url>/2018/09/16/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Pytorch%20backward()%E6%B5%85%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>最近在看pytorch文档的时候，看到backward内有一个参数gradient，在经过查阅了相关资料和进行了实验后，对backward有了更深的认识。</p><h2 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h2><p>1️⃣如果调用backward的是一个标量，如：<code>loss.backward()</code><br>则gradient不需要手动传入，会自动求导。<br>例子:<br>$a=[x_1,x_2],b=\frac{x_1+x_2}{2}$<br>则b对a求导，有：<br>$\dfrac {\partial b}{\partial x_{1}}=\frac{1}{2}，\dfrac {\partial b}{\partial x_{2}}=\frac{1}{2}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a=torch.Tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">a.requires_grad=<span class="keyword">True</span></span><br><span class="line">b=torch.mean(a)  <span class="comment">#tensor(2.5000, grad_fn=&lt;MeanBackward1&gt;)</span></span><br><span class="line">b.backward()</span><br><span class="line">a.grad   <span class="comment">#tensor([0.5000, 0.5000])</span></span><br></pre></td></tr></table></figure><p>gradient此时只是在缩放原grad的大小，也即不指定gradient和gradient=1是等价的</p><p>当然，也可以指定gradient，其中指定gradient的shape必须和b的维度相同<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gradient=torch.tensor(<span class="number">10.0</span>)</span><br><span class="line">b.backward(gradient)</span><br><span class="line">a.grad   <span class="comment">#tensor([5., 5.])</span></span><br></pre></td></tr></table></figure></p><p>2️⃣如果调用backward的是一个向量<br>例子：<br>$a=[x_1,x_2],b=[b_1,b_2]$, 其中 $b_1=x_1+x_2,b_2=x_1*x_2$<br>b对a求导，有：<br>$\dfrac {\partial b_1}{\partial x_{1}}=1,\dfrac {\partial b_1}{\partial x_{2}}=1$</p><p>$\dfrac {\partial b_2}{\partial x_{1}}=x_2,\dfrac {\partial b_2}{\partial x_{2}}=x_1$</p><p>在backward的时候则必须指定gradient。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a=torch.FloatTensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">a.requires_grad=<span class="keyword">True</span></span><br><span class="line">b=torch.zeros(<span class="number">2</span>)</span><br><span class="line">b[<span class="number">0</span>]=a[<span class="number">0</span>]+a[<span class="number">1</span>]</span><br><span class="line">b[<span class="number">1</span>]=a[<span class="number">0</span>]*a[<span class="number">1</span>]    <span class="comment"># b=tensor([5., 6.], grad_fn=&lt;CopySlices&gt;)</span></span><br><span class="line">gradient=torch.tensor([<span class="number">1.0</span>,<span class="number">0.0</span>])</span><br><span class="line">b.backward(gradient,retain_graph=<span class="keyword">True</span>)</span><br><span class="line">a.grad   <span class="comment">#tensor([1., 1.])，说明是对b_1进行求导</span></span><br><span class="line">a.grad.zero_()  <span class="comment">#将梯度清空，否则会叠加</span></span><br><span class="line"><span class="comment">#-------------- #</span></span><br><span class="line">gradient=torch.tensor([<span class="number">0.0</span>,<span class="number">1.0</span>])</span><br><span class="line">b.backward(gradient,retain_graph=<span class="keyword">True</span>)</span><br><span class="line">a.grad  <span class="comment"># tensor([3., 2.])，说明对b_2进行求导</span></span><br><span class="line">a.grad.zero_()</span><br><span class="line"><span class="comment"># ------------- #</span></span><br><span class="line">gradient=torch.tensor([<span class="number">1.0</span>,<span class="number">1.0</span>])</span><br><span class="line">b.backward(gradient,retain_graph=<span class="keyword">True</span>)</span><br><span class="line">a.grad   <span class="comment"># tensor([4., 3.])，即b_1,b_2的导数的叠加</span></span><br><span class="line">a.grad.zero_()</span><br></pre></td></tr></table></figure><p>注意到b.backward()时需要retain_graph设为True，否则在计算完后会自动释放计算图的内存，这样就没法进行二次反向传播了。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.pytorchtutorial.com/pytorch-backward/" target="_blank" rel="noopener">https://www.pytorchtutorial.com/pytorch-backward/</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> backward </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词8</title>
      <link href="/2018/09/09/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D8/"/>
      <url>/2018/09/09/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D8/</url>
      
        <content type="html"><![CDATA[<h3 id="望月怀远"><a href="#望月怀远" class="headerlink" title="望月怀远"></a>望月怀远</h3><p>[唐] 张九龄<br><strong>海上生明月，天涯共此时。</strong><br>情人怨遥夜，竟夕起相思。<br>灭烛怜光满，披衣觉露滋。<br>不堪盈手赠，还寝梦佳期。</p><p>遥夜，长夜。</p><p><a href="http://m.xichuangzhu.com/work/57aca120a341310060e2a09f" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57aca120a341310060e2a09f</a></p><hr><h3 id="无题"><a href="#无题" class="headerlink" title="无题"></a>无题</h3><p>萨镇冰<br>五十七载犹如梦，举国沦亡缘汉城。<br><strong>龙游浅水勿自弃，终有扬眉吐气天。</strong></p><p>1951年，中国人民志愿军在抗美援朝战争第三次战役后打进了汉城，萨镇冰得知此事，回想起57年前的甲午悲歌，当即作诗一首。</p><hr><h3 id="白雪歌送武判官归京"><a href="#白雪歌送武判官归京" class="headerlink" title="白雪歌送武判官归京"></a>白雪歌送武判官归京</h3><p>[唐] 岑参<br>北风卷地白草折，胡天八月即飞雪。<br><strong>忽如一夜春风来，千树万树梨花开。</strong><br>散入珠帘湿罗幕，狐裘不暖锦衾薄。<br>将军角弓不得控，都护铁衣冷难着。<br>瀚海阑干百丈冰，愁云惨淡万里凝。<br>中军置酒饮归客，胡琴琵琶与羌笛。<br>纷纷暮雪下辕门，风掣红旗冻不翻。<br><strong>轮台东门送君去，去时雪满天山路。<br>山回路转不见君，雪上空留马行处</strong>。</p><p><a href="http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词7</title>
      <link href="/2018/09/02/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D7/"/>
      <url>/2018/09/02/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D7/</url>
      
        <content type="html"><![CDATA[<h3 id="滕王阁序"><a href="#滕王阁序" class="headerlink" title="滕王阁序"></a>滕王阁序</h3><p>遥襟甫畅，逸兴遄飞。爽籁发而清风生，纤歌凝而白云遏。睢园绿竹，气凌彭泽之樽；邺水朱华，光照临川之笔。四美具，二难并。穷睇眄于中天，极娱游于暇日。天高地迥，觉宇宙之无穷；兴尽悲来，识盈虚之有数。望长安于日下，目吴会于云间。地势极而南溟深，天柱高而北辰远。关山难越，谁悲失路之人；萍水相逢，尽是他乡之客。怀帝阍而不见，奉宣室以何年？</p><hr><p><strong>注释：</strong><br>遥襟甫畅，逸兴遄（chuán）飞：登高望远的胸怀顿时舒畅，飘欲脱俗的兴致油然而生。</p><p>爽籁（lài）发而清风生，纤歌凝而白云遏：宴会上，排箫响起，好像清风拂来；柔美的歌声缭绕不散，遏止了白云飞动。爽：形容籁的发音清脆。籁：排箫，一种由多根竹管编排而成的管乐器。</p><p>睢（suī）园绿竹，气凌彭泽之樽：今日的宴会，好比当年睢园竹林的聚会，在座的文人雅士，豪爽善饮的气概超过了陶渊明。睢园：西汉梁孝王在睢水旁修建的竹园，他常和一些文人在此饮酒赋诗。</p><p>邺（yè）水朱华，光照临川之笔：这是借诗人曹植、谢灵运来比拟参加宴会的文人。邺：今河北临漳，是曹魏兴起的地方。曹植曾在这里作过《公宴诗》，诗中有“朱华冒绿池”的句子。临川之笔：指谢灵运，他曾任临川（今属江西）内史。</p><p>四美：指良辰、美景、赏心、乐事。</p><p>二难：贤主、嘉宾。</p><p>地势极而南溟深，天柱高而北辰远：地势偏远，南海深邃；天柱高耸，北极星远悬。</p><p>帝阍（hūn）：原指天帝的守门者。这里指皇帝的宫门。</p><p>奉宣室以何年：什么时候才能像贾谊那样去侍奉君王呢</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识6</title>
      <link href="/2018/09/02/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%866/"/>
      <url>/2018/09/02/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%866/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-dropout"><a href="#1️⃣-dropout" class="headerlink" title="1️⃣[dropout]"></a>1️⃣[dropout]</h3><p>dropout形式:<br><img src="/images/2018-09-02-15358857481798.jpg" width="70%" height="50%"><br>RNN的形式有多种：</p><ul><li><p>recurrent dropout<br>RNN: $h_t=f(W_h ⊙ [x_t,h_{t-1}]+b_h)$<br>加上dropout的RNN：$h_t=f(W_h ⊙ [x_t,d(h_{t-1})]+b_h)$，其中$d(\cdot)$为dropout函数<br>同理：<br>LSTM:$c_t=f_t ⊙c_{t-1} + i_t ⊙ d(g_t)$<br>GRU:$h_t=(1-z_t)⊙c_{t-1}+z_t⊙d(g_t)$</p></li><li><p>垂直连接的dropout<br>dropout的作用即是否允许L层某个LSTM单元的隐状态信息流入L+1层对应单元。<br><img src="/images/2018-09-02-15358866404870.jpg" width="50%" height="50%"></p></li></ul><p>Reference:<br><a href="https://blog.csdn.net/falianghuang/article/details/72910161" target="_blank" rel="noopener">https://blog.csdn.net/falianghuang/article/details/72910161</a></p><hr><h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>pack_padded_sequence用于RNN中，将padding矩阵压缩:<br><img src="/images/2018-09-02-15358868858836.jpg" width="60%" height="50%"><br>这样就可以实现在RNN传输过程中短句提前结束。</p><p>pad_packed_sequence是pack_padded_sequence的逆运算。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> dropout </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>我没有说话</title>
      <link href="/2018/08/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%88%91%E6%B2%A1%E6%9C%89%E8%AF%B4%E8%AF%9D/"/>
      <url>/2018/08/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%88%91%E6%B2%A1%E6%9C%89%E8%AF%B4%E8%AF%9D/</url>
      
        <content type="html"><![CDATA[<p>《我没有说话》</p><p>纳粹杀共产党时，<br>我没有出声<br>——因为我不是共产党员；<br>接着他们迫害犹太人，<br>我没有出声<br>——因为我不是犹太人；<br>然后他们杀工会成员，<br>我没有出声<br>——因为我不是工会成员；<br>后来他们迫害天主教徒，<br>我没有出声<br>——因为我是新教徒；<br>最后当他们开始对付我的时候，<br>已经没有人能站出来为我发声了</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Deep Learning NLP best practices笔记</title>
      <link href="/2018/08/26/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Deep%20Learning%20NLP%20best%20practices%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/08/26/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Deep%20Learning%20NLP%20best%20practices%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>博客地址：<a href="http://ruder.io/deep-learning-nlp-best-practices/index.html" target="_blank" rel="noopener">http://ruder.io/deep-learning-nlp-best-practices/index.html</a><br>个人觉得这篇文章写得很好，有许多实践得到的经验，通过这篇可以避免走一些弯路。</p><h2 id="Practices"><a href="#Practices" class="headerlink" title="Practices"></a>Practices</h2><h3 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h3><blockquote><p>The optimal dimensionality of word embeddings is mostly task-dependent: a smaller dimensionality works better for more syntactic tasks such as named entity recognition or part-of-speech (POS) tagging, while a larger dimensionality is more useful for more semantic tasks such as sentiment analysis.</p></blockquote><p>对于偏向语法的，使用维度低一些的词向量；而对于偏向语义内容的，使用维度大一些的词向量，如情感分析。</p><h3 id="LSTM-Depth"><a href="#LSTM-Depth" class="headerlink" title="LSTM Depth"></a>LSTM Depth</h3><blockquote><p>performance improvements of making the model deeper than 2 layers are minimal </p></blockquote><p>LSTM深度最好不要超过两层。</p><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><blockquote><p>It is often thought that Adam clearly outperforms vanilla stochastic gradient descent (SGD). However, while it converges much faster than SGD, it has been observed that SGD with learning rate annealing slightly outperforms Adam. Recent work furthermore shows that SGD with properly tuned momentum outperforms Adam .</p></blockquote><p>Adam可以更早拟合，而SGD效果可能会更好一些。</p><p>可以采用优化策略，比如说使用Adam训练直到拟合，然后将学习率减半，并重新导入之前训练好的最好的模型。这样Adam能够忘记之前的信息并重新开始训练。</p><blockquote><p>Denkowski &amp; Neubig (2017) show that Adam with 2 restarts and learning rate annealing is faster and performs better than SGD with annealing</p></blockquote><h3 id="Ensembling"><a href="#Ensembling" class="headerlink" title="Ensembling"></a>Ensembling</h3><blockquote><p>Combining multiple models into an ensemble by averaging their predictions is a proven strategy to improve model performance.</p></blockquote><p>Ensembling很重要的一点是需要保证多样性：</p><blockquote><p>Ensembling is an important way to ensure that results are still reliable if the diversity of the evaluated models increases (Denkowski &amp; Neubig, 2017). While ensembling different checkpoints of a model has been shown to be effective (Jean et al., 2015; Sennrich et al., 2016) [51, 52], it comes at the cost of model diversity. Cyclical learning rates can help to mitigate this effect</p></blockquote><h3 id="LSTM-tricks"><a href="#LSTM-tricks" class="headerlink" title="LSTM tricks"></a>LSTM tricks</h3><ul><li><p>在initial state中我们常常使用全0向量，实际上可以将其作为参数学习。</p><blockquote><p>Instead of fixing the initial state, we can learn it like any other parameter, which can improve performance</p></blockquote></li><li><p>将input和output embedding的参数共享，如果是做language model或者机器翻译之类的，可以让他们共享。</p></li><li><p>Gradient Norm Clipping</p><blockquote><p>Rather than clipping each gradient independently, clipping the global norm of the gradient yields more significant improvements</p></blockquote></li></ul><p>这点我没看懂。</p><h3 id="Classification-practices"><a href="#Classification-practices" class="headerlink" title="Classification practices"></a>Classification practices</h3><p>关于CNN</p><blockquote><p>CNN filters:Combining filter sizes near the optimal filter size, e.g. (3,4,5) performs best (Kim, 2014; Kim et al., 2016). The optimal number of feature maps is in the range of 50-600 (Zhang &amp; Wallace, 2015) [59].</p><p>Aggregation function:1-max-pooling outperforms average-pooling and k-max pooling (Zhang &amp; Wallace, 2015).</p></blockquote><p>这在我之前的关于CNN文本分类指南中有更详尽的分析。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>这是一篇干货满满的博客，实际上我还是有许多地方没有读懂，这适合多看几遍，慢慢理解。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 指南 </tag>
            
            <tag> 调参 </tag>
            
            <tag> NLP🤖 </tag>
            
            <tag> 笔记📒 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识5</title>
      <link href="/2018/08/26/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%865/"/>
      <url>/2018/08/26/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%865/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[Paper]<br>Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components</p><p>基本框架和CBOW一致，主要贡献在于针对中文词向量添加了偏旁、字的组件作为训练信息。</p><p><img src="/images/2018-08-26-15352530482345.jpg" width="50%" height="50%"></p><hr><p>2️⃣[Paper]<br>Highway Networks</p><p>为了解决神经网络深度过深时导致的反向传播困难的问题。<br>前向传播的公式：</p><script type="math/tex; mode=display">y=H(x,W_H)</script><p>而论文所做的改进：</p><script type="math/tex; mode=display">y=H(x,W_H) \cdot T(x,W_T)+ x \cdot C(x,W_C)</script><p>其中$T$是transform gate，$C$是carry gate。方便起见，可以将 $C=1-T$，最终有：</p><script type="math/tex; mode=display">y=H(x,W_H) \cdot T(x,W_T)+ x \cdot (1-T(x,W_T))</script><p>可以看出思想和LSTM很类似，都是gate的思想。</p><hr><p>3️⃣[调参方法]<br>博客：<a href="https://blog.goodaudience.com/how-to-make-your-model-happy-again-part-1-40d94a9ffb41" target="_blank" rel="noopener">https://blog.goodaudience.com/how-to-make-your-model-happy-again-part-1-40d94a9ffb41</a></p><ul><li><strong>学习率</strong>：</li></ul><p>一条原则：当validation loss开始上升时，减少学习率。</p><p>如何减少？</p><p><img src="/images/2018-08-26-15352871548330.jpg" width="50%" height="50%"></p><p>或者：</p><p><img src="/images/2018-08-26-15352873283890.jpg" width="50%" height="50%"><br>设定一定的epoch作为一个stepsize，在训练过程中线性增加学习率，然后在到达最大值后再线性减小。<br>实验表明，使用该方法可以在一半的epoch内达到相同的效果。</p><ul><li><strong>batch size</strong>：</li></ul><p><img src="/images/2018-08-26-15352891425729.jpg" width="50%" height="50%"></p><p>由于batch size和学习率的强相关性，<a href="https://arxiv.org/pdf/1711.00489.pdf" target="_blank" rel="noopener">相关论文</a>提出提高batch size而不是降低学习率的方法来提升模型表现。</p><blockquote><p>increasing the batch size during training, instead of decaying learning rate. — L. Smith<br><a href="https://arxiv.org/pdf/1711.00489.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1711.00489.pdf</a></p></blockquote><p>一个trick：保持学习率不变，提高batch size，直到batch size~训练集/10，接下来再采用学习率下降的策略。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Paper </tag>
            
            <tag> 调参方法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录6</title>
      <link href="/2018/08/26/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB6/"/>
      <url>/2018/08/26/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB6/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣将数据整理成batch"><a href="#1️⃣将数据整理成batch" class="headerlink" title="1️⃣将数据整理成batch"></a>1️⃣将数据整理成batch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_iter_batch</span><span class="params">(paras,labels,batch_size,shuffle=True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param paras:</span></span><br><span class="line"><span class="string">    :param labels:</span></span><br><span class="line"><span class="string">    :param batch_size:</span></span><br><span class="line"><span class="string">    :param shuffle:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">assert</span> len(paras)==len(labels)</span><br><span class="line">    paras_size=len(paras)</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        indices=np.arange(paras_size)</span><br><span class="line">        np.random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> start_idx <span class="keyword">in</span> range(<span class="number">0</span>,paras_size-batch_size+<span class="number">1</span>,batch_size):</span><br><span class="line">        <span class="keyword">if</span> shuffle:</span><br><span class="line">            excerpt=indices[start_idx:start_idx+batch_size]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            excerpt=slice(start_idx,start_idx+batch_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> paras[excerpt],labels[excerpt]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词6</title>
      <link href="/2018/08/26/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D6/"/>
      <url>/2018/08/26/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D6/</url>
      
        <content type="html"><![CDATA[<p>1️⃣</p><h3 id="戏为六绝句"><a href="#戏为六绝句" class="headerlink" title="戏为六绝句"></a>戏为六绝句</h3><p>[唐] 杜甫<br>【其二】<br>王杨卢骆当时体，轻薄为文哂未休。<br><strong>尔曹身与名俱灭，不废江河万古流</strong>。</p><p>哂（shěn）：讥笑。</p><p><a href="http://m.xichuangzhu.com/work/57b9658c0a2b58005c95d2a7" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b9658c0a2b58005c95d2a7</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CNN文本分类任务指南</title>
      <link href="/2018/08/25/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/CNN%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%8C%87%E5%8D%97/"/>
      <url>/2018/08/25/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/CNN%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p>最近因为比赛的缘故对文本分类有一定的了解。其中使用CNN方法做情感分析任务存在着许多优势。虽然模型简单，但如何设置超参有时候对结果有很大的影响。本文记录了关于CNN文本分类的一些学习历程和指南，基本参考了论文。</p><h2 id="做法"><a href="#做法" class="headerlink" title="做法"></a>做法</h2><p>基本上目前较为浅层的CNN文本分类的做法都是如下图：<br><img src="/images/2018-08-25-15351860103617.jpg" alt=""></p><p>将词向量堆积成为二维的矩阵，通过CNN的卷积单元对矩阵进行卷积处理，同时使用pooling（通常是1max-pooling）操作，将不等长的卷积结果变为等长，对不同的卷积单元的结果进行拼接后生成单个向量，最后再通过线性层转化成类别概率分布。</p><p>另一张图也说明了该流程。</p><p><img src="/images/2018-08-25-15351863867337.jpg" alt=""></p><h2 id="建议与指导"><a href="#建议与指导" class="headerlink" title="建议与指导"></a>建议与指导</h2><h3 id="超参及其对结果的影响"><a href="#超参及其对结果的影响" class="headerlink" title="超参及其对结果的影响"></a>超参及其对结果的影响</h3><p>接下来的内容参考了论文<a href="https://arxiv.org/pdf/1510.03820.pdf" target="_blank" rel="noopener">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional<br>Neural Networks for Sentence Classification</a></p><p>CNN文本分类的超参：</p><ul><li>输入向量</li><li>卷积大小</li><li>输出通道（feature maps）</li><li>激活函数</li><li>池化策略</li><li>正则化</li></ul><h4 id="输入向量的影响"><a href="#输入向量的影响" class="headerlink" title="输入向量的影响"></a>输入向量的影响</h4><p>实验表明，使用word2vec和GloVe不分伯仲，但将word2vec和GloVe简单拼接在一起并不能带来提升。</p><blockquote><p>unfortunately, simply concatenating these representations does necessarily seem helpful</p></blockquote><p>当句子长度很长（document classification）时，使用one-hot可能会有效果，但在句子长度不是很长时，效果不好。</p><h5 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h5><p>对于新任务，可以word2vec或GloVe或者其他词向量都试一下，如果句子长，可以试着使用one-hot。</p><h4 id="卷积大小"><a href="#卷积大小" class="headerlink" title="卷积大小"></a>卷积大小</h4><p>由于卷积的长度是固定的，也就是词向量的长度，因此只需讨论宽度。<br>实验表明，不同的数据集会有不同的最佳大小，但似乎对于长度越长的句子，最佳大小有越大的趋势。</p><blockquote><p>However, for datasets comprising longer sentences, such as CR (maximum sentence length is 105, whereas it ranges from 36-56 on the other sentiment datasets used here), the optimal region size may be larger.</p></blockquote><p>同时，当增加不同卷积大小作为组合时，如果组合的卷积核大小接近于最佳大小（optimal region size），有助于结果的提升；相反，如果卷积核大小离最佳大小很远时，反而会产生负面影响。</p><h5 id="建议-1"><a href="#建议-1" class="headerlink" title="建议"></a>建议</h5><p>首先试着找到最优的卷积核大小，然后在这个基础上添加和该卷积核大小类似的卷积核。</p><h4 id="feature-maps"><a href="#feature-maps" class="headerlink" title="feature maps"></a>feature maps</h4><p>也就是输出通道（out channel），表明该卷积核大小的卷积核有多少个。</p><p>实验表明，最佳的feature maps和数据集相关，但一般不超过600。</p><blockquote><p>it would seem that increasing the number of maps beyond 600 yields at best very marginal returns, and often hurts performance.</p></blockquote><h5 id="建议-2"><a href="#建议-2" class="headerlink" title="建议"></a>建议</h5><p>在600内搜索最优，如果在600的边缘还没有明显的效果下降，那么可以尝试大于600的feature maps。</p><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>实验结果：<br><img src="/images/2018-08-25-15351889835594.jpg" alt=""></p><p>结果表明，tanh、ReLU和不使用激活函数效果较好。tanh的优点是以0为中心，ReLU能够加速拟合，至于为什么不使用的效果会好，可能是因为模型较为简单：</p><blockquote><p>This indicates that on some datasets, a linear transformation is enough to capture the<br>correlation between the word embedding and the output label.</p></blockquote><h5 id="建议-3"><a href="#建议-3" class="headerlink" title="建议"></a>建议</h5><p>使用tanh、ReLU或者干脆不使用。但如果模型更为复杂，有多层的结构，还是需要使用激活函数的。</p><h4 id="pooling策略"><a href="#pooling策略" class="headerlink" title="pooling策略"></a>pooling策略</h4><p>所有的实验都表明了，1-max pooling的效果比其他好，如k-max pooling。在pooling这一步可以直接选择1-max pooling。</p><blockquote><p>This may be because the location of predictive contexts does not matter, and certain n-grams in the sentence can be more predictive on their own than the entire sentence considered jointly.</p></blockquote><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>主要是dropout和l2 norm constraint。<br>dropout就是随机将一些神经元置为0，l2 norm constraint是对参数矩阵W进行整体缩放，使其不超过一定阈值。（与通常的l2 regularization不同，最早可追溯到Hinton的<a href="https://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="noopener">Improving neural networks by preventing<br>co-adaptation of feature detectors</a>）</p><blockquote><p>the l2 norm of a weight vector is linearly scaled to a constraint c when it exceeds this threshold, so a smaller c implies stronger regularization</p></blockquote><p>实验表明，dropout起的作用很小，l2 norm没有提升甚至还会导致下降。可能是因为模型参数不多，因此过拟合的可能性较低。</p><h5 id="建议-4"><a href="#建议-4" class="headerlink" title="建议"></a>建议</h5><p>设置较小的dropout和较大的l2 norm，当feature maps增大时，可以试着调节较大的dropout以避免过拟合。</p><h3 id="建议及结论"><a href="#建议及结论" class="headerlink" title="建议及结论"></a>建议及结论</h3><ul><li>刚开始的使用使用word2vec或者GloVe，如果数据量够大，可以尝试one-hot</li><li>线性搜索最佳的卷积核大小，如果句子够长，那么可以扩大搜索范围。一旦确定了最佳卷积核大小，尝试在该卷积核大小的附近进行组合，如最佳卷积核宽度是5，那么尝试[3,4,5]或者[2,3,4,5]等</li><li>使用较小的dropout和较大的max norm constraint，然后在[100,600]范围内搜索feature maps，如果最佳的feature maps在600附近，可以试着选择比600更大的范围</li><li>尝试不同的激活函数，通常tanh和ReLU是较好的，但也可以尝试什么都不加。</li><li>使用1-max pooling。</li><li>如果模型复杂，比如feature maps很大，那么可以尝试更为严格的正则化，如更大的dropout rate和较小的max norm constraint。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="http://www.aclweb.org/anthology/D14-1181" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a></p><p><a href="https://arxiv.org/pdf/1510.03820.pdf" target="_blank" rel="noopener">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional<br>Neural Networks for Sentence Classification</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> 情感分析 </tag>
            
            <tag> 指南 </tag>
            
            <tag> 调参 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Pytorch中的inplace的操作</title>
      <link href="/2018/08/20/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADin-place%E7%9A%84%E6%93%8D%E4%BD%9C/"/>
      <url>/2018/08/20/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADin-place%E7%9A%84%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>最近在写Hierarchical attention network的时候遇到了如下的bug：</p><blockquote><p>one of the variables needed for gradient computation has been modified by an inplace operation</p></blockquote><p>在查阅了文档和请教了其他人之后，最终找到了bug。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(seq_len):</span><br><span class="line">    h_i = rnn_outputs[i]  <span class="comment"># batch,hidden*2</span></span><br><span class="line">    a_i = attn_weights[i].unsqueeze_(<span class="number">1</span>)  <span class="comment"># take in-place opt may cause an error</span></span><br><span class="line">    a_i = a_i.expand_as(h_i)  <span class="comment"># batch,hidden*2</span></span><br></pre></td></tr></table></figure><p>这是我原来的逻辑，我在无意中做了inplace操作，导致了bug的发生。正确的做法应该是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(seq_len):</span><br><span class="line">    h_i = rnn_outputs[i]  <span class="comment"># batch,hidden*2</span></span><br><span class="line">    <span class="comment"># a_i = attn_weights[i].unsqueeze_(1)  # take in-place opt may cause an error</span></span><br><span class="line">    a_i = attn_weights[i].unsqueeze(<span class="number">1</span>)  <span class="comment"># batch,1</span></span><br><span class="line">    a_i = a_i.expand_as(h_i)  <span class="comment"># batch,hidden*2</span></span><br></pre></td></tr></table></figure><p>实际上，在实践过程中应当尽量避免inplace操作，在官方文档中也提到了（存疑）这点，虽然提供了inplace操作，但并不推荐使用。</p><p>具体的原因是，在Pytorch构建计算图的过程中，会记录每个节点是怎么来的，但inplace会破坏这种关系，使得在回传的时候没法正常求导。</p><p>特别地，有两种情况不应该使用inplace操作（摘自知乎）：</p><ol><li>对于requires_grad=True的叶子张量(leaf tensor)不能使用inplace operation</li><li>对于在求梯度阶段需要用到的张量不能使用inplace operation</li></ol><p>Reference:<br><a href="https://zhuanlan.zhihu.com/p/38475183" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38475183</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>愿中国青年都摆脱冷气</title>
      <link href="/2018/08/19/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%84%BF%E4%B8%AD%E5%9B%BD%E9%9D%92%E5%B9%B4%E9%83%BD%E6%91%86%E8%84%B1%E5%86%B7%E6%B0%94/"/>
      <url>/2018/08/19/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%84%BF%E4%B8%AD%E5%9B%BD%E9%9D%92%E5%B9%B4%E9%83%BD%E6%91%86%E8%84%B1%E5%86%B7%E6%B0%94/</url>
      
        <content type="html"><![CDATA[<p>近期的新闻常让人感到愤怒以致绝望…</p><hr><p>愿中国青年都摆脱冷气，只是向上走，不必听自暴自弃者流的话。能做事的做事，能发声的发声。有一分热，发一分光。就令萤火一般，也可以在黑暗里发一点光，不必等候炬火。</p><p>—鲁迅《热风》</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录5</title>
      <link href="/2018/08/19/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB5/"/>
      <url>/2018/08/19/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB5/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣sklearn模型的保存与恢复"><a href="#1️⃣sklearn模型的保存与恢复" class="headerlink" title="1️⃣sklearn模型的保存与恢复"></a>1️⃣sklearn模型的保存与恢复</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">clf = svm.SVC()</span><br><span class="line">clf.fit(X, y)  </span><br><span class="line">clf.fit(train_X,train_y)</span><br><span class="line">joblib.dump(clf, <span class="string">"train_model.m"</span>)</span><br><span class="line">clf = joblib.load(<span class="string">"train_model.m"</span>)</span><br><span class="line">clf.predit(test_X)</span><br></pre></td></tr></table></figure><hr><h3 id="2️⃣Dictionary类"><a href="#2️⃣Dictionary类" class="headerlink" title="2️⃣Dictionary类"></a>2️⃣Dictionary类</h3><p>在构造字典时需要用到<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dictionary</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.word2idx = &#123;&#125;</span><br><span class="line">        self.idx2word = []</span><br><span class="line">        self.__vocab_size = <span class="number">0</span></span><br><span class="line">        self.add_word(<span class="string">'&lt;pad&gt;'</span>)</span><br><span class="line">        self.add_word(<span class="string">'&lt;UNK&gt;'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_word</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.word2idx:</span><br><span class="line">            self.idx2word.append(word)</span><br><span class="line">            self.word2idx[word] = self.__vocab_size</span><br><span class="line">            self.__vocab_size += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_index</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> self.word2idx:</span><br><span class="line">            <span class="keyword">return</span> self.word2idx[word]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.word2idx[<span class="string">'&lt;UNK&gt;'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.idx2word[idx]</span><br></pre></td></tr></table></figure></p><hr><h3 id="3️⃣对dict按元素排序的三种方法"><a href="#3️⃣对dict按元素排序的三种方法" class="headerlink" title="3️⃣对dict按元素排序的三种方法"></a>3️⃣对dict按元素排序的三种方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">d=&#123;<span class="string">'apple'</span>:<span class="number">10</span>,<span class="string">'orange'</span>:<span class="number">20</span>,<span class="string">'banana'</span>:<span class="number">5</span>,<span class="string">'watermelon'</span>:<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#法1</span></span><br><span class="line">print(sorted(d.items(),key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])) <span class="comment">#[('watermelon', 1), ('banana', 5), ('apple', 10), ('orange', 20)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#法2</span></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"></span><br><span class="line">print(sorted(d.items(),key=itemgetter(<span class="number">1</span>))) <span class="comment">#[('watermelon', 1), ('banana', 5), ('apple', 10), ('orange', 20)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#法3</span></span><br><span class="line"></span><br><span class="line">print(sorted(d,key=d.get))  <span class="comment">#['watermelon', 'banana', 'apple', 'orange'] 没有value了</span></span><br></pre></td></tr></table></figure><hr><h3 id="4️⃣合并dict的三种方法"><a href="#4️⃣合并dict的三种方法" class="headerlink" title="4️⃣合并dict的三种方法"></a>4️⃣合并dict的三种方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d1=&#123;<span class="string">'a'</span>:<span class="number">1</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d2=&#123;<span class="string">'b'</span>:<span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#法1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d=&#123;**d1,**d2&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d</span><br><span class="line">&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#法2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dd=dict(d1.items()|d2.items())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dd</span><br><span class="line">&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#法3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d1.update(d2)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d1</span><br><span class="line">&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="5️⃣找到list最大最小值的index"><a href="#5️⃣找到list最大最小值的index" class="headerlink" title="5️⃣找到list最大最小值的index"></a>5️⃣找到list最大最小值的index</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">lst = [<span class="number">40</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minIndex</span><span class="params">(lst)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> min(range(len(lst)),key=lst.__getitem__)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxIndex</span><span class="params">(lst)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> max(range(len(lst)),key=lst.__getitem__)</span><br><span class="line">    </span><br><span class="line">print(minIndex(lst))</span><br><span class="line">print(maxIndex(lst))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Pytorch中的Embedding padding</title>
      <link href="/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADEmbedding%E7%9A%84padding/"/>
      <url>/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADEmbedding%E7%9A%84padding/</url>
      
        <content type="html"><![CDATA[<p>在Pytorch中，nn.Embedding()代表embedding矩阵，其中有一个参数<code>padding_idx</code>指定用以padding的索引位置。所谓padding，就是在将不等长的句子组成一个batch时，对那些空缺的位置补0，以形成一个统一的矩阵。</p><p>用法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.embedding = nn.Embedding(vocab_size, embed_dim,padding_idx=<span class="number">0</span>) <span class="comment">#也可以是别的数值</span></span><br></pre></td></tr></table></figure></p><p>在显式设定<code>padding_idx=0</code>后，在自定义的词典内也应当在相应位置添加<code>&lt;pad&gt;</code>作为一个词。如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dictionary</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.word2idx = &#123;&#125;</span><br><span class="line">        self.idx2word = []</span><br><span class="line">        self.__vocab_size = <span class="number">0</span></span><br><span class="line">        self.add_word(<span class="string">'&lt;pad&gt;'</span>)  <span class="comment"># should add &lt;pad&gt; first</span></span><br><span class="line">        self.add_word(<span class="string">'&lt;UNK&gt;'</span>)</span><br></pre></td></tr></table></figure><p>那么对于<code>padding_idx</code>，内部是如何操作的呢？</p><p>在查看了Embedding的源码后，发现设置了<code>padding_idx</code>，类内部会有如下操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-----Embedding __init__ 内部--------------</span></span><br><span class="line"><span class="keyword">if</span> _weight <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">    self.weight = Parameter(torch.Tensor(num_embeddings, embedding_dim))</span><br><span class="line">    self.reset_parameters()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#---------reset_parameters()--------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.weight.data.normal_(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> self.padding_idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        self.weight.data[self.padding_idx].fill_(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>也就是说，当Embedding是随机初始化的矩阵时，会对<code>padding_idx</code>所在的行进行填0。保证了padding行为的正确性。</p><p>那么，还需要保证一个问题，就是在反向回传的时候，<code>padding_idx</code>是不会更新的.</p><p>在查看了源码后发现在Embedding类内有如下注释：</p><blockquote><p>.. note::<br>        With :attr:<code>padding_idx</code> set, the embedding vector at<br>        :attr:<code>padding_idx</code> is initialized to all zeros. However, note that this<br>        vector can be modified afterwards, e.g., using a customized<br>        initialization method, and thus changing the vector used to pad the<br>        output. The gradient for this vector from :class:<code>~torch.nn.Embedding</code><br>        is always zero.</p></blockquote><p>并且在查阅了其他资料后，发现该行确实会不更新。有意思的是，查阅源码并没有找到如何使其不更新的机制，因为在F.embedding函数中，返回：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)</span><br></pre></td></tr></table></figure><p>但我并不能跳转到torch.embedding中，大概是因为这部分被隐藏了吧。我也没有再深究下去。我猜测有可能是在autograd内部有对该部分进行单独的处理，用mask屏蔽这部分的更新；或者一个更简单的方法，就是任其更新，但每一次都reset，将第一行手动设为全0。</p><p><strong>附记</strong>：</p><p>假如说没有显式设置该行，是否padding就没有效果呢？<br>我认为是的。</p><p>一般来说，我们都是以0作为padding的填充，如：</p><div class="table-container"><table><thead><tr><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>12</td><td>44</td><td>22</td><td>67</td><td>85</td></tr><tr><td>12</td><td>13</td><td>534</td><td>31</td><td>0</td></tr><tr><td>87</td><td>23</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div><p>每一行代表一个句子，其中0作为填充。然后将该矩阵送入到embedding_lookup中，获得三维的tensor，那么0填充的部分，所获得的embedding表示应当是要全0。</p><p>假如不显式设置<code>padding_idx=0</code>，就可能会出现两个结果（个人推测)：</p><p>①本应该全0的地方，被词典中第一个词的词向量表示给替代了，因为将0作为索引去embedding矩阵获取到的词向量，就是第一个词的词向量，而该词并不全0。</p><p>②词典的最后一个词被全0覆盖。F.embedding中有如下片段：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> padding_idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">    <span class="keyword">if</span> padding_idx &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">assert</span> padding_idx &lt; weight.size(<span class="number">0</span>), <span class="string">'Padding_idx must be within num_embeddings'</span></span><br><span class="line">    <span class="keyword">elif</span> padding_idx &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">assert</span> padding_idx &gt;= -weight.size(<span class="number">0</span>), <span class="string">'Padding_idx must be within num_embeddings'</span></span><br><span class="line">        padding_idx = weight.size(<span class="number">0</span>) + padding_idx</span><br><span class="line"><span class="keyword">elif</span> padding_idx <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        padding_idx = <span class="number">-1</span></span><br></pre></td></tr></table></figure><p>上面片段显示，<code>padding_idx</code>被设置为-1，也就是最后一个单词。做完这步紧接着就返回：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)</span><br></pre></td></tr></table></figure><p>还是由于torch.embedding无法查看的原因，我不知道内部是如何实现的，但应该来说，最后一个词就是被覆盖了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Embedding </tag>
            
            <tag> padding </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python Tricks[转]</title>
      <link href="/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Python%20Tricks%5B%E8%BD%AC%5D/"/>
      <url>/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Python%20Tricks%5B%E8%BD%AC%5D/</url>
      
        <content type="html"><![CDATA[<p>原文地址:<a href="https://hackernoon.com/python-tricks-101-2836251922e0" target="_blank" rel="noopener">https://hackernoon.com/python-tricks-101-2836251922e0</a></p><p>我觉得这个介绍Python一些tricks的文章很好，能够更加熟悉Python的一些非常方便的用法。<br>以下是我觉得有用的几个点。</p><p>1️⃣Reverse a String/List</p><p><img src="/images/2018-08-19-15346465152976.jpg" width="70%" height="50%"></p><p><img src="/images/2018-08-19-15346467215597.jpg" width="70%" height="50%"></p><p>[::-1]解释：<br>[:]表示取所有的元素，-1表示步进。[1:5:2]表示的就是从元素1到元素5，每2个距离取一个。</p><hr><p>2️⃣transpose 2d array</p><p><img src="/images/2018-08-19-15346470165919.jpg" width="70%" height="50%"></p><p>zip()相当于压缩，zip(*)相当于解压。</p><hr><p>3️⃣Chained function call</p><p><img src="/images/2018-08-19-15346471756442.jpg" width="70%" height="50%"></p><p>非常简洁的写法。</p><hr><p>4️⃣Copy List</p><p><img src="/images/2018-08-19-15346472744350.jpg" width="50%" height="50%"></p><p>之前谈过的Python的赋值、浅拷贝、深拷贝。</p><hr><p>5️⃣Dictionary get</p><p><img src="/images/2018-08-19-15346473929918.jpg" width="70%" height="50%"></p><p>避免了dict不存在该元素的问题。</p><hr><p>6️⃣✨Sort Dictionary by Value</p><p><img src="/images/2018-08-19-15346475170316.jpg" width="90%" height="50%"></p><p>其中第三种返回的是[‘watermelon’, ‘banana’, ‘apple’, ‘orange’]，没有value了。</p><hr><p>7️⃣For…else</p><p><img src="/images/2018-08-19-15346481408714.jpg" width="90%" height="50%"></p><p>注意到如果for在中途break了，就不会进入到else了；只有顺利循环完才会进入到else。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> e <span class="keyword">in</span> a:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">if</span> e==<span class="number">0</span>:</span><br><span class="line"><span class="meta">... </span>            <span class="keyword">break</span></span><br><span class="line"><span class="meta">... </span><span class="keyword">else</span>:</span><br><span class="line"><span class="meta">... </span>    print(<span class="string">'hello'</span>)</span><br><span class="line"><span class="meta">... </span><span class="comment">#什么都没有print</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> e <span class="keyword">in</span> a:</span><br><span class="line"><span class="meta">... </span>    print(e)</span><br><span class="line"><span class="meta">... </span><span class="keyword">else</span>:</span><br><span class="line"><span class="meta">... </span>    print(<span class="string">'hello'</span>)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line">hello</span><br></pre></td></tr></table></figure><hr><p>8️⃣Merge dict’s</p><p><img src="/images/2018-08-19-15346483785515.jpg" width="90%" height="50%"></p><p>合并dict的方法。</p><hr><p>9️⃣Min and Max index in List</p><p><img src="/images/2018-08-19-15346487918895.jpg" width="80%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> Python tricks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识4</title>
      <link href="/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%864/"/>
      <url>/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%864/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[概率校准(Probability Calibration)]<br>一种对机器学习算法输出结果的校准，通过几个实验可以发现，概率校准能够一定程度提高表现。<br>几个参考资料：<br>直观理解:  <a href="http://www.bubuko.com/infodetail-2133893.html" target="_blank" rel="noopener">http://www.bubuko.com/infodetail-2133893.html</a><br>SVC的概率校准在sklearn上的应用: <a href="https://blog.csdn.net/ericcchen/article/details/79337716" target="_blank" rel="noopener">https://blog.csdn.net/ericcchen/article/details/79337716</a><br>✨完全手册: <a href="http://users.dsic.upv.es/~flip/papers/BFHRHandbook2010.pdf" target="_blank" rel="noopener">Calibration of Machine Learning Models</a></p><hr><p>2️⃣[Paper]<br><a href="https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf" target="_blank" rel="noopener">Hierarchical Attention Networks for Document Classification</a></p><p>亮点在使用层次的RNN结构，以及使用了attention方法。<br><img src="/images/2018-08-19-15346447273228.jpg" width="50%" height="50%"></p><p>参考了其他人的代码自己也试着实现了一个，GitHub地址：<a href="https://github.com/linzehui/pytorch-hierarchical-attention-network" target="_blank" rel="noopener">https://github.com/linzehui/pytorch-hierarchical-attention-network</a></p><hr><p>3️⃣[XGBoost]<br>kaggle神器XGBoost，一篇原理的详细介绍：<br><a href="http://www.cnblogs.com/willnote/p/6801496.html" target="_blank" rel="noopener">http://www.cnblogs.com/willnote/p/6801496.html</a><br>虽然还是有好些地方没搞懂，有必要从头学起。</p><hr><p>4️⃣[Python]<br>关于函数列表中单星号(*)和双星号(**)<br>单星号：</p><ul><li>代表接收任意多个非关键字参数，将其转换成元组：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one</span><span class="params">(a,*b)</span>:</span></span><br><span class="line">    <span class="string">"""a是一个普通传入参数，*b是一个非关键字星号参数"""</span></span><br><span class="line">    print(b)</span><br><span class="line">one(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)  <span class="comment">#输出：(2, 3, 4, 5, 6)</span></span><br></pre></td></tr></table></figure><ul><li>对一个普通变量使用单星号，表示对该变量拆分成单个元素</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(a,b)</span>:</span></span><br><span class="line">    print(a,b)</span><br><span class="line">l=[<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">fun(*l)  <span class="comment">#输出 1,2</span></span><br></pre></td></tr></table></figure><p>双星号：</p><ul><li>获得字典值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two</span><span class="params">(a=<span class="number">1</span>,**b)</span>:</span></span><br><span class="line">    <span class="string">"""a是一个普通关键字参数，**b是一个关键字双星号参数"""</span></span><br><span class="line">    print(b)</span><br><span class="line">two(a=<span class="number">1</span>,b=<span class="number">2</span>,c=<span class="number">3</span>,d=<span class="number">4</span>,e=<span class="number">5</span>,f=<span class="number">6</span>)  <span class="comment">#输出&#123;'b': 2, 'c': 3, 'e': 5, 'f': 6, 'd': 4&#125;</span></span><br></pre></td></tr></table></figure><hr><p>5️⃣[Pytorch]<br>在Pytorch中，只要一个tensor的requires_grad是true，那么两个tensor的加减乘除后的结果的requires_grad也会是true。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Paper </tag>
            
            <tag> 概率校准 </tag>
            
            <tag> Probability Calibration </tag>
            
            <tag> HAN </tag>
            
            <tag> XGBoost </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词5</title>
      <link href="/2018/08/19/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D5/"/>
      <url>/2018/08/19/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D5/</url>
      
        <content type="html"><![CDATA[<p>本周太忙了，没背什么诗词，只背（复习）了部分的《滕王阁序》。</p><p>1️⃣</p><h3 id="滕王阁序"><a href="#滕王阁序" class="headerlink" title="滕王阁序"></a>滕王阁序</h3><p>嗟乎！时运不齐，命途多舛。冯唐易老，李广难封。屈贾谊于长沙，非无圣主；窜梁鸿于海曲，岂乏明时？所赖<strong>君子见机，达人知命</strong>。老当益壮，宁移白首之心？<strong>穷且益坚，不坠青云之志</strong>。酌贪泉而觉爽，处涸辙以犹欢。<strong>北海虽赊，扶摇可接；东隅已逝，桑榆非晚。</strong>孟尝高洁，空馀报国之情；阮籍猖狂，岂效穷途之哭！</p><p>勃，三尺微命，一介书生。无路请缨，等终军之弱冠；有怀投笔，慕宗慤之长风。舍簪笏于百龄，奉晨昏于万里。非谢家之宝树，接孟氏之芳邻。他日趋庭，叨陪鲤对；今兹捧袂，喜托龙门。杨意不逢，抚凌云而自惜；锺期既遇，奏流水以何惭？</p><hr><p><strong>注释：</strong><br>冯唐：西汉人，有才能却一直不受重用。汉武帝时选求贤良，有人举荐冯唐，可是他已九十多岁，难再做官了。李广：汉武帝时的名将，多年抗击匈奴，军功很大，却终身没有封侯。</p><p>贾谊：汉文帝本想任贾谊为公卿，但因朝中权贵反对，就疏远了贾谊，任他为长沙王太傅。梁鸿：东汉人，因作诗讽刺君王，得罪了汉章帝，被迫逃到齐鲁一带躲避。</p><p>酌（zhuó）贪泉而觉爽：喝下贪泉的水，仍觉得心境清爽。古代传说广州有水名贪泉，人喝了这里的水就会变得贪婪。这句是说有德行的人在污浊的环境中也能保持纯正，不被污染。处涸辙以犹欢：处在奄奄待毙的时候，仍然乐观开朗。处河辙：原指鲋鱼处在干涸的车辙旦。比喻人陷入危急之中。</p><p>孟尝：东汉人，为官清正贤能，但不被重用，后来归田。阮籍：三国魏诗人，他有时独自驾车出行，到无路处便恸哭而返，借此宣泄不满于现实的苦闷心情。</p><p>终军：《汉书·终军传》记载，汉武帝想让南越（今广东、广西一带）王归顺，派终军前往劝说，终军请求给他长缨，必缚住南越王，带回到皇宫门前（意思是一定完成使命）。后来用“请缨”指投军报国。</p><p>宗悫（què）：南朝宋人，少年时很有抱负，说“愿乘长风破万里浪”。</p><p>簪（zān）笏（hù）：这里代指官职。晨昏：晨昏定省，出自 《礼记·曲礼上》，释义为旧时侍奉父母的日常礼节。</p><p>非谢家之宝树，接孟氏之芳邻：自己并不是像谢玄那样出色的人才，却能在今日的宴会上结识各位名士。谢家之宝树：指谢玄。《晋书·谢玄传》记载，晋朝谢安曾问子侄们：为什么人们总希望自己的子弟好？侄子谢玄回答：“譬如芝兰玉树，欲使其生于庭阶耳。”后来就称谢玄为谢家宝树。孟氏之芳邻：这里借孟子的母亲为寻找邻居而三次搬家的故事，来指赴宴的嘉宾。</p><p>他日趋庭，叨陪鲤对：过些时候自己将到父亲那里陪侍和聆听教诲。趋庭：快步走过庭院，这是表示对长辈的恭敬。叨：惭愧地承受，表示自谦。鲤对：孔鲤是孔子的儿子，鲤对指接受父亲教诲。事见《论语·季氏》：（孔子）尝独立，（孔）鲤趋而过庭。（子）曰：“学诗乎？”对曰：“未也。”“不学诗，无以言。”鲤退而学诗。他日，又独立，鲤趋而过庭。（子）曰：“学礼乎？”对曰：‘未也。”“不学礼，无以立。”鲤退而学礼。</p><p>捧袂（mèi）：举起双袖作揖，指谒见阎公。喜托龙门：（受到阎公的接待）十分高兴，好像登上龙门一样。</p><p>杨意：即蜀人杨得意，任掌管天子猎犬的官，西汉辞赋家司马相如是由他推荐给汉武帝的。凌云：这里指司马相如的赋，《史记·司马相如传》说，相如献《大人赋》，“天子大悦，飘飘有凌云之气，似游天地之间”。钟期：即钟子期。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python中的拷贝</title>
      <link href="/2018/08/18/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Python%E4%B8%AD%E7%9A%84%E6%8B%B7%E8%B4%9D/"/>
      <url>/2018/08/18/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Python%E4%B8%AD%E7%9A%84%E6%8B%B7%E8%B4%9D/</url>
      
        <content type="html"><![CDATA[<p>Python的拷贝和C/C++的差别很大，很经常就容易搞混，因此记录一下。</p><h3 id="赋值、拷贝"><a href="#赋值、拷贝" class="headerlink" title="赋值、拷贝"></a>赋值、拷贝</h3><ul><li>赋值：实际上就是对象的引用，没有开辟新的内存空间<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lst=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">l=lst</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>浅拷贝:创建了新对象，但是<strong>内容是对原对象的引用</strong>，有三种形式</p><ol><li><p>切片  </p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">l=lst[:]</span><br><span class="line">l=[i <span class="keyword">for</span> i <span class="keyword">in</span> lst]</span><br></pre></td></tr></table></figure></li><li><p>工厂</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l=list(lst)</span><br></pre></td></tr></table></figure></li><li><p>copy </p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">l=copy.copy(lst)</span><br></pre></td></tr></table></figure></li></ol></li><li><p>深拷贝:copy中的deepcopy，生成一个全新的对象，与原来的对象无关</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">l=copy.deepcopy(lst)</span><br></pre></td></tr></table></figure></li></ul><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 引用https://www.cnblogs.com/huangbiquan/p/7795152.html 的例子###</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> copy</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,[<span class="string">'a'</span>,<span class="string">'b'</span>]] <span class="comment">#定义一个列表a</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a <span class="comment">#赋值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = copy.copy(a) <span class="comment">#浅拷贝</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = copy.deepcopy(a) <span class="comment">#深拷贝</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.append(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(a)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>], <span class="number">5</span>] <span class="comment">#a添加一个元素5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(b) </span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>], <span class="number">5</span>] <span class="comment">#b跟着添加一个元素5 </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(c)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>]] <span class="comment">#c保持不变</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(d)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>]] <span class="comment">#d保持不变</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">4</span>].append(<span class="string">'c'</span>) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(a)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>], <span class="number">5</span>] <span class="comment">#a中的list(即a[4])添加一个元素c</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(b)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>], <span class="number">5</span>] <span class="comment">#b跟着添加一个元素c</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(c)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]] <span class="comment">#c跟着添加一个元素c</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(d)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>]] <span class="comment">#d保持不变</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#说明如下：</span></span><br><span class="line"><span class="comment">#1.外层添加元素时， 浅拷贝c不会随原列表a变化而变化；内层list添加元素时，浅拷贝c才会变化。</span></span><br><span class="line"><span class="comment">#2.无论原列表a如何变化，深拷贝d都保持不变。</span></span><br><span class="line"><span class="comment">#3.赋值对象随着原列表一起变化</span></span><br></pre></td></tr></table></figure><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.cnblogs.com/huangbiquan/p/7795152.html" target="_blank" rel="noopener">https://www.cnblogs.com/huangbiquan/p/7795152.html</a><br><a href="https://www.cnblogs.com/xueli/p/4952063.html" target="_blank" rel="noopener">https://www.cnblogs.com/xueli/p/4952063.html</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> 拷贝 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何将ELMo词向量用于中文</title>
      <link href="/2018/08/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E5%B0%86ELMo%E8%AF%8D%E5%90%91%E9%87%8F%E7%94%A8%E4%BA%8E%E4%B8%AD%E6%96%87/"/>
      <url>/2018/08/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E5%B0%86ELMo%E8%AF%8D%E5%90%91%E9%87%8F%E7%94%A8%E4%BA%8E%E4%B8%AD%E6%96%87/</url>
      
        <content type="html"><![CDATA[<p>10.10更新：ELMo已经由哈工大组用PyTorch重写了，并且提供了中文的预训练好的language model，可以直接使用。</p><hr><p>ELMo于今年二月由AllenNLP提出，与word2vec或GloVe不同的是其动态词向量的思想，其本质即通过训练language model，对于一句话进入到language model获得不同的词向量。根据实验可得，使用了Elmo词向量之后，许多NLP任务都有了大幅的提高。</p><p>论文:<a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">Deep contextualized word representations</a></p><p>AllenNLP一共release了两份ELMo的代码，一份是Pytorch版本的，另一份是Tensorflow版本的。Pytorch版本的只开放了使用预训练好的词向量的接口，但没有给出自己训练的接口，因此无法使用到中文语料中。Tensorflow版本有提供训练的代码，因此本文记录如何将ELMo用于中文语料中，但本文只记录使用到的部分，而不会分析全部的代码。</p><p>需求:<br>使用预训练好的词向量作为句子表示直接传入到RNN中(也就是不使用代码中默认的先过CNN)，在训练完后，将模型保存，在需要用的时候load进来，对于一个特定的句子，首先将其转换成预训练的词向量，传入language model之后最终得到ELMo词向量。</p><p>准备工作:</p><ol><li>将中文语料分词</li><li>训练好GloVe词向量或者word2vec</li><li>下载<a href="https://github.com/allenai/bilm-tf" target="_blank" rel="noopener">bilm-tf代码</a></li><li>生成词表 vocab_file （训练的时候要用到）</li><li>optional:阅读Readme</li><li>optional:通读bilm-tf的代码，对代码结构有一定的认识</li></ol><p>思路:</p><ol><li>将预训练的词向量读入</li><li>修改bilm-tf代码<ol><li>option部分</li><li>添加给embedding weight赋初值</li><li>添加保存embedding weight的代码</li></ol></li><li>开始训练，获得checkpoint和option文件</li><li>运行脚本，获得language model的weight文件</li><li>将embedding weight保存为hdf5文件形式</li><li>运行脚本，将语料转化成ELMo embedding。</li></ol><h3 id="训练GloVe或word2vec"><a href="#训练GloVe或word2vec" class="headerlink" title="训练GloVe或word2vec"></a>训练GloVe或word2vec</h3><p>可参见我以前的博客或者网上的教程。<br>注意到，如果要用gensim导入GloVe训好的词向量，需要在开头添加num_word embedding_dim。 如：<br><img src="/images/2018-08-10-15338861462682.jpg" width="70%" height="50%"></p><h3 id="获得vocab词表文件"><a href="#获得vocab词表文件" class="headerlink" title="获得vocab词表文件"></a>获得vocab词表文件</h3><p>注意到，词表文件的开头必须要有<code>&lt;S&gt; &lt;/S&gt; &lt;UNK&gt;</code>，且大小写敏感。并且应当按照单词的词频降序排列。可以通过手动添加这三个特殊符号。<br>如：<br><img src="/images/2018-08-11-15339757184030.jpg" width="10%" height="50%"></p><p>代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model=gensim.models.KeyedVectors.load_word2vec_format(</span><br><span class="line">    fname=<span class="string">'/home/zhlin/GloVe/vectors.txt'</span>,binary=<span class="keyword">False</span></span><br><span class="line">)</span><br><span class="line">words=model.vocab</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'vocab.txt'</span>,<span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'&lt;S&gt;'</span>)</span><br><span class="line">    f.write(<span class="string">'\n'</span>）</span><br><span class="line">    f.write(<span class="string">'&lt;/S&gt;'</span>)</span><br><span class="line">    f.write(<span class="string">'\n'</span>)</span><br><span class="line">    f.write(<span class="string">'&lt;UNK&gt;'</span>)</span><br><span class="line">    f.write(<span class="string">'\n'</span>)    <span class="comment"># bilm-tf 要求vocab有这三个符号，并且在最前面</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        f.write(word)</span><br><span class="line">        f.write(<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure></p><h3 id="修改bilm-tf代码"><a href="#修改bilm-tf代码" class="headerlink" title="修改bilm-tf代码"></a>修改bilm-tf代码</h3><p>注意到，在使用该代码之前，需要安装好相应的环境。</p><p><img src="/images/2018-08-10-15338879402377.jpg" width="50%" height="50%"></p><p>如果使用的是conda作为默认的Python解释器，强烈建议使用conda安装，否则可能会出现一些莫名的错误。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install tensorflow-gpu=1.4</span><br><span class="line">conda install h5py</span><br><span class="line">python setup.py install <span class="comment">#应在bilm-tf的文件夹下执行该指令</span></span><br></pre></td></tr></table></figure></p><p>然后再运行测试代码，通过说明安装成功。</p><h4 id="修改train-elmo-py"><a href="#修改train-elmo-py" class="headerlink" title="修改train_elmo.py"></a>修改train_elmo.py</h4><p>bin文件夹下的train_elmo.py是程序的入口。<br>主要修改的地方：</p><ol><li>load_vocab的第二个参数应该改为None</li><li>n_gpus CUDA_VISIBLE_DEVICES 根据自己需求改</li><li>n_train_tokens 可改可不改，影响的是输出信息。要查看自己语料的行数，可以通过<code>wc -l corpus.txt</code> 查看。</li><li><strong>option的修改</strong>，将char_cnn部分都注释掉，其他根据自己需求修改</li></ol><p>如：<br><img src="/images/2018-08-10-15338888745894.jpg" width="70%" height="50%"></p><h4 id="修改LanguageModel类"><a href="#修改LanguageModel类" class="headerlink" title="修改LanguageModel类"></a>修改LanguageModel类</h4><p>由于我需要传入预训练好的GloVe embedding，那么还需要修改embedding部分，这部分在bilm文件夹下的training.py，进入到LanguageModel类中_build_word_embeddings函数中。注意到，由于前三个是<code>&lt;S&gt; &lt;/S&gt; &lt;UNK&gt;</code>，而这三个字符在GloVe里面是没有的，因此这三个字符的embedding应当在训练的时候逐渐学习到，而正因此 <code>embedding_weights</code>的<code>trainable</code>应当设为<code>True</code></p><p>如:</p><p><img src="/images/2018-08-12-15340585073779.jpg" alt=""></p><h4 id="修改train函数"><a href="#修改train函数" class="headerlink" title="修改train函数"></a>修改train函数</h4><p>添加代码，使得在train函数的最后保存embedding文件。<br><img src="/images/2018-08-12-15340607132103.jpg" alt=""></p><h3 id="训练并获得weights文件"><a href="#训练并获得weights文件" class="headerlink" title="训练并获得weights文件"></a>训练并获得weights文件</h3><p>训练需要语料文件corpus.txt，词表文件vocab.txt。</p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>cd到bilm-tf文件夹下，运行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=4</span><br><span class="line">nohup python -u bin/train_elmo.py \</span><br><span class="line">--train_prefix=<span class="string">'/home/zhlin/bilm-tf/corpus.txt'</span> \</span><br><span class="line">--vocab_file /home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab.txt \</span><br><span class="line">--save_dir /home/zhlin/bilm-tf/try &gt;bilm_out.txt 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p><p>根据实际情况设定不同的值和路径。</p><p>运行情况：<br><img src="/images/2018-08-10-15339015862848.jpg" width="50%" height="50%"></p><p>PS:运行过程中可能会有warning:</p><blockquote><p>‘list’ object has no attribute ‘name’<br>WARNING:tensorflow:Error encountered when serializing lstm_output_embeddings.<br>Type is unsupported, or the types of the items don’t match field type in CollectionDef.</p></blockquote><p>应该不用担心，还是能够继续运行的，后面也不受影响。</p><p>在等待了相当长的时间后，在save_dir文件夹内生成了几个文件，其中checkpoint和options是关键，checkpoint能够进一步生成language model的weights文件，而options记录language model的参数。</p><p><img src="/images/2018-08-11-15339734319058.jpg" alt=""></p><h4 id="获得language-model的weights"><a href="#获得language-model的weights" class="headerlink" title="获得language model的weights"></a>获得language model的weights</h4><p>接下来运行bin/dump_weights.py将checkpoint转换成hdf5文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nohup python -u  /home/zhlin/bilm-tf/bin/dump_weights.py  \</span><br><span class="line">--save_dir /home/zhlin/bilm-tf/try  \</span><br><span class="line">--outfile /home/zhlin/bilm-tf/try/weights.hdf5 &gt;outfile.txt 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>其中save_dir是checkpoint和option文件保存的地址。</p><p>接下来等待程序运行：</p><p><img src="/images/2018-08-11-15339740970081.jpg" width="70%" height="50%"></p><p><img src="/images/2018-08-11-15339745511775.jpg" width="70%" height="50%"></p><p>最终获得了想要的weights和option：<br><img src="/images/2018-08-11-15339978499136.jpg" alt=""></p><h3 id="将语料转化成ELMo-embedding"><a href="#将语料转化成ELMo-embedding" class="headerlink" title="将语料转化成ELMo embedding"></a>将语料转化成ELMo embedding</h3><p>由于我们有了vocab_file、与vocab_file一一对应的embedding h5py文件、以及language model的weights.hdf5和options.json。<br>接下来参考usage_token.py将一句话转化成ELMo embedding。</p><p>参考代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> bilm <span class="keyword">import</span> TokenBatcher, BidirectionalLanguageModel, weight_layers, \</span><br><span class="line">    dump_token_embeddings</span><br><span class="line"></span><br><span class="line"><span class="comment"># Our small dataset.</span></span><br><span class="line">raw_context = [</span><br><span class="line">    <span class="string">'这 是 测试 .'</span>,</span><br><span class="line">    <span class="string">'好的 .'</span></span><br><span class="line">]</span><br><span class="line">tokenized_context = [sentence.split() <span class="keyword">for</span> sentence <span class="keyword">in</span> raw_context]</span><br><span class="line">tokenized_question = [</span><br><span class="line">    [<span class="string">'这'</span>, <span class="string">'是'</span>, <span class="string">'什么'</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">vocab_file=<span class="string">'/home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab.txt'</span></span><br><span class="line">options_file=<span class="string">'/home/zhlin/bilm-tf/try/options.json'</span></span><br><span class="line">weight_file=<span class="string">'/home/zhlin/bilm-tf/try/weights.hdf5'</span></span><br><span class="line">token_embedding_file=<span class="string">'/home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab_embedding.hdf5'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Now we can do inference.</span></span><br><span class="line"><span class="comment"># Create a TokenBatcher to map text to token ids.</span></span><br><span class="line">batcher = TokenBatcher(vocab_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Input placeholders to the biLM.</span></span><br><span class="line">context_token_ids = tf.placeholder(<span class="string">'int32'</span>, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>))</span><br><span class="line">question_token_ids = tf.placeholder(<span class="string">'int32'</span>, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the biLM graph.</span></span><br><span class="line">bilm = BidirectionalLanguageModel(</span><br><span class="line">    options_file,</span><br><span class="line">    weight_file,</span><br><span class="line">    use_character_inputs=<span class="keyword">False</span>,</span><br><span class="line">    embedding_weight_file=token_embedding_file</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get ops to compute the LM embeddings.</span></span><br><span class="line">context_embeddings_op = bilm(context_token_ids)</span><br><span class="line">question_embeddings_op = bilm(question_token_ids)</span><br><span class="line"></span><br><span class="line">elmo_context_input = weight_layers(<span class="string">'input'</span>, context_embeddings_op, l2_coef=<span class="number">0.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">''</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">    <span class="comment"># the reuse=True scope reuses weights from the context for the question</span></span><br><span class="line">    elmo_question_input = weight_layers(</span><br><span class="line">        <span class="string">'input'</span>, question_embeddings_op, l2_coef=<span class="number">0.0</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elmo_context_output = weight_layers(</span><br><span class="line">    <span class="string">'output'</span>, context_embeddings_op, l2_coef=<span class="number">0.0</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">''</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">    <span class="comment"># the reuse=True scope reuses weights from the context for the question</span></span><br><span class="line">    elmo_question_output = weight_layers(</span><br><span class="line">        <span class="string">'output'</span>, question_embeddings_op, l2_coef=<span class="number">0.0</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># It is necessary to initialize variables once before running inference.</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create batches of data.</span></span><br><span class="line">    context_ids = batcher.batch_sentences(tokenized_context)</span><br><span class="line">    question_ids = batcher.batch_sentences(tokenized_question)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute ELMo representations (here for the input only, for simplicity).</span></span><br><span class="line">    elmo_context_input_, elmo_question_input_ = sess.run(</span><br><span class="line">        [elmo_context_input[<span class="string">'weighted_op'</span>], elmo_question_input[<span class="string">'weighted_op'</span>]],</span><br><span class="line">        feed_dict=&#123;context_token_ids: context_ids,</span><br><span class="line">                   question_token_ids: question_ids&#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">print(elmo_context_input_,elmo_context_input_)</span><br></pre></td></tr></table></figure></p><p>可以修改代码以适应自己的需求。</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://github.com/allenai/bilm-tf" target="_blank" rel="noopener">https://github.com/allenai/bilm-tf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> ELMo </tag>
            
            <tag> 词向量 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录4</title>
      <link href="/2018/08/12/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB4/"/>
      <url>/2018/08/12/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB4/</url>
      
        <content type="html"><![CDATA[<p>本周没有什么代码要记录的。</p><h3 id="1️⃣sklearn之Pipeline例子"><a href="#1️⃣sklearn之Pipeline例子" class="headerlink" title="1️⃣sklearn之Pipeline例子"></a>1️⃣sklearn之Pipeline例子</h3><p>用机器学习解决问题的流程：<br>(去掉部分数据）—&gt; 获取feature（Tf-idf等） —&gt; （feature selection，chi2、互信息等） —&gt; （缩放/正则化） —&gt; 分类器 —&gt; GridSearch/RandomizedSearch调参</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pipe=Pipeline([     <span class="comment">#建立pipeline</span></span><br><span class="line">    (<span class="string">'vect'</span>,TfidfVectorizer()),</span><br><span class="line">    (<span class="string">'select'</span>,SelectKBest(chi2),</span><br><span class="line">    (<span class="string">'norm'</span>,MaxAbsScaler()),   </span><br><span class="line">    (<span class="string">'svm'</span>,svm.LinearSVC())</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">parameters=&#123;</span><br><span class="line">    <span class="string">'vect__ngram_range'</span>:[(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">3</span>),(<span class="number">2</span>,<span class="number">3</span>)],</span><br><span class="line">    <span class="string">'vect__max_df'</span>:[<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>],</span><br><span class="line">    <span class="string">'vect__min_df'</span>:[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>],</span><br><span class="line">    <span class="string">'vect__norm'</span>:[<span class="string">'l1'</span>,<span class="string">'l2'</span>],</span><br><span class="line">    <span class="string">'svm__penalty'</span>:[<span class="string">'l1'</span>,<span class="string">'l2'</span>],</span><br><span class="line">    <span class="string">'svm__loss'</span>:[<span class="string">'squared_hinge'</span>],  </span><br><span class="line">    <span class="string">'svm__dual'</span>:[<span class="keyword">False</span>,<span class="keyword">True</span>],</span><br><span class="line">    <span class="string">'svm__tol'</span>:[<span class="number">1e-5</span>,<span class="number">1e-4</span>],</span><br><span class="line">    <span class="string">'svm__C'</span>:[<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>,<span class="number">1.0</span>,<span class="number">1.1</span>],</span><br><span class="line">    <span class="string">'svm__class_weight'</span>:[<span class="keyword">None</span>,<span class="string">'balanced'</span>],</span><br><span class="line">    <span class="string">'svm__max_iter'</span>:[<span class="number">1000</span>,<span class="number">5000</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">grid_search_model=GridSearchCV(pipe,parameters,error_score=<span class="number">0</span>,n_jobs=<span class="number">5</span>)</span><br><span class="line">grid_search_model.fit(train[column],train[<span class="string">'class'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> para_name <span class="keyword">in</span> sorted(parameters.keys()):</span><br><span class="line">    print(para_name,grid_search_model.best_params_[para_name])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"cv_result:"</span>)</span><br><span class="line">print(grid_search_model.cv_results_)</span><br></pre></td></tr></table></figure><hr>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识3</title>
      <link href="/2018/08/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%863/"/>
      <url>/2018/08/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%863/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[Python]<br>在服务器上跑代码时，如 <code>python project/folder1/a.py</code>，如果a.py引用了一个自定义的模块但又不在folder1内，此时interpreter就会报错，提示找不到该模块。这是因为解释器默认只会在同一个folder下查找。解决方案是在运行前显式添加查找范围。如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PYTHONPATH=/home/zhlin/bilm-tf:<span class="variable">$PYTHONPATH</span></span><br></pre></td></tr></table></figure></p><p>那么python解释器就会到该目录下去找。</p><hr><p>2️⃣[度量标准]<br><img src="/images/2018-08-12-15340420442670.jpg" alt=""></p><ul><li>准确率(accuracy):  $ACC=\frac{TP+TN}{TP+TN+FP+FN}$<br> 衡量的是分类器预测准确的比例</li><li>召回率(recall): $Recall=\frac{TP}{TP+FN}$<br>  正例中被分对的比例，衡量了分类器对正例的识别能力。</li><li>精确率(Precision): $P=\frac{TP}{TP+FP}$<br>度量了被分为正例的示例中实际为正例的比例。</li><li>F-Measure: $F=\frac{(\alpha^2 +1)P*R}{\alpha^2 (P+R)}$<br>  其中P是Precision,R是Recall。综合考量了两种度量。<br>  当$\alpha=1$时，称为F1值 $F1=\frac{2PR}{P+R}$</li></ul><hr><p>3️⃣[调参技巧]<br>在google发布的一份关于text-classification的<a href="https://developers.google.com/machine-learning/guides/text-classification/" target="_blank" rel="noopener">guide</a>中，提到了几个调参的trick。</p><ol><li>在feature selection步骤中，卡方检验chi2和方差分析的F值 f_classif的表现相当，在大约选择20k的feature时，准确率达到顶峰，当feature越多，效果并没有提升甚至会下降。<br><img src="/images/2018-08-12-15340434326365.jpg" width="90%" height="50%"></li><li>在文本分类中，似乎使用normalization并没有多少用处，建议跳过。<blockquote><p>Normalization converts all feature/sample values to small and similar values. This simplifies gradient descent convergence in learning algorithms. From what we have seen, normalization during data preprocessing does not seem to add much value in text classification problems; we recommend skipping this step.</p></blockquote></li></ol><p>实际上我也测试过，发现确实normalization对于准确率的提高没什么帮助，甚至还有一点下降。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> 度量标准 </tag>
            
            <tag> 调参技巧 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词4</title>
      <link href="/2018/08/12/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D4/"/>
      <url>/2018/08/12/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D4/</url>
      
        <content type="html"><![CDATA[<p>1️⃣</p><h3 id="灞上秋居"><a href="#灞上秋居" class="headerlink" title="灞上秋居"></a>灞上秋居</h3><p>[唐] 马戴<br>灞原风雨定，晚见雁行频。<br>落叶他乡树，寒灯独夜人。<br>空园白露滴，孤壁野僧邻。<br><strong>寄卧郊扉久，何年致此身。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b8a4080a2b58005c9108d9" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8a4080a2b58005c9108d9</a></p><hr><p>2️⃣</p><h3 id="唐多令"><a href="#唐多令" class="headerlink" title="唐多令"></a>唐多令</h3><p>[宋] 刘过<br>芦叶满汀洲，寒沙带浅流。二十年重过南楼。柳下系船犹未稳，能几日，又中秋。<br>黄鹤断矶头，故人今在否？旧江山浑是新愁。<strong>欲买桂花同载酒，终不似、少年游。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b922e7c4c9710055904842" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b922e7c4c9710055904842</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Vim常用快捷键</title>
      <link href="/2018/08/10/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Vim%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
      <url>/2018/08/10/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Vim%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/</url>
      
        <content type="html"><![CDATA[<p>在服务器经常要用到Vim，因此记录常用的快捷键并熟悉之。</p><h3 id="退出"><a href="#退出" class="headerlink" title="退出"></a>退出</h3><p>:q 退出<br>:wq 写入并退出<br>:q! 退出并忽略所有更改<br>:e! 放弃修改并打开原来的文件</p><h3 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h3><p>i 在当前位置前插入<br>a 在当前位置后插入</p><h3 id="撤销"><a href="#撤销" class="headerlink" title="撤销"></a>撤销</h3><p>:u 撤销<br>:U 撤销整行操作<br>Ctrl+r 重做</p><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p>:md 删除第m行<br>nd 删除当前行开始的n行(一共n+1行)<br>dd 删除当前行<br>D 删除当前字符至行尾<br>:m,nd 删除从m到n行的内容，如: <code>:100,10000d</code><br>:m,$d 删除m行及以后所有的行<br>:10d</p><h3 id="移动"><a href="#移动" class="headerlink" title="移动"></a>移动</h3><p>:n 跳转到行号  如， :100<br>gg 跳到行首<br>G(shift+g)移动到文件尾</p><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><p>/text 搜索text，n搜索下一个，N搜索上一个<br>?text 反向查找<br>:set ignorecase 忽略大小写查找<br>:set noignorecase 不忽略大小写查找<br>*或# 对光标处的单词搜索</p><h3 id="复制粘贴"><a href="#复制粘贴" class="headerlink" title="复制粘贴"></a>复制粘贴</h3><p>v 从当前位置开始，光标经过的地方被选中，再按一下v结束</p><h3 id="环境设置"><a href="#环境设置" class="headerlink" title="环境设置"></a>环境设置</h3><p>:set nu 显示行号<br>:set nonu 隐藏行号<br>:set hlsearch 设置搜索结果高亮</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.cnblogs.com/wangrx/p/5907013.html" target="_blank" rel="noopener">https://www.cnblogs.com/wangrx/p/5907013.html</a><br><a href="https://www.cnblogs.com/yangjig/p/6014198.html" target="_blank" rel="noopener">https://www.cnblogs.com/yangjig/p/6014198.html</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 技巧 </tag>
            
            <tag> 杂七杂八 </tag>
            
            <tag> 快捷键 </tag>
            
            <tag> Vim </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pycharm常用技巧</title>
      <link href="/2018/08/10/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Pycharm%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
      <url>/2018/08/10/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Pycharm%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<p>记录Pycharm的一些技巧，让Pycharm更顺手</p><h3 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h3><p>0️⃣Double Shift 万能搜索<br>可以搜索<strong>文件名、类名、方法名、目录名</strong>（在关键字前面加/ ），并不能用来搜索任意关键字</p><p>1️⃣ Command+F 在页面搜索</p><p>2️⃣ Ctrl+Shift+F Find in Path 在路径下搜索</p><p>3️⃣✨Command+E 快速查找文件<br>显示最近打开的文件</p><p>4️⃣ Shift+Enter 任意位置换行<br>无论光标在何处都可以直接另起一行</p><p>5️⃣ Option+Enter 自动导入模块；万能提示键<br>自动导入如何设置见小技巧#0️⃣</p><p>6️⃣ Ctrl+F10 运行<br>我已经添加了Ctrl+R作为另一对运行快捷键</p><p>7️⃣ Command+Shift+ +/-  展开/收缩代码 </p><p>8️⃣ Option+F 在Dash中搜索</p><p>9️⃣ Ctrl+J 不跳转查看代码</p><h3 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h3><p>0️⃣ Pycharm自动导入模块<br><a href="https://blog.csdn.net/lantian_123/article/details/78094148" target="_blank" rel="noopener">https://blog.csdn.net/lantian_123/article/details/78094148</a></p><p>1️⃣ ✨远程部署工程 强烈推荐<br>两步走：配置服务器映射+配置服务器解释器</p><p>2️⃣跳转后如何回退<br>开启toolbar即可<br><a href="https://segmentfault.com/a/1190000010205945" target="_blank" rel="noopener">https://segmentfault.com/a/1190000010205945</a></p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://foofish.net/pycharm-tips.html" target="_blank" rel="noopener">https://foofish.net/pycharm-tips.html</a><br><a href="https://blog.csdn.net/lantian_123/article/details/78094148" target="_blank" rel="noopener">https://blog.csdn.net/lantian_123/article/details/78094148</a><br><a href="https://segmentfault.com/a/1190000010205945" target="_blank" rel="noopener">https://segmentfault.com/a/1190000010205945</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 技巧 </tag>
            
            <tag> 杂七杂八 </tag>
            
            <tag> Pycharm </tag>
            
            <tag> 快捷键 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>无题</title>
      <link href="/2018/08/06/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%97%A0%E9%A2%98/"/>
      <url>/2018/08/06/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%97%A0%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>人这辈子一共会死三次。</p><p>第一次是你的心脏停止跳动，那么从生物的角度来说，你死了；</p><p>第二次是在葬礼上，认识你的人都来祭奠，那么你在社会关系上的事实存在就死了；</p><p>第三次是在最后一个记得你的人死后，那你就真的死了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>人可以卑微如尘土,不可扭曲如蛆虫</title>
      <link href="/2018/08/05/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BA%BA%E5%8F%AF%E4%BB%A5%E5%8D%91%E5%BE%AE%E5%A6%82%E5%B0%98%E5%9C%9F,%E4%B8%8D%E5%8F%AF%E6%89%AD%E6%9B%B2%E5%A6%82%E8%9B%86%E8%99%AB/"/>
      <url>/2018/08/05/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BA%BA%E5%8F%AF%E4%BB%A5%E5%8D%91%E5%BE%AE%E5%A6%82%E5%B0%98%E5%9C%9F,%E4%B8%8D%E5%8F%AF%E6%89%AD%E6%9B%B2%E5%A6%82%E8%9B%86%E8%99%AB/</url>
      
        <content type="html"><![CDATA[<p>如果天总也不亮，那就摸黑过生活; </p><p>如果发出声音是危险的，那就保持沉默; </p><p>如果自觉无力发光，那就别去照亮别人。 </p><p>但是——不要习惯了黑暗就为黑暗辩护; </p><p>不要为自己的苟且而得意洋洋; </p><p>不要嘲讽那些比自己更勇敢、更有热量的人们。 </p><p><strong>可以卑微如尘土，不可扭曲如蛆虫</strong>。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python惯例[转]</title>
      <link href="/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Python%E6%83%AF%E4%BE%8B/"/>
      <url>/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Python%E6%83%AF%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<p>fork from <a href="https://github.com/jackfrued/Python-100-Days/blob/master/Python惯例.md" target="_blank" rel="noopener">https://github.com/jackfrued/Python-100-Days/blob/master/Python惯例.md</a></p><h2 id="Python惯例"><a href="#Python惯例" class="headerlink" title="Python惯例"></a>Python惯例</h2><p>“惯例”这个词指的是“习惯的做法，常规的办法，一贯的做法”，与这个词对应的英文单词叫“idiom”。由于Python跟其他很多编程语言在语法和使用上还是有比较显著的差别，因此作为一个Python开发者如果不能掌握这些惯例，就无法写出“Pythonic”的代码。下面我们总结了一些在Python开发中的惯用的代码。</p><ol><li><p>让代码既可以被导入又可以被执行。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br></pre></td></tr></table></figure></li><li><p>用下面的方式判断逻辑“真”或“假”。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> x:</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> x:</span><br></pre></td></tr></table></figure><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">'jackfrued'</span></span><br><span class="line">fruits = [<span class="string">'apple'</span>, <span class="string">'orange'</span>, <span class="string">'grape'</span>]</span><br><span class="line">owners = &#123;<span class="string">'1001'</span>: <span class="string">'骆昊'</span>, <span class="string">'1002'</span>: <span class="string">'王大锤'</span>&#125;</span><br><span class="line"><span class="keyword">if</span> name <span class="keyword">and</span> fruits <span class="keyword">and</span> owners:</span><br><span class="line">    print(<span class="string">'I love fruits!'</span>)</span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">'jackfrued'</span></span><br><span class="line">fruits = [<span class="string">'apple'</span>, <span class="string">'orange'</span>, <span class="string">'grape'</span>]</span><br><span class="line">owners = &#123;<span class="string">'1001'</span>: <span class="string">'骆昊'</span>, <span class="string">'1002'</span>: <span class="string">'王大锤'</span>&#125;</span><br><span class="line"><span class="keyword">if</span> name != <span class="string">''</span> <span class="keyword">and</span> len(fruits) &gt; <span class="number">0</span> <span class="keyword">and</span> owners != &#123;&#125;:</span><br><span class="line">    print(<span class="string">'I love fruits!'</span>)</span><br></pre></td></tr></table></figure></li><li><p>善于使用in运算符。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> x <span class="keyword">in</span> items: <span class="comment"># 包含</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> items: <span class="comment"># 迭代</span></span><br></pre></td></tr></table></figure><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">'Hao LUO'</span></span><br><span class="line"><span class="keyword">if</span> <span class="string">'L'</span> <span class="keyword">in</span> name:</span><br><span class="line">    print(<span class="string">'The name has an L in it.'</span>)</span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">'Hao LUO'</span></span><br><span class="line"><span class="keyword">if</span> name.find(<span class="string">'L'</span>) != <span class="number">-1</span>:</span><br><span class="line">    print(<span class="string">'This name has an L in it!'</span>)</span><br></pre></td></tr></table></figure></li><li><p>不使用临时变量交换两个值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a, b = b, a</span><br></pre></td></tr></table></figure></li><li><p><strong>用序列构建字符串</strong>。</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chars = [<span class="string">'j'</span>, <span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'k'</span>, <span class="string">'f'</span>, <span class="string">'r'</span>, <span class="string">'u'</span>, <span class="string">'e'</span>, <span class="string">'d'</span>]</span><br><span class="line">name = <span class="string">''</span>.join(chars)</span><br><span class="line">print(name)  <span class="comment"># jackfrued</span></span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">chars = [<span class="string">'j'</span>, <span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'k'</span>, <span class="string">'f'</span>, <span class="string">'r'</span>, <span class="string">'u'</span>, <span class="string">'e'</span>, <span class="string">'d'</span>]</span><br><span class="line">name = <span class="string">''</span></span><br><span class="line"><span class="keyword">for</span> char <span class="keyword">in</span> chars:</span><br><span class="line">    name += char</span><br><span class="line">print(name)  <span class="comment"># jackfrued</span></span><br></pre></td></tr></table></figure></li><li><p><strong>EAFP优于LBYL</strong>。</p><p>EAFP - <strong>E</strong>asier to <strong>A</strong>sk <strong>F</strong>orgiveness than <strong>P</strong>ermission.</p><p>LBYL - <strong>L</strong>ook <strong>B</strong>efore <strong>Y</strong>ou <strong>L</strong>eap.</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'x'</span>: <span class="string">'5'</span>&#125;</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    value = int(d[<span class="string">'x'</span>])</span><br><span class="line">    print(value)</span><br><span class="line"><span class="keyword">except</span> (KeyError, TypeError, ValueError):</span><br><span class="line">    value = <span class="keyword">None</span></span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'x'</span>: <span class="string">'5'</span>&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="string">'x'</span> <span class="keyword">in</span> d <span class="keyword">and</span> isinstance(d[<span class="string">'x'</span>], str) \</span><br><span class="line"><span class="keyword">and</span> d[<span class="string">'x'</span>].isdigit():</span><br><span class="line">    value = int(d[<span class="string">'x'</span>])</span><br><span class="line">    print(value)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    value = <span class="keyword">None</span></span><br></pre></td></tr></table></figure></li><li><p>使用enumerate进行迭代。</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fruits = [<span class="string">'orange'</span>, <span class="string">'grape'</span>, <span class="string">'pitaya'</span>, <span class="string">'blueberry'</span>]</span><br><span class="line"><span class="keyword">for</span> index, fruit <span class="keyword">in</span> enumerate(fruits):</span><br><span class="line">print(index, <span class="string">':'</span>, fruit)</span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fruits = [<span class="string">'orange'</span>, <span class="string">'grape'</span>, <span class="string">'pitaya'</span>, <span class="string">'blueberry'</span>]</span><br><span class="line">index = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> fruit <span class="keyword">in</span> fruits:</span><br><span class="line">    print(index, <span class="string">':'</span>, fruit)</span><br><span class="line">    index += <span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p>用生成式生成列表。</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = [<span class="number">7</span>, <span class="number">20</span>, <span class="number">3</span>, <span class="number">15</span>, <span class="number">11</span>]</span><br><span class="line">result = [num * <span class="number">3</span> <span class="keyword">for</span> num <span class="keyword">in</span> data <span class="keyword">if</span> num &gt; <span class="number">10</span>]</span><br><span class="line">print(result)  <span class="comment"># [60, 45, 33]</span></span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = [<span class="number">7</span>, <span class="number">20</span>, <span class="number">3</span>, <span class="number">15</span>, <span class="number">11</span>]</span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">    <span class="keyword">if</span> i &gt; <span class="number">10</span>:</span><br><span class="line">        result.append(i * <span class="number">3</span>)</span><br><span class="line">print(result)  <span class="comment"># [60, 45, 33]</span></span><br></pre></td></tr></table></figure></li><li><p>用zip组合键和值来创建字典。</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keys = [<span class="string">'1001'</span>, <span class="string">'1002'</span>, <span class="string">'1003'</span>]</span><br><span class="line">values = [<span class="string">'骆昊'</span>, <span class="string">'王大锤'</span>, <span class="string">'白元芳'</span>]</span><br><span class="line">d = dict(zip(keys, values))</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keys = [<span class="string">'1001'</span>, <span class="string">'1002'</span>, <span class="string">'1003'</span>]</span><br><span class="line">values = [<span class="string">'骆昊'</span>, <span class="string">'王大锤'</span>, <span class="string">'白元芳'</span>]</span><br><span class="line">d = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i, key <span class="keyword">in</span> enumerate(keys):</span><br><span class="line">    d[key] = values[i]</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure></li></ol><blockquote><p><strong>说明</strong>：这篇文章的内容来自于网络，有兴趣的读者可以阅读<a href="http://safehammad.com/downloads/python-idioms-2014-01-16.pdf" target="_blank" rel="noopener">原文</a>。</p></blockquote><p>注：<br>许多原则我认为非常有意义，能够摆脱C/C++的风格，真正写出Pythonic的代码。让我有很大感触的是1、3、8，能够写出非常简洁优雅的代码。同时6我之前从没注意过，习惯了C/C++风格之后总是会在执行之前考虑所有情况，但确实不够优雅，今后可以尝试EAFP风格（<a href="https://stackoverflow.com/questions/11360858/what-is-the-eafp-principle-in-python" target="_blank" rel="noopener">什么是EAFP</a>）。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 杂七杂八 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何训练GloVe中文词向量</title>
      <link href="/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83GloVe%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F/"/>
      <url>/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83GloVe%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<h3 id="准备语料"><a href="#准备语料" class="headerlink" title="准备语料"></a>准备语料</h3><p>准备好自己的语料，保存为txt，每行一个句子或一段话，注意要分好词。</p><p><img src="/images/2018-08-05-15334388069130.jpg" width="60%" height="60%"></p><h3 id="准备源码"><a href="#准备源码" class="headerlink" title="准备源码"></a>准备源码</h3><p>从GitHub下载代码，<a href="https://github.com/stanfordnlp/GloVe" target="_blank" rel="noopener">https://github.com/stanfordnlp/GloVe</a><br>将语料corpus.txt放入到Glove的主文件夹下。</p><h3 id="修改bash"><a href="#修改bash" class="headerlink" title="修改bash"></a>修改bash</h3><p>打开demo.sh，修改相应的内容</p><ol><li>因为demo默认是下载网上的语料来训练的，因此如果要训练自己的语料，需要注释掉</li></ol><p><img src="/images/2018-08-05-15334390298383.jpg" width="70%" height="50%"></p><ol><li>修改参数设置，将CORPUS设置成语料的名字</li></ol><p><img src="/images/2018-08-05-15334391029224.jpg" width="50%" height="50%"></p><h3 id="执行bash文件"><a href="#执行bash文件" class="headerlink" title="执行bash文件"></a>执行bash文件</h3><p>进入到主文件夹下</p><ol><li>make</li></ol><p><img src="/images/2018-08-05-15334392348665.jpg" width="70%" height="50%"></p><ol><li>bash demo.sh</li></ol><p><img src="/images/2018-08-05-15334392595148.jpg" width="70%" height="70%"></p><p>注意，如果训练数据较大，则训练时间较长，那么建议使用nohup来运行程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup bash demo.sh &gt;output.txt 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>坐等训练，最后会得到vectors.txt 以及其他的相应的文件。如果要用gensim的word2ve load进来，那么需要在vectors.txt的第一行加上vacob_size vector_size，第一个数指明一共有多少个向量，第二个数指明每个向量有多少维。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.cnblogs.com/echo-cheng/p/8561171.html" target="_blank" rel="noopener">https://www.cnblogs.com/echo-cheng/p/8561171.html</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> GloVe </tag>
            
            <tag> 教程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识2</title>
      <link href="/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%862/"/>
      <url>/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%862/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[Pytorch]<br>避免写出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.zeros(...), requires_grad=<span class="keyword">True</span>).cuda()</span><br></pre></td></tr></table></figure><p>而是应该要：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.zeros(...).cuda(), requires_grad=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>Reference:<br><a href="https://discuss.pytorch.org/t/variable-grad-is-always-none-when-extending-autograd/12187" target="_blank" rel="noopener">https://discuss.pytorch.org/t/variable-grad-is-always-none-when-extending-autograd/12187</a></p><hr><p>2️⃣[Tf-idf]<br>本周因为比赛的原因了解了一下各种文本建模的方法。Tf-idf能够取得不错的成绩，但有一定的缺陷。</p><blockquote><p>TF-IDF用于向量空间模型，进行文档相似度计算是相当有效的。但在文本分类中单纯使用TF-IDF来判断一个特征是否有区分度是不够的。</p><ol><li>它仅仅综合考虑了该词在文档中的重要程度和文档区分度。</li><li>它没有考虑特征词在类间的分布。特征选择所选择的特征应该在某类出现多，而其它类出现少，即考察各类的文档频率的差异。如果一个特征词，在各个类间分布比较均匀，这样的词对分类基本没有贡献；但是如果一个特征词比较集中的分布在某个类中，而在其它类中几乎不出现，这样的词却能够很好代表这个类的特征，而TF-IDF不能区分这两种情况。</li><li>它没有考虑特征词在类内部文档中的分布情况。在类内部的文档中，如果特征词均匀分布在其中，则这个特征词能够很好的代表这个类的特征，如果只在几篇文档中出现，而在此类的其它文档中不出现，显然这样的特征词不能够代表这个类的特征。</li></ol></blockquote><p>Reference:<br><a href="https://blog.csdn.net/mmc2015/article/details/46771791" target="_blank" rel="noopener">https://blog.csdn.net/mmc2015/article/details/46771791</a></p><hr><p>3️⃣[卡方检验CHI]<br>在文本分类中，用于选择最相关的特征。</p><p>Reference:<br><a href="https://blog.csdn.net/blockheadls/article/details/49977361" target="_blank" rel="noopener">https://blog.csdn.net/blockheadls/article/details/49977361</a></p><hr><p>4️⃣[文本分类]<br>各种文本分类方法的简单介绍。</p><p>Reference:<br><a href="https://github.com/wangjiang0624/Note/blob/master/MachineLearning/文本分类.md" target="_blank" rel="noopener">https://github.com/wangjiang0624/Note/blob/master/MachineLearning/文本分类.md</a></p><hr><p>5️⃣[Python]<br>collections的两个有用的类</p><ol><li>named_tuple：快速建立一个类，使得可以使用属性来访问而非索引，提高了代码可读性</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line">Point = namedtuple(<span class="string">'Point'</span>,[<span class="string">'x'</span>,<span class="string">'y'</span>])</span><br><span class="line">p = Point(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">print(p.x)  <span class="comment"># 1</span></span><br><span class="line">print(p.y)  <span class="comment"># 2</span></span><br></pre></td></tr></table></figure><ol><li>Counter：统计字符出现的次数</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">count = Counter([...]).most_commom()  <span class="comment">#会按照出现的次数排序，通常可用于构建词典</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> count:     <span class="comment"># c是一个tuple，c[0]是词，c[1]是频率</span></span><br><span class="line">    <span class="keyword">if</span> c[<span class="number">1</span>]&gt;= threshold:</span><br><span class="line">        vocab.add_word(c[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>Counter用法：<br><a href="https://blog.csdn.net/u014755493/article/details/69812244" target="_blank" rel="noopener">https://blog.csdn.net/u014755493/article/details/69812244</a></p><hr><p>6️⃣[nohup]<br>本周在服务器上跑代码的时候遇到一个问题，使用nohup执行python程序时，发现输出文件没有显示。以为是代码的问题，但经过排查并非是代码的问题。通过查阅资料，发现问题所在：<br>因为python输出有缓冲，导致output不能<strong>马上</strong>看到输出。实际上，在等待了一段时间后，输出文件终于显示出来了。</p><p>解决方案：使用python的参数 -u 使得python不启用缓冲。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python -u test.py &gt; nohup.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>Reference:<br><a href="https://blog.csdn.net/sunlylorn/article/details/19127107" target="_blank" rel="noopener">https://blog.csdn.net/sunlylorn/article/details/19127107</a></p><hr><p>7️⃣[hexo配置]</p><ol><li>mathjax配置: <a href="https://www.jianshu.com/p/7ab21c7f0674" target="_blank" rel="noopener">https://www.jianshu.com/p/7ab21c7f0674</a></li><li>配置域名:<a href="https://www.zhihu.com/question/31377141" target="_blank" rel="noopener">https://www.zhihu.com/question/31377141</a></li><li>配置sitemap:<a href="http://www.yuan-ji.me/Hexo-优化：提交sitemap及解决百度爬虫抓取-GitHub-Pages-问题/" target="_blank" rel="noopener">http://www.yuan-ji.me/Hexo-优化：提交sitemap及解决百度爬虫抓取-GitHub-Pages-问题/</a></li></ol><hr><p>8️⃣[Paper]<br><a href="http://aclweb.org/anthology/D17-1025" target="_blank" rel="noopener">Learning Chinese Word Representations From Glyphs Of Characters</a></p><p>使用图像的卷积来生成词向量:<br><img src="/images/2018-08-05-15334453515509.jpg" width="60%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Paper </tag>
            
            <tag> Tf-idf </tag>
            
            <tag> 文本分类 </tag>
            
            <tag> hexo </tag>
            
            <tag> nohup </tag>
            
            <tag> CHI </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录3</title>
      <link href="/2018/08/05/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB3/"/>
      <url>/2018/08/05/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB3/</url>
      
        <content type="html"><![CDATA[<p>本周只有简单的代码。</p><h3 id="1️⃣使用gensim训练word2vec"><a href="#1️⃣使用gensim训练word2vec" class="headerlink" title="1️⃣使用gensim训练word2vec"></a>1️⃣使用gensim训练word2vec</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#encoding=utf-8</span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line">sentences=word2vec.Text8Corpus(<span class="string">u'分词后的爽肤水评论.txt'</span>)   <span class="comment">#sentence:[ [ a b ],[c d]... ]</span></span><br><span class="line">model=word2vec.Word2Vec(sentences, size=<span class="number">50</span>)  <span class="comment">#size:dim </span></span><br><span class="line"></span><br><span class="line">y2=model.similarity(<span class="string">u"好"</span>, <span class="string">u"还行"</span>)  <span class="comment">#计算相似度</span></span><br><span class="line">print(y2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> model.most_similar(<span class="string">u"滋润"</span>):</span><br><span class="line">    <span class="keyword">print</span> i[<span class="number">0</span>],i[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment">#保存</span></span><br><span class="line">model.save(<span class="string">'/model/word2vec_model'</span>)</span><br><span class="line"></span><br><span class="line">new_model=gensim.models.Word2Vec.load(<span class="string">'/model/word2vec_model'</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="2️⃣使用Counter建立词表"><a href="#2️⃣使用Counter建立词表" class="headerlink" title="2️⃣使用Counter建立词表"></a>2️⃣使用Counter建立词表</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dict</span><span class="params">(dataset,min_freq=<span class="number">5</span>)</span>:</span></span><br><span class="line">    dictionary=Dictionary() </span><br><span class="line">    count=Counter(flat(dataset)).most_common()  </span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> count:</span><br><span class="line">        <span class="keyword">if</span> c[<span class="number">1</span>]&gt;=min_freq:</span><br><span class="line">            dictionary.add_word(c[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> dictionary</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词3</title>
      <link href="/2018/08/05/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D3/"/>
      <url>/2018/08/05/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D3/</url>
      
        <content type="html"><![CDATA[<p>本周背的都是比较简单的。</p><p>1️⃣</p><h3 id="把酒问月"><a href="#把酒问月" class="headerlink" title="把酒问月"></a>把酒问月</h3><p>[唐] 李白<br>青天有月来几时？我今停杯一问之。<br>人攀明月不可得，月行却与人相随。<br>皎如飞镜临丹阙，绿烟灭尽清辉发。<br>但见宵从海上来，宁知晓向云间没。<br>白兔捣药秋复春，嫦娥孤栖与谁邻？<br><strong>今人不见古时月，今月曾经照古人。<br>古人今人若流水，共看明月皆如此。</strong><br><strong>唯愿当歌对酒时，月光长照金樽里。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b8f6f4165abd0054bf8c13" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8f6f4165abd0054bf8c13</a></p><hr><p>2️⃣</p><h3 id="金缕衣"><a href="#金缕衣" class="headerlink" title="金缕衣"></a>金缕衣</h3><p>[唐] 杜秋娘<br>劝君莫惜金缕衣，劝君惜取少年时。<br><strong>花开堪折直须折，莫待无花空折枝。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b92bdca633bd00665eb99e" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b92bdca633bd00665eb99e</a></p><hr><p>3️⃣</p><h3 id="北青萝"><a href="#北青萝" class="headerlink" title="北青萝"></a>北青萝</h3><p>[唐] 李商隐<br>残阳西入崦，茅屋访孤僧。<br>落叶人何在，寒云路几层。<br>独敲初夜磬，闲倚一枝藤。<br><strong>世界微尘里，吾宁爱与憎。</strong></p><p>崦（yān）：即“崦嵫（zī）”，山名，在甘肃。古时常用来指太阳落山的地方。<br><strong>磬（qìng）</strong>：古代打击乐器，形状像曲尺，用玉、石制成，可悬挂。</p><p><a href="http://m.xichuangzhu.com/work/57b8ee240a2b58005c91c99e" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8ee240a2b58005c91c99e</a></p><hr><p>4️⃣</p><h3 id="夏日绝句"><a href="#夏日绝句" class="headerlink" title="夏日绝句"></a>夏日绝句</h3><p>[宋] 李清照<br>生当作人杰，死亦为鬼雄。<br><strong>至今思项羽，不肯过江东。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b911dac4c97100558fb30e" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b911dac4c97100558fb30e</a></p><hr><p>5️⃣</p><h3 id="雨霖铃"><a href="#雨霖铃" class="headerlink" title="雨霖铃"></a>雨霖铃</h3><p>[宋] 柳永<br>寒蝉凄切，对长亭晚，骤雨初歇。都门帐饮无绪，留恋处，兰舟催发。执手相看泪眼，竟无语凝噎。念去去，千里烟波，暮霭沉沉楚天阔。<br><strong>多情自古伤离别</strong>，更那堪、冷落清秋节。今宵酒醒何处？杨柳岸，晓风残月。此去经年，应是良辰好景虚设。<strong>便纵有千种风情，更与何人说</strong>？</p><p><a href="http://m.xichuangzhu.com/work/57ad742f5bbb500062bc7c9c" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57ad742f5bbb500062bc7c9c</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Pytorch中grad的理解</title>
      <link href="/2018/08/03/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADgrad%E7%9A%84%E7%90%86%E8%A7%A3/"/>
      <url>/2018/08/03/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADgrad%E7%9A%84%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>事情起源于我写了一个CNN用于文本分类，但loss一直没降，因此我尝试<code>print(loss.grad)</code>的grad，发现神奇的是loss grad显示为None，接着尝试<code>print(y_pred.grad)</code>，同样是None，但再print loss和y_pred的requires_grad发现是正常的True。</p><p>在查阅了资料，以及问了学长之后发现原来并不是bug，而是因为，Pytorch默认不会保存中间节点(intermediate variable)的grad，此举是为了节省内存。</p><blockquote><p>By default, gradients are only retained for leaf variables. non-leaf variables’ gradients are not retained to be inspected later. This was done by design, to save memory.</p></blockquote><p><a href="https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94" target="_blank" rel="noopener">https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94</a></p><p>实际上可以通过retain_grad()或者hook来查看中间节点的grad。</p><p>我后面尝试print了叶子节点，如 <code>print(CNN_model.fc.weight.grad)</code>，最终获得了正确的grad。</p><p>ps：所谓中间节点，是<strong>由其他节点计算所得</strong>的tensor，而叶子节点则是<strong>自己定义</strong>出来的。</p><p>最后我发现，原来loss一直没降的原因是因为我定义的CNN过于复杂，并且数据集偏小，无法快速收敛导致的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> grad </tag>
            
            <tag> bug </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Linux常用命令</title>
      <link href="/2018/08/03/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2018/08/03/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h3 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h3><p>记录自己常用的命令。</p><p>1️⃣ls：显式当前目录下的文件和目录<br>    -a 包括隐藏文件<br>    -h 将文件的容量以易读方式列出（配合-s使用）<br>    -s 以块数形式显示每个文件分配的尺寸<br>    -l 以较长格式列出信息，可以直接写成 <code>ll</code><br><img src="/images/2018-08-10-15339070264416.jpg" width="70%" height="50%"></p><hr><p>2️⃣cd 到达指定地址</p><hr><p>3️⃣kill 杀死程序<br>    -l 信息编号。<strong>当l=9时，无条件终止，其他信号可能忽略</strong><br>    killall -u <user_name> 杀死该用户全部进程</user_name></p><p><img src="/images/2018-08-03-15332830170623.jpg" width="50%" height="50%"></p><hr><p>4️⃣ps 报告当前系统的进程状态<br>    -a 所有<br>    -p 指定程序<br>    -u 指定用户<br>    -x 列出该用户的进程的详细信息(我的理解应该是)<br>    如：<br><img src="/images/2018-08-10-15339036207642.jpg" width="70%" height="50%"></p><hr><p>5️⃣htop 比top更优，交互更好，同时可以直观看到资源占用情况<br>基本命令与top一致<br><img src="/images/2018-08-04-15333484387393.jpg" width="70%" height="50%"></p><hr><p>6️⃣top：动态查看系统运行状态<br>    -u 指定用户名<br>    -p 指定进程</p><p>7️⃣nvidia-smi 查看显卡状态<br>watch nvidia-smi 实时查看显卡状态，定时刷新</p><hr><p>8️⃣tail 显示指定文件的末尾若干行<br>    -f 显示文件最新追加的内容<br>    -n 显示文件尾部n行内容<br>    -c 显示文件尾部最后c个字符</p><p>如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tail file 显示最后10行</span><br><span class="line">tail -n +20 file 显示从第20行至末尾</span><br><span class="line">tail -c 10 file 显示文件file的最后10个字符</span><br></pre></td></tr></table></figure><pre><code>-------</code></pre><p>9️⃣echo 用于打印指定的字符串<br><img src="/images/2018-08-04-15333497266470.jpg" width="50%" height="50%"></p><hr><p>🔟which 用于查找并显示给定命令的绝对路径，which指令会在环境变量$PATH设置的目录里查找符合条件的文件。使用which命令，可以看到某个系统命令是否存在，以及执行的是哪个位置的命令。如：</p><p><img src="/images/2018-08-04-15333500837235.jpg" width="50%" height="50%"></p><hr><p>1️⃣1️⃣nohup 将程序以忽略挂起信号的方式运行，经常用于在服务器跑代码<br>如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python xxx.py &gt;output.txt 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p><p>即，将输出重定向到output.txt ；最后一个<code>&amp;</code>表示后台挂起</p><hr><p>1️⃣2️⃣cp 复制文件   cp [文件] [目标文件夹]<br>    -r 递归复制，用于目录的复制</p><hr><p>1️⃣3️⃣mv 移动文件、目录或更名  mv [文件/文件夹] [文件夹]<br>    -f 强制，当目标文件存在，直接覆盖<br>    -i 会询问</p><hr><p>1️⃣4️⃣rm 删除文件或目录<br>    -f 强制删除<br>    -r 递归删除，用于目录删除</p><hr><p>1️⃣5️⃣file 用于判断文件的基本数据<br>如：</p><p><img src="/images/2018-08-05-15334547949248.jpg" width="80%" height="50%"></p><hr><p>1️⃣6️⃣tar 对文件打包/压缩<br>    -t 查看打包文件的内容含有哪些文件名<br>    -x 解压缩<br>    -c 新建打包文件<br>    -C 指定压缩/解压目录<br>    -v 解压/压缩过程中将处理的文件名显示出来<br>常用的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">压缩：tar -jcv -f filename.tar.bz2 要被处理的文件或目录名称</span><br><span class="line">查询：tar -jtv -f filename.tar.bz2</span><br><span class="line">解压：tar -jxv -f filename.tar.bz2 -C 欲解压缩的目录</span><br></pre></td></tr></table></figure><hr><p>1️⃣7️⃣wc word count 统计文件内容信息，如行数、字符数<br>    -l 显示文件行数<br>    -c 显示字节数<br>    -m 显示字符数<br>    -w 显示字数  字被定义为由空白、跳格、换行字符分隔的字符串<br>    -L 显示最长行的长度<br>    不加参数，所有的都显示，依次是行数、单词数、字节数、文件名</p><hr><p>1️⃣8️⃣df 显示磁盘相关信息<br>    -h 以可读性较高的方式显示信息</p><hr><p>1️⃣9️⃣scp 服务器之间的文件复制<br>    如:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r /test1 zhlin@123.12.1.12:/home/zhlin</span><br></pre></td></tr></table></figure><h3 id="✨快捷键"><a href="#✨快捷键" class="headerlink" title="✨快捷键"></a>✨快捷键</h3><p>Ctrl+a 跳到行首<br>Ctrl+c 退出当前进程<br>Ctrl+e 跳到页尾<br>Ctrl+k 删除当前光标后面的文字<br>Ctrl+l 清屏，等价于clear<br>Ctrl+r 搜索之前打过的命令<br>Ctrl+u 删除当前光标前面的文字<br>✨Ctrl+左右键 单词之间跳转 在Mac上可以使用option+左右键<br>Ctrl+y 进行恢复删除<br>Ctrl+z 将当前进程转到后台，使用fg恢复</p><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://blog.csdn.net/leo_618/article/details/53003111" target="_blank" rel="noopener">https://blog.csdn.net/leo_618/article/details/53003111</a></p><p>———-持续更新———-</p>]]></content>
      
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 碎片知识 </tag>
            
            <tag> 技巧 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>一个关于yield的重新认识</title>
      <link href="/2018/07/31/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E4%B8%80%E4%B8%AA%E5%85%B3%E4%BA%8Eyield%E7%9A%84%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86/"/>
      <url>/2018/07/31/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E4%B8%80%E4%B8%AA%E5%85%B3%E4%BA%8Eyield%E7%9A%84%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<p>今天遇到了一个神奇的”bug”，让我对yield的理解更深一步。</p><p>这是一个函数，我本来打算试着print一下line内部的格式和内容。<br><img src="/images/2018-07-31-15330184801079.jpg" width="40%" height="40%"></p><p>这是调用的主函数：<br><img src="/images/2018-07-31-15330185414299.jpg" width="50%" height="50%"></p><p>结果跑出的结果是：<br><img src="/images/2018-07-31-15330185762441.jpg" width="90%" height="90%"></p><p>？？？<br><img src="/images/2018-07-31-15330186003254.jpg" alt=""></p><p>我尝试在函数的开头添加print：<br><img src="/images/2018-07-31-15330186689312.jpg" width="40%" height="50%"></p><p>结果仍然没有任何的输出。</p><p>我试着在main函数添加print：<br><img src="/images/2018-07-31-15330187249603.jpg" width="50%" height="50%"></p><p>结果：</p><p><img src="/images/2018-07-31-15330187445810.jpg" width="50%" height="50%"></p><p>也就是说，根本没有进入到get_dataset_from_txt函数啊。</p><p>我以为是pycharm的问题还重启了一遍，然而并没有任何作用。问了其他人，他们也觉得很神奇。最后一个同学看了一下函数，发现了问题所在：<strong>yield</strong></p><p>我突然想起来，<strong>yield返回的是一个generator，只有在对generator进行遍历时，才会开始运行</strong>…</p><p>于是，我试着这么写，试着对generator遍历：</p><p><img src="/images/2018-07-31-15330189280379.jpg" width="50%" height="50%"></p><p>虽然报错了，但函数终于是进去了…</p><p><img src="/images/2018-07-31-15330189613535.jpg" width="70%" height="50%"></p><p><strong>结论：有yield的函数会返回一个generator，当对其进行遍历时，函数才会开始运行。</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Python </tag>
            
            <tag> yield </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录2</title>
      <link href="/2018/07/29/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB2/"/>
      <url>/2018/07/29/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB2/</url>
      
        <content type="html"><![CDATA[<p>本周主要看了<a href="https://github.com/allenai/bilm-tf" target="_blank" rel="noopener">AllenNLP/ELMO</a>的代码，但并没有找到很多可复用的代码。本周也没有比较有意义的代码。</p><hr><h3 id="1️⃣get-time-diff"><a href="#1️⃣get-time-diff" class="headerlink" title="1️⃣get_time_diff"></a>1️⃣get_time_diff</h3><p>获取已使用的时间<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta</span><br><span class="line"></span><br><span class="line">start_time=time.time()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time_dif</span><span class="params">(start_time)</span>:</span></span><br><span class="line">    <span class="string">"""获取已使用时间"""</span></span><br><span class="line">    end_time = time.time()</span><br><span class="line">    time_dif = end_time - start_time</span><br><span class="line">    <span class="keyword">return</span> timedelta(seconds=int(round(time_dif)))</span><br></pre></td></tr></table></figure></p><hr><h3 id="2️⃣parser使用"><a href="#2️⃣parser使用" class="headerlink" title="2️⃣parser使用"></a>2️⃣parser使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--save_dir'</span>, help=<span class="string">'Location of checkpoint files'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--vocab_file'</span>, help=<span class="string">'Vocabulary file'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--train_prefix'</span>, help=<span class="string">'Prefix for train files'</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line">    </span><br><span class="line">main(args)   <span class="comment">#使用</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识1</title>
      <link href="/2018/07/29/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%861/"/>
      <url>/2018/07/29/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%861/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[Python]<br>assert用法：</p><p><code>assert expression</code><br>等价于<br><code>if not expression: raise AssertionError</code></p><hr><p>2️⃣[Pytorch]<br>Pytorch view：<br>创建一个新的tensor，但他们的<strong>data是共享的</strong>。</p><p><img src="/images/2018-07-29-15328360404485.jpg" width="50%" height="50%"></p><hr><p>3️⃣[Pytorch]<br>在Pytorch中，embedding的index是不能requires_grad=True的，否则会出错。<br><a href="https://github.com/pytorch/pytorch/issues/7021" target="_blank" rel="noopener">https://github.com/pytorch/pytorch/issues/7021</a></p><p>之前看过一份代码，设置volatile=false但没有出错，是因为在Pytorch0.4之后volatile已经被弃用了，因此volatile=false不起作用，而默认requires_grad=false</p><hr><p>4️⃣[Pytorch]<br>在Pytorch中，<code>nn.Linear(self.hidden_dim,self.vocab_size)</code>的维度是vocab_size<em>hidden_dim，之前居然没有注意到这个问题。<br>因为nn.Linear的<em>*第一个参数表示输入维度，第二个参数表示输出维度</em></em></p><hr><p>5️⃣[Pytorch]<br>Pytorch中，使用view一般来说必须要用 .contiguous()。也即：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch.view(batch_size, <span class="number">-1</span>).t().contiguous()</span><br></pre></td></tr></table></figure><p>contiguous()的官方解释：<br><a href="https://discuss.pytorch.org/t/runtimeerror-input-is-not-contiguous/930" target="_blank" rel="noopener">https://discuss.pytorch.org/t/runtimeerror-input-is-not-contiguous/930</a></p><blockquote><p>It means that your tensor is not a single block of memory, but a block with holes. view can be only used with contiguous tensors, so if you need to use it here, just call .contiguous() before.</p></blockquote><p>也就是说，contiguous会将数据存到一个连续的空间内（block）。</p><hr><p>6️⃣[Pytorch]<br>调用Cross_entropy时，Pytorch会帮助你加log和softmax。</p><p><img src="/images/2018-07-29-15328370301774.jpg" alt=""></p><hr><p>7️⃣[Paper]<br><a href="https://arxiv.org/abs/1807.02291" target="_blank" rel="noopener">Sliced_RNN</a></p><p>将RNN分块以提高并行性，甚至每层的RNN都可以不一样，达到抽取不同程度的抽象语义信息的目的。实验证明，在不同任务上都有一定的提升，但速度的提升很大。</p><p><img src="/images/2018-07-29-15328372609264.jpg" width="50%" height="50%"></p><hr><p>8️⃣[Tf-idf]<br>计算词语对于句子的重要程度</p><p><a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/Tf-idf</a></p><p>tf是词频，idf是逆向文件频率。也即如果词在该句出现的次数越多，在所有文本的出现次数越少，则词对于句子的重要程度越高。</p><hr><p>9️⃣[Numpy]<br>在Numpy中，一个列表虽然是横着表示的，但它是列向量。我之前居然没有注意到这个问题。</p><p><img src="/images/2018-07-29-15328375536418.jpg" width="50%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Paper </tag>
            
            <tag> Tf-idf </tag>
            
            <tag> Numpy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Mac配置复旦有线网</title>
      <link href="/2018/07/29/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Mac%E9%85%8D%E7%BD%AE%E5%A4%8D%E6%97%A6%E6%9C%89%E7%BA%BF%E7%BD%91/"/>
      <url>/2018/07/29/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Mac%E9%85%8D%E7%BD%AE%E5%A4%8D%E6%97%A6%E6%9C%89%E7%BA%BF%E7%BD%91/</url>
      
        <content type="html"><![CDATA[<h3 id="配置ip、子网掩码、DNS、路由器"><a href="#配置ip、子网掩码、DNS、路由器" class="headerlink" title="配置ip、子网掩码、DNS、路由器"></a>配置ip、子网掩码、DNS、路由器</h3><p>有线似乎不支持DHCP，因此只好自己设置。<br>首先连接上有线，将配置iPv4选为手动。问实验室的学长具体的ip地址、子网掩码、路由器、DNS服务器。其中ip地址最后三位要自己设定，只要不和其他人冲突就好。</p><p><img src="/images/2018-07-29-15328333841584.jpg" width="50%" height="50%"></p><p><img src="/images/2018-07-29-15328335236981.jpg" width="50%" height="50%"></p><h3 id="手动认证"><a href="#手动认证" class="headerlink" title="手动认证"></a>手动认证</h3><p>到认证平台，下载Mac客户端，其实就是一个.sh文件：<br><a href="http://10.108.255.249/srun_portal_pc.php?ac_id=1&amp;&amp;phone=1" target="_blank" rel="noopener">http://10.108.255.249/srun_portal_pc.php?ac_id=1&amp;&amp;phone=1</a></p><p><img src="/images/2018-07-29-15328336395029.jpg" width="30%" height="30%"></p><p>然后，打开文件配置用户名密码，注意到等号后面要有双引号：</p><p><img src="/images/2018-07-29-15328337135244.jpg" width="50%" height="50%"></p><p>保存并放入终端运行，接下来就可以使用有线网了。</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>似乎，每次重新连接都要这样配置，我没有试过不清楚；<br>有线网好像也没有比无线网快多少，但应该会稳定一些。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 网络 </tag>
            
            <tag> 配置 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ssh快速登录配置</title>
      <link href="/2018/07/29/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/ssh%E5%BF%AB%E9%80%9F%E7%99%BB%E5%BD%95%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/07/29/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/ssh%E5%BF%AB%E9%80%9F%E7%99%BB%E5%BD%95%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>分配了服务器之后，每次要ssh进入都很麻烦：<code>ssh user_name@ip_address</code> 然后还要输入密码。</p><p>特别是如果分配了多个服务器，那有时候还容易忘记ip地址。因此如果能够一条命令就进入服务器能够减少麻烦。<br>主要有三点：</p><ol><li>创建rsa key</li><li>上传public key到服务器</li><li>设置alias</li></ol><h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><h2 id="创建rsa-key"><a href="#创建rsa-key" class="headerlink" title="创建rsa key"></a>创建rsa key</h2><p>在终端输入命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p>当然如果以前有创建过的可以不用。</p><p>结果：</p><p><img src="/images/2018-07-29-Xnip2018-07-29_10-43-15.jpg" width="50%" height="50%"></p><h2 id="上传public-key到服务器"><a href="#上传public-key到服务器" class="headerlink" title="上传public key到服务器"></a>上传public key到服务器</h2><p>使用命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub user@127.0.0.1</span><br></pre></td></tr></table></figure></p><p>输入密码即可</p><p>结果：</p><p><img src="/images/2018-07-29-15328324683354.jpg" width="60%" height="60%"></p><h2 id="设置alias"><a href="#设置alias" class="headerlink" title="设置alias"></a>设置alias</h2><p>完成以上步骤就可以不输入密码登录，但还是需要输入ip地址和用户名，为了更简化操作，给命令起个别名。需要配置 .bash_profile文件。<br>输入命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure><p>在文件后面添加以下文字：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># alias </span><br><span class="line">alias ssh×××=&quot;ssh user_name@ip_address&quot;</span><br><span class="line">alias ssh×××=&quot;ssh user_name@ip_address&quot;</span><br></pre></td></tr></table></figure><p>其中 ×××是你自己起的名字，可以是服务器的名字，user_name和ip_address是自己服务器的用户名和地址。保存更改退出。</p><p>然后还要使其生效:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><p>这样，输入别名，就可以直接登录了：</p><p><img src="/images/2018-07-29-15328329815456.jpg" width="50%" height="50%"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.jianshu.com/p/66d658c7cb9e" target="_blank" rel="noopener">https://www.jianshu.com/p/66d658c7cb9e</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 配置 </tag>
            
            <tag> ssh </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于困惑度</title>
      <link href="/2018/07/29/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8E%E5%9B%B0%E6%83%91%E5%BA%A6/"/>
      <url>/2018/07/29/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8E%E5%9B%B0%E6%83%91%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<p>前几天在写新手任务<a href="https://github.com/FudanNLP/nlp-beginner" target="_blank" rel="noopener">task3</a>的时候，参考了Pytorch官方example的word language model，官方example在训练过程中计算困惑度是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">math.exp(cur_loss)</span><br></pre></td></tr></table></figure><p>其中，cur_loss表示交叉熵的loss，即 $-P(\hat{x})logP(x)$，$\hat{x}$表示ground truth。</p><p>然而，在查阅了困惑度相关资料后，我发现，困惑度的定义是这样的：</p><script type="math/tex; mode=display">\begin{aligned}PP(S)= &{P(w_{1}w_{2}...w_{N})}^{-\frac{1}{N}} \\= &\sqrt[N]{\frac{1}{p(w_1 w_2 ... w_N)}} \\= & \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} }\end{aligned}</script><p>这是另一种形式:</p><script type="math/tex; mode=display">\begin{aligned}Perplexity (W)=& 2^{H(W)} \\= & {P(w_{1}w_{2}...w_{N})}^{-\frac{1}{N}} \\= & \sqrt[N]{\frac{1}{p(w_1 w_2 ... w_N)}} \\= & \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} }\end{aligned}</script><p>可以看到，二者本质是一样的。</p><p><strong>那么，为什么在代码中以e为底去计算困惑度，而不是2呢?</strong></p><p>实际上，是因为在上述公式中，log是以2为底的，但在Pytorch中，log默认是以e为底的。因此在代码中，需要用e作为指数的底来还原成困惑度的原本形式： </p><script type="math/tex; mode=display">\begin{aligned}\sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} }\end{aligned}</script><p>最后这是perplexity的数学推导：<br><a href="https://www.zhihu.com/question/58482430" target="_blank" rel="noopener">https://www.zhihu.com/question/58482430</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 困惑度 </tag>
            
            <tag> perplexity </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词2</title>
      <link href="/2018/07/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D2/"/>
      <url>/2018/07/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D2/</url>
      
        <content type="html"><![CDATA[<p>本周的诗词有两篇是已经背过的，权当是复习了一遍。</p><hr><p>1️⃣</p><h3 id="下终南山过斛斯山人宿置酒"><a href="#下终南山过斛斯山人宿置酒" class="headerlink" title="下终南山过斛斯山人宿置酒"></a>下终南山过斛斯山人宿置酒</h3><p>[唐] 李白<br>暮从碧山下，山月随人归。<br>却顾所来径，苍苍横翠微。<br>相携及田家，童稚开荆扉。<br>绿竹入幽径，青萝拂行衣。<br>欢言得所憩，美酒聊共挥。<br>长歌吟松风，曲尽河星稀。<br>我醉君复乐，陶然共忘机。</p><p><a href="http://m.xichuangzhu.com/work/57b900307db2a20054269a2a" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b900307db2a20054269a2a</a></p><hr><p>2️⃣</p><h3 id="逢入京使"><a href="#逢入京使" class="headerlink" title="逢入京使"></a>逢入京使</h3><p>[唐] 岑参<br>故园东望路漫漫，双袖龙钟泪不乾。<br><strong>马上相逢无纸笔，凭君传语报平安。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b92218df0eea006335f923" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b92218df0eea006335f923</a></p><hr><p>3️⃣</p><h3 id="念奴娇·赤壁怀古"><a href="#念奴娇·赤壁怀古" class="headerlink" title="念奴娇·赤壁怀古"></a>念奴娇·赤壁怀古</h3><p>[宋] 苏轼<br>大江东去，浪淘尽、千古风流人物。故垒西边，人道是、三国周郎赤壁。乱石穿空，惊涛拍岸，卷起千堆雪。江山如画，一时多少豪杰。<br>遥想公瑾当年，小乔初嫁了，雄姿英发。羽扇纶巾，谈笑间，樯橹灰飞烟灭。故国神游，多情应笑我，早生华发。<strong>人生如梦，一尊还酹江月。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b8bda2df0eea006333ecd2" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8bda2df0eea006333ecd2</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录1</title>
      <link href="/2018/07/23/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB1/"/>
      <url>/2018/07/23/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB1/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-get-batch"><a href="#1️⃣-get-batch" class="headerlink" title="1️⃣ get_batch"></a>1️⃣ get_batch</h3><p>注意到shuffle的标准做法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">(self,data,batch_size=<span class="number">32</span>,is_shuffle)</span>:</span></span><br><span class="line">  N=len(data)  <span class="comment">#获得数据的长度</span></span><br><span class="line">  <span class="keyword">if</span> is_shuffle <span class="keyword">is</span> <span class="keyword">True</span>:</span><br><span class="line">    r=random.Random()</span><br><span class="line">    r.seed()</span><br><span class="line">    r.shuffle(data) <span class="comment">#如果is_shuffle为真则打乱</span></span><br><span class="line">  <span class="comment">#开始获得batch，使用[ for in ]</span></span><br><span class="line">  batch=[data[k:k+batch_size] <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>,N,batch_size)]</span><br><span class="line">  <span class="keyword">if</span> N%batch_size!=<span class="number">0</span>:  <span class="comment">#处理不整除问题，如果有显式要求丢掉则不需要处理，这里默认处理</span></span><br><span class="line">    remainder=N-N%batch_size  <span class="comment">#剩下的部分</span></span><br><span class="line">    batch.append(data[temp:N])</span><br><span class="line">  <span class="keyword">return</span> batch</span><br></pre></td></tr></table></figure><hr><h3 id="2️⃣使用gensim将GloVe读入"><a href="#2️⃣使用gensim将GloVe读入" class="headerlink" title="2️⃣使用gensim将GloVe读入"></a>2️⃣使用gensim将GloVe读入</h3><p>实际上这份代码有点问题，在使用过程中，发现glove文件需要放在gensim的文件夹下才能被读到(7.20 updated,应该使用绝对地址)，并不好。</p><p>教程地址：<a href="https://radimrehurek.com/gensim/scripts/glove2word2vec.html" target="_blank" rel="noopener">gensim: scripts.glove2word2vec – Convert glove format to word2vec</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1. 使用gensim读入word2vec</span></span><br><span class="line"></span><br><span class="line">model = gensim.models.KeyedVectors.load_word2vec_format(</span><br><span class="line">        fname=<span class="string">'GoogleNews-vectors-negative300-SLIM.bin'</span>, binary=<span class="keyword">True</span>)</span><br><span class="line">words = model.vocab  <span class="comment">#获得词表</span></span><br><span class="line">vector= model[word]  <span class="comment">#word是words里面的元素</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2. 使用gensim读入glove</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line"><span class="keyword">from</span> gensim.test.utils <span class="keyword">import</span> datapath, get_tmpfile</span><br><span class="line"><span class="keyword">from</span> gensim.scripts.glove2word2vec <span class="keyword">import</span> glove2word2vec</span><br><span class="line">glove_file=datapath(<span class="string">'glove.txt'</span>)  <span class="comment">#最好使用绝对地址</span></span><br><span class="line">tmp_file=get_tmpfile(<span class="string">'word2vec.txt'</span>)</span><br><span class="line">glove2word2vec(glove_file,tmp_file)</span><br><span class="line">model=KeyedVectors.load_word2vec_format(tmp_file)</span><br><span class="line"><span class="comment">#接下来使用的方法是一样的</span></span><br></pre></td></tr></table></figure></p><hr><h3 id="3️⃣data-split方法"><a href="#3️⃣data-split方法" class="headerlink" title="3️⃣data_split方法"></a>3️⃣data_split方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_split</span><span class="params">(seed=<span class="number">1</span>, proportion=<span class="number">0.7</span>)</span>:</span> </span><br><span class="line">    data = list(iter_corpus())</span><br><span class="line">    ids = list(range(len(data)))</span><br><span class="line"></span><br><span class="line">    N = int(len(ids) * proportion)  <span class="comment"># number of training data</span></span><br><span class="line"></span><br><span class="line">    rng = random.Random(seed)</span><br><span class="line">    rng.shuffle(ids)</span><br><span class="line">    test_ids = set(ids[N:])</span><br><span class="line">    train_data = []</span><br><span class="line">    test_data = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> x[<span class="number">1</span>] <span class="keyword">in</span> test_ids:  <span class="comment"># x[1]: sentence id</span></span><br><span class="line">            test_data.append(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train_data.append(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_data, test_data</span><br></pre></td></tr></table></figure><hr><h3 id="4️⃣对string预处理"><a href="#4️⃣对string预处理" class="headerlink" title="4️⃣对string预处理"></a>4️⃣对string预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_str</span><span class="params">(string)</span>:</span></span><br><span class="line">    string = re.sub(<span class="string">r"[^A-Za-z0-9()!?\'\`]"</span>, <span class="string">""</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'s"</span>, <span class="string">" \'s"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'m"</span>, <span class="string">" \'m"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'ve"</span>, <span class="string">" \'ve"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"n\'t"</span>, <span class="string">" n\'t"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'re"</span>, <span class="string">" \'re"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'d"</span>, <span class="string">" \'d"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'ll"</span>, <span class="string">" \'ll"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r","</span>, <span class="string">" , "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"!"</span>, <span class="string">" ! "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\("</span>, <span class="string">" \( "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\)"</span>, <span class="string">" \) "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\?"</span>, <span class="string">" \? "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\s&#123;2,&#125;"</span>, <span class="string">" "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\@.*?[\s\n]"</span>, <span class="string">""</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"https*://.+[\s]"</span>, <span class="string">""</span>, string)</span><br><span class="line">    <span class="keyword">return</span> string.strip().lower()</span><br></pre></td></tr></table></figure><hr><h3 id="5️⃣collate-fn-batch）"><a href="#5️⃣collate-fn-batch）" class="headerlink" title="5️⃣collate_fn(batch）"></a>5️⃣collate_fn(batch）</h3><p>重写collate_fn组建mini-batch，在NLP中常用，句子的不等长性<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(batch)</span>:</span>  <span class="comment"># rewrite collate_fn to form a mini-batch</span></span><br><span class="line">    lengths = np.array([len(data[<span class="string">'sentence'</span>]) <span class="keyword">for</span> data <span class="keyword">in</span> batch])</span><br><span class="line">    sorted_index = np.argsort(-lengths)</span><br><span class="line">    lengths = lengths[sorted_index]  <span class="comment"># descend order</span></span><br><span class="line"></span><br><span class="line">    max_length = lengths[<span class="number">0</span>]</span><br><span class="line">    batch_size = len(batch)</span><br><span class="line">    sentence_tensor = torch.LongTensor(batch_size, int(max_length)).zero_()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, index <span class="keyword">in</span> enumerate(sorted_index):</span><br><span class="line">        sentence_tensor[i][:lengths[i]] = torch.LongTensor(batch[index][<span class="string">'sentence'</span>][:max_length])</span><br><span class="line"></span><br><span class="line">    sentiments = torch.autograd.Variable(torch.LongTensor([batch[i][<span class="string">'sentiment'</span>] <span class="keyword">for</span> i <span class="keyword">in</span> sorted_index]))</span><br><span class="line">    <span class="keyword">if</span> config.use_cuda:</span><br><span class="line">        packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(Variable(sentence_tensor.t()).cuda(), lengths)  <span class="comment">#remember to transpose</span></span><br><span class="line">        sentiments = sentiments.cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(Variable(sentence_tensor.t()),lengths)  <span class="comment"># remember to transpose</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'sentence'</span>: packed_sequences, <span class="string">'sentiment'</span>: sentiments&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">## 重写collate_fn(batch)以用于dataloader使用，使用方法如下：</span></span><br><span class="line"></span><br><span class="line">train_dataloader=DataLoader(train_data,batch_size=<span class="number">32</span>,shuffle=<span class="keyword">True</span>,collate_fn=collate_fn)</span><br><span class="line">​</span><br><span class="line"><span class="comment">## 其中，train_dataloader可循环遍历​​。</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p><hr><h3 id="6️⃣使用yield获得数据的generator"><a href="#6️⃣使用yield获得数据的generator" class="headerlink" title="6️⃣使用yield获得数据的generator"></a>6️⃣使用yield获得数据的generator</h3><p>yield的用法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span><span class="params">(txt_file)</span>:</span>     <span class="comment"># return generator</span></span><br><span class="line">    <span class="keyword">with</span> open(txt_file,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="keyword">if</span> len(line.strip())==<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            sentence=list(line.strip())+[<span class="string">'&lt;eos&gt;'</span>]</span><br><span class="line">            <span class="keyword">yield</span> sentence</span><br><span class="line">            </span><br><span class="line"><span class="comment">#在使用的时候：</span></span><br><span class="line">dataset=get_dataset(txt_file)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> dataset:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果需要还可以改成list形式</span></span><br><span class="line">dataset=list(get_dataset(txt_file))</span><br></pre></td></tr></table></figure></p><hr><h3 id="7️⃣动态创建RNN实例"><a href="#7️⃣动态创建RNN实例" class="headerlink" title="7️⃣动态创建RNN实例"></a>7️⃣动态创建RNN实例</h3><p>根据rnn_type动态创建对象实例，使用了getattr<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rnn in ['GRU','LSTM','RNN']</span></span><br><span class="line"></span><br><span class="line">self.rnn = getattr(nn, self.rnn_type)(self.embedding_dim, self.hidden_dim, self.num_layers, dropout=self.dropout)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词1</title>
      <link href="/2018/07/23/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D1/"/>
      <url>/2018/07/23/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D1/</url>
      
        <content type="html"><![CDATA[<p>本周背了四篇。</p><hr><p>1️⃣</p><h3 id="临江仙·夜归临皋"><a href="#临江仙·夜归临皋" class="headerlink" title="临江仙·夜归临皋"></a>临江仙·夜归临皋</h3><p>[宋] 苏轼<br>夜饮东坡醒复醉，归来彷彿三更。家童鼻息已雷鸣，敲门都不应，倚杖听江声。<br><strong>长恨此身非我有，何时忘却营营？</strong>夜阑风静縠纹平，小舟从此逝，江海寄馀生。</p><p>縠（hú）纹<br>皋（gao）<br><a href="http://m.xichuangzhu.com/work/57ae79400a2b580063150e39" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57ae79400a2b580063150e39</a></p><hr><p>2️⃣</p><h3 id="蝶恋花·阅尽天涯离别苦"><a href="#蝶恋花·阅尽天涯离别苦" class="headerlink" title="蝶恋花·阅尽天涯离别苦"></a>蝶恋花·阅尽天涯离别苦</h3><p>[清] 王国维<br>阅尽天涯离别苦。不道归来，零落花如许。花底相看无一语，绿窗春与天俱莫。<br>待把相思灯下诉。一缕新欢，旧恨千千缕。<strong>最是人间留不住，朱颜辞镜花辞树。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b8ef70128fe10054c91d17" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8ef70128fe10054c91d17</a></p><hr><p>3️⃣</p><h3 id="送友人"><a href="#送友人" class="headerlink" title="送友人"></a>送友人</h3><p>[唐] 李白<br>青山横北郭，白水绕东城。<br>此地一为别，孤蓬万里征。<br><strong>浮云游子意，落日故人情。</strong><br>挥手自兹去，萧萧班马鸣。</p><p><a href="http://m.xichuangzhu.com/work/57b8facfd342d3005ac6ffb4" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8facfd342d3005ac6ffb4</a></p><hr><p>4️⃣</p><h3 id="黄鹤楼送孟浩然之广陵"><a href="#黄鹤楼送孟浩然之广陵" class="headerlink" title="黄鹤楼送孟浩然之广陵"></a>黄鹤楼送孟浩然之广陵</h3><p>[唐] 李白<br>故人西辞黄鹤楼，烟花三月下扬州。<br>孤帆远影碧空尽，唯见长江天际流。</p><p><a href="http://m.xichuangzhu.com/work/57b8f306128fe10054c92fb8" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8f306128fe10054c92fb8</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>安装conda错误</title>
      <link href="/2018/07/23/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/%E5%AE%89%E8%A3%85conda%E9%94%99%E8%AF%AF/"/>
      <url>/2018/07/23/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/%E5%AE%89%E8%A3%85conda%E9%94%99%E8%AF%AF/</url>
      
        <content type="html"><![CDATA[<p>在服务器上安装conda的时候，一开始使用了pip安装<br><code>pip install conda</code><br>在安装好conda之后想要使用conda命令，出现：</p><p>ERROR: The install method you used for conda—probably either <code>pip install conda</code> or <code>easy_install conda</code>—is not compatible with using conda as an application. If your intention is to install conda as a standalone application, currently supported install methods include the Anaconda installer and the miniconda installer. You can download the miniconda installer from <a href="https://conda.io/miniconda.html" target="_blank" rel="noopener">https://conda.io/miniconda.html</a>.</p><p><img src="/images/2018-07-23-15323331261104.jpg" alt=""></p><p>然后到官网下载.sh文件并bash安装，仍然没有解决该问题；接着尝试pip uninstall conda，出现<br><img src="/images/2018-07-23-15323337042406.jpg" alt=""></p><p>最后在查阅了网上之后，使用 <code>which conda</code>找到conda的地址，并删除<code>rm ×××</code><br><img src="/images/2018-07-23-15323337894186.jpg" alt=""></p><p>最后重新bash安装即可。</p>]]></content>
      
      
      
        <tags>
            
            <tag> conda </tag>
            
            <tag> 遇到的问题 </tag>
            
            <tag> 杂七杂八 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
