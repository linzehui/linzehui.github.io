<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>深度炼丹tricks合集</title>
      <link href="/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%B7%B1%E5%BA%A6%E7%82%BC%E4%B8%B9tricks%E5%90%88%E9%9B%86/"/>
      <url>/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%B7%B1%E5%BA%A6%E7%82%BC%E4%B8%B9tricks%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p>长期更新。受知乎深度炼丹的启发，以及个人在实践过程和阅读中会接触到一些tricks，认为有必要做一个合集，将一些可能有用的tricks做记录。有实践过的会特别标注。</p><h2 id="调参技巧"><a href="#调参技巧" class="headerlink" title="调参技巧"></a>调参技巧</h2><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><h4 id="1️⃣zero-center"><a href="#1️⃣zero-center" class="headerlink" title="1️⃣zero-center"></a>1️⃣zero-center</h4><p>[9]将数据中心化</p><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><h4 id="1️⃣Xavier-initialization-7-方法"><a href="#1️⃣Xavier-initialization-7-方法" class="headerlink" title="1️⃣Xavier initialization[7]方法"></a>1️⃣Xavier initialization[7]方法</h4><p>适用[9]于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)</p><h4 id="2️⃣He-initialization-8-方法"><a href="#2️⃣He-initialization-8-方法" class="headerlink" title="2️⃣He initialization[8]方法"></a>2️⃣He initialization[8]方法</h4><p>适用[9]于ReLU：scale = np.sqrt(6/n)</p><h4 id="3️⃣Batch-normalization-10"><a href="#3️⃣Batch-normalization-10" class="headerlink" title="3️⃣Batch normalization[10]"></a>3️⃣Batch normalization[10]</h4><h4 id="4️⃣RNN-LSTM-init-hidden-state"><a href="#4️⃣RNN-LSTM-init-hidden-state" class="headerlink" title="4️⃣RNN/LSTM init hidden state"></a>4️⃣RNN/LSTM init hidden state</h4><p>Hinton[3]提到将RNN/LSTM的初始hidden state设置为可学习的weight</p><h3 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h3><h4 id="1️⃣Gradient-Clipping-5-6"><a href="#1️⃣Gradient-Clipping-5-6" class="headerlink" title="1️⃣Gradient Clipping[5,6]"></a>1️⃣Gradient Clipping[5,6]</h4><h4 id="2️⃣learning-rate"><a href="#2️⃣learning-rate" class="headerlink" title="2️⃣learning rate"></a>2️⃣learning rate</h4><p>原则：当validation loss开始上升时，减少学习率。<br>[1]Time/Drop-based/Cyclical Learning Rate</p><h4 id="3️⃣batch-size"><a href="#3️⃣batch-size" class="headerlink" title="3️⃣batch size"></a>3️⃣batch size</h4><p>[2]中详细论述了增加batch size而不是减小learning rate能够提升模型表现。保持学习率不变，提高batch size，直到batch size~训练集/10，接下来再采用学习率下降的策略。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]<a href="https://blog.goodaudience.com/how-to-make-your-model-happy-again-part-1-40d94a9ffb41" target="_blank" rel="noopener">How to make your model happy again — part 1</a></p><p>[2]<a href="https://arxiv.org/abs/1711.00489" target="_blank" rel="noopener">Don’t Decay the Learning Rate, Increase the Batch Size</a></p><p>[3]<a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf" target="_blank" rel="noopener">CSC2535 2013: Advanced Machine Learning Lecture 10 Recurrent neural networks</a></p><p>[4]<a href="https://zhuanlan.zhihu.com/p/25110150" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25110150</a></p><p>[5]<a href="https://arxiv.org/abs/1211.5063" target="_blank" rel="noopener">On the difficulty of training Recurrent Neural Networks</a></p><p>[6]<a href="https://arxiv.org/abs/1612.08083" target="_blank" rel="noopener">Language Modeling with Gated Convolutional Networks</a></p><p>[7]<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Understanding the difficulty of training deep feedforward neural networks</a></p><p>[8]<a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></p><p>[9]<a href="https://www.zhihu.com/question/41631631" target="_blank" rel="noopener">知乎：你有哪些deep learning（rnn、cnn）调参的经验？</a></p><p>[10]<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 调参 </tag>
            
            <tag> tricks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词11</title>
      <link href="/2018/10/07/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D11/"/>
      <url>/2018/10/07/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D11/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣赋得古原草送别"><a href="#1️⃣赋得古原草送别" class="headerlink" title="1️⃣赋得古原草送别"></a>1️⃣赋得古原草送别</h3><p>[唐] 白居易<br>离离原上草，一岁一枯荣。<br><strong>野火烧不尽，春风吹又生</strong>。<br>远芳侵古道，晴翠接荒城。<br>又送王孙去，萋萋满别情。</p><p>萋萋（qī）：形容草木长得茂盛的样子。</p><p><a href="http://m.xichuangzhu.com/work/57b8a5371532bc005b99da51" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8a5371532bc005b99da51</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PRML第三章 回归的线性模型</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%9B%9E%E5%BD%92%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%9B%9E%E5%BD%92%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="线性基函数模型"><a href="#线性基函数模型" class="headerlink" title="线性基函数模型"></a>线性基函数模型</h1><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-Xnip2018-10-07_15-51-50.jpg" alt="0"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-Xnip2018-10-07_15-53-54.jpg" alt="1"></p><h1 id="偏置-⽅差分解"><a href="#偏置-⽅差分解" class="headerlink" title="偏置-⽅差分解"></a>偏置-⽅差分解</h1><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-Xnip2018-10-07_15-56-30.jpg" alt="0"></p><h1 id="贝叶斯线性回归"><a href="#贝叶斯线性回归" class="headerlink" title="贝叶斯线性回归"></a>贝叶斯线性回归</h1><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-Xnip2018-10-07_16-13-57.jpg" alt="1"></p><h1 id="贝叶斯模型⽐较"><a href="#贝叶斯模型⽐较" class="headerlink" title="贝叶斯模型⽐较"></a>贝叶斯模型⽐较</h1><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-Xnip2018-10-07_23-30-33.jpg" alt="1"></p><h1 id="证据近似"><a href="#证据近似" class="headerlink" title="证据近似"></a>证据近似</h1><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-09-Xnip2018-10-09_22-06-22.jpg" alt="1"></p><p>—-未完—-</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> PRML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 16:SVM</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2015:%20SVM/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2015:%20SVM/</url>
      
        <content type="html"><![CDATA[<p><strong>Hinge Loss+kernel method = SVM</strong></p><h3 id="Hinge-Loss"><a href="#Hinge-Loss" class="headerlink" title="Hinge Loss"></a>Hinge Loss</h3><p>SVM与logistic regression的区别即在于loss function的不同，logistic是cross entropy，而SVM是hinge loss<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388878731499.jpg" width="50%" height="50%"></p><p>也即如果分类间隔大于1，则 $L(m_i)=max(0,1−m_i(w))$，则损失为0。因此SVM更具鲁棒性，因为对离群点不敏感。</p><p>对于linear SVM：</p><ul><li>定义函数 $f(x)=\sum_i w_i x_i +b=w^T x$</li><li>定义损失函数  $L(f)=\sum_n l(f(x^n),\hat{y}^n)+\lambda ||w||_2$，其中$l(f(x^n),\hat{y}^n)=max(0,1-\hat{y}^n f(x))$</li><li><p>梯度下降求解（省略了正则化）</p><script type="math/tex; mode=display">\frac{\partial{l(f(x^n),\hat{y}^n})}{\partial{w_i}}=  \frac{\partial{l(f(x^n),\hat{y}^n})}{\partial{f(x^n)}}  \frac{\partial{f(x^n)}}{\partial{w_i}} x_i^n</script><p>  而</p><script type="math/tex; mode=display">f(x^n)=w^T \cdot x^n</script></li></ul><script type="math/tex; mode=display">\frac{\partial{max(0,1-\hat{y}^n f(x^n)})}{\partial{f(x^n)}}=\left\{               \begin{array}{**lr**}                -\hat{y}^n & if  \hat{y}^n f(x^n)<1 \\                 0  & otherwise &                 \end{array}  \right.</script><p>因此最终有：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388891611785.jpg" width="55%" height="50%"><br>我们接下来用$c^n(w)$替代$-\delta(\hat{y}^n f(x^n)&lt;1) \hat{y}^n$</p><h3 id="Kernel-Method"><a href="#Kernel-Method" class="headerlink" title="Kernel Method"></a>Kernel Method</h3><p>一个事实：$w$是$x$的线性加和，其中$α$不等于0对应的$x$就是support vectors</p><p>证明：<br>我们前面说过，更新过程：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388894194698.jpg" width="30%" height="50%"></p><p>将其组织成向量形式：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388894627632.jpg" width="25%" height="50%"></p><p><strong>如果我们将$w$初始化成0向量</strong>，那么$w$最终就是$x$的线性组合。证毕</p><p>因为$c(w)$是hinge loss，因此大多数的值是0，会造成$α$稀疏。<br>如果我们将训练数据$x$组织成一个矩阵，那么有：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388895570090.jpg" width="25%" height="50%"><br>也即：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388895870336.jpg" width="40%" height="50%"></p><p>所以对于$f(x)$，有：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388896378645.jpg" width="50%" height="50%"></p><p>实际上$X^Tx$就是每个训练数据和$x$进行点积的结果，但实际上线性函数往往表达能力不强，我们希望$x$能够变成非线性的。如果我们引入kernel，将点积换成kernel，则会有：</p><script type="math/tex; mode=display">f(x)=\sum_n \alpha_n (x_n\cdot x)=\sum_n \alpha_n K(x_n,x)</script><p>所以我们的问题就变成了：</p><ul><li>定义函数 $f(x)=\sum_n \alpha_n K(x_n,x)$</li><li>找到最佳的α，最小化loss function：$L(f)=\sum_n l(f(x^n),\hat{y}^n)=\sum_n l(\sum_{n’} \alpha_{n’} K(x^{n^{‘}},x^n),\hat{y}^n)$</li></ul><p>实际上我们不需要真的知道$x$的非线性的具体形式，我们只需要会算$K$就行，这种绕过$x$的具体形式的方法就是<strong>kernel trick</strong>。直接计算$K$，比先将$x$非线性转化再做点积来得高效。甚至有时候，我们对$x$做的非线性是无穷多维的，是无法直接做非线性化的。比如RBF核:</p><script type="math/tex; mode=display">K(x,z)=exp(-\frac{1}{2}||x-z||_2)</script><p>通过泰勒展开可以知道，RBF核是无穷维的。</p><p>另一个kernel的例子是sigmoid kernel：</p><script type="math/tex; mode=display">K(x,z)=tanh(x\cdot z)</script><p>当我们使用sigmoid kernel时，就相当于一层hidden layer的神经网络，如图：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388901736757.jpg" width="40%" height="50%"></p><p>给定一个输入，共有n个neuron，其中的weight就是每个训练数据的向量值，然后再将这些neuron加和得到输出。当然大部分的α的值是0，因此实质上神经元的个数和support vector的个数一致。</p><p>我们可以直接设计kernel，而不需要考虑x的非线性变换的形式，只要kernel符合mercer’s theory即可。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 15:Transfer Learning</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2015:%20Transfer%20Learning/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2015:%20Transfer%20Learning/</url>
      
        <content type="html"><![CDATA[<h3 id="Model-Fine-tuning"><a href="#Model-Fine-tuning" class="headerlink" title="Model Fine-tuning"></a>Model Fine-tuning</h3><p>假设我们有很多的source data $(x^s,y^s )$，与任务相关的target data $(x^t,y^t )$  很少。<br>我们利用source data训练一个模型，然后用target data来fine tune模型。</p><h4 id="conservative-training"><a href="#conservative-training" class="headerlink" title="conservative training"></a>conservative training</h4><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388871902739.jpg" width="50%" height="50%"></p><p>我们可以用source data训练好的模型的weight作为新的模型的weight，然后设定一些限制，比如source data作为输入的output应和target data作为输入的output尽量相似，或者参数尽量相似等。</p><h4 id="layer-transfer"><a href="#layer-transfer" class="headerlink" title="layer transfer"></a>layer transfer</h4><p>也就是新模型有几层是直接copy旧模型的，只训练其它层。注意到不同任务所应copy的层是不同的，语音任务最后几层效果好，图像识别前面几层效果好</p><h3 id="Multitask-Learning"><a href="#Multitask-Learning" class="headerlink" title="Multitask Learning"></a>Multitask Learning</h3><p>不同任务之间共享相同的中间层，如：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388872452545.jpg" width="30%" height="50%"><br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388872627707.jpg" width="30%" height="50%"></p><p>还有一种progressive neural networks：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388872920224.jpg" width="50%" height="50%"><br>首先训练好第一个任务的模型，然后在训练第二个模型的时候将第一个模型的隐层加入到第二个模型的隐层中；训练第三个模型则将第二个和第一个模型的隐层加入到第三个模型的隐层中，以此类推</p><h3 id="Domain-adversarial-training"><a href="#Domain-adversarial-training" class="headerlink" title="Domain-adversarial training"></a>Domain-adversarial training</h3><p>source data是有标签的，而target data是无标签的，都属于同一个任务，但数据是mismatch的，如：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388873325090.jpg" width="50%" height="50%"></p><p>因为NN的隐层可以理解成是在抽取图像的特征，我们希望能够在训练NN的过程中去掉source data的一些domain specific的特性，这样就可以用在target data上了。因此我们在feature exactor后面连接两个模块：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388873772888.jpg" width="50%" height="50%"></p><p>一方面我们希望抽取的特征能够使得分类器正确地分类，另一方面我们希望这些特征能够让domain classifier能够无法识别特征是从哪些data抽取得到的，这样得到的特征就是被去掉domain specific特征的。</p><p>具体训练：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388874447304.jpg" width="50%" height="50%"></p><h3 id="Zero-shot-Learning"><a href="#Zero-shot-Learning" class="headerlink" title="Zero-shot Learning"></a>Zero-shot Learning</h3><p>source data有标签，target data无标签，但任务不同，如：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388874838165.jpg" width="50%" height="50%"></p><h4 id="Representing-each-class-by-its-attributes"><a href="#Representing-each-class-by-its-attributes" class="headerlink" title="Representing each class by its attributes"></a>Representing each class by its attributes</h4><p>一种方法是将每一个类都用特征表示，但特征要足够丰富：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388875088114.jpg" width="50%" height="50%"></p><p>在训练的时候，输入是图片，输出则是这些特征：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388875558853.jpg" width="40%" height="50%"><br>这样在将target data放入训练好的NN后也会得到一个这样的attribute，查表即可找到最相似的特征对应的类。</p><h4 id="Attribute-embedding"><a href="#Attribute-embedding" class="headerlink" title="Attribute embedding"></a>Attribute embedding</h4><p>如果特征维度太高，也可以将特征压缩成一个向量表示，这样在训练的时候，输出则是这样的向量特征，输入target data，输出向量特征，找到最近的特征对应的类即可<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388875888699.jpg" width="50%" height="50%"></p><h4 id="Attribute-embedding-word-embedding"><a href="#Attribute-embedding-word-embedding" class="headerlink" title="Attribute embedding + word embedding"></a>Attribute embedding + word embedding</h4><p>如果没有attribute数据，利用word embedding也可以达到不错的效果。<br>在zero-shot learning中，光是让相同类的f和g相似是不够的，还应该让不同的f和g尽量远。</p><script type="math/tex; mode=display">f^∗,g^∗=arg min_{(f,g)}⁡∑_nmax(0,k−f(x^n )\cdot g(y^n )+max_{(m≠n)} ⁡f(x^m )\cdot g(x^m ) )</script>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Transfer Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 14:Unsupervised Learning:Generation</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2014:%20Unsupervised%20Learning:%20Generation/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2014:%20Unsupervised%20Learning:%20Generation/</url>
      
        <content type="html"><![CDATA[<h3 id="Component-by-component"><a href="#Component-by-component" class="headerlink" title="Component-by-component"></a>Component-by-component</h3><p>对于图像来说，每次生成一个pixel：PixelRNN</p><h3 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h3><p>架构：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388837191574.jpg" width="50%" height="50%"></p><p>其中e是噪声，σ是方差，目标是最小化reconstruction error，以及一个限制。该限制的目的即防止σ=0，m是正则化项。</p><p><del>中间的推导以及为什么是这样的架构我还不是很懂，之后再更新。</del><br>实际上可以这么理解，有几个要点：</p><ul><li>首先我们是基于这么一个假设：中间的code应当是服从正态分布的，而encoder的作用即在于拟合该正态分布的均值与方差的对数（因为方差应当恒为正，但神经网络的输出可能有正有负）</li><li>如果生成出来的code不符合正态分布，会有一个惩罚项，也就是上图的constraint（可以通过KL散度推导获得）</li><li>按理说，应当是在生成了均值和方差后，定义好该正态分布，然后再从中采样，但是这样没办法回传更新梯度，因此这里使用重参数技巧(Reparameterization Trick)，也即从$N(\mu,\sigma^2)$中采样$Z$，相当于从$N(0,I)$中采样$\varepsilon$，然后让$Z=\mu + \varepsilon \times \mu$</li></ul><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-08-15389638077301.jpg" width="70%" height="50%"></p><p><strong>Reference</strong>:<br><a href="https://www.sohu.com/a/226209674_500659" target="_blank" rel="noopener">https://www.sohu.com/a/226209674_500659</a></p><p>VAE的主要问题在于，网络只试图去记住见过的图像，但没法真正去生成没见过的图像。</p><h3 id="Generative-Adversarial-Network-GAN"><a href="#Generative-Adversarial-Network-GAN" class="headerlink" title="Generative Adversarial Network (GAN)"></a>Generative Adversarial Network (GAN)</h3><p>GAN包含一个discriminator和一个generator，generator试图生成能够骗过discriminator的样本，而generator试图能够将generator生成的样本和真实的样本区分。</p><p>之后会有详细的介绍。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> Unsupervised Learning </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Generation </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 13:Unsupervised Learning:Auto-encoder</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2013:%20Unsupervised%20Learning:%20Auto-encoder/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2013:%20Unsupervised%20Learning:%20Auto-encoder/</url>
      
        <content type="html"><![CDATA[<h3 id="Auto-encoder"><a href="#Auto-encoder" class="headerlink" title="Auto-encoder"></a>Auto-encoder</h3><p>由一个encoder和一个decoder组成，encoder负责将输入转成一个向量表示（维度通常小于输入），decoder负责将这段向量表示恢复成原来的输入。那么中间的code就可以作为输入的一个低维表示：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388832782913.jpg" width="50%" height="50%"></p><h3 id="Auto-encoder-for-CNN"><a href="#Auto-encoder-for-CNN" class="headerlink" title="Auto-encoder for CNN"></a>Auto-encoder for CNN</h3><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388833149617.jpg" width="50%" height="50%"></p><h4 id="Unpooling"><a href="#Unpooling" class="headerlink" title="Unpooling"></a>Unpooling</h4><p>有两种方法，一种在pooling的时候记录最大值的位置，在unpooling时在相对位置填充最大值，其他位置填充0；另一种不记录最大值位置，直接在pooling区域全部填充最大值。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388833530548.jpg" width="50%" height="50%"></p><h4 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h4><p>其实本质就是convolution。</p><p>这是convolution:</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388834044149.jpg" width="10%" height="50%"></p><p>我们期待的convolution：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388834434741.jpg" width="15%" height="50%"></p><p>实际上就等价在两边做padding，然后直接convolution：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388834751493.jpg" width="15%" height="50%"></p><h3 id="Auto-encoder的用处"><a href="#Auto-encoder的用处" class="headerlink" title="Auto-encoder的用处"></a>Auto-encoder的用处</h3><p>可以预训练每一层的DNN：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388835335550.jpg" width="50%" height="50%"></p><p>同理其它层也是一样，每次fix住其他层然后做Auto-encoder。那么在bp的时候只需要fine-tune就行。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> Unsupervised Learning </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Auto-encoder </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 12:Unsupervised Learning:Neighbor Embedding</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2012:%20Unsupervised%20Learning:%20Neighbor%20Embedding/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2012:%20Unsupervised%20Learning:%20Neighbor%20Embedding/</url>
      
        <content type="html"><![CDATA[<h3 id="Locally-Linear-Embedding-LLE"><a href="#Locally-Linear-Embedding-LLE" class="headerlink" title="Locally Linear Embedding (LLE)"></a>Locally Linear Embedding (LLE)</h3><p>一种降维方法<br>思想：假设每个点可以由其周围的点来表示<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388822769215.jpg" width="25%" height="50%"></p><p>我们需要找到这样的$w_{ij}$，使得：</p><script type="math/tex; mode=display">∑_i‖x^i−∑_j w_{ij} x^j ‖_2</script><p>这样在降维的时候，我们仍然保持x之间的这样的关系:<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388823792351.jpg" width="50%" height="50%"></p><h3 id="Laplacian-Eigenmaps"><a href="#Laplacian-Eigenmaps" class="headerlink" title="Laplacian Eigenmaps"></a>Laplacian Eigenmaps</h3><p>一种降维方法<br>基本思想：如果$x^1$与$x^2$在高维空间中相近，则降维后也应该接近：</p><script type="math/tex; mode=display">S=1/2 ∑_{i,j} w_{i,j} (z^i−z^j )^2</script><p>其中：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388824984809.jpg" width="30%" height="50%"></p><p>如果将z全设为0，显然S最小，因此我们需要给z一个限制：z应当充满空间，也即假如z是M维，那么$\{z^1,z^2…,z^N\}$的秩应该等于M</p><h3 id="T-distributed-Stochastic-Neighbor-Embedding-t-SNE"><a href="#T-distributed-Stochastic-Neighbor-Embedding-t-SNE" class="headerlink" title="T-distributed Stochastic Neighbor Embedding (t-SNE)"></a>T-distributed Stochastic Neighbor Embedding (t-SNE)</h3><p>也是一种降维方法<br>前面提到的方法有一个问题：同一类的点确实聚在一起，但不同类的点并没有尽量分开<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388826477983.jpg" width="50%" height="50%"></p><p>t-SNE的主要思想：将数据点映射到概率分布，我们希望降维前和降维后，数据分布的概率应当尽可能一致。<br>t-SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。t-SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。</p><p>如何做？<br>在高维空间中，我们定义：</p><script type="math/tex; mode=display">P(x^j |x^i )=\frac{S(x^i,x^j )}{∑_{k≠i}S(x^i,x^k )}</script><p>其中S表示i与j之间的相似度。</p><p>在低维空间中，同样有：</p><script type="math/tex; mode=display">Q(z^j |z^i )=\frac{S′(z^i,z^j )}{∑_{k≠i}S′(z^i,z^k )}</script><p>使用KL散度去计算两个分布之间的差异：</p><script type="math/tex; mode=display">L=∑_i KL(P(∗|x^i )||Q(∗|z^i )) =∑_i∑_j P(x^j |x^i )\frac{log P(x^j |x^i )}{Q(z^j |z^i )}</script><p>t-SNE中，高维空间和低维空间计算相似度的公式不大一样：</p><script type="math/tex; mode=display">S(x^i,x^j )=exp(−‖x^i−x^j ‖_2 )</script><script type="math/tex; mode=display">S′(z^i,z^j )=\frac{1}{(1+‖z^i−z^j ‖_2)}</script><p>两个公式的图示：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388830652023.jpg" width="70%" height="50%"></p><p>也即<strong>低维空间会拉长距离，使得距离远的点尽可能被拉开</strong>。</p><p>t-SNE的问题在于：t-SNE无法对新的数据点进行降维。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> Unsupervised Learning </tag>
            
            <tag> Neighbor Embedding </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 11:Unsupervised Learning:Linear Dimension Reduction</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2011:%20Unsupervised%20Learning:%20Linear%20Dimension%20Reduction/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2011:%20Unsupervised%20Learning:%20Linear%20Dimension%20Reduction/</url>
      
        <content type="html"><![CDATA[<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><h3 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h3><p>算法步骤：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388800377875.jpg" width="70%" height="50%"></p><p>迭代更新使得最后聚类中心收敛。但事先需要定好有多少类。</p><h3 id="Hierarchical-Agglomerative-Clustering-HAC"><a href="#Hierarchical-Agglomerative-Clustering-HAC" class="headerlink" title="Hierarchical Agglomerative Clustering (HAC)"></a>Hierarchical Agglomerative Clustering (HAC)</h3><p>自下而上，每次选两个最近的聚为一类，直到所有的都分成一类<br>最后选择一个阈值划分，如蓝色绿色和红色的线<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388801021791.jpg" width="50%" height="50%"></p><h2 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h2><p>找到一个映射，使得x能够映射到低维z</p><h3 id="Principle-Component-Analysis-PCA"><a href="#Principle-Component-Analysis-PCA" class="headerlink" title="Principle Component Analysis (PCA)"></a>Principle Component Analysis (PCA)</h3><p>目的是找到一个维度，使得投影得到的variance最大，也即最大程度保留数据的差异性。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388801830659.jpg" width="50%" height="50%"></p><p>形式化可以写成（一维情形）：</p><script type="math/tex; mode=display">Var(z_1 )=\frac{1}{N} ∑_{z_1}(z_1−\overline{z_1} )^2</script><p>其中：</p><script type="math/tex; mode=display">‖w^1 ‖_2=1</script><script type="math/tex; mode=display">z_1=w^1 \cdot x</script><p>$\overline{z_1}$表示z的均值</p><p>假如我们要投影到多维，其他维度也有同样的目标。其中每个维度之间都应该是相互正交的。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388804752506.jpg" width="20%" height="50%"></p><h4 id="如何做？"><a href="#如何做？" class="headerlink" title="如何做？"></a>如何做？</h4><p>找到$ \frac{1}{N}∑(x−\overline{x} ) (x−\overline{x})^T$的前k个最大的特征值对应的特征向量，组合起来即是我们要找的$W$</p><h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><p>—-Warning of Math—-<br>目的：$Var(z_1 )=\frac{1}{N} ∑_{z_1}(z_1−\overline{z_1} )^2 $<br>其中 $\overline{z_1} =\frac{1}{N} ∑{z_1} = \frac{1}{N} ∑ w^1 \cdot x=w^1\cdot \overline{x}$</p><p>推导：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388811276042.jpg" width="35%" height="50%"><br>改变符号 $S=Cov(x)$</p><p>利用拉格朗日乘数法，有：<br>$Sw^1=αw^1$<br>等式两边各左乘$(w^1)^T$，有：<br>$(w^1 )^T Sw^1=α(w^1 )^T w^1=α$</p><p>也即，$α$是$S$的特征值，选择最大的特征值，就能够最大化我们的目标。</p><p>同理，我们要找$w^2$，最大化$(w^2 )^T Sw^2$，其中有：<br>$(w^2 )^T w^2=1$<br>$(w^2 )^T w^1=0$ （与第一维正交）</p><p>因此利用拉格朗日乘数法：</p><script type="math/tex; mode=display">g(w^2 )= (w^2 )^T Sw^2−α((w^2 )^T w^2−1)−β((w^2 )^T w^1−0)</script><p>最终得到，w2对应第二大的特征值的特征向量。</p><p>以此类推，其他维也同理。<br>—-End of Math—-</p><h4 id="PCA的其他"><a href="#PCA的其他" class="headerlink" title="PCA的其他"></a>PCA的其他</h4><p>实际上最终得到的z，每一维之间的协方差都为0<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388815546680.jpg" width="50%" height="50%"></p><p>证明如下：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388815837458.jpg" width="50%" height="50%"></p><p>PCA也可以用SVD来做：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388816250075.jpg" width="60%" height="50%"></p><p>U中保存了K个特征向量。</p><p>从另一种角度理解PCA，也可以认为PCA是一种autoencoder：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388816896369.jpg" width="50%" height="50%"></p><h4 id="PCA的问题"><a href="#PCA的问题" class="headerlink" title="PCA的问题"></a>PCA的问题</h4><p>PCA是无监督学习，如果有标签，则无法按照类别来进行正确降维，如：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388817393283.jpg" width="30%" height="50%"></p><p>第二就是PCA是线性变换，对于一些需要非线性变换的无能为力<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388817566149.jpg" width="28%" height="50%"></p><h3 id="Matrix-Factorization"><a href="#Matrix-Factorization" class="headerlink" title="Matrix Factorization"></a>Matrix Factorization</h3><p>定义：矩阵分解，就是将一个矩阵D分解为U和V的乘积，即对于一个特定的规模为m*n的矩阵D，估计出规模分别为m*k和n*k的矩阵U和V，使得$UV^T$的值尽可能逼近矩阵D。常用于推荐系统。</p><p>思想：<br>假如有一个矩阵：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388819053983.jpg" width="60%" height="50%"></p><p>假设横轴和纵轴每一维都有一个向量代表该维，矩阵的每个元素就是横轴和纵轴对应维的点积。我们的目的是尽可能减小：</p><script type="math/tex; mode=display">L=\sum_{(i,j)} (r^i \cdot r^j -n_{ij})^2</script><p>其中$r_i$ $r_j$就是向量表示，$n_{ij}$就是矩阵的内容。</p><p>可以使用SVD求解上式：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388820642382.jpg" width="50%" height="50%"></p><p>实际上，考虑每一行或列本身的特性，我们对Loss进行扩展：</p><script type="math/tex; mode=display">Minimizing \ \ L=\sum_{(i,j)} (r^i \cdot r^j +b_i+b_j-n_{ij})^2</script><p>使用SGD可以求解。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> Unsupervised Learning </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Linear Dimension Reduction </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>梯度消失与梯度爆炸的推导</title>
      <link href="/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E6%8E%A8%E5%AF%BC/"/>
      <url>/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E6%8E%A8%E5%AF%BC/</url>
      
        <content type="html"><![CDATA[<p> 记RNN中每一步的损失为$E_t$，则损失对$h_{t-1}$的权重$W$的导数有：</p><script type="math/tex; mode=display">\frac{\partial{E_t}}{\partial{W}}=\sum_{k=1}^{t}    \frac{\partial{E_t}}{\partial{y_t}} \frac{\partial{y_t}}{\partial{h_t}} \frac{\partial{h_t}}{\partial{h_k}} \frac{\partial{h_k}}{\partial{W}}</script><p>其中$\frac{\partial{h_t}}{\partial{h_k}}$使用链式法则有：</p><script type="math/tex; mode=display">\frac{\partial{h_t}}{\partial{h_k}} =     \prod_{j=k+1}^{t} \frac{\partial{h_j}}{\partial{h_{j-1}}} =    \prod_{j=k+1}^{t} W^T \times diag[f^{\prime}(h_{j-1})]</script><p>其中$\frac{\partial{h_j}}{\partial{h_{j-1}}}$ 是雅克比矩阵。对其取模(norm)，有：</p><script type="math/tex; mode=display">\rVert \frac{\partial{h_j}}{\partial{h_{j-1}}}\rVert ≤ \rVert W^T \rVert \rVert diag[f^{\prime}(h_{j-1})] \rVert ≤ \beta_W \beta_h</script><p>当$f$为sigmoid时，$f^{\prime}(h_{j-1})$最大值为1。</p><p>最终我们有：</p><script type="math/tex; mode=display">\rVert \frac{\partial{h_t}}{\partial{h_{k}}}\rVert ≤ \rVert \prod_{j=k+1}^{t} \frac{\partial{h_j}}{\partial{h_{j-1}}} \rVert ≤ (\beta_W \beta_h)^{t-k}</script><p>从上式可以看出，当t-k足够大时，如果$(\beta_W \beta_h)$小于1则$(\beta_W \beta_h)^{t-k}$则会变得非常小，相反，若$(\beta_W \beta_h)$大于1则$(\beta_W \beta_h)^{t-k}$则会变得非常大。</p><p>在计算机中，当梯度值很大时，会造成上溢(NaN)，也即梯度爆炸问题，当梯度值很小时，会变成0，也即梯度消失。注意到，t-k的损失实际上评估的是一个较远的词对当前t的贡献，梯度消失也即意味着对当前的贡献消失。</p><p>Reference:<br>CS224d: Deep Learning for NLP Lecture4</p>]]></content>
      
      
      
        <tags>
            
            <tag> 梯度消失 </tag>
            
            <tag> 梯度爆炸 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识9</title>
      <link href="/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8610/"/>
      <url>/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8610/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-正态分布"><a href="#1️⃣-正态分布" class="headerlink" title="1️⃣[正态分布]"></a>1️⃣[正态分布]</h3><p>高维正态分布是从一维发展而来的：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388761009977.jpg" width="70%" height="50%"></p><p><a href="https://www.zhihu.com/question/36339816" target="_blank" rel="noopener">https://www.zhihu.com/question/36339816</a></p><hr><h3 id="2️⃣-RNN"><a href="#2️⃣-RNN" class="headerlink" title="2️⃣[RNN]"></a>2️⃣[RNN]</h3><p>from <a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf" target="_blank" rel="noopener">https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf</a></p><p>通常而言，我们都会将RNN的initial state设为全0，但在Hinton的slide中提到，我们可以将初始状态作为可学习的变量，和我们在学习权重矩阵一样。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-07-15388770544817.jpg" width="80%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 调参 </tag>
            
            <tag> 正态分布 </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PRML第二章 概率分布</title>
      <link href="/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83/"/>
      <url>/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83/</url>
      
        <content type="html"><![CDATA[<h1 id="二元变量"><a href="#二元变量" class="headerlink" title="二元变量"></a>二元变量</h1><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-Xnip2018-09-30_14-25-21.jpg" alt="二元变量1"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-Xnip2018-09-30_14-26-46.jpg" alt="贝塔分布"></p><h1 id="多项式分布"><a href="#多项式分布" class="headerlink" title="多项式分布"></a>多项式分布</h1><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-Xnip2018-09-30_14-31-55.jpg" alt="多项式分布"></p><h1 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h1><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-Xnip2018-09-30_14-40-32.jpg" alt="高斯分布"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-Xnip2018-09-30_14-43-09.jpg" alt="2"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-Xnip2018-09-30_14-45-15.jpg" alt="3"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-Xnip2018-09-30_14-48-58.jpg" alt="4"></p><h1 id="指数族分布"><a href="#指数族分布" class="headerlink" title="指数族分布"></a>指数族分布</h1><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-Xnip2018-09-30_14-51-25.jpg" alt="1"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-03-Xnip2018-10-03_10-03-54.jpg" alt="2"></p><h1 id="非参数优化"><a href="#非参数优化" class="headerlink" title="非参数优化"></a>非参数优化</h1><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-03-Xnip2018-10-03_10-05-24.jpg" alt="1"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-10-03-Xnip2018-10-03_10-06-55.jpg" alt="2"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> PRML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 10:Semi-supervised learning</title>
      <link href="/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2010:%20Semi-supervised/"/>
      <url>/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2010:%20Semi-supervised/</url>
      
        <content type="html"><![CDATA[<p>什么是semi-supervised learning</p><p>给定数据${(x^r,\hat{y}^r)}_{r=1}^{R},{(x_u)}_{u=R}^{R+U}$，其中未标记数据远远多于标记数据 $U&gt;&gt;R$</p><p>为什么半监督学习有用？<br>因为未标记数据的分布可能能够给我们一些信息。</p><h3 id="生成模型的半监督学习"><a href="#生成模型的半监督学习" class="headerlink" title="生成模型的半监督学习"></a>生成模型的半监督学习</h3><p>给定两类$C_1$、$C_2$，要求得到后验概率分布</p><script type="math/tex; mode=display">P(C_1 |x)=\frac{P(x|C_1 )P(C_1 )}{(P(x|C_1 )P(C_1 )+P(x|C_2 )P(C_2 ) )}</script><p>其中联合概率分布服从高斯分布。未标记数据此时的作用即帮我们重新估计$P(C_1),P(C_2),\mu,\Sigma$</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382829251218.jpg" width="50%" height="50%"></p><p>如何做?<br>先初始化$P(C_1),P(C_2),\mu,\Sigma$，通常可以先用有标记数据进行估计</p><ol><li>计算每个未标记数据的后验概率分布</li><li>以该概率分布更新模型<br>不断重复直至拟合</li></ol><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382829987091.jpg" width="70%" height="50%"></p><p>原因：<br>当我们在做监督学习时，使用最大似然求解：</p><script type="math/tex; mode=display">logL(θ)=∑_{x^r,\hat{y}^r} logP_θ (x^r |\hat{y}^r )</script><p>加上了未标记数据后，同样也要做最大似然：</p><script type="math/tex; mode=display">logL(θ)=∑_{(x^r,\hat{y}^r)} logP_θ (x^r |\hat{y}^r )+∑_{x^u} logP_θ (x^u)</script><h3 id="Low-density-Separation"><a href="#Low-density-Separation" class="headerlink" title="Low-density Separation"></a>Low-density Separation</h3><p>假设不同类别之间有一条明显的分界线，也即存在一个区域，其密度比其他区域小</p><h4 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h4><p>如何做?</p><ol><li>先用有标签数据训练一个模型$f$；</li><li>利用模型对未标记数据进行标记，这些标签称为伪标签（pseudo-label）</li><li>将部分有伪标签的数据放入有标签数据中，重新训练<br>重复直到拟合</li></ol><p>这种方式和生成模型的区别：该方法使用的是hard label而生成模型使用的是soft label</p><h4 id="Entropy-based-Regularization"><a href="#Entropy-based-Regularization" class="headerlink" title="Entropy-based Regularization"></a>Entropy-based Regularization</h4><p>将未标记数据充当正则化的效果，我们希望模型预测标签的概率较为集中，也即熵应该尽可能小。也就是说，未标记数据使得分类边界尽可能划在低密度区域。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382837957204.jpg" width="30%" height="50%"></p><h3 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h3><p>假设：位于稠密数据区域的两个距离很近的样例的类标签相似，通过high density path连接。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382840889274.jpg" width="40%" height="50%"><br>x1与x2之间较为稠密，因此x2与x1比x2与x3更为接近。</p><p><strong>如何知道x1与x2通过high density path连接？</strong><br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382841851160.jpg" width="50%" height="50%"></p><p>基于图的方法：</p><ol><li>定义xi与xj之间的相似度$s(x^i,x^j)$</li><li>添加边，有两种选择<ol><li>k nearest neighbor</li><li>e-neighborhood<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382842669412.jpg" width="50%" height="50%"></li></ol></li><li>边之间的权重通过相似度来衡量。如： $s(x^i,x^j )=exp(−γ‖x^i−x^j‖^2)$</li></ol><p>该方法本质即利用有标签数据去影响未标记数据，通过图的传播。但一个问题是如果数据不够多，就可能没办法传播。如：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382844101208.jpg" width="30%" height="50%"></p><p>在建立好图后，如何使用?</p><ul><li>定义图的平滑程度，$y$表示标签。$S$越小表示越平滑。<script type="math/tex; mode=display">S=1/2∑_{i,j} w_{i,j} (y^i−y^j )^2=y^T Ly</script><script type="math/tex; mode=display">y=[⋯y^i⋯y^j⋯]^T</script><script type="math/tex; mode=display">L=D−W</script></li></ul><p>D是邻接矩阵，第ij个元素即xi与xj之间的weight，W是对角矩阵，ii个元素是D的第i行的加和；L称为Graph Laplacian<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382847006975.jpg" width="50%" height="50%"></p><ul><li>我们最终在计算Loss的时候要加上这项正则项<script type="math/tex; mode=display">L=∑_{x^r}C(y^r,\hat{y}^r ) +λS</script></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Semi-supervised learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 7:Tips for DL</title>
      <link href="/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%207:%20Tips%20for%20DL/"/>
      <url>/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%207:%20Tips%20for%20DL/</url>
      
        <content type="html"><![CDATA[<p>大纲<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382757690955.jpg" width="50%" height="50%"></p><h2 id="new-activation-function"><a href="#new-activation-function" class="headerlink" title="new activation function"></a>new activation function</h2><p>梯度消失问题：由于sigmoid会将值压缩，所以在反向传播时，越到后面值越小。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382758841507.jpg" width="30%" height="50%"><br>所以后层的更新会比前层的更新更快，导致前层还没converge，后层就根据前层的数据（random）达到converge了</p><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382759503401.jpg" width="30%" height="50%"><br>能够快速计算，且能够解决梯度消失问题。</p><p>因为会有部分neuron的值是0，所以相当于每次训练一个瘦长的神经网络。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382760030965.jpg" width="50%" height="50%"></p><h4 id="ReLU的变体"><a href="#ReLU的变体" class="headerlink" title="ReLU的变体"></a>ReLU的变体</h4><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382928024258.jpg" width="50%" height="50%"></p><h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><p>首先将几个neuron归为一组，然后每次前向传播时取最大的作为输出。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382761367509.jpg" width="50%" height="50%"></p><p>实际上ReLU是maxout的一种特殊形式：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382761741870.jpg" width="40%" height="50%"></p><p>更一般的，有：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382762237369.jpg" width="40%" height="50%"></p><p>因为w和b的变化，所以该activation function实际上就是一个learnable activation function</p><p>这样一个learnable activation function有这样的特点：</p><blockquote><p>Activation function in maxout network can be any piecewise linear convex function<br>How many pieces depending on how many elements in a group</p></blockquote><p>如：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382763537888.jpg" width="60%" height="50%"></p><p>maxout应如何训练？</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382764343880.jpg" width="50%" height="50%"></p><p>实际上就是一个普通的瘦长network，常规训练即可。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382764564859.jpg" width="50%" height="50%"></p><h2 id="Adaptive-learning-rate"><a href="#Adaptive-learning-rate" class="headerlink" title="Adaptive learning rate"></a>Adaptive learning rate</h2><p>在adagrad中:<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382765516383.jpg" width="30%" height="50%"></p><p>越到后面learning rate越来越小，但实际上在dl里面，error surface是非常复杂的，越来越小的learning rate可能不适用于dl。如：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382765821828.jpg" width="50%" height="50%"></p><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382766372355.jpg" width="60%" height="50%"><br>$σ^t$是历史信息，也就是说$σ^t$参考了过去的梯度和当前的梯度获得一个新的放缩大小</p><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>引入惯性作为参考，也即参考了上一次梯度的方向。引入惯性后，可能有机会越过local minimum。<br>普通的gradient descent:<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382769712428.jpg" width="40%" height="50%"><br>每次朝着梯度的反方向走。</p><p>Momentum:<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382770120104.jpg" width="40%" height="50%"></p><p>考虑了上一步走的方向。</p><p>具体算法：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382771516587.jpg" width="30%" height="50%"></p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>结合了RMSprop和Momentum，也即综合考虑了历史信息决定当前步长；考虑了上一步的方向决定当前走的方向。<br>具体算法：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382772986250.jpg" width="60%" height="50%"></p><h2 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h2><p>就是在validation set的loss不再减小时停止<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382814784406.jpg" width="50%" height="50%"></p><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382815175056.jpg" width="50%" height="50%"><br>其中<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382815336088.jpg" width="30%" height="50%"><br>因此更新公式为：<br>    <img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382815641437.jpg" width="50%" height="50%"></p><p>也即每次以$1-\eta \lambda$对w进行放缩，使w更接近0<br>正则化在DL中也称为weight decay</p><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382816962676.jpg" width="25%" height="50%"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382817122102.jpg" width="25%" height="50%"><br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382817409633.jpg" width="25%" height="50%"></p><p>则更新公式为：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382817897319.jpg" width="50%" height="50%"></p><p>也即每次以$ηλsgn(w)$ 使w往0靠（sgn表示符号函数）</p><p>可以看出，L1每次都加减相同的值，而L2按比例进行缩放。因此L1更为稀疏(sparse)。</p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>训练的时候每一层采样p%的神经元设为0，让其不工作<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382819376579.jpg" width="50%" height="50%"></p><p>实际上就是每个batch改变了网络结构，使得网络更细长<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382819772611.jpg" width="50%" height="50%"></p><p>测试的时候所有的weight都乘以1-p%</p><p>从ensemble的角度看待dropout：<br>在训练的时候训练一堆不同结构的network，最多有$2^N$种组合，N为neuron个数，可以称为终极的ensemble方法了。而在测试的时候对这些不同的网络进行平均。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382821089494.jpg" width="50%" height="50%"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382821379688.jpg" width="50%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Tips for DL </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>采样浅析</title>
      <link href="/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/%E9%87%87%E6%A0%B7%E6%B5%85%E6%9E%90/"/>
      <url>/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/%E9%87%87%E6%A0%B7%E6%B5%85%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>总结在NLP中的采样方法（持续更新）。</p><h2 id="采样方法"><a href="#采样方法" class="headerlink" title="采样方法"></a>采样方法</h2><h3 id="1️⃣逆变换采样-Inverse-Sampling"><a href="#1️⃣逆变换采样-Inverse-Sampling" class="headerlink" title="1️⃣逆变换采样(Inverse Sampling)"></a>1️⃣逆变换采样(Inverse Sampling)</h3><p>目的：已知任意概率分布的<strong>累积分布函数</strong>时，用于从该分布中生成随机样本。</p><p>—-什么是累积分布函数(CDF)—-<br>是概率密度函数(PDF)的积分，定义：</p><script type="math/tex; mode=display">F_X(x)=P(X≤x)=\int_{-∞}^{x}f_X(t)dt</script><p>—-END—-</p><p>想象我们知道高斯分布的概率密度函数，我们应该如何采样？本质上我们只能对均匀分布进行直接采样（高斯分布有<a href="https://www.zhihu.com/question/29971598" target="_blank" rel="noopener">算法</a>可以生成采样，但无法一般化）。对于这种连续的随机变量，我们只能通过间接的方法进行采样。</p><p>逆变换采样即是通过累积分布函数的反函数来采样。因为累积分布函数的值域为$[0,1]$，因此我们通过在$[0,1]$上进行采样，再映射到原分布。<br>例子:<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382714567064.jpg" width="80%" height="50%"><br>映射关系如图：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-30-15382715821631.jpg" width="50%" height="50%"></p><h3 id="2️⃣重要性采样-Importance-Sampling"><a href="#2️⃣重要性采样-Importance-Sampling" class="headerlink" title="2️⃣重要性采样(Importance Sampling)"></a>2️⃣重要性采样(Importance Sampling)</h3><p>目的：已知某个分布$P$，希望能估计$f(x)$的期望。亦即：</p><script type="math/tex; mode=display">E[f(x)]=\int_{x}f(x)p(x)dx≈\frac{1}{n}\sum_{i=1}^{n}f(x_i)</script><p>其中$x\sim p$。<br>假设$p(x)$的分布复杂或样本不好生成，另一分布$q(x)$方便生成样本。因此我们引入$q(x)$对原先分布进行估计。</p><script type="math/tex; mode=display">E[f(x)]=\int_{x}f(x)p(x)dx=\int_{x}f(x)\frac{p(x)}{q(x)}q(x)dx≈\frac{1}{n}\sum_{i=1}^{n}f(x_i)\frac{p(x_i)}{q(x_i)}</script><p>其中，$x \sim q$。$w(x)=\frac{p(x)}{q(x)}$称为Importance Weight</p><p>根据上式，实际上就是每次采样的加权求和。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>逆变换采样<br><a href="https://zh.wikipedia.org/wiki/%E9%80%86%E5%8F%98%E6%8D%A2%E9%87%87%E6%A0%B7" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E9%80%86%E5%8F%98%E6%8D%A2%E9%87%87%E6%A0%B7</a></p><p>重要性采样<br><a href="https://www.youtube.com/watch?v=S3LAOZxGcnk" target="_blank" rel="noopener">https://www.youtube.com/watch?v=S3LAOZxGcnk</a></p><p>——持续更新——</p>]]></content>
      
      
      
        <tags>
            
            <tag> 采样 </tag>
            
            <tag> sampling </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识9</title>
      <link href="/2018/09/30/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%869/"/>
      <url>/2018/09/30/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%869/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>Pytorch中保存checkpoint是一个dict形式，可以保存任意多个模型到一个checkpoint中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#save</span></span><br><span class="line">torch.save(&#123;            <span class="string">'epoch'</span>: epoch,            <span class="string">'model_state_dict'</span>: model.state_dict(),            <span class="string">'optimizer_state_dict'</span>: optimizer.state_dict(),            <span class="string">'loss'</span>: loss,            ...            &#125;, PATH)</span><br><span class="line"><span class="comment">#load</span></span><br><span class="line">model = TheModelClass(*args, **kwargs)optimizer = TheOptimizerClass(*args, **kwargs)</span><br><span class="line">checkpoint = torch.load(PATH)model.load_state_dict(checkpoint[<span class="string">'model_state_dict'</span>])optimizer.load_state_dict(checkpoint[<span class="string">'optimizer_state_dict'</span>])epoch = checkpoint[<span class="string">'epoch'</span>]loss = checkpoint[<span class="string">'loss'</span>]</span><br><span class="line">model.eval()<span class="comment"># - or -</span>model.train()</span><br></pre></td></tr></table></figure></p><hr><h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>Pytorch可以load部分模型，也就是只load进来部分我们需要的层，这在transfer learning中用到。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">torch.save(modelA.state_dict(), PATH)</span><br><span class="line">modelB = TheModelBClass(*args, **kwargs)modelB.load_state_dict(torch.load(PATH), strict=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>no title</title>
      <link href="/2018/09/30/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%9C%89%E5%8A%9B%E9%87%8F%E7%9A%84%E6%96%87%E5%AD%97/"/>
      <url>/2018/09/30/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%9C%89%E5%8A%9B%E9%87%8F%E7%9A%84%E6%96%87%E5%AD%97/</url>
      
        <content type="html"><![CDATA[<p>每当我遇到自己不敢直视的困难时，我就会闭上双眼，想象自己是一个80岁的老人，为人生中曾放弃和逃避过的无数困难而懊悔不已，我会对自己说，能再年轻一次该有多好，然后我睁开眼睛：砰！我又年轻一次了！</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词10</title>
      <link href="/2018/09/30/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D10/"/>
      <url>/2018/09/30/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D10/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣次北固山下"><a href="#1️⃣次北固山下" class="headerlink" title="1️⃣次北固山下"></a>1️⃣次北固山下</h3><p>[唐] 王湾<br>客路青山外，行舟绿水前。<br>潮平两岸阔，风正一帆悬。<br><strong>海日生残夜，江春入旧年。</strong><br>乡书何处达，归雁洛阳边。</p><p>次：旅途中暂时停宿，这里是停泊的意思。</p><p><a href="http://m.xichuangzhu.com/work/57b95de92e958a005fa8919e" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b95de92e958a005fa8919e</a></p><hr><h3 id="2️⃣将赴吴兴登乐游原"><a href="#2️⃣将赴吴兴登乐游原" class="headerlink" title="2️⃣将赴吴兴登乐游原"></a>2️⃣将赴吴兴登乐游原</h3><p>[唐] 杜牧<br>清时有味是无能，闲爱孤云静爱僧。<br><strong>欲把一麾江海去，乐游原上望昭陵。</strong></p><p>无能：无所作为。</p><p><a href="http://m.xichuangzhu.com/work/57b99db9165abd005a6da742" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b99db9165abd005a6da742</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PRML第一章 绪论</title>
      <link href="/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E7%BB%AA%E8%AE%BA/"/>
      <url>/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E7%BB%AA%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<p>记录PRML学习过程。<br>笔记共享链接：<a href="https://1drv.ms/u/s!Apsp2510NHF6rIRjMclFB16v7B0FWg" target="_blank" rel="noopener">https://1drv.ms/u/s!Apsp2510NHF6rIRjMclFB16v7B0FWg</a></p><hr><h1 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h1><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-Xnip2018-09-23_10-27-21.jpg" alt="概率论"></p><h1 id="决策论"><a href="#决策论" class="headerlink" title="决策论"></a>决策论</h1><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-Xnip2018-09-23_10-36-52.jpg" alt="决策论"></p><h1 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h1><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-Xnip2018-09-23_10-38-46.jpg" alt="信息论"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> PRML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 6:Backpropagation</title>
      <link href="/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%206:%20Backpropagation/"/>
      <url>/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%206:%20Backpropagation/</url>
      
        <content type="html"><![CDATA[<h3 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h3><p>基本公式<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376684598125.jpg" width="50%" height="50%"><br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376684674095.jpg" width="50%" height="50%"></p><h3 id="forward-pass和backward-pass"><a href="#forward-pass和backward-pass" class="headerlink" title="forward pass和backward pass"></a>forward pass和backward pass</h3><p>可以将backpropagation分为两步</p><h4 id="forward-pass"><a href="#forward-pass" class="headerlink" title="forward pass"></a>forward pass</h4><p>在前向传播的时候提前计算/保存好，因为该梯度很简单<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376686358832.jpg" width="50%" height="50%"></p><p>比如z对w1的梯度就是x1，就是和w1相连的项<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376687025289.jpg" width="20%" height="50%"></p><h4 id="backward-pass"><a href="#backward-pass" class="headerlink" title="backward pass"></a>backward pass</h4><p>回传的时候逐层相乘下去，类似动态规划，获得了后一层的梯度才能求出前一层的梯度。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376687488493.jpg" width="50%" height="50%"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376687824826.jpg" width="50%" height="50%"></p><p>先前向，提前算出最邻近的梯度，直到output layer，计算完该梯度，再不断回传逐层相乘获得output对各层的梯度。</p><h3 id="代码实现例子"><a href="#代码实现例子" class="headerlink" title="代码实现例子"></a>代码实现例子</h3><p>relu实现forward pass和backward pass<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return</span></span><br><span class="line"><span class="string">        a Tensor containing the output. ctx is a context object that can be used</span></span><br><span class="line"><span class="string">        to stash information for backward computation. You can cache arbitrary</span></span><br><span class="line"><span class="string">        objects for use in the backward pass using the ctx.save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ctx.save_for_backward(input)  <span class="comment">#为了之后的backward计算</span></span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Backpropagation </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 5:Classification:Logistic Regression</title>
      <link href="/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%205%20Classification:%20Logistic%20Regression/"/>
      <url>/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%205%20Classification:%20Logistic%20Regression/</url>
      
        <content type="html"><![CDATA[<h3 id="logistic-regression如何做？"><a href="#logistic-regression如何做？" class="headerlink" title="logistic regression如何做？"></a>logistic regression如何做？</h3><p>step1: 定义function set<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376666391912.jpg" width="30%" height="50%"></p><p>step2: 更新<br>使用最大似然更新</p><script type="math/tex; mode=display">L(w,b)=f_{w,b}(x^1 )f_{w,b}(x^2 )(1−f_{w,b} (x^3 ))⋯f_{w,b} (x^N )</script><p>找到w，b使得L最大</p><p>对似然函数取负对数，则有：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376667791380.jpg" width="60%" height="50%"></p><p>将式子的每个元素写成伯努利分布形式：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376669013511.jpg" width="60%" height="50%"></p><p>上式就是cross-entropy损失函数。</p><p>求导该式子可得：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376669743224.jpg" width="30%" height="50%"><br>更新公式：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376669980138.jpg" width="40%" height="50%"><br>可以看出上式很直观：和答案差距越大，更新步伐越大。</p><p>同时发现上式和linear regression的更新公式是一致的。</p><h3 id="为什么不像linear-regression那样设loss为square？"><a href="#为什么不像linear-regression那样设loss为square？" class="headerlink" title="为什么不像linear regression那样设loss为square？"></a>为什么不像linear regression那样设loss为square？</h3><p>假设我们使用square loss，则求导得到的梯度：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376671202521.jpg" width="50%" height="50%"><br>上式可以看出，当接近target时，梯度小；远离target时，梯度也小。难以达到全局最小<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376672527230.jpg" width="60%" height="50%"></p><p>下图是cross entropy和square error的图像示意：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376672892502.jpg" width="60%" height="50%"></p><p>如图，square loss难以到达全局最小。</p><h3 id="生成式模型与判别式模型的区别"><a href="#生成式模型与判别式模型的区别" class="headerlink" title="生成式模型与判别式模型的区别"></a>生成式模型与判别式模型的区别</h3><p>生成式对联合概率分布进行建模，再通过贝叶斯定理获得后验概率；而判别式模型直接对后验概率建模。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376674213503.jpg" width="60%" height="50%"><br>二者所定义的function set是一致的，但同一组数据可能会得到不同的w和b。</p><p>二者优劣对比：</p><ul><li>数据量多时，一般来说判别式模型会更好。因为判别式模型没有先验假设，完全依赖于数据。但如果数据有噪声，容易受影响。</li><li>生成式模型是有一定的假设的，当假设错误，会影响分类效果。</li><li>正因为有一定的先验假设，当数据量很少时，可能效果会不错；对于噪声更具有鲁棒性。</li><li>先验可以从其他数据源获得来帮助特定任务，如语音识别问题。</li></ul><h3 id="logistic的局限"><a href="#logistic的局限" class="headerlink" title="logistic的局限"></a>logistic的局限</h3><p>本质仍是一个线性分类器，没办法分类非线性的数据。<br>如何解决该问题?<br><strong>将logistic regression model拼接起来</strong>，前面的model对数据进行feature transformation，然后再对新的feature进行分类。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376677559470.jpg" width="70%" height="50%"></p><p>logistic与deep learning的联系：<br>如果将logistic regression的一个单元称为neuron，拼起来就是neural network了！！！</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Classification </tag>
            
            <tag> Logistic Regression </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识8</title>
      <link href="/2018/09/23/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%868/"/>
      <url>/2018/09/23/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%868/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>torch.max()有两种不同写法。<br>torch.max(input) → Tensor 返回其中最大的元素<br>torch.max(input, dim, keepdim=False, out=None) → (Tensor, LongTensor) 返回该维度上最大值，以及对应的index</p><hr><h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>将模型同时部署到多张卡上训练，本质就是将一个batch的数据split，送到各个model，然后合并结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model)</span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> rand_loader:</span><br><span class="line">    input = data.to(device)</span><br><span class="line">    output = model(input)</span><br></pre></td></tr></table></figure><hr><h3 id="3️⃣-求导"><a href="#3️⃣-求导" class="headerlink" title="3️⃣[求导]"></a>3️⃣[求导]</h3><p>标量、向量、矩阵之间的求导有两种布局，即分子布局和分母布局。分子布局和分母布局只差一个转置。<br>我的记法：在求导过程中，假设分母为m*n，分子为 k*n，则导数矩阵应该为 k*m 。一些特殊的如标量对矩阵求导等除外。<br>具体直接查表：<a href="https://en.m.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">https://en.m.wikipedia.org/wiki/Matrix_calculus</a></p><p>按位计算求导：<br>假设一个函数$f(x)$的输入是标量$x$。对于一组K个标量$x_1,··· ,x_K$，我们<br>可以通过$f(x)$得到另外一组K个标量$z_1,··· ,z_K$，<br>$z_k = f(x_k),∀k = 1,··· ,K$<br>其中，$f(x)$是按位运算的，即$[f(x)]_i = f(x_i)$<br>其导数是一个对角矩阵：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-23-15376727095200.jpg" width="50%" height="50%"></p><p><strong>Reference</strong>：<br><a href="https://en.m.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">https://en.m.wikipedia.org/wiki/Matrix_calculus</a><br><a href="https://blog.csdn.net/uncle_gy/article/details/78879131" target="_blank" rel="noopener">https://blog.csdn.net/uncle_gy/article/details/78879131</a><br><a href="https://nndl.github.io/chap-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.pdf" target="_blank" rel="noopener">https://nndl.github.io/chap-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.pdf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> 求导 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录7</title>
      <link href="/2018/09/23/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%957/"/>
      <url>/2018/09/23/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%957/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣softmax的numpy实现"><a href="#1️⃣softmax的numpy实现" class="headerlink" title="1️⃣softmax的numpy实现"></a>1️⃣softmax的numpy实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x,axis=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Compute softmax values for each sets of scores in x."""</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(x) / np.sum(np.exp(x), axis=axis)</span><br></pre></td></tr></table></figure><hr><h3 id="2️⃣numpy-手动求导relu"><a href="#2️⃣numpy-手动求导relu" class="headerlink" title="2️⃣numpy 手动求导relu"></a>2️⃣numpy 手动求导relu</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.dot(w1)</span><br><span class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.dot(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = np.square(y_pred - y).sum()</span><br><span class="line">    print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><hr><h3 id="3️⃣Pytorch实现relu"><a href="#3️⃣Pytorch实现relu" class="headerlink" title="3️⃣Pytorch实现relu"></a>3️⃣Pytorch实现relu</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return</span></span><br><span class="line"><span class="string">        a Tensor containing the output. ctx is a context object that can be used</span></span><br><span class="line"><span class="string">        to stash information for backward computation. You can cache arbitrary</span></span><br><span class="line"><span class="string">        objects for use in the backward pass using the ctx.save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ctx.save_for_backward(input)  <span class="comment">#为了之后的backward计算</span></span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br></pre></td></tr></table></figure><hr><h3 id="4️⃣Pytorch在多张卡上部署"><a href="#4️⃣Pytorch在多张卡上部署" class="headerlink" title="4️⃣Pytorch在多张卡上部署"></a>4️⃣Pytorch在多张卡上部署</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model)</span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> rand_loader:</span><br><span class="line">    input = data.to(device)</span><br><span class="line">    output = model(input)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>听达观杯现场答辩有感</title>
      <link href="/2018/09/19/%E8%A7%81%E9%97%BB&amp;%E6%83%B3%E6%B3%95/%E5%90%AC%E8%BE%BE%E8%A7%82%E6%9D%AF%E7%8E%B0%E5%9C%BA%E7%AD%94%E8%BE%A9%E6%9C%89%E6%84%9F/"/>
      <url>/2018/09/19/%E8%A7%81%E9%97%BB&amp;%E6%83%B3%E6%B3%95/%E5%90%AC%E8%BE%BE%E8%A7%82%E6%9D%AF%E7%8E%B0%E5%9C%BA%E7%AD%94%E8%BE%A9%E6%9C%89%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>前几日（周日）去了达观杯答辩现场听了前10名做了报告，有了一些感想，但一直没有抽出时间写一下自己的感想（懒）。</p><p>自己大概花了十来天做了一下比赛，实际上也就是一个文本分类的<a href="http://www.dcjingsai.com/common/cmpt/%E2%80%9C%E8%BE%BE%E8%A7%82%E6%9D%AF%E2%80%9D%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98%E8%B5%9B_%E8%B5%9B%E4%BD%93%E4%B8%8E%E6%95%B0%E6%8D%AE.html" target="_blank" rel="noopener">比赛</a>，因为没有比赛经验的缘故，走了很多弯路。不过也学到了一些东西。</p><p>现记录前十名的一些idea/trick：</p><ul><li>数据增强<ul><li>因为给的句子长度很长，因此在做截断的时候后面的就没法训练到了，可以将文本倒序作为新的数据训练模型。可以充分利用到数据</li><li>将数据打乱、随机删除，实际上就是对一个句子的词进行sample再组合</li><li>打乱词序以增加数据量</li><li>使用pseudo labeling，但有的队伍使用这个做出效果了，但有的没有</li></ul></li><li>特征工程<ul><li>假设开头中间结尾的信息对分类有帮助，因此截取该部分信息做训练</li><li>改进baseline的tfidf的特征工程方法，使用基于熵的词权重计算</li><li>降维，留下最重要的特征。先用卡方分布降到20万，再用SVD降到8000</li><li>将word2vec和GloVe拼接起来作为deep learning模型的输入</li><li>将文章分段，每段取前20后20拼起来</li></ul></li><li>模型融合<br>  所有队伍都无一例外使用了模型融合，stacking或者简单的投票<ul><li>DL+ML —&gt; lgbm model —&gt; voting</li><li>深度模型+传统模型，在深度模型最后一层加入传统模型的信息/feature</li><li>后向选择剔除冗余模型</li></ul></li><li>DL&amp;其他<ul><li>HAN，选择10个attention vector</li><li>对易错类增加权重，通过改变损失函数来增加权重</li><li>CNN, [1,2,3,4,5,6]*600</li><li>提出新的模型（第一名）</li></ul></li></ul><p>其实除了一些trick，我还是有些失望的，因为都是用模型融合堆出来的，这也让我对比赛失去了一些兴趣。虽然能理解现在的比赛都是这样的，但感觉实在太暴力了。<br>当然，其中还是有一些亮点的，有一支队伍立意很高，从理解业务的角度出发而不是堆模型，也取得了很好的效果；还有一个使用了最新论文中的特征工程改进方法，令我耳目一新；以及第一名在比赛过程中提出来三个新的模型。</p><p>Anyway，我目前还是太菜了，还是安心搞科研吧。_(:з」∠)</p>]]></content>
      
      
      
        <tags>
            
            <tag> 有感 </tag>
            
            <tag> 达观杯 </tag>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识7</title>
      <link href="/2018/09/16/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%867/"/>
      <url>/2018/09/16/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%867/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>只有一个元素的tensor，可用.item()来获取元素</p><p>tensor &lt;—&gt; numpy 相互转化会共享内部数据，因此改变其中一个会改变另一个</p><p>可用使用 .to 来移动到设备<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370671350548.jpg" alt=""></p><p>.detech()  detach it from the computation history, and to prevent future computation from being tracked. 将其从计算图中分离，变为叶子节点，并且requires_grad=False</p><p>Function 记录了这个tensor是怎么来的，所有的tensor都有，除非是用户自定义的：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370672806253.jpg" width="65%" height="50%"></p><hr><h3 id="2️⃣-协方差"><a href="#2️⃣-协方差" class="headerlink" title="2️⃣[协方差]"></a>2️⃣[协方差]</h3><p>关于协方差的理解，x与y关于某个自变量的变化程度，即度量了x与y之间的联系。<br><a href="https://www.zhihu.com/question/20852004" target="_blank" rel="noopener">https://www.zhihu.com/question/20852004</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> 协方差 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 4:Classification:Probabilistic Generative Model</title>
      <link href="/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%204%20Classification%20%20Probabilistic%20Generative%20Model/"/>
      <url>/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%204%20Classification%20%20Probabilistic%20Generative%20Model/</url>
      
        <content type="html"><![CDATA[<h3 id="为什么不使用regression来分类？"><a href="#为什么不使用regression来分类？" class="headerlink" title="为什么不使用regression来分类？"></a>为什么不使用regression来分类？</h3><p>1️⃣如果使用regression的思想来分类，会对离边界较远的点进行惩罚：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370662150910.jpg" width="70%" height="50%"></p><p>2️⃣如果多分类使用regression，如class 1, class 2, class 3；则隐式地假设了class 1 和 class 2较为接近，如果没有这种接近关系，则分类会不正确。</p><h3 id="问题描述与定义"><a href="#问题描述与定义" class="headerlink" title="问题描述与定义"></a>问题描述与定义</h3><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370662762802.jpg" width="70%" height="50%"></p><p>当P大于0.5则是C1类，反之是C2类<br>先验P(C1)和P(C2)都好计算，计算C1占总的比例即可<br>因此，我们需要计算的就是p(x|C)</p><p>这一想法，本质是得到了生成式模型：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370663099515.jpg" width="70%" height="50%"></p><h3 id="原理概述"><a href="#原理概述" class="headerlink" title="原理概述"></a>原理概述</h3><p>现<strong>假设训练数据点的分布服从高斯分布</strong>：（显然可以自己设任何分布）<br>即数据从高斯分布采样得到：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370663870205.jpg" width="55%" height="50%"></p><p>根据最大似然估计，可以获得每个类别的μ和Σ：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370664041246.jpg" width="70%" height="50%"></p><p>得到了参数后，即可代入得到P(C|x) ：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370664355955.jpg" width="80%" height="50%"></p><p>刚刚假设$Σ$对于不同类别不同，现我们<strong>令不同类别共享相同$Σ$</strong>：<br>（因为协方差代表的是不同feature之间的联系，可以认为是和类别无关的）</p><p>$Σ$的计算公式是加权求和：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370665362081.jpg" width="24%" height="50%"></p><p>在使用了相同的协方差矩阵后，边界就是线性的（后面会提到为什么是这样）：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370665553962.jpg" alt=""></p><p> 总结：<br> 三步走，定义function set，计算μ和协方差矩阵，得到best function：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370665888683.jpg" width="60%" height="50%"></p><p>注意到，如果我们认为，不同feature之间没有关系，每个feature符合特定的高斯分布，则该分类器则是朴素贝叶斯分类器：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370666092459.jpg" width="60%" height="50%"></p><h3 id="分类与logistics-regression"><a href="#分类与logistics-regression" class="headerlink" title="分类与logistics regression"></a>分类与logistics regression</h3><p>现推导，该分类问题与logistics regression之间的联系：<br>即：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370666567324.jpg" width="60%" height="50%"></p><h4 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h4><p>数据服从高斯分布，共享$Σ$</p><h4 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h4><p>①总框架：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370667000974.jpg" width="50%" height="50%"></p><p>令<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370667135868.jpg" width="24%" height="50%"></p><p>则有：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370667376312.jpg" width="50%" height="50%"></p><p>②z的进一步推导与简化：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370667582564.jpg" width="30%" height="50%"></p><p>将z展开：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370667763267.jpg" width="50%" height="50%"></p><p>而第一部分有：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370667880942.jpg" width="50%" height="50%"></p><p>第一部分相除，有：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370668274551.jpg" width="50%" height="50%"></p><p>再进行展开，有：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370668394919.jpg" width="50%" height="50%"></p><p>最终z的公式为：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370668522531.jpg" width="50%" height="50%"></p><p>由于共享协方差矩阵，则可以消去部分，得到：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370668957564.jpg" width="50%" height="50%"></p><p>替换成w和b：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370669103562.jpg" width="50%" height="50%"></p><p>③最终，将z带回到原式：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370669221472.jpg" width="25%" height="50%"></p><p>所以我们不需要再估计N1,N2,μ和Σ，直接计算w和b即可。也因此，分界线是线性的。</p><p>全过程：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370669594744.jpg" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Classification </tag>
            
            <tag> Probabilistic Generative Model </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 3:Gradient Descent</title>
      <link href="/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%203%20Gradient%20Descent/"/>
      <url>/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%203%20Gradient%20Descent/</url>
      
        <content type="html"><![CDATA[<h2 id="Gradient-Descent-tips"><a href="#Gradient-Descent-tips" class="headerlink" title="Gradient Descent tips"></a>Gradient Descent tips</h2><h3 id="tip-1：Adaptive-Learning-Rates"><a href="#tip-1：Adaptive-Learning-Rates" class="headerlink" title="tip 1：Adaptive Learning Rates"></a>tip 1：Adaptive Learning Rates</h3><h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><h5 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h5><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370654467771.jpg" width="30%" height="50%"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370654502774.jpg" width="20%" height="50%"></p><p>其中σ是之前所有的梯度的平方根<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370654728457.jpg" width="30%" height="50%"></p><p>化简形式：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370654853172.jpg" width="50%" height="50%"></p><h5 id="为什么要怎么做？"><a href="#为什么要怎么做？" class="headerlink" title="为什么要怎么做？"></a>为什么要怎么做？</h5><p>考虑一个开口向上的二次函数<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370655091732.jpg" width="50%" height="50%"></p><p>也即，最好的步长是一次导除以二次导，但二次导计算量大，因此使用近似的方式：<br><strong>对一次导作多次的sample</strong>。<br>下图显示，如果二次导小，那么多次sample获得的一次导也小，反之则大，也就是说，一次导在某种程度上可以反映二次导的大小，所以直接用一次导近似，可以减少计算量。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370655850876.jpg" width="50%" height="50%"></p><h3 id="tip-2：feature-scaling"><a href="#tip-2：feature-scaling" class="headerlink" title="tip 2：feature scaling"></a>tip 2：feature scaling</h3><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370656836724.jpg" width="50%" height="50%"></p><p>能够改变loss的分布，上图1中w2对loss的影响较大，则较陡峭，参数更新就较困难，需要adaptive learning rate；如果进行feature scaling，能够更好达到local optimal</p><h2 id="Gradient-Descent-Theory"><a href="#Gradient-Descent-Theory" class="headerlink" title="Gradient Descent Theory"></a>Gradient Descent Theory</h2><p>另一种角度看gradient descent：</p><p>基本思想：<br>我们希望每一次都在当前点附近找到一个最小的点，即在一个范围内：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370657785697.jpg" width="40%" height="50%"></p><p>应该如何找到该最小点？</p><p>我们知道，泰勒级数的形式：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370658066669.jpg" width="50%" height="50%"></p><p>当x接近x0时，会有如下近似：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370658167935.jpg" width="30%" height="50%"></p><p>推广到多元泰勒级数则有：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370658315314.jpg" width="60%" height="50%"></p><p>那么，如前所述，x接近x0，对于图中，即圆圈足够小时：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370658494091.jpg" width="50%" height="50%"></p><p>简化符号：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370658736683.jpg" width="12%" height="50%"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370658623258.jpg" width="30%" height="50%"></p><p>所以可以简写成：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370658855882.jpg" width="30%" height="50%"></p><p>由于s,u,v都是常数，在圆圈范围内寻找最小值对应的参数可以简化成：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370658981519.jpg" width="40%" height="50%"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370659061025.jpg" width="30%" height="50%"></p><p>再度简化，可以表达成：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370659680601.jpg" width="40%" height="50%"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370659747339.jpg" width="30%" height="50%"></p><p>在图中可以画为两个向量的点积<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370660126195.jpg" width="40%" height="50%"></p><p>显然，当反方向时，最小：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370660243469.jpg" width="40%" height="50%"></p><p>也即：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370660628961.jpg" width="50%" height="50%"></p><p>最终完整的式子：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370660794436.jpg" width="55%" height="50%"></p><p>因此，当learning rate不够小时，是不满足泰勒级数近似的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Gradient Descent </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 2:Bias and Variance</title>
      <link href="/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%202%20Bias%20and%20Variance/"/>
      <url>/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%202%20Bias%20and%20Variance/</url>
      
        <content type="html"><![CDATA[<h3 id="如何理解bias-amp-variance"><a href="#如何理解bias-amp-variance" class="headerlink" title="如何理解bias&amp;variance"></a>如何理解bias&amp;variance</h3><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370650140053.jpg" width="40%" height="50%"><br>bias是function space中心离optimal model的差距，variance是某次实验所得模型离function space中心的距离。</p><p>比如说，简单地模型的function space小，随机性小，因此variance小，但也因为function space小，表示能力有限，因此bias大。</p><p>如图：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-16-15370651353167.jpg" width="70%" height="50%"><br>该图中蓝色圈代表模型所能表达的范围。</p><h3 id="如何解决variance大的问题"><a href="#如何解决variance大的问题" class="headerlink" title="如何解决variance大的问题"></a>如何解决variance大的问题</h3><p>①更多的data<br>②regularization：强迫function更平滑，因此减小variance，但因为调整了function space，可能会增加bias。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> bias&amp;variance </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词9</title>
      <link href="/2018/09/16/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D9/"/>
      <url>/2018/09/16/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D9/</url>
      
        <content type="html"><![CDATA[<h3 id="白雪歌送武判官归京"><a href="#白雪歌送武判官归京" class="headerlink" title="白雪歌送武判官归京"></a>白雪歌送武判官归京</h3><p>[唐] 岑参<br>北风卷地白草折，胡天八月即飞雪。<br><strong>忽如一夜春风来，千树万树梨花开</strong>。<br>散入珠帘湿罗幕，狐裘不暖锦衾薄。<br>将军角弓不得控，都护铁衣冷难着。<br>瀚海阑干百丈冰，愁云惨淡万里凝。<br>中军置酒饮归客，胡琴琵琶与羌笛。<br>纷纷暮雪下辕门，风掣红旗冻不翻。<br><strong>轮台东门送君去，去时雪满天山路。<br>山回路转不见君，雪上空留马行处。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290</a></p><hr><h3 id="绝命诗"><a href="#绝命诗" class="headerlink" title="绝命诗"></a>绝命诗</h3><p>谭嗣同<br>望门投止思张俭，<br>忍死须臾待杜根。<br><strong>我自横刀向天笑，<br>去留肝胆两昆仑！</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pytorch backward()浅析</title>
      <link href="/2018/09/16/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/Pytorch%20backward()%E6%B5%85%E6%9E%90/"/>
      <url>/2018/09/16/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/Pytorch%20backward()%E6%B5%85%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>最近在看pytorch文档的时候，看到backward内有一个参数gradient，在经过查阅了相关资料和进行了实验后，对backward有了更深的认识。</p><h2 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h2><p>1️⃣如果调用backward的是一个标量，如：<code>loss.backward()</code><br>则gradient不需要手动传入，会自动求导。<br>例子:<br>$a=[x_1,x_2],b=\frac{x_1+x_2}{2}$<br>则b对a求导，有：<br>$\dfrac {\partial b}{\partial x_{1}}=\frac{1}{2}，\dfrac {\partial b}{\partial x_{2}}=\frac{1}{2}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a=torch.Tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">a.requires_grad=<span class="keyword">True</span></span><br><span class="line">b=torch.mean(a)  <span class="comment">#tensor(2.5000, grad_fn=&lt;MeanBackward1&gt;)</span></span><br><span class="line">b.backward()</span><br><span class="line">a.grad   <span class="comment">#tensor([0.5000, 0.5000])</span></span><br></pre></td></tr></table></figure><p>gradient此时只是在缩放原grad的大小，也即不指定gradient和gradient=1是等价的</p><p>当然，也可以指定gradient，其中指定gradient的shape必须和b的维度相同<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gradient=torch.tensor(<span class="number">10.0</span>)</span><br><span class="line">b.backward(gradient)</span><br><span class="line">a.grad   <span class="comment">#tensor([5., 5.])</span></span><br></pre></td></tr></table></figure></p><p>2️⃣如果调用backward的是一个向量<br>例子：<br>$a=[x_1,x_2],b=[b_1,b_2]$, 其中 $b_1=x_1+x_2,b_2=x_1*x_2$<br>b对a求导，有：<br>$\dfrac {\partial b_1}{\partial x_{1}}=1,\dfrac {\partial b_1}{\partial x_{2}}=1$</p><p>$\dfrac {\partial b_2}{\partial x_{1}}=x_2,\dfrac {\partial b_2}{\partial x_{2}}=x_1$</p><p>在backward的时候则必须指定gradient。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a=torch.FloatTensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">a.requires_grad=<span class="keyword">True</span></span><br><span class="line">b=torch.zeros(<span class="number">2</span>)</span><br><span class="line">b[<span class="number">0</span>]=a[<span class="number">0</span>]+a[<span class="number">1</span>]</span><br><span class="line">b[<span class="number">1</span>]=a[<span class="number">0</span>]*a[<span class="number">1</span>]    <span class="comment"># b=tensor([5., 6.], grad_fn=&lt;CopySlices&gt;)</span></span><br><span class="line">gradient=torch.tensor([<span class="number">1.0</span>,<span class="number">0.0</span>])</span><br><span class="line">b.backward(gradient,retain_graph=<span class="keyword">True</span>)</span><br><span class="line">a.grad   <span class="comment">#tensor([1., 1.])，说明是对b_1进行求导</span></span><br><span class="line">a.grad.zero_()  <span class="comment">#将梯度清空，否则会叠加</span></span><br><span class="line"><span class="comment">#-------------- #</span></span><br><span class="line">gradient=torch.tensor([<span class="number">0.0</span>,<span class="number">1.0</span>])</span><br><span class="line">b.backward(gradient,retain_graph=<span class="keyword">True</span>)</span><br><span class="line">a.grad  <span class="comment"># tensor([3., 2.])，说明对b_2进行求导</span></span><br><span class="line">a.grad.zero_()</span><br><span class="line"><span class="comment"># ------------- #</span></span><br><span class="line">gradient=torch.tensor([<span class="number">1.0</span>,<span class="number">1.0</span>])</span><br><span class="line">b.backward(gradient,retain_graph=<span class="keyword">True</span>)</span><br><span class="line">a.grad   <span class="comment"># tensor([4., 3.])，即b_1,b_2的导数的叠加</span></span><br><span class="line">a.grad.zero_()</span><br></pre></td></tr></table></figure><p>注意到b.backward()时需要retain_graph设为True，否则在计算完后会自动释放计算图的内存，这样就没法进行二次反向传播了。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.pytorchtutorial.com/pytorch-backward/" target="_blank" rel="noopener">https://www.pytorchtutorial.com/pytorch-backward/</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> backward </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词8</title>
      <link href="/2018/09/09/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D8/"/>
      <url>/2018/09/09/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D8/</url>
      
        <content type="html"><![CDATA[<h3 id="望月怀远"><a href="#望月怀远" class="headerlink" title="望月怀远"></a>望月怀远</h3><p>[唐] 张九龄<br><strong>海上生明月，天涯共此时。</strong><br>情人怨遥夜，竟夕起相思。<br>灭烛怜光满，披衣觉露滋。<br>不堪盈手赠，还寝梦佳期。</p><p>遥夜，长夜。</p><p><a href="http://m.xichuangzhu.com/work/57aca120a341310060e2a09f" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57aca120a341310060e2a09f</a></p><hr><h3 id="无题"><a href="#无题" class="headerlink" title="无题"></a>无题</h3><p>萨镇冰<br>五十七载犹如梦，举国沦亡缘汉城。<br><strong>龙游浅水勿自弃，终有扬眉吐气天。</strong></p><p>1951年，中国人民志愿军在抗美援朝战争第三次战役后打进了汉城，萨镇冰得知此事，回想起57年前的甲午悲歌，当即作诗一首。</p><hr><h3 id="白雪歌送武判官归京"><a href="#白雪歌送武判官归京" class="headerlink" title="白雪歌送武判官归京"></a>白雪歌送武判官归京</h3><p>[唐] 岑参<br>北风卷地白草折，胡天八月即飞雪。<br><strong>忽如一夜春风来，千树万树梨花开。</strong><br>散入珠帘湿罗幕，狐裘不暖锦衾薄。<br>将军角弓不得控，都护铁衣冷难着。<br>瀚海阑干百丈冰，愁云惨淡万里凝。<br>中军置酒饮归客，胡琴琵琶与羌笛。<br>纷纷暮雪下辕门，风掣红旗冻不翻。<br><strong>轮台东门送君去，去时雪满天山路。<br>山回路转不见君，雪上空留马行处</strong>。</p><p><a href="http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词7</title>
      <link href="/2018/09/02/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D7/"/>
      <url>/2018/09/02/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D7/</url>
      
        <content type="html"><![CDATA[<h3 id="滕王阁序"><a href="#滕王阁序" class="headerlink" title="滕王阁序"></a>滕王阁序</h3><p>遥襟甫畅，逸兴遄飞。爽籁发而清风生，纤歌凝而白云遏。睢园绿竹，气凌彭泽之樽；邺水朱华，光照临川之笔。四美具，二难并。穷睇眄于中天，极娱游于暇日。天高地迥，觉宇宙之无穷；兴尽悲来，识盈虚之有数。望长安于日下，目吴会于云间。地势极而南溟深，天柱高而北辰远。关山难越，谁悲失路之人；萍水相逢，尽是他乡之客。怀帝阍而不见，奉宣室以何年？</p><hr><p><strong>注释：</strong><br>遥襟甫畅，逸兴遄（chuán）飞：登高望远的胸怀顿时舒畅，飘欲脱俗的兴致油然而生。</p><p>爽籁（lài）发而清风生，纤歌凝而白云遏：宴会上，排箫响起，好像清风拂来；柔美的歌声缭绕不散，遏止了白云飞动。爽：形容籁的发音清脆。籁：排箫，一种由多根竹管编排而成的管乐器。</p><p>睢（suī）园绿竹，气凌彭泽之樽：今日的宴会，好比当年睢园竹林的聚会，在座的文人雅士，豪爽善饮的气概超过了陶渊明。睢园：西汉梁孝王在睢水旁修建的竹园，他常和一些文人在此饮酒赋诗。</p><p>邺（yè）水朱华，光照临川之笔：这是借诗人曹植、谢灵运来比拟参加宴会的文人。邺：今河北临漳，是曹魏兴起的地方。曹植曾在这里作过《公宴诗》，诗中有“朱华冒绿池”的句子。临川之笔：指谢灵运，他曾任临川（今属江西）内史。</p><p>四美：指良辰、美景、赏心、乐事。</p><p>二难：贤主、嘉宾。</p><p>地势极而南溟深，天柱高而北辰远：地势偏远，南海深邃；天柱高耸，北极星远悬。</p><p>帝阍（hūn）：原指天帝的守门者。这里指皇帝的宫门。</p><p>奉宣室以何年：什么时候才能像贾谊那样去侍奉君王呢</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识6</title>
      <link href="/2018/09/02/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%866/"/>
      <url>/2018/09/02/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%866/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-dropout"><a href="#1️⃣-dropout" class="headerlink" title="1️⃣[dropout]"></a>1️⃣[dropout]</h3><p>dropout形式:<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-02-15358857481798.jpg" width="70%" height="50%"><br>RNN的形式有多种：</p><ul><li><p>recurrent dropout<br>RNN: $h_t=f(W_h ⊙ [x_t,h_{t-1}]+b_h)$<br>加上dropout的RNN：$h_t=f(W_h ⊙ [x_t,d(h_{t-1})]+b_h)$，其中$d(\cdot)$为dropout函数<br>同理：<br>LSTM:$c_t=f_t ⊙c_{t-1} + i_t ⊙ d(g_t)$<br>GRU:$h_t=(1-z_t)⊙c_{t-1}+z_t⊙d(g_t)$</p></li><li><p>垂直连接的dropout<br>dropout的作用即是否允许L层某个LSTM单元的隐状态信息流入L+1层对应单元。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-02-15358866404870.jpg" width="50%" height="50%"></p></li></ul><p>Reference:<br><a href="https://blog.csdn.net/falianghuang/article/details/72910161" target="_blank" rel="noopener">https://blog.csdn.net/falianghuang/article/details/72910161</a></p><hr><h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>pack_padded_sequence用于RNN中，将padding矩阵压缩:<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-09-02-15358868858836.jpg" width="60%" height="50%"><br>这样就可以实现在RNN传输过程中短句提前结束。</p><p>pad_packed_sequence是pack_padded_sequence的逆运算。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> dropout </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>我没有说话</title>
      <link href="/2018/08/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%88%91%E6%B2%A1%E6%9C%89%E8%AF%B4%E8%AF%9D/"/>
      <url>/2018/08/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%88%91%E6%B2%A1%E6%9C%89%E8%AF%B4%E8%AF%9D/</url>
      
        <content type="html"><![CDATA[<p>《我没有说话》</p><p>纳粹杀共产党时，<br>我没有出声<br>——因为我不是共产党员；<br>接着他们迫害犹太人，<br>我没有出声<br>——因为我不是犹太人；<br>然后他们杀工会成员，<br>我没有出声<br>——因为我不是工会成员；<br>后来他们迫害天主教徒，<br>我没有出声<br>——因为我是新教徒；<br>最后当他们开始对付我的时候，<br>已经没有人能站出来为我发声了</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Deep Learning NLP best practices笔记</title>
      <link href="/2018/08/26/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/Deep%20Learning%20NLP%20best%20practices%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/08/26/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/Deep%20Learning%20NLP%20best%20practices%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>博客地址：<a href="http://ruder.io/deep-learning-nlp-best-practices/index.html" target="_blank" rel="noopener">http://ruder.io/deep-learning-nlp-best-practices/index.html</a><br>个人觉得这篇文章写得很好，有许多实践得到的经验，通过这篇可以避免走一些弯路。</p><h2 id="Practices"><a href="#Practices" class="headerlink" title="Practices"></a>Practices</h2><h3 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h3><blockquote><p>The optimal dimensionality of word embeddings is mostly task-dependent: a smaller dimensionality works better for more syntactic tasks such as named entity recognition or part-of-speech (POS) tagging, while a larger dimensionality is more useful for more semantic tasks such as sentiment analysis.</p></blockquote><p>对于偏向语法的，使用维度低一些的词向量；而对于偏向语义内容的，使用维度大一些的词向量，如情感分析。</p><h3 id="LSTM-Depth"><a href="#LSTM-Depth" class="headerlink" title="LSTM Depth"></a>LSTM Depth</h3><blockquote><p>performance improvements of making the model deeper than 2 layers are minimal </p></blockquote><p>LSTM深度最好不要超过两层。</p><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><blockquote><p>It is often thought that Adam clearly outperforms vanilla stochastic gradient descent (SGD). However, while it converges much faster than SGD, it has been observed that SGD with learning rate annealing slightly outperforms Adam. Recent work furthermore shows that SGD with properly tuned momentum outperforms Adam .</p></blockquote><p>Adam可以更早拟合，而SGD效果可能会更好一些。</p><p>可以采用优化策略，比如说使用Adam训练直到拟合，然后将学习率减半，并重新导入之前训练好的最好的模型。这样Adam能够忘记之前的信息并重新开始训练。</p><blockquote><p>Denkowski &amp; Neubig (2017) show that Adam with 2 restarts and learning rate annealing is faster and performs better than SGD with annealing</p></blockquote><h3 id="Ensembling"><a href="#Ensembling" class="headerlink" title="Ensembling"></a>Ensembling</h3><blockquote><p>Combining multiple models into an ensemble by averaging their predictions is a proven strategy to improve model performance.</p></blockquote><p>Ensembling很重要的一点是需要保证多样性：</p><blockquote><p>Ensembling is an important way to ensure that results are still reliable if the diversity of the evaluated models increases (Denkowski &amp; Neubig, 2017). While ensembling different checkpoints of a model has been shown to be effective (Jean et al., 2015; Sennrich et al., 2016) [51, 52], it comes at the cost of model diversity. Cyclical learning rates can help to mitigate this effect</p></blockquote><h3 id="LSTM-tricks"><a href="#LSTM-tricks" class="headerlink" title="LSTM tricks"></a>LSTM tricks</h3><ul><li><p>在initial state中我们常常使用全0向量，实际上可以将其作为参数学习。</p><blockquote><p>Instead of fixing the initial state, we can learn it like any other parameter, which can improve performance</p></blockquote></li><li><p>将input和output embedding的参数共享，如果是做language model或者机器翻译之类的，可以让他们共享。</p></li><li><p>Gradient Norm Clipping</p><blockquote><p>Rather than clipping each gradient independently, clipping the global norm of the gradient yields more significant improvements</p></blockquote></li></ul><p>这点我没看懂。</p><h3 id="Classification-practices"><a href="#Classification-practices" class="headerlink" title="Classification practices"></a>Classification practices</h3><p>关于CNN</p><blockquote><p>CNN filters:Combining filter sizes near the optimal filter size, e.g. (3,4,5) performs best (Kim, 2014; Kim et al., 2016). The optimal number of feature maps is in the range of 50-600 (Zhang &amp; Wallace, 2015) [59].</p><p>Aggregation function:1-max-pooling outperforms average-pooling and k-max pooling (Zhang &amp; Wallace, 2015).</p></blockquote><p>这在我之前的关于CNN文本分类指南中有更详尽的分析。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>这是一篇干货满满的博客，实际上我还是有许多地方没有读懂，这适合多看几遍，慢慢理解。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 指南 </tag>
            
            <tag> 调参 </tag>
            
            <tag> NLP🤖 </tag>
            
            <tag> 笔记📒 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识5</title>
      <link href="/2018/08/26/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%865/"/>
      <url>/2018/08/26/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%865/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[Paper]<br>Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components</p><p>基本框架和CBOW一致，主要贡献在于针对中文词向量添加了偏旁、字的组件作为训练信息。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-26-15352530482345.jpg" width="50%" height="50%"></p><hr><p>2️⃣[Paper]<br>Highway Networks</p><p>为了解决神经网络深度过深时导致的反向传播困难的问题。<br>前向传播的公式：</p><script type="math/tex; mode=display">y=H(x,W_H)</script><p>而论文所做的改进：</p><script type="math/tex; mode=display">y=H(x,W_H) \cdot T(x,W_T)+ x \cdot C(x,W_C)</script><p>其中$T$是transform gate，$C$是carry gate。方便起见，可以将 $C=1-T$，最终有：</p><script type="math/tex; mode=display">y=H(x,W_H) \cdot T(x,W_T)+ x \cdot (1-T(x,W_T))</script><p>可以看出思想和LSTM很类似，都是gate的思想。</p><hr><p>3️⃣[调参方法]<br>博客：<a href="https://blog.goodaudience.com/how-to-make-your-model-happy-again-part-1-40d94a9ffb41" target="_blank" rel="noopener">https://blog.goodaudience.com/how-to-make-your-model-happy-again-part-1-40d94a9ffb41</a></p><ul><li><strong>学习率</strong>：</li></ul><p>一条原则：当validation loss开始上升时，减少学习率。</p><p>如何减少？</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-26-15352871548330.jpg" width="50%" height="50%"></p><p>或者：</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-26-15352873283890.jpg" width="50%" height="50%"><br>设定一定的epoch作为一个stepsize，在训练过程中线性增加学习率，然后在到达最大值后再线性减小。<br>实验表明，使用该方法可以在一半的epoch内达到相同的效果。</p><ul><li><strong>batch size</strong>：</li></ul><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-26-15352891425729.jpg" width="50%" height="50%"></p><p>由于batch size和学习率的强相关性，<a href="https://arxiv.org/pdf/1711.00489.pdf" target="_blank" rel="noopener">相关论文</a>提出提高batch size而不是降低学习率的方法来提升模型表现。</p><blockquote><p>increasing the batch size during training, instead of decaying learning rate. — L. Smith<br><a href="https://arxiv.org/pdf/1711.00489.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1711.00489.pdf</a></p></blockquote><p>一个trick：保持学习率不变，提高batch size，直到batch size~训练集/10，接下来再采用学习率下降的策略。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Paper </tag>
            
            <tag> 调参方法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录6</title>
      <link href="/2018/08/26/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB6/"/>
      <url>/2018/08/26/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB6/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣将数据整理成batch"><a href="#1️⃣将数据整理成batch" class="headerlink" title="1️⃣将数据整理成batch"></a>1️⃣将数据整理成batch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_iter_batch</span><span class="params">(paras,labels,batch_size,shuffle=True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param paras:</span></span><br><span class="line"><span class="string">    :param labels:</span></span><br><span class="line"><span class="string">    :param batch_size:</span></span><br><span class="line"><span class="string">    :param shuffle:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">assert</span> len(paras)==len(labels)</span><br><span class="line">    paras_size=len(paras)</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        indices=np.arange(paras_size)</span><br><span class="line">        np.random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> start_idx <span class="keyword">in</span> range(<span class="number">0</span>,paras_size-batch_size+<span class="number">1</span>,batch_size):</span><br><span class="line">        <span class="keyword">if</span> shuffle:</span><br><span class="line">            excerpt=indices[start_idx:start_idx+batch_size]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            excerpt=slice(start_idx,start_idx+batch_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> paras[excerpt],labels[excerpt]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词6</title>
      <link href="/2018/08/26/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D6/"/>
      <url>/2018/08/26/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D6/</url>
      
        <content type="html"><![CDATA[<p>1️⃣</p><h3 id="戏为六绝句"><a href="#戏为六绝句" class="headerlink" title="戏为六绝句"></a>戏为六绝句</h3><p>[唐] 杜甫<br>【其二】<br>王杨卢骆当时体，轻薄为文哂未休。<br><strong>尔曹身与名俱灭，不废江河万古流</strong>。</p><p>哂（shěn）：讥笑。</p><p><a href="http://m.xichuangzhu.com/work/57b9658c0a2b58005c95d2a7" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b9658c0a2b58005c95d2a7</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CNN文本分类任务指南</title>
      <link href="/2018/08/25/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/CNN%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%8C%87%E5%8D%97/"/>
      <url>/2018/08/25/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/CNN%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p>最近因为比赛的缘故对文本分类有一定的了解。其中使用CNN方法做情感分析任务存在着许多优势。虽然模型简单，但如何设置超参有时候对结果有很大的影响。本文记录了关于CNN文本分类的一些学习历程和指南，基本参考了论文。</p><h2 id="做法"><a href="#做法" class="headerlink" title="做法"></a>做法</h2><p>基本上目前较为浅层的CNN文本分类的做法都是如下图：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-25-15351860103617.jpg" alt=""></p><p>将词向量堆积成为二维的矩阵，通过CNN的卷积单元对矩阵进行卷积处理，同时使用pooling（通常是1max-pooling）操作，将不等长的卷积结果变为等长，对不同的卷积单元的结果进行拼接后生成单个向量，最后再通过线性层转化成类别概率分布。</p><p>另一张图也说明了该流程。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-25-15351863867337.jpg" alt=""></p><h2 id="建议与指导"><a href="#建议与指导" class="headerlink" title="建议与指导"></a>建议与指导</h2><h3 id="超参及其对结果的影响"><a href="#超参及其对结果的影响" class="headerlink" title="超参及其对结果的影响"></a>超参及其对结果的影响</h3><p>接下来的内容参考了论文<a href="https://arxiv.org/pdf/1510.03820.pdf" target="_blank" rel="noopener">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional<br>Neural Networks for Sentence Classification</a></p><p>CNN文本分类的超参：</p><ul><li>输入向量</li><li>卷积大小</li><li>输出通道（feature maps）</li><li>激活函数</li><li>池化策略</li><li>正则化</li></ul><h4 id="输入向量的影响"><a href="#输入向量的影响" class="headerlink" title="输入向量的影响"></a>输入向量的影响</h4><p>实验表明，使用word2vec和GloVe不分伯仲，但将word2vec和GloVe简单拼接在一起并不能带来提升。</p><blockquote><p>unfortunately, simply concatenating these representations does necessarily seem helpful</p></blockquote><p>当句子长度很长（document classification）时，使用one-hot可能会有效果，但在句子长度不是很长时，效果不好。</p><h5 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h5><p>对于新任务，可以word2vec或GloVe或者其他词向量都试一下，如果句子长，可以试着使用one-hot。</p><h4 id="卷积大小"><a href="#卷积大小" class="headerlink" title="卷积大小"></a>卷积大小</h4><p>由于卷积的长度是固定的，也就是词向量的长度，因此只需讨论宽度。<br>实验表明，不同的数据集会有不同的最佳大小，但似乎对于长度越长的句子，最佳大小有越大的趋势。</p><blockquote><p>However, for datasets comprising longer sentences, such as CR (maximum sentence length is 105, whereas it ranges from 36-56 on the other sentiment datasets used here), the optimal region size may be larger.</p></blockquote><p>同时，当增加不同卷积大小作为组合时，如果组合的卷积核大小接近于最佳大小（optimal region size），有助于结果的提升；相反，如果卷积核大小离最佳大小很远时，反而会产生负面影响。</p><h5 id="建议-1"><a href="#建议-1" class="headerlink" title="建议"></a>建议</h5><p>首先试着找到最优的卷积核大小，然后在这个基础上添加和该卷积核大小类似的卷积核。</p><h4 id="feature-maps"><a href="#feature-maps" class="headerlink" title="feature maps"></a>feature maps</h4><p>也就是输出通道（out channel），表明该卷积核大小的卷积核有多少个。</p><p>实验表明，最佳的feature maps和数据集相关，但一般不超过600。</p><blockquote><p>it would seem that increasing the number of maps beyond 600 yields at best very marginal returns, and often hurts performance.</p></blockquote><h5 id="建议-2"><a href="#建议-2" class="headerlink" title="建议"></a>建议</h5><p>在600内搜索最优，如果在600的边缘还没有明显的效果下降，那么可以尝试大于600的feature maps。</p><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>实验结果：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-25-15351889835594.jpg" alt=""></p><p>结果表明，tanh、ReLU和不使用激活函数效果较好。tanh的优点是以0为中心，ReLU能够加速拟合，至于为什么不使用的效果会好，可能是因为模型较为简单：</p><blockquote><p>This indicates that on some datasets, a linear transformation is enough to capture the<br>correlation between the word embedding and the output label.</p></blockquote><h5 id="建议-3"><a href="#建议-3" class="headerlink" title="建议"></a>建议</h5><p>使用tanh、ReLU或者干脆不使用。但如果模型更为复杂，有多层的结构，还是需要使用激活函数的。</p><h4 id="pooling策略"><a href="#pooling策略" class="headerlink" title="pooling策略"></a>pooling策略</h4><p>所有的实验都表明了，1-max pooling的效果比其他好，如k-max pooling。在pooling这一步可以直接选择1-max pooling。</p><blockquote><p>This may be because the location of predictive contexts does not matter, and certain n-grams in the sentence can be more predictive on their own than the entire sentence considered jointly.</p></blockquote><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>主要是dropout和l2 norm constraint。<br>dropout就是随机将一些神经元置为0，l2 norm constraint是对参数矩阵W进行整体缩放，使其不超过一定阈值。（与通常的l2 regularization不同，最早可追溯到Hinton的<a href="https://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="noopener">Improving neural networks by preventing<br>co-adaptation of feature detectors</a>）</p><blockquote><p>the l2 norm of a weight vector is linearly scaled to a constraint c when it exceeds this threshold, so a smaller c implies stronger regularization</p></blockquote><p>实验表明，dropout起的作用很小，l2 norm没有提升甚至还会导致下降。可能是因为模型参数不多，因此过拟合的可能性较低。</p><h5 id="建议-4"><a href="#建议-4" class="headerlink" title="建议"></a>建议</h5><p>设置较小的dropout和较大的l2 norm，当feature maps增大时，可以试着调节较大的dropout以避免过拟合。</p><h3 id="建议及结论"><a href="#建议及结论" class="headerlink" title="建议及结论"></a>建议及结论</h3><ul><li>刚开始的使用使用word2vec或者GloVe，如果数据量够大，可以尝试one-hot</li><li>线性搜索最佳的卷积核大小，如果句子够长，那么可以扩大搜索范围。一旦确定了最佳卷积核大小，尝试在该卷积核大小的附近进行组合，如最佳卷积核宽度是5，那么尝试[3,4,5]或者[2,3,4,5]等</li><li>使用较小的dropout和较大的max norm constraint，然后在[100,600]范围内搜索feature maps，如果最佳的feature maps在600附近，可以试着选择比600更大的范围</li><li>尝试不同的激活函数，通常tanh和ReLU是较好的，但也可以尝试什么都不加。</li><li>使用1-max pooling。</li><li>如果模型复杂，比如feature maps很大，那么可以尝试更为严格的正则化，如更大的dropout rate和较小的max norm constraint。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="http://www.aclweb.org/anthology/D14-1181" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a></p><p><a href="https://arxiv.org/pdf/1510.03820.pdf" target="_blank" rel="noopener">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional<br>Neural Networks for Sentence Classification</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> 情感分析 </tag>
            
            <tag> 指南 </tag>
            
            <tag> 调参 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Pytorch中的inplace的操作</title>
      <link href="/2018/08/20/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADin-place%E7%9A%84%E6%93%8D%E4%BD%9C/"/>
      <url>/2018/08/20/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADin-place%E7%9A%84%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>最近在写Hierarchical attention network的时候遇到了如下的bug：</p><blockquote><p>one of the variables needed for gradient computation has been modified by an inplace operation</p></blockquote><p>在查阅了文档和请教了其他人之后，最终找到了bug。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(seq_len):</span><br><span class="line">    h_i = rnn_outputs[i]  <span class="comment"># batch,hidden*2</span></span><br><span class="line">    a_i = attn_weights[i].unsqueeze_(<span class="number">1</span>)  <span class="comment"># take in-place opt may cause an error</span></span><br><span class="line">    a_i = a_i.expand_as(h_i)  <span class="comment"># batch,hidden*2</span></span><br></pre></td></tr></table></figure><p>这是我原来的逻辑，我在无意中做了inplace操作，导致了bug的发生。正确的做法应该是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(seq_len):</span><br><span class="line">    h_i = rnn_outputs[i]  <span class="comment"># batch,hidden*2</span></span><br><span class="line">    <span class="comment"># a_i = attn_weights[i].unsqueeze_(1)  # take in-place opt may cause an error</span></span><br><span class="line">    a_i = attn_weights[i].unsqueeze(<span class="number">1</span>)  <span class="comment"># batch,1</span></span><br><span class="line">    a_i = a_i.expand_as(h_i)  <span class="comment"># batch,hidden*2</span></span><br></pre></td></tr></table></figure><p>实际上，在实践过程中应当尽量避免inplace操作，在官方文档中也提到了（存疑）这点，虽然提供了inplace操作，但并不推荐使用。</p><p>具体的原因是，在Pytorch构建计算图的过程中，会记录每个节点是怎么来的，但inplace会破坏这种关系，使得在回传的时候没法正常求导。</p><p>特别地，有两种情况不应该使用inplace操作（摘自知乎）：</p><ol><li>对于requires_grad=True的叶子张量(leaf tensor)不能使用inplace operation</li><li>对于在求梯度阶段需要用到的张量不能使用inplace operation</li></ol><p>Reference:<br><a href="https://zhuanlan.zhihu.com/p/38475183" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38475183</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 遇到的问题 </tag>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>愿中国青年都摆脱冷气</title>
      <link href="/2018/08/19/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%84%BF%E4%B8%AD%E5%9B%BD%E9%9D%92%E5%B9%B4%E9%83%BD%E6%91%86%E8%84%B1%E5%86%B7%E6%B0%94/"/>
      <url>/2018/08/19/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%84%BF%E4%B8%AD%E5%9B%BD%E9%9D%92%E5%B9%B4%E9%83%BD%E6%91%86%E8%84%B1%E5%86%B7%E6%B0%94/</url>
      
        <content type="html"><![CDATA[<p>近期的新闻常让人感到愤怒以致绝望…</p><hr><p>愿中国青年都摆脱冷气，只是向上走，不必听自暴自弃者流的话。能做事的做事，能发声的发声。有一分热，发一分光。就令萤火一般，也可以在黑暗里发一点光，不必等候炬火。</p><p>—鲁迅《热风》</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录5</title>
      <link href="/2018/08/19/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB5/"/>
      <url>/2018/08/19/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB5/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣sklearn模型的保存与恢复"><a href="#1️⃣sklearn模型的保存与恢复" class="headerlink" title="1️⃣sklearn模型的保存与恢复"></a>1️⃣sklearn模型的保存与恢复</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">clf = svm.SVC()</span><br><span class="line">clf.fit(X, y)  </span><br><span class="line">clf.fit(train_X,train_y)</span><br><span class="line">joblib.dump(clf, <span class="string">"train_model.m"</span>)</span><br><span class="line">clf = joblib.load(<span class="string">"train_model.m"</span>)</span><br><span class="line">clf.predit(test_X)</span><br></pre></td></tr></table></figure><hr><h3 id="2️⃣Dictionary类"><a href="#2️⃣Dictionary类" class="headerlink" title="2️⃣Dictionary类"></a>2️⃣Dictionary类</h3><p>在构造字典时需要用到<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dictionary</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.word2idx = &#123;&#125;</span><br><span class="line">        self.idx2word = []</span><br><span class="line">        self.__vocab_size = <span class="number">0</span></span><br><span class="line">        self.add_word(<span class="string">'&lt;pad&gt;'</span>)</span><br><span class="line">        self.add_word(<span class="string">'&lt;UNK&gt;'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_word</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.word2idx:</span><br><span class="line">            self.idx2word.append(word)</span><br><span class="line">            self.word2idx[word] = self.__vocab_size</span><br><span class="line">            self.__vocab_size += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_index</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> self.word2idx:</span><br><span class="line">            <span class="keyword">return</span> self.word2idx[word]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.word2idx[<span class="string">'&lt;UNK&gt;'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.idx2word[idx]</span><br></pre></td></tr></table></figure></p><hr><h3 id="3️⃣对dict按元素排序的三种方法"><a href="#3️⃣对dict按元素排序的三种方法" class="headerlink" title="3️⃣对dict按元素排序的三种方法"></a>3️⃣对dict按元素排序的三种方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">d=&#123;<span class="string">'apple'</span>:<span class="number">10</span>,<span class="string">'orange'</span>:<span class="number">20</span>,<span class="string">'banana'</span>:<span class="number">5</span>,<span class="string">'watermelon'</span>:<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#法1</span></span><br><span class="line">print(sorted(d.items(),key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])) <span class="comment">#[('watermelon', 1), ('banana', 5), ('apple', 10), ('orange', 20)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#法2</span></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"></span><br><span class="line">print(sorted(d.items(),key=itemgetter(<span class="number">1</span>))) <span class="comment">#[('watermelon', 1), ('banana', 5), ('apple', 10), ('orange', 20)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#法3</span></span><br><span class="line"></span><br><span class="line">print(sorted(d,key=d.get))  <span class="comment">#['watermelon', 'banana', 'apple', 'orange'] 没有value了</span></span><br></pre></td></tr></table></figure><hr><h3 id="4️⃣合并dict的三种方法"><a href="#4️⃣合并dict的三种方法" class="headerlink" title="4️⃣合并dict的三种方法"></a>4️⃣合并dict的三种方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d1=&#123;<span class="string">'a'</span>:<span class="number">1</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d2=&#123;<span class="string">'b'</span>:<span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#法1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d=&#123;**d1,**d2&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d</span><br><span class="line">&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#法2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dd=dict(d1.items()|d2.items())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dd</span><br><span class="line">&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#法3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d1.update(d2)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d1</span><br><span class="line">&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="5️⃣找到list最大最小值的index"><a href="#5️⃣找到list最大最小值的index" class="headerlink" title="5️⃣找到list最大最小值的index"></a>5️⃣找到list最大最小值的index</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">lst = [<span class="number">40</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minIndex</span><span class="params">(lst)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> min(range(len(lst)),key=lst.__getitem__)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxIndex</span><span class="params">(lst)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> max(range(len(lst)),key=lst.__getitem__)</span><br><span class="line">    </span><br><span class="line">print(minIndex(lst))</span><br><span class="line">print(maxIndex(lst))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Pytorch中的Embedding padding</title>
      <link href="/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADEmbedding%E7%9A%84padding/"/>
      <url>/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADEmbedding%E7%9A%84padding/</url>
      
        <content type="html"><![CDATA[<p>在Pytorch中，nn.Embedding()代表embedding矩阵，其中有一个参数<code>padding_idx</code>指定用以padding的索引位置。所谓padding，就是在将不等长的句子组成一个batch时，对那些空缺的位置补0，以形成一个统一的矩阵。</p><p>用法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.embedding = nn.Embedding(vocab_size, embed_dim,padding_idx=<span class="number">0</span>) <span class="comment">#也可以是别的数值</span></span><br></pre></td></tr></table></figure></p><p>在显式设定<code>padding_idx=0</code>后，在自定义的词典内也应当在相应位置添加<code>&lt;pad&gt;</code>作为一个词。如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dictionary</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.word2idx = &#123;&#125;</span><br><span class="line">        self.idx2word = []</span><br><span class="line">        self.__vocab_size = <span class="number">0</span></span><br><span class="line">        self.add_word(<span class="string">'&lt;pad&gt;'</span>)  <span class="comment"># should add &lt;pad&gt; first</span></span><br><span class="line">        self.add_word(<span class="string">'&lt;UNK&gt;'</span>)</span><br></pre></td></tr></table></figure><p>那么对于<code>padding_idx</code>，内部是如何操作的呢？</p><p>在查看了Embedding的源码后，发现设置了<code>padding_idx</code>，类内部会有如下操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-----Embedding __init__ 内部--------------</span></span><br><span class="line"><span class="keyword">if</span> _weight <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">    self.weight = Parameter(torch.Tensor(num_embeddings, embedding_dim))</span><br><span class="line">    self.reset_parameters()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#---------reset_parameters()--------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.weight.data.normal_(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> self.padding_idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        self.weight.data[self.padding_idx].fill_(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>也就是说，当Embedding是随机初始化的矩阵时，会对<code>padding_idx</code>所在的行进行填0。保证了padding行为的正确性。</p><p>那么，还需要保证一个问题，就是在反向回传的时候，<code>padding_idx</code>是不会更新的.</p><p>在查看了源码后发现在Embedding类内有如下注释：</p><blockquote><p>.. note::<br>        With :attr:<code>padding_idx</code> set, the embedding vector at<br>        :attr:<code>padding_idx</code> is initialized to all zeros. However, note that this<br>        vector can be modified afterwards, e.g., using a customized<br>        initialization method, and thus changing the vector used to pad the<br>        output. The gradient for this vector from :class:<code>~torch.nn.Embedding</code><br>        is always zero.</p></blockquote><p>并且在查阅了其他资料后，发现该行确实会不更新。有意思的是，查阅源码并没有找到如何使其不更新的机制，因为在F.embedding函数中，返回：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)</span><br></pre></td></tr></table></figure><p>但我并不能跳转到torch.embedding中，大概是因为这部分被隐藏了吧。我也没有再深究下去。我猜测有可能是在autograd内部有对该部分进行单独的处理，用mask屏蔽这部分的更新；或者一个更简单的方法，就是任其更新，但每一次都reset，将第一行手动设为全0。</p><p><strong>附记</strong>：</p><p>假如说没有显式设置该行，是否padding就没有效果呢？<br>我认为是的。</p><p>一般来说，我们都是以0作为padding的填充，如：</p><div class="table-container"><table><thead><tr><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>12</td><td>44</td><td>22</td><td>67</td><td>85</td></tr><tr><td>12</td><td>13</td><td>534</td><td>31</td><td>0</td></tr><tr><td>87</td><td>23</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div><p>每一行代表一个句子，其中0作为填充。然后将该矩阵送入到embedding_lookup中，获得三维的tensor，那么0填充的部分，所获得的embedding表示应当是要全0。</p><p>假如不显式设置<code>padding_idx=0</code>，就可能会出现两个结果（个人推测)：</p><p>①本应该全0的地方，被词典中第一个词的词向量表示给替代了，因为将0作为索引去embedding矩阵获取到的词向量，就是第一个词的词向量，而该词并不全0。</p><p>②词典的最后一个词被全0覆盖。F.embedding中有如下片段：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> padding_idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">    <span class="keyword">if</span> padding_idx &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">assert</span> padding_idx &lt; weight.size(<span class="number">0</span>), <span class="string">'Padding_idx must be within num_embeddings'</span></span><br><span class="line">    <span class="keyword">elif</span> padding_idx &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">assert</span> padding_idx &gt;= -weight.size(<span class="number">0</span>), <span class="string">'Padding_idx must be within num_embeddings'</span></span><br><span class="line">        padding_idx = weight.size(<span class="number">0</span>) + padding_idx</span><br><span class="line"><span class="keyword">elif</span> padding_idx <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        padding_idx = <span class="number">-1</span></span><br></pre></td></tr></table></figure><p>上面片段显示，<code>padding_idx</code>被设置为-1，也就是最后一个单词。做完这步紧接着就返回：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)</span><br></pre></td></tr></table></figure><p>还是由于torch.embedding无法查看的原因，我不知道内部是如何实现的，但应该来说，最后一个词就是被覆盖了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Embedding </tag>
            
            <tag> padding </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python Tricks[转]</title>
      <link href="/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/Python%20Tricks%5B%E8%BD%AC%5D/"/>
      <url>/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/Python%20Tricks%5B%E8%BD%AC%5D/</url>
      
        <content type="html"><![CDATA[<p>原文地址:<a href="https://hackernoon.com/python-tricks-101-2836251922e0" target="_blank" rel="noopener">https://hackernoon.com/python-tricks-101-2836251922e0</a></p><p>我觉得这个介绍Python一些tricks的文章很好，能够更加熟悉Python的一些非常方便的用法。<br>以下是我觉得有用的几个点。</p><p>1️⃣Reverse a String/List</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-19-15346465152976.jpg" width="70%" height="50%"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-19-15346467215597.jpg" width="70%" height="50%"></p><p>[::-1]解释：<br>[:]表示取所有的元素，-1表示步进。[1:5:2]表示的就是从元素1到元素5，每2个距离取一个。</p><hr><p>2️⃣transpose 2d array</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-19-15346470165919.jpg" width="70%" height="50%"></p><p>zip()相当于压缩，zip(*)相当于解压。</p><hr><p>3️⃣Chained function call</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-19-15346471756442.jpg" width="70%" height="50%"></p><p>非常简洁的写法。</p><hr><p>4️⃣Copy List</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-19-15346472744350.jpg" width="50%" height="50%"></p><p>之前谈过的Python的赋值、浅拷贝、深拷贝。</p><hr><p>5️⃣Dictionary get</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-19-15346473929918.jpg" width="70%" height="50%"></p><p>避免了dict不存在该元素的问题。</p><hr><p>6️⃣✨Sort Dictionary by Value</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-19-15346475170316.jpg" width="90%" height="50%"></p><p>其中第三种返回的是[‘watermelon’, ‘banana’, ‘apple’, ‘orange’]，没有value了。</p><hr><p>7️⃣For…else</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-19-15346481408714.jpg" width="90%" height="50%"></p><p>注意到如果for在中途break了，就不会进入到else了；只有顺利循环完才会进入到else。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> e <span class="keyword">in</span> a:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">if</span> e==<span class="number">0</span>:</span><br><span class="line"><span class="meta">... </span>            <span class="keyword">break</span></span><br><span class="line"><span class="meta">... </span><span class="keyword">else</span>:</span><br><span class="line"><span class="meta">... </span>    print(<span class="string">'hello'</span>)</span><br><span class="line"><span class="meta">... </span><span class="comment">#什么都没有print</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> e <span class="keyword">in</span> a:</span><br><span class="line"><span class="meta">... </span>    print(e)</span><br><span class="line"><span class="meta">... </span><span class="keyword">else</span>:</span><br><span class="line"><span class="meta">... </span>    print(<span class="string">'hello'</span>)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line">hello</span><br></pre></td></tr></table></figure><hr><p>8️⃣Merge dict’s</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-19-15346483785515.jpg" width="90%" height="50%"></p><p>合并dict的方法。</p><hr><p>9️⃣Min and Max index in List</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-19-15346487918895.jpg" width="80%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> Python tricks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识4</title>
      <link href="/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%864/"/>
      <url>/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%864/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[概率校准(Probability Calibration)]<br>一种对机器学习算法输出结果的校准，通过几个实验可以发现，概率校准能够一定程度提高表现。<br>几个参考资料：<br>直观理解:  <a href="http://www.bubuko.com/infodetail-2133893.html" target="_blank" rel="noopener">http://www.bubuko.com/infodetail-2133893.html</a><br>SVC的概率校准在sklearn上的应用: <a href="https://blog.csdn.net/ericcchen/article/details/79337716" target="_blank" rel="noopener">https://blog.csdn.net/ericcchen/article/details/79337716</a><br>✨完全手册: <a href="http://users.dsic.upv.es/~flip/papers/BFHRHandbook2010.pdf" target="_blank" rel="noopener">Calibration of Machine Learning Models</a></p><hr><p>2️⃣[Paper]<br><a href="https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf" target="_blank" rel="noopener">Hierarchical Attention Networks for Document Classification</a></p><p>亮点在使用层次的RNN结构，以及使用了attention方法。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-19-15346447273228.jpg" width="50%" height="50%"></p><p>参考了其他人的代码自己也试着实现了一个，GitHub地址：<a href="https://github.com/linzehui/pytorch-hierarchical-attention-network" target="_blank" rel="noopener">https://github.com/linzehui/pytorch-hierarchical-attention-network</a></p><hr><p>3️⃣[XGBoost]<br>kaggle神器XGBoost，一篇原理的详细介绍：<br><a href="http://www.cnblogs.com/willnote/p/6801496.html" target="_blank" rel="noopener">http://www.cnblogs.com/willnote/p/6801496.html</a><br>虽然还是有好些地方没搞懂，有必要从头学起。</p><hr><p>4️⃣[Python]<br>关于函数列表中单星号(*)和双星号(**)<br>单星号：</p><ul><li>代表接收任意多个非关键字参数，将其转换成元组：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one</span><span class="params">(a,*b)</span>:</span></span><br><span class="line">    <span class="string">"""a是一个普通传入参数，*b是一个非关键字星号参数"""</span></span><br><span class="line">    print(b)</span><br><span class="line">one(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)  <span class="comment">#输出：(2, 3, 4, 5, 6)</span></span><br></pre></td></tr></table></figure><ul><li>对一个普通变量使用单星号，表示对该变量拆分成单个元素</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(a,b)</span>:</span></span><br><span class="line">    print(a,b)</span><br><span class="line">l=[<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">fun(*l)  <span class="comment">#输出 1,2</span></span><br></pre></td></tr></table></figure><p>双星号：</p><ul><li>获得字典值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two</span><span class="params">(a=<span class="number">1</span>,**b)</span>:</span></span><br><span class="line">    <span class="string">"""a是一个普通关键字参数，**b是一个关键字双星号参数"""</span></span><br><span class="line">    print(b)</span><br><span class="line">two(a=<span class="number">1</span>,b=<span class="number">2</span>,c=<span class="number">3</span>,d=<span class="number">4</span>,e=<span class="number">5</span>,f=<span class="number">6</span>)  <span class="comment">#输出&#123;'b': 2, 'c': 3, 'e': 5, 'f': 6, 'd': 4&#125;</span></span><br></pre></td></tr></table></figure><hr><p>5️⃣[Pytorch]<br>在Pytorch中，只要一个tensor的requires_grad是true，那么两个tensor的加减乘除后的结果的requires_grad也会是true。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Paper </tag>
            
            <tag> 概率校准 </tag>
            
            <tag> Probability Calibration </tag>
            
            <tag> HAN </tag>
            
            <tag> XGBoost </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词5</title>
      <link href="/2018/08/19/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D5/"/>
      <url>/2018/08/19/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D5/</url>
      
        <content type="html"><![CDATA[<p>本周太忙了，没背什么诗词，只背（复习）了部分的《滕王阁序》。</p><p>1️⃣</p><h3 id="滕王阁序"><a href="#滕王阁序" class="headerlink" title="滕王阁序"></a>滕王阁序</h3><p>嗟乎！时运不齐，命途多舛。冯唐易老，李广难封。屈贾谊于长沙，非无圣主；窜梁鸿于海曲，岂乏明时？所赖<strong>君子见机，达人知命</strong>。老当益壮，宁移白首之心？<strong>穷且益坚，不坠青云之志</strong>。酌贪泉而觉爽，处涸辙以犹欢。<strong>北海虽赊，扶摇可接；东隅已逝，桑榆非晚。</strong>孟尝高洁，空馀报国之情；阮籍猖狂，岂效穷途之哭！</p><p>勃，三尺微命，一介书生。无路请缨，等终军之弱冠；有怀投笔，慕宗慤之长风。舍簪笏于百龄，奉晨昏于万里。非谢家之宝树，接孟氏之芳邻。他日趋庭，叨陪鲤对；今兹捧袂，喜托龙门。杨意不逢，抚凌云而自惜；锺期既遇，奏流水以何惭？</p><hr><p><strong>注释：</strong><br>冯唐：西汉人，有才能却一直不受重用。汉武帝时选求贤良，有人举荐冯唐，可是他已九十多岁，难再做官了。李广：汉武帝时的名将，多年抗击匈奴，军功很大，却终身没有封侯。</p><p>贾谊：汉文帝本想任贾谊为公卿，但因朝中权贵反对，就疏远了贾谊，任他为长沙王太傅。梁鸿：东汉人，因作诗讽刺君王，得罪了汉章帝，被迫逃到齐鲁一带躲避。</p><p>酌（zhuó）贪泉而觉爽：喝下贪泉的水，仍觉得心境清爽。古代传说广州有水名贪泉，人喝了这里的水就会变得贪婪。这句是说有德行的人在污浊的环境中也能保持纯正，不被污染。处涸辙以犹欢：处在奄奄待毙的时候，仍然乐观开朗。处河辙：原指鲋鱼处在干涸的车辙旦。比喻人陷入危急之中。</p><p>孟尝：东汉人，为官清正贤能，但不被重用，后来归田。阮籍：三国魏诗人，他有时独自驾车出行，到无路处便恸哭而返，借此宣泄不满于现实的苦闷心情。</p><p>终军：《汉书·终军传》记载，汉武帝想让南越（今广东、广西一带）王归顺，派终军前往劝说，终军请求给他长缨，必缚住南越王，带回到皇宫门前（意思是一定完成使命）。后来用“请缨”指投军报国。</p><p>宗悫（què）：南朝宋人，少年时很有抱负，说“愿乘长风破万里浪”。</p><p>簪（zān）笏（hù）：这里代指官职。晨昏：晨昏定省，出自 《礼记·曲礼上》，释义为旧时侍奉父母的日常礼节。</p><p>非谢家之宝树，接孟氏之芳邻：自己并不是像谢玄那样出色的人才，却能在今日的宴会上结识各位名士。谢家之宝树：指谢玄。《晋书·谢玄传》记载，晋朝谢安曾问子侄们：为什么人们总希望自己的子弟好？侄子谢玄回答：“譬如芝兰玉树，欲使其生于庭阶耳。”后来就称谢玄为谢家宝树。孟氏之芳邻：这里借孟子的母亲为寻找邻居而三次搬家的故事，来指赴宴的嘉宾。</p><p>他日趋庭，叨陪鲤对：过些时候自己将到父亲那里陪侍和聆听教诲。趋庭：快步走过庭院，这是表示对长辈的恭敬。叨：惭愧地承受，表示自谦。鲤对：孔鲤是孔子的儿子，鲤对指接受父亲教诲。事见《论语·季氏》：（孔子）尝独立，（孔）鲤趋而过庭。（子）曰：“学诗乎？”对曰：“未也。”“不学诗，无以言。”鲤退而学诗。他日，又独立，鲤趋而过庭。（子）曰：“学礼乎？”对曰：‘未也。”“不学礼，无以立。”鲤退而学礼。</p><p>捧袂（mèi）：举起双袖作揖，指谒见阎公。喜托龙门：（受到阎公的接待）十分高兴，好像登上龙门一样。</p><p>杨意：即蜀人杨得意，任掌管天子猎犬的官，西汉辞赋家司马相如是由他推荐给汉武帝的。凌云：这里指司马相如的赋，《史记·司马相如传》说，相如献《大人赋》，“天子大悦，飘飘有凌云之气，似游天地之间”。钟期：即钟子期。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python中的拷贝</title>
      <link href="/2018/08/18/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/Python%E4%B8%AD%E7%9A%84%E6%8B%B7%E8%B4%9D/"/>
      <url>/2018/08/18/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/Python%E4%B8%AD%E7%9A%84%E6%8B%B7%E8%B4%9D/</url>
      
        <content type="html"><![CDATA[<p>Python的拷贝和C/C++的差别很大，很经常就容易搞混，因此记录一下。</p><h3 id="赋值、拷贝"><a href="#赋值、拷贝" class="headerlink" title="赋值、拷贝"></a>赋值、拷贝</h3><ul><li>赋值：实际上就是对象的引用，没有开辟新的内存空间<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lst=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">l=lst</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>浅拷贝:创建了新对象，但是<strong>内容是对原对象的引用</strong>，有三种形式</p><ol><li><p>切片  </p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">l=lst[:]</span><br><span class="line">l=[i <span class="keyword">for</span> i <span class="keyword">in</span> lst]</span><br></pre></td></tr></table></figure></li><li><p>工厂</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l=list(lst)</span><br></pre></td></tr></table></figure></li><li><p>copy </p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">l=copy.copy(lst)</span><br></pre></td></tr></table></figure></li></ol></li><li><p>深拷贝:copy中的deepcopy，生成一个全新的对象，与原来的对象无关</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">l=copy.deepcopy(lst)</span><br></pre></td></tr></table></figure></li></ul><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 引用https://www.cnblogs.com/huangbiquan/p/7795152.html 的例子###</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> copy</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,[<span class="string">'a'</span>,<span class="string">'b'</span>]] <span class="comment">#定义一个列表a</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a <span class="comment">#赋值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = copy.copy(a) <span class="comment">#浅拷贝</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = copy.deepcopy(a) <span class="comment">#深拷贝</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.append(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(a)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>], <span class="number">5</span>] <span class="comment">#a添加一个元素5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(b) </span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>], <span class="number">5</span>] <span class="comment">#b跟着添加一个元素5 </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(c)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>]] <span class="comment">#c保持不变</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(d)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>]] <span class="comment">#d保持不变</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">4</span>].append(<span class="string">'c'</span>) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(a)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>], <span class="number">5</span>] <span class="comment">#a中的list(即a[4])添加一个元素c</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(b)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>], <span class="number">5</span>] <span class="comment">#b跟着添加一个元素c</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(c)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]] <span class="comment">#c跟着添加一个元素c</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(d)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>]] <span class="comment">#d保持不变</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#说明如下：</span></span><br><span class="line"><span class="comment">#1.外层添加元素时， 浅拷贝c不会随原列表a变化而变化；内层list添加元素时，浅拷贝c才会变化。</span></span><br><span class="line"><span class="comment">#2.无论原列表a如何变化，深拷贝d都保持不变。</span></span><br><span class="line"><span class="comment">#3.赋值对象随着原列表一起变化</span></span><br></pre></td></tr></table></figure><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.cnblogs.com/huangbiquan/p/7795152.html" target="_blank" rel="noopener">https://www.cnblogs.com/huangbiquan/p/7795152.html</a><br><a href="https://www.cnblogs.com/xueli/p/4952063.html" target="_blank" rel="noopener">https://www.cnblogs.com/xueli/p/4952063.html</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> 拷贝 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何将ELMo词向量用于中文</title>
      <link href="/2018/08/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E5%A6%82%E4%BD%95%E5%B0%86ELMo%E8%AF%8D%E5%90%91%E9%87%8F%E7%94%A8%E4%BA%8E%E4%B8%AD%E6%96%87/"/>
      <url>/2018/08/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E5%A6%82%E4%BD%95%E5%B0%86ELMo%E8%AF%8D%E5%90%91%E9%87%8F%E7%94%A8%E4%BA%8E%E4%B8%AD%E6%96%87/</url>
      
        <content type="html"><![CDATA[<p>10.10更新：ELMo已经由哈工大组用PyTorch重写了，并且提供了中文的预训练好的language model，可以直接使用，但代码中似乎还不能对生成的词向量进行加权求和，这部分可能需要自己改。如果可能我应该会再写一篇如何改代码的博客。</p><hr><p>ELMo于今年二月由AllenNLP提出，与word2vec或GloVe不同的是其动态词向量的思想，其本质即通过训练language model，对于一句话进入到language model获得不同的词向量。根据实验可得，使用了Elmo词向量之后，许多NLP任务都有了大幅的提高。</p><p>论文:<a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">Deep contextualized word representations</a></p><p>AllenNLP一共release了两份ELMo的代码，一份是Pytorch版本的，另一份是Tensorflow版本的。Pytorch版本的只开放了使用预训练好的词向量的接口，但没有给出自己训练的接口，因此无法使用到中文语料中。Tensorflow版本有提供训练的代码，因此本文记录如何将ELMo用于中文语料中，但本文只记录使用到的部分，而不会分析全部的代码。</p><p>需求:<br>使用预训练好的词向量作为句子表示直接传入到RNN中(也就是不使用代码中默认的先过CNN)，在训练完后，将模型保存，在需要用的时候load进来，对于一个特定的句子，首先将其转换成预训练的词向量，传入language model之后最终得到ELMo词向量。</p><p>准备工作:</p><ol><li>将中文语料分词</li><li>训练好GloVe词向量或者word2vec</li><li>下载<a href="https://github.com/allenai/bilm-tf" target="_blank" rel="noopener">bilm-tf代码</a></li><li>生成词表 vocab_file （训练的时候要用到）</li><li>optional:阅读Readme</li><li>optional:通读bilm-tf的代码，对代码结构有一定的认识</li></ol><p>思路:</p><ol><li>将预训练的词向量读入</li><li>修改bilm-tf代码<ol><li>option部分</li><li>添加给embedding weight赋初值</li><li>添加保存embedding weight的代码</li></ol></li><li>开始训练，获得checkpoint和option文件</li><li>运行脚本，获得language model的weight文件</li><li>将embedding weight保存为hdf5文件形式</li><li>运行脚本，将语料转化成ELMo embedding。</li></ol><h3 id="训练GloVe或word2vec"><a href="#训练GloVe或word2vec" class="headerlink" title="训练GloVe或word2vec"></a>训练GloVe或word2vec</h3><p>可参见我以前的博客或者网上的教程。<br>注意到，如果要用gensim导入GloVe训好的词向量，需要在开头添加num_word embedding_dim。 如：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-10-15338861462682.jpg" width="70%" height="50%"></p><h3 id="获得vocab词表文件"><a href="#获得vocab词表文件" class="headerlink" title="获得vocab词表文件"></a>获得vocab词表文件</h3><p>注意到，词表文件的开头必须要有<code>&lt;S&gt; &lt;/S&gt; &lt;UNK&gt;</code>，且大小写敏感。并且应当按照单词的词频降序排列。可以通过手动添加这三个特殊符号。<br>如：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-11-15339757184030.jpg" width="10%" height="50%"></p><p>代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model=gensim.models.KeyedVectors.load_word2vec_format(</span><br><span class="line">    fname=<span class="string">'/home/zhlin/GloVe/vectors.txt'</span>,binary=<span class="keyword">False</span></span><br><span class="line">)</span><br><span class="line">words=model.vocab</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'vocab.txt'</span>,<span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'&lt;S&gt;'</span>)</span><br><span class="line">    f.write(<span class="string">'\n'</span>）</span><br><span class="line">    f.write(<span class="string">'&lt;/S&gt;'</span>)</span><br><span class="line">    f.write(<span class="string">'\n'</span>)</span><br><span class="line">    f.write(<span class="string">'&lt;UNK&gt;'</span>)</span><br><span class="line">    f.write(<span class="string">'\n'</span>)    <span class="comment"># bilm-tf 要求vocab有这三个符号，并且在最前面</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        f.write(word)</span><br><span class="line">        f.write(<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure></p><h3 id="修改bilm-tf代码"><a href="#修改bilm-tf代码" class="headerlink" title="修改bilm-tf代码"></a>修改bilm-tf代码</h3><p>注意到，在使用该代码之前，需要安装好相应的环境。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-10-15338879402377.jpg" width="50%" height="50%"></p><p>如果使用的是conda作为默认的Python解释器，强烈建议使用conda安装，否则可能会出现一些莫名的错误。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install tensorflow-gpu=1.4</span><br><span class="line">conda install h5py</span><br><span class="line">python setup.py install <span class="comment">#应在bilm-tf的文件夹下执行该指令</span></span><br></pre></td></tr></table></figure></p><p>然后再运行测试代码，通过说明安装成功。</p><h4 id="修改train-elmo-py"><a href="#修改train-elmo-py" class="headerlink" title="修改train_elmo.py"></a>修改train_elmo.py</h4><p>bin文件夹下的train_elmo.py是程序的入口。<br>主要修改的地方：</p><ol><li>load_vocab的第二个参数应该改为None</li><li>n_gpus CUDA_VISIBLE_DEVICES 根据自己需求改</li><li>n_train_tokens 可改可不改，影响的是输出信息。要查看自己语料的行数，可以通过<code>wc -l corpus.txt</code> 查看。</li><li><strong>option的修改</strong>，将char_cnn部分都注释掉，其他根据自己需求修改</li></ol><p>如：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-10-15338888745894.jpg" width="70%" height="50%"></p><h4 id="修改LanguageModel类"><a href="#修改LanguageModel类" class="headerlink" title="修改LanguageModel类"></a>修改LanguageModel类</h4><p>由于我需要传入预训练好的GloVe embedding，那么还需要修改embedding部分，这部分在bilm文件夹下的training.py，进入到LanguageModel类中_build_word_embeddings函数中。注意到，由于前三个是<code>&lt;S&gt; &lt;/S&gt; &lt;UNK&gt;</code>，而这三个字符在GloVe里面是没有的，因此这三个字符的embedding应当在训练的时候逐渐学习到，而正因此 <code>embedding_weights</code>的<code>trainable</code>应当设为<code>True</code></p><p>如:</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-12-15340585073779.jpg" alt=""></p><h4 id="修改train函数"><a href="#修改train函数" class="headerlink" title="修改train函数"></a>修改train函数</h4><p>添加代码，使得在train函数的最后保存embedding文件。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-12-15340607132103.jpg" alt=""></p><h3 id="训练并获得weights文件"><a href="#训练并获得weights文件" class="headerlink" title="训练并获得weights文件"></a>训练并获得weights文件</h3><p>训练需要语料文件corpus.txt，词表文件vocab.txt。</p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>cd到bilm-tf文件夹下，运行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=4</span><br><span class="line">nohup python -u bin/train_elmo.py \</span><br><span class="line">--train_prefix=<span class="string">'/home/zhlin/bilm-tf/corpus.txt'</span> \</span><br><span class="line">--vocab_file /home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab.txt \</span><br><span class="line">--save_dir /home/zhlin/bilm-tf/try &gt;bilm_out.txt 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p><p>根据实际情况设定不同的值和路径。</p><p>运行情况：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-10-15339015862848.jpg" width="50%" height="50%"></p><p>PS:运行过程中可能会有warning:</p><blockquote><p>‘list’ object has no attribute ‘name’<br>WARNING:tensorflow:Error encountered when serializing lstm_output_embeddings.<br>Type is unsupported, or the types of the items don’t match field type in CollectionDef.</p></blockquote><p>应该不用担心，还是能够继续运行的，后面也不受影响。</p><p>在等待了相当长的时间后，在save_dir文件夹内生成了几个文件，其中checkpoint和options是关键，checkpoint能够进一步生成language model的weights文件，而options记录language model的参数。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-11-15339734319058.jpg" alt=""></p><h4 id="获得language-model的weights"><a href="#获得language-model的weights" class="headerlink" title="获得language model的weights"></a>获得language model的weights</h4><p>接下来运行bin/dump_weights.py将checkpoint转换成hdf5文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nohup python -u  /home/zhlin/bilm-tf/bin/dump_weights.py  \</span><br><span class="line">--save_dir /home/zhlin/bilm-tf/try  \</span><br><span class="line">--outfile /home/zhlin/bilm-tf/try/weights.hdf5 &gt;outfile.txt 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>其中save_dir是checkpoint和option文件保存的地址。</p><p>接下来等待程序运行：</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-11-15339740970081.jpg" width="70%" height="50%"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-11-15339745511775.jpg" width="70%" height="50%"></p><p>最终获得了想要的weights和option：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-11-15339978499136.jpg" alt=""></p><h3 id="将语料转化成ELMo-embedding"><a href="#将语料转化成ELMo-embedding" class="headerlink" title="将语料转化成ELMo embedding"></a>将语料转化成ELMo embedding</h3><p>由于我们有了vocab_file、与vocab_file一一对应的embedding h5py文件、以及language model的weights.hdf5和options.json。<br>接下来参考usage_token.py将一句话转化成ELMo embedding。</p><p>参考代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> bilm <span class="keyword">import</span> TokenBatcher, BidirectionalLanguageModel, weight_layers, \</span><br><span class="line">    dump_token_embeddings</span><br><span class="line"></span><br><span class="line"><span class="comment"># Our small dataset.</span></span><br><span class="line">raw_context = [</span><br><span class="line">    <span class="string">'这 是 测试 .'</span>,</span><br><span class="line">    <span class="string">'好的 .'</span></span><br><span class="line">]</span><br><span class="line">tokenized_context = [sentence.split() <span class="keyword">for</span> sentence <span class="keyword">in</span> raw_context]</span><br><span class="line">tokenized_question = [</span><br><span class="line">    [<span class="string">'这'</span>, <span class="string">'是'</span>, <span class="string">'什么'</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">vocab_file=<span class="string">'/home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab.txt'</span></span><br><span class="line">options_file=<span class="string">'/home/zhlin/bilm-tf/try/options.json'</span></span><br><span class="line">weight_file=<span class="string">'/home/zhlin/bilm-tf/try/weights.hdf5'</span></span><br><span class="line">token_embedding_file=<span class="string">'/home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab_embedding.hdf5'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Now we can do inference.</span></span><br><span class="line"><span class="comment"># Create a TokenBatcher to map text to token ids.</span></span><br><span class="line">batcher = TokenBatcher(vocab_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Input placeholders to the biLM.</span></span><br><span class="line">context_token_ids = tf.placeholder(<span class="string">'int32'</span>, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>))</span><br><span class="line">question_token_ids = tf.placeholder(<span class="string">'int32'</span>, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the biLM graph.</span></span><br><span class="line">bilm = BidirectionalLanguageModel(</span><br><span class="line">    options_file,</span><br><span class="line">    weight_file,</span><br><span class="line">    use_character_inputs=<span class="keyword">False</span>,</span><br><span class="line">    embedding_weight_file=token_embedding_file</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get ops to compute the LM embeddings.</span></span><br><span class="line">context_embeddings_op = bilm(context_token_ids)</span><br><span class="line">question_embeddings_op = bilm(question_token_ids)</span><br><span class="line"></span><br><span class="line">elmo_context_input = weight_layers(<span class="string">'input'</span>, context_embeddings_op, l2_coef=<span class="number">0.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">''</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">    <span class="comment"># the reuse=True scope reuses weights from the context for the question</span></span><br><span class="line">    elmo_question_input = weight_layers(</span><br><span class="line">        <span class="string">'input'</span>, question_embeddings_op, l2_coef=<span class="number">0.0</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elmo_context_output = weight_layers(</span><br><span class="line">    <span class="string">'output'</span>, context_embeddings_op, l2_coef=<span class="number">0.0</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">''</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">    <span class="comment"># the reuse=True scope reuses weights from the context for the question</span></span><br><span class="line">    elmo_question_output = weight_layers(</span><br><span class="line">        <span class="string">'output'</span>, question_embeddings_op, l2_coef=<span class="number">0.0</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># It is necessary to initialize variables once before running inference.</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create batches of data.</span></span><br><span class="line">    context_ids = batcher.batch_sentences(tokenized_context)</span><br><span class="line">    question_ids = batcher.batch_sentences(tokenized_question)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute ELMo representations (here for the input only, for simplicity).</span></span><br><span class="line">    elmo_context_input_, elmo_question_input_ = sess.run(</span><br><span class="line">        [elmo_context_input[<span class="string">'weighted_op'</span>], elmo_question_input[<span class="string">'weighted_op'</span>]],</span><br><span class="line">        feed_dict=&#123;context_token_ids: context_ids,</span><br><span class="line">                   question_token_ids: question_ids&#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">print(elmo_context_input_,elmo_context_input_)</span><br></pre></td></tr></table></figure></p><p>可以修改代码以适应自己的需求。</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://github.com/allenai/bilm-tf" target="_blank" rel="noopener">https://github.com/allenai/bilm-tf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> ELMo </tag>
            
            <tag> 教程 </tag>
            
            <tag> 词向量 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录4</title>
      <link href="/2018/08/12/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB4/"/>
      <url>/2018/08/12/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB4/</url>
      
        <content type="html"><![CDATA[<p>本周没有什么代码要记录的。</p><h3 id="1️⃣sklearn之Pipeline例子"><a href="#1️⃣sklearn之Pipeline例子" class="headerlink" title="1️⃣sklearn之Pipeline例子"></a>1️⃣sklearn之Pipeline例子</h3><p>用机器学习解决问题的流程：<br>(去掉部分数据）—&gt; 获取feature（Tf-idf等） —&gt; （feature selection，chi2、互信息等） —&gt; （缩放/正则化） —&gt; 分类器 —&gt; GridSearch/RandomizedSearch调参</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pipe=Pipeline([     <span class="comment">#建立pipeline</span></span><br><span class="line">    (<span class="string">'vect'</span>,TfidfVectorizer()),</span><br><span class="line">    (<span class="string">'select'</span>,SelectKBest(chi2),</span><br><span class="line">    (<span class="string">'norm'</span>,MaxAbsScaler()),   </span><br><span class="line">    (<span class="string">'svm'</span>,svm.LinearSVC())</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">parameters=&#123;</span><br><span class="line">    <span class="string">'vect__ngram_range'</span>:[(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">3</span>),(<span class="number">2</span>,<span class="number">3</span>)],</span><br><span class="line">    <span class="string">'vect__max_df'</span>:[<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>],</span><br><span class="line">    <span class="string">'vect__min_df'</span>:[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>],</span><br><span class="line">    <span class="string">'vect__norm'</span>:[<span class="string">'l1'</span>,<span class="string">'l2'</span>],</span><br><span class="line">    <span class="string">'svm__penalty'</span>:[<span class="string">'l1'</span>,<span class="string">'l2'</span>],</span><br><span class="line">    <span class="string">'svm__loss'</span>:[<span class="string">'squared_hinge'</span>],  </span><br><span class="line">    <span class="string">'svm__dual'</span>:[<span class="keyword">False</span>,<span class="keyword">True</span>],</span><br><span class="line">    <span class="string">'svm__tol'</span>:[<span class="number">1e-5</span>,<span class="number">1e-4</span>],</span><br><span class="line">    <span class="string">'svm__C'</span>:[<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>,<span class="number">1.0</span>,<span class="number">1.1</span>],</span><br><span class="line">    <span class="string">'svm__class_weight'</span>:[<span class="keyword">None</span>,<span class="string">'balanced'</span>],</span><br><span class="line">    <span class="string">'svm__max_iter'</span>:[<span class="number">1000</span>,<span class="number">5000</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">grid_search_model=GridSearchCV(pipe,parameters,error_score=<span class="number">0</span>,n_jobs=<span class="number">5</span>)</span><br><span class="line">grid_search_model.fit(train[column],train[<span class="string">'class'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> para_name <span class="keyword">in</span> sorted(parameters.keys()):</span><br><span class="line">    print(para_name,grid_search_model.best_params_[para_name])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"cv_result:"</span>)</span><br><span class="line">print(grid_search_model.cv_results_)</span><br></pre></td></tr></table></figure><hr>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识3</title>
      <link href="/2018/08/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%863/"/>
      <url>/2018/08/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%863/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[Python]<br>在服务器上跑代码时，如 <code>python project/folder1/a.py</code>，如果a.py引用了一个自定义的模块但又不在folder1内，此时interpreter就会报错，提示找不到该模块。这是因为解释器默认只会在同一个folder下查找。解决方案是在运行前显式添加查找范围。如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PYTHONPATH=/home/zhlin/bilm-tf:<span class="variable">$PYTHONPATH</span></span><br></pre></td></tr></table></figure></p><p>那么python解释器就会到该目录下去找。</p><hr><p>2️⃣[度量标准]<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-12-15340420442670.jpg" alt=""></p><ul><li>准确率(accuracy):  $ACC=\frac{TP+TN}{TP+TN+FP+FN}$<br> 衡量的是分类器预测准确的比例</li><li>召回率(recall): $Recall=\frac{TP}{TP+FN}$<br>  正例中被分对的比例，衡量了分类器对正例的识别能力。</li><li>精确率(Precision): $P=\frac{TP}{TP+FP}$<br>度量了被分为正例的示例中实际为正例的比例。</li><li>F-Measure: $F=\frac{(\alpha^2 +1)P*R}{\alpha^2 (P+R)}$<br>  其中P是Precision,R是Recall。综合考量了两种度量。<br>  当$\alpha=1$时，称为F1值 $F1=\frac{2PR}{P+R}$</li></ul><hr><p>3️⃣[调参技巧]<br>在google发布的一份关于text-classification的<a href="https://developers.google.com/machine-learning/guides/text-classification/" target="_blank" rel="noopener">guide</a>中，提到了几个调参的trick。</p><ol><li>在feature selection步骤中，卡方检验chi2和方差分析的F值 f_classif的表现相当，在大约选择20k的feature时，准确率达到顶峰，当feature越多，效果并没有提升甚至会下降。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-12-15340434326365.jpg" width="90%" height="50%"></li><li>在文本分类中，似乎使用normalization并没有多少用处，建议跳过。<blockquote><p>Normalization converts all feature/sample values to small and similar values. This simplifies gradient descent convergence in learning algorithms. From what we have seen, normalization during data preprocessing does not seem to add much value in text classification problems; we recommend skipping this step.</p></blockquote></li></ol><p>实际上我也测试过，发现确实normalization对于准确率的提高没什么帮助，甚至还有一点下降。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> 度量标准 </tag>
            
            <tag> 调参技巧 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词4</title>
      <link href="/2018/08/12/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D4/"/>
      <url>/2018/08/12/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D4/</url>
      
        <content type="html"><![CDATA[<p>1️⃣</p><h3 id="灞上秋居"><a href="#灞上秋居" class="headerlink" title="灞上秋居"></a>灞上秋居</h3><p>[唐] 马戴<br>灞原风雨定，晚见雁行频。<br>落叶他乡树，寒灯独夜人。<br>空园白露滴，孤壁野僧邻。<br><strong>寄卧郊扉久，何年致此身。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b8a4080a2b58005c9108d9" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8a4080a2b58005c9108d9</a></p><hr><p>2️⃣</p><h3 id="唐多令"><a href="#唐多令" class="headerlink" title="唐多令"></a>唐多令</h3><p>[宋] 刘过<br>芦叶满汀洲，寒沙带浅流。二十年重过南楼。柳下系船犹未稳，能几日，又中秋。<br>黄鹤断矶头，故人今在否？旧江山浑是新愁。<strong>欲买桂花同载酒，终不似、少年游。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b922e7c4c9710055904842" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b922e7c4c9710055904842</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Vim常用快捷键</title>
      <link href="/2018/08/10/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Vim%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
      <url>/2018/08/10/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Vim%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/</url>
      
        <content type="html"><![CDATA[<p>在服务器经常要用到Vim，因此记录常用的快捷键并熟悉之。</p><h3 id="退出"><a href="#退出" class="headerlink" title="退出"></a>退出</h3><p>:q 退出<br>:wq 写入并退出<br>:q! 退出并忽略所有更改<br>:e! 放弃修改并打开原来的文件</p><h3 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h3><p>i 在当前位置前插入<br>a 在当前位置后插入</p><h3 id="撤销"><a href="#撤销" class="headerlink" title="撤销"></a>撤销</h3><p>:u 撤销<br>:U 撤销整行操作<br>Ctrl+r 重做</p><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p>:md 删除第m行<br>nd 删除当前行开始的n行(一共n+1行)<br>dd 删除当前行<br>D 删除当前字符至行尾<br>:m,nd 删除从m到n行的内容，如: <code>:100,10000d</code><br>:m,$d 删除m行及以后所有的行<br>:10d</p><h3 id="移动"><a href="#移动" class="headerlink" title="移动"></a>移动</h3><p>:n 跳转到行号  如， :100<br>gg 跳到行首<br>G(shift+g)移动到文件尾</p><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><p>/text 搜索text，n搜索下一个，N搜索上一个<br>?text 反向查找<br>:set ignorecase 忽略大小写查找<br>:set noignorecase 不忽略大小写查找<br>*或# 对光标处的单词搜索</p><h3 id="复制粘贴"><a href="#复制粘贴" class="headerlink" title="复制粘贴"></a>复制粘贴</h3><p>v 从当前位置开始，光标经过的地方被选中，再按一下v结束</p><h3 id="环境设置"><a href="#环境设置" class="headerlink" title="环境设置"></a>环境设置</h3><p>:set nu 显示行号<br>:set nonu 隐藏行号<br>:set hlsearch 设置搜索结果高亮</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.cnblogs.com/wangrx/p/5907013.html" target="_blank" rel="noopener">https://www.cnblogs.com/wangrx/p/5907013.html</a><br><a href="https://www.cnblogs.com/yangjig/p/6014198.html" target="_blank" rel="noopener">https://www.cnblogs.com/yangjig/p/6014198.html</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Vim </tag>
            
            <tag> 杂七杂八 </tag>
            
            <tag> 快捷键 </tag>
            
            <tag> 技巧 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pycharm常用技巧</title>
      <link href="/2018/08/10/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Pycharm%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
      <url>/2018/08/10/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Pycharm%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<p>记录Pycharm的一些技巧，让Pycharm更顺手</p><h3 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h3><p>0️⃣Double Shift 万能搜索<br>可以搜索<strong>文件名、类名、方法名、目录名</strong>（在关键字前面加/ ），并不能用来搜索任意关键字</p><p>1️⃣ Command+F 在页面搜索</p><p>2️⃣ Ctrl+Shift+F Find in Path 在路径下搜索</p><p>3️⃣✨Command+E 快速查找文件<br>显示最近打开的文件</p><p>4️⃣ Shift+Enter 任意位置换行<br>无论光标在何处都可以直接另起一行</p><p>5️⃣ Option+Enter 自动导入模块；万能提示键<br>自动导入如何设置见小技巧#0️⃣</p><p>6️⃣ Ctrl+F10 运行<br>我已经添加了Ctrl+R作为另一对运行快捷键</p><p>7️⃣ Command+Shift+ +/-  展开/收缩代码 </p><p>8️⃣ Option+F 在Dash中搜索</p><h3 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h3><p>0️⃣ Pycharm自动导入模块<br><a href="https://blog.csdn.net/lantian_123/article/details/78094148" target="_blank" rel="noopener">https://blog.csdn.net/lantian_123/article/details/78094148</a></p><p>1️⃣ ✨远程部署工程 强烈推荐<br>两步走：配置服务器映射+配置服务器解释器</p><p>2️⃣跳转后如何回退<br>开启toolbar即可<br><a href="https://segmentfault.com/a/1190000010205945" target="_blank" rel="noopener">https://segmentfault.com/a/1190000010205945</a></p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://foofish.net/pycharm-tips.html" target="_blank" rel="noopener">https://foofish.net/pycharm-tips.html</a><br><a href="https://blog.csdn.net/lantian_123/article/details/78094148" target="_blank" rel="noopener">https://blog.csdn.net/lantian_123/article/details/78094148</a><br><a href="https://segmentfault.com/a/1190000010205945" target="_blank" rel="noopener">https://segmentfault.com/a/1190000010205945</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 杂七杂八 </tag>
            
            <tag> 快捷键 </tag>
            
            <tag> 技巧 </tag>
            
            <tag> Pycharm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>无题</title>
      <link href="/2018/08/06/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%97%A0%E9%A2%98/"/>
      <url>/2018/08/06/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%97%A0%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>人这辈子一共会死三次。</p><p>第一次是你的心脏停止跳动，那么从生物的角度来说，你死了；</p><p>第二次是在葬礼上，认识你的人都来祭奠，那么你在社会关系上的事实存在就死了；</p><p>第三次是在最后一个记得你的人死后，那你就真的死了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>人可以卑微如尘土,不可扭曲如蛆虫</title>
      <link href="/2018/08/05/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BA%BA%E5%8F%AF%E4%BB%A5%E5%8D%91%E5%BE%AE%E5%A6%82%E5%B0%98%E5%9C%9F,%E4%B8%8D%E5%8F%AF%E6%89%AD%E6%9B%B2%E5%A6%82%E8%9B%86%E8%99%AB/"/>
      <url>/2018/08/05/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BA%BA%E5%8F%AF%E4%BB%A5%E5%8D%91%E5%BE%AE%E5%A6%82%E5%B0%98%E5%9C%9F,%E4%B8%8D%E5%8F%AF%E6%89%AD%E6%9B%B2%E5%A6%82%E8%9B%86%E8%99%AB/</url>
      
        <content type="html"><![CDATA[<p>如果天总也不亮，那就摸黑过生活; </p><p>如果发出声音是危险的，那就保持沉默; </p><p>如果自觉无力发光，那就别去照亮别人。 </p><p>但是——不要习惯了黑暗就为黑暗辩护; </p><p>不要为自己的苟且而得意洋洋; </p><p>不要嘲讽那些比自己更勇敢、更有热量的人们。 </p><p><strong>可以卑微如尘土，不可扭曲如蛆虫</strong>。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python惯例[转]</title>
      <link href="/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/Python%E6%83%AF%E4%BE%8B/"/>
      <url>/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/Python%E6%83%AF%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<p>fork from <a href="https://github.com/jackfrued/Python-100-Days/blob/master/Python惯例.md" target="_blank" rel="noopener">https://github.com/jackfrued/Python-100-Days/blob/master/Python惯例.md</a></p><h2 id="Python惯例"><a href="#Python惯例" class="headerlink" title="Python惯例"></a>Python惯例</h2><p>“惯例”这个词指的是“习惯的做法，常规的办法，一贯的做法”，与这个词对应的英文单词叫“idiom”。由于Python跟其他很多编程语言在语法和使用上还是有比较显著的差别，因此作为一个Python开发者如果不能掌握这些惯例，就无法写出“Pythonic”的代码。下面我们总结了一些在Python开发中的惯用的代码。</p><ol><li><p>让代码既可以被导入又可以被执行。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br></pre></td></tr></table></figure></li><li><p>用下面的方式判断逻辑“真”或“假”。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> x:</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> x:</span><br></pre></td></tr></table></figure><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">'jackfrued'</span></span><br><span class="line">fruits = [<span class="string">'apple'</span>, <span class="string">'orange'</span>, <span class="string">'grape'</span>]</span><br><span class="line">owners = &#123;<span class="string">'1001'</span>: <span class="string">'骆昊'</span>, <span class="string">'1002'</span>: <span class="string">'王大锤'</span>&#125;</span><br><span class="line"><span class="keyword">if</span> name <span class="keyword">and</span> fruits <span class="keyword">and</span> owners:</span><br><span class="line">    print(<span class="string">'I love fruits!'</span>)</span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">'jackfrued'</span></span><br><span class="line">fruits = [<span class="string">'apple'</span>, <span class="string">'orange'</span>, <span class="string">'grape'</span>]</span><br><span class="line">owners = &#123;<span class="string">'1001'</span>: <span class="string">'骆昊'</span>, <span class="string">'1002'</span>: <span class="string">'王大锤'</span>&#125;</span><br><span class="line"><span class="keyword">if</span> name != <span class="string">''</span> <span class="keyword">and</span> len(fruits) &gt; <span class="number">0</span> <span class="keyword">and</span> owners != &#123;&#125;:</span><br><span class="line">    print(<span class="string">'I love fruits!'</span>)</span><br></pre></td></tr></table></figure></li><li><p>善于使用in运算符。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> x <span class="keyword">in</span> items: <span class="comment"># 包含</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> items: <span class="comment"># 迭代</span></span><br></pre></td></tr></table></figure><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">'Hao LUO'</span></span><br><span class="line"><span class="keyword">if</span> <span class="string">'L'</span> <span class="keyword">in</span> name:</span><br><span class="line">    print(<span class="string">'The name has an L in it.'</span>)</span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">'Hao LUO'</span></span><br><span class="line"><span class="keyword">if</span> name.find(<span class="string">'L'</span>) != <span class="number">-1</span>:</span><br><span class="line">    print(<span class="string">'This name has an L in it!'</span>)</span><br></pre></td></tr></table></figure></li><li><p>不使用临时变量交换两个值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a, b = b, a</span><br></pre></td></tr></table></figure></li><li><p><strong>用序列构建字符串</strong>。</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chars = [<span class="string">'j'</span>, <span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'k'</span>, <span class="string">'f'</span>, <span class="string">'r'</span>, <span class="string">'u'</span>, <span class="string">'e'</span>, <span class="string">'d'</span>]</span><br><span class="line">name = <span class="string">''</span>.join(chars)</span><br><span class="line">print(name)  <span class="comment"># jackfrued</span></span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">chars = [<span class="string">'j'</span>, <span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'k'</span>, <span class="string">'f'</span>, <span class="string">'r'</span>, <span class="string">'u'</span>, <span class="string">'e'</span>, <span class="string">'d'</span>]</span><br><span class="line">name = <span class="string">''</span></span><br><span class="line"><span class="keyword">for</span> char <span class="keyword">in</span> chars:</span><br><span class="line">    name += char</span><br><span class="line">print(name)  <span class="comment"># jackfrued</span></span><br></pre></td></tr></table></figure></li><li><p><strong>EAFP优于LBYL</strong>。</p><p>EAFP - <strong>E</strong>asier to <strong>A</strong>sk <strong>F</strong>orgiveness than <strong>P</strong>ermission.</p><p>LBYL - <strong>L</strong>ook <strong>B</strong>efore <strong>Y</strong>ou <strong>L</strong>eap.</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'x'</span>: <span class="string">'5'</span>&#125;</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    value = int(d[<span class="string">'x'</span>])</span><br><span class="line">    print(value)</span><br><span class="line"><span class="keyword">except</span> (KeyError, TypeError, ValueError):</span><br><span class="line">    value = <span class="keyword">None</span></span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'x'</span>: <span class="string">'5'</span>&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="string">'x'</span> <span class="keyword">in</span> d <span class="keyword">and</span> isinstance(d[<span class="string">'x'</span>], str) \</span><br><span class="line"><span class="keyword">and</span> d[<span class="string">'x'</span>].isdigit():</span><br><span class="line">    value = int(d[<span class="string">'x'</span>])</span><br><span class="line">    print(value)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    value = <span class="keyword">None</span></span><br></pre></td></tr></table></figure></li><li><p>使用enumerate进行迭代。</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fruits = [<span class="string">'orange'</span>, <span class="string">'grape'</span>, <span class="string">'pitaya'</span>, <span class="string">'blueberry'</span>]</span><br><span class="line"><span class="keyword">for</span> index, fruit <span class="keyword">in</span> enumerate(fruits):</span><br><span class="line">print(index, <span class="string">':'</span>, fruit)</span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fruits = [<span class="string">'orange'</span>, <span class="string">'grape'</span>, <span class="string">'pitaya'</span>, <span class="string">'blueberry'</span>]</span><br><span class="line">index = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> fruit <span class="keyword">in</span> fruits:</span><br><span class="line">    print(index, <span class="string">':'</span>, fruit)</span><br><span class="line">    index += <span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p>用生成式生成列表。</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = [<span class="number">7</span>, <span class="number">20</span>, <span class="number">3</span>, <span class="number">15</span>, <span class="number">11</span>]</span><br><span class="line">result = [num * <span class="number">3</span> <span class="keyword">for</span> num <span class="keyword">in</span> data <span class="keyword">if</span> num &gt; <span class="number">10</span>]</span><br><span class="line">print(result)  <span class="comment"># [60, 45, 33]</span></span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = [<span class="number">7</span>, <span class="number">20</span>, <span class="number">3</span>, <span class="number">15</span>, <span class="number">11</span>]</span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">    <span class="keyword">if</span> i &gt; <span class="number">10</span>:</span><br><span class="line">        result.append(i * <span class="number">3</span>)</span><br><span class="line">print(result)  <span class="comment"># [60, 45, 33]</span></span><br></pre></td></tr></table></figure></li><li><p>用zip组合键和值来创建字典。</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keys = [<span class="string">'1001'</span>, <span class="string">'1002'</span>, <span class="string">'1003'</span>]</span><br><span class="line">values = [<span class="string">'骆昊'</span>, <span class="string">'王大锤'</span>, <span class="string">'白元芳'</span>]</span><br><span class="line">d = dict(zip(keys, values))</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keys = [<span class="string">'1001'</span>, <span class="string">'1002'</span>, <span class="string">'1003'</span>]</span><br><span class="line">values = [<span class="string">'骆昊'</span>, <span class="string">'王大锤'</span>, <span class="string">'白元芳'</span>]</span><br><span class="line">d = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i, key <span class="keyword">in</span> enumerate(keys):</span><br><span class="line">    d[key] = values[i]</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure></li></ol><blockquote><p><strong>说明</strong>：这篇文章的内容来自于网络，有兴趣的读者可以阅读<a href="http://safehammad.com/downloads/python-idioms-2014-01-16.pdf" target="_blank" rel="noopener">原文</a>。</p></blockquote><p>注：<br>许多原则我认为非常有意义，能够摆脱C/C++的风格，真正写出Pythonic的代码。让我有很大感触的是1、3、8，能够写出非常简洁优雅的代码。同时6我之前从没注意过，习惯了C/C++风格之后总是会在执行之前考虑所有情况，但确实不够优雅，今后可以尝试EAFP风格（<a href="https://stackoverflow.com/questions/11360858/what-is-the-eafp-principle-in-python" target="_blank" rel="noopener">什么是EAFP</a>）。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 杂七杂八 </tag>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何训练GloVe中文词向量</title>
      <link href="/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83GloVe%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F/"/>
      <url>/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83GloVe%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<h3 id="准备语料"><a href="#准备语料" class="headerlink" title="准备语料"></a>准备语料</h3><p>准备好自己的语料，保存为txt，每行一个句子或一段话，注意要分好词。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-05-15334388069130.jpg" width="60%" height="60%"></p><h3 id="准备源码"><a href="#准备源码" class="headerlink" title="准备源码"></a>准备源码</h3><p>从GitHub下载代码，<a href="https://github.com/stanfordnlp/GloVe" target="_blank" rel="noopener">https://github.com/stanfordnlp/GloVe</a><br>将语料corpus.txt放入到Glove的主文件夹下。</p><h3 id="修改bash"><a href="#修改bash" class="headerlink" title="修改bash"></a>修改bash</h3><p>打开demo.sh，修改相应的内容</p><ol><li>因为demo默认是下载网上的语料来训练的，因此如果要训练自己的语料，需要注释掉</li></ol><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-05-15334390298383.jpg" width="70%" height="50%"></p><ol><li>修改参数设置，将CORPUS设置成语料的名字</li></ol><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-05-15334391029224.jpg" width="50%" height="50%"></p><h3 id="执行bash文件"><a href="#执行bash文件" class="headerlink" title="执行bash文件"></a>执行bash文件</h3><p>进入到主文件夹下</p><ol><li>make</li></ol><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-05-15334392348665.jpg" width="70%" height="50%"></p><ol><li>bash demo.sh</li></ol><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-05-15334392595148.jpg" width="70%" height="70%"></p><p>注意，如果训练数据较大，则训练时间较长，那么建议使用nohup来运行程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup bash demo.sh &gt;output.txt 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>坐等训练，最后会得到vectors.txt 以及其他的相应的文件。如果要用gensim的word2ve load进来，那么需要在vectors.txt的第一行加上vacob_size vector_size，第一个数指明一共有多少个向量，第二个数指明每个向量有多少维。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.cnblogs.com/echo-cheng/p/8561171.html" target="_blank" rel="noopener">https://www.cnblogs.com/echo-cheng/p/8561171.html</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> GloVe </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识2</title>
      <link href="/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%862/"/>
      <url>/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%862/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[Pytorch]<br>避免写出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.zeros(...), requires_grad=<span class="keyword">True</span>).cuda()</span><br></pre></td></tr></table></figure><p>而是应该要：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.zeros(...).cuda(), requires_grad=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>Reference:<br><a href="https://discuss.pytorch.org/t/variable-grad-is-always-none-when-extending-autograd/12187" target="_blank" rel="noopener">https://discuss.pytorch.org/t/variable-grad-is-always-none-when-extending-autograd/12187</a></p><hr><p>2️⃣[Tf-idf]<br>本周因为比赛的原因了解了一下各种文本建模的方法。Tf-idf能够取得不错的成绩，但有一定的缺陷。</p><blockquote><p>TF-IDF用于向量空间模型，进行文档相似度计算是相当有效的。但在文本分类中单纯使用TF-IDF来判断一个特征是否有区分度是不够的。</p><ol><li>它仅仅综合考虑了该词在文档中的重要程度和文档区分度。</li><li>它没有考虑特征词在类间的分布。特征选择所选择的特征应该在某类出现多，而其它类出现少，即考察各类的文档频率的差异。如果一个特征词，在各个类间分布比较均匀，这样的词对分类基本没有贡献；但是如果一个特征词比较集中的分布在某个类中，而在其它类中几乎不出现，这样的词却能够很好代表这个类的特征，而TF-IDF不能区分这两种情况。</li><li>它没有考虑特征词在类内部文档中的分布情况。在类内部的文档中，如果特征词均匀分布在其中，则这个特征词能够很好的代表这个类的特征，如果只在几篇文档中出现，而在此类的其它文档中不出现，显然这样的特征词不能够代表这个类的特征。</li></ol></blockquote><p>Reference:<br><a href="https://blog.csdn.net/mmc2015/article/details/46771791" target="_blank" rel="noopener">https://blog.csdn.net/mmc2015/article/details/46771791</a></p><hr><p>3️⃣[卡方检验CHI]<br>在文本分类中，用于选择最相关的特征。</p><p>Reference:<br><a href="https://blog.csdn.net/blockheadls/article/details/49977361" target="_blank" rel="noopener">https://blog.csdn.net/blockheadls/article/details/49977361</a></p><hr><p>4️⃣[文本分类]<br>各种文本分类方法的简单介绍。</p><p>Reference:<br><a href="https://github.com/wangjiang0624/Note/blob/master/MachineLearning/文本分类.md" target="_blank" rel="noopener">https://github.com/wangjiang0624/Note/blob/master/MachineLearning/文本分类.md</a></p><hr><p>5️⃣[Python]<br>collections的两个有用的类</p><ol><li>named_tuple：快速建立一个类，使得可以使用属性来访问而非索引，提高了代码可读性</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line">Point = namedtuple(<span class="string">'Point'</span>,[<span class="string">'x'</span>,<span class="string">'y'</span>])</span><br><span class="line">p = Point(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">print(p.x)  <span class="comment"># 1</span></span><br><span class="line">print(p.y)  <span class="comment"># 2</span></span><br></pre></td></tr></table></figure><ol><li>Counter：统计字符出现的次数</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">count = Counter([...]).most_commom()  <span class="comment">#会按照出现的次数排序，通常可用于构建词典</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> count:     <span class="comment"># c是一个tuple，c[0]是词，c[1]是频率</span></span><br><span class="line">    <span class="keyword">if</span> c[<span class="number">1</span>]&gt;= threshold:</span><br><span class="line">        vocab.add_word(c[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>Counter用法：<br><a href="https://blog.csdn.net/u014755493/article/details/69812244" target="_blank" rel="noopener">https://blog.csdn.net/u014755493/article/details/69812244</a></p><hr><p>6️⃣[nohup]<br>本周在服务器上跑代码的时候遇到一个问题，使用nohup执行python程序时，发现输出文件没有显示。以为是代码的问题，但经过排查并非是代码的问题。通过查阅资料，发现问题所在：<br>因为python输出有缓冲，导致output不能<strong>马上</strong>看到输出。实际上，在等待了一段时间后，输出文件终于显示出来了。</p><p>解决方案：使用python的参数 -u 使得python不启用缓冲。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python -u test.py &gt; nohup.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>Reference:<br><a href="https://blog.csdn.net/sunlylorn/article/details/19127107" target="_blank" rel="noopener">https://blog.csdn.net/sunlylorn/article/details/19127107</a></p><hr><p>7️⃣[hexo配置]</p><ol><li>mathjax配置: <a href="https://www.jianshu.com/p/7ab21c7f0674" target="_blank" rel="noopener">https://www.jianshu.com/p/7ab21c7f0674</a></li><li>配置域名:<a href="https://www.zhihu.com/question/31377141" target="_blank" rel="noopener">https://www.zhihu.com/question/31377141</a></li><li>配置sitemap:<a href="http://www.yuan-ji.me/Hexo-优化：提交sitemap及解决百度爬虫抓取-GitHub-Pages-问题/" target="_blank" rel="noopener">http://www.yuan-ji.me/Hexo-优化：提交sitemap及解决百度爬虫抓取-GitHub-Pages-问题/</a></li></ol><hr><p>8️⃣[Paper]<br><a href="http://aclweb.org/anthology/D17-1025" target="_blank" rel="noopener">Learning Chinese Word Representations From Glyphs Of Characters</a></p><p>使用图像的卷积来生成词向量:<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-05-15334453515509.jpg" width="60%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Paper </tag>
            
            <tag> Tf-idf </tag>
            
            <tag> 文本分类 </tag>
            
            <tag> hexo </tag>
            
            <tag> nohup </tag>
            
            <tag> CHI </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录3</title>
      <link href="/2018/08/05/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB3/"/>
      <url>/2018/08/05/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB3/</url>
      
        <content type="html"><![CDATA[<p>本周只有简单的代码。</p><h3 id="1️⃣使用gensim训练word2vec"><a href="#1️⃣使用gensim训练word2vec" class="headerlink" title="1️⃣使用gensim训练word2vec"></a>1️⃣使用gensim训练word2vec</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#encoding=utf-8</span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line">sentences=word2vec.Text8Corpus(<span class="string">u'分词后的爽肤水评论.txt'</span>)   <span class="comment">#sentence:[ [ a b ],[c d]... ]</span></span><br><span class="line">model=word2vec.Word2Vec(sentences, size=<span class="number">50</span>)  <span class="comment">#size:dim </span></span><br><span class="line"></span><br><span class="line">y2=model.similarity(<span class="string">u"好"</span>, <span class="string">u"还行"</span>)  <span class="comment">#计算相似度</span></span><br><span class="line">print(y2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> model.most_similar(<span class="string">u"滋润"</span>):</span><br><span class="line">    <span class="keyword">print</span> i[<span class="number">0</span>],i[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment">#保存</span></span><br><span class="line">model.save(<span class="string">'/model/word2vec_model'</span>)</span><br><span class="line"></span><br><span class="line">new_model=gensim.models.Word2Vec.load(<span class="string">'/model/word2vec_model'</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="2️⃣使用Counter建立词表"><a href="#2️⃣使用Counter建立词表" class="headerlink" title="2️⃣使用Counter建立词表"></a>2️⃣使用Counter建立词表</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dict</span><span class="params">(dataset,min_freq=<span class="number">5</span>)</span>:</span></span><br><span class="line">    dictionary=Dictionary() </span><br><span class="line">    count=Counter(flat(dataset)).most_common()  </span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> count:</span><br><span class="line">        <span class="keyword">if</span> c[<span class="number">1</span>]&gt;=min_freq:</span><br><span class="line">            dictionary.add_word(c[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> dictionary</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词3</title>
      <link href="/2018/08/05/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D3/"/>
      <url>/2018/08/05/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D3/</url>
      
        <content type="html"><![CDATA[<p>本周背的都是比较简单的。</p><p>1️⃣</p><h3 id="把酒问月"><a href="#把酒问月" class="headerlink" title="把酒问月"></a>把酒问月</h3><p>[唐] 李白<br>青天有月来几时？我今停杯一问之。<br>人攀明月不可得，月行却与人相随。<br>皎如飞镜临丹阙，绿烟灭尽清辉发。<br>但见宵从海上来，宁知晓向云间没。<br>白兔捣药秋复春，嫦娥孤栖与谁邻？<br><strong>今人不见古时月，今月曾经照古人。<br>古人今人若流水，共看明月皆如此。</strong><br><strong>唯愿当歌对酒时，月光长照金樽里。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b8f6f4165abd0054bf8c13" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8f6f4165abd0054bf8c13</a></p><hr><p>2️⃣</p><h3 id="金缕衣"><a href="#金缕衣" class="headerlink" title="金缕衣"></a>金缕衣</h3><p>[唐] 杜秋娘<br>劝君莫惜金缕衣，劝君惜取少年时。<br><strong>花开堪折直须折，莫待无花空折枝。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b92bdca633bd00665eb99e" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b92bdca633bd00665eb99e</a></p><hr><p>3️⃣</p><h3 id="北青萝"><a href="#北青萝" class="headerlink" title="北青萝"></a>北青萝</h3><p>[唐] 李商隐<br>残阳西入崦，茅屋访孤僧。<br>落叶人何在，寒云路几层。<br>独敲初夜磬，闲倚一枝藤。<br><strong>世界微尘里，吾宁爱与憎。</strong></p><p>崦（yān）：即“崦嵫（zī）”，山名，在甘肃。古时常用来指太阳落山的地方。<br><strong>磬（qìng）</strong>：古代打击乐器，形状像曲尺，用玉、石制成，可悬挂。</p><p><a href="http://m.xichuangzhu.com/work/57b8ee240a2b58005c91c99e" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8ee240a2b58005c91c99e</a></p><hr><p>4️⃣</p><h3 id="夏日绝句"><a href="#夏日绝句" class="headerlink" title="夏日绝句"></a>夏日绝句</h3><p>[宋] 李清照<br>生当作人杰，死亦为鬼雄。<br><strong>至今思项羽，不肯过江东。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b911dac4c97100558fb30e" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b911dac4c97100558fb30e</a></p><hr><p>5️⃣</p><h3 id="雨霖铃"><a href="#雨霖铃" class="headerlink" title="雨霖铃"></a>雨霖铃</h3><p>[宋] 柳永<br>寒蝉凄切，对长亭晚，骤雨初歇。都门帐饮无绪，留恋处，兰舟催发。执手相看泪眼，竟无语凝噎。念去去，千里烟波，暮霭沉沉楚天阔。<br><strong>多情自古伤离别</strong>，更那堪、冷落清秋节。今宵酒醒何处？杨柳岸，晓风残月。此去经年，应是良辰好景虚设。<strong>便纵有千种风情，更与何人说</strong>？</p><p><a href="http://m.xichuangzhu.com/work/57ad742f5bbb500062bc7c9c" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57ad742f5bbb500062bc7c9c</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Pytorch中grad的理解</title>
      <link href="/2018/08/03/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADgrad%E7%9A%84%E7%90%86%E8%A7%A3/"/>
      <url>/2018/08/03/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADgrad%E7%9A%84%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>事情起源于我写了一个CNN用于文本分类，但loss一直没降，因此我尝试<code>print(loss.grad)</code>的grad，发现神奇的是loss grad显示为None，接着尝试<code>print(y_pred.grad)</code>，同样是None，但再print loss和y_pred的requires_grad发现是正常的True。</p><p>在查阅了资料，以及问了学长之后发现原来并不是bug，而是因为，Pytorch默认不会保存中间节点(intermediate variable)的grad，此举是为了节省内存。</p><blockquote><p>By default, gradients are only retained for leaf variables. non-leaf variables’ gradients are not retained to be inspected later. This was done by design, to save memory.</p></blockquote><p><a href="https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94" target="_blank" rel="noopener">https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94</a></p><p>实际上可以通过retain_grad()或者hook来查看中间节点的grad。</p><p>我后面尝试print了叶子节点，如 <code>print(CNN_model.fc.weight.grad)</code>，最终获得了正确的grad。</p><p>ps：所谓中间节点，是<strong>由其他节点计算所得</strong>的tensor，而叶子节点则是<strong>自己定义</strong>出来的。</p><p>最后我发现，原来loss一直没降的原因是因为我定义的CNN过于复杂，并且数据集偏小，无法快速收敛导致的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> grad </tag>
            
            <tag> bug </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Linux常用命令</title>
      <link href="/2018/08/03/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2018/08/03/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h3 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h3><p>记录自己常用的命令。</p><p>1️⃣ls：显式当前目录下的文件和目录<br>    -a 包括隐藏文件<br>    -h 将文件的容量以易读方式列出（配合-s使用）<br>    -s 以块数形式显示每个文件分配的尺寸<br>    -l 以较长格式列出信息，可以直接写成 <code>ll</code><br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-10-15339070264416.jpg" width="70%" height="50%"></p><hr><p>2️⃣cd 到达指定地址</p><hr><p>3️⃣kill 杀死程序<br>    -l 信息编号。<strong>当l=9时，无条件终止，其他信号可能忽略</strong><br>    killall -u <user_name> 杀死该用户全部进程</user_name></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-03-15332830170623.jpg" width="50%" height="50%"></p><hr><p>4️⃣ps 报告当前系统的进程状态<br>    -a 所有<br>    -p 指定程序<br>    -u 指定用户<br>    -x 列出该用户的进程的详细信息(我的理解应该是)<br>    如：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-10-15339036207642.jpg" width="70%" height="50%"></p><hr><p>5️⃣htop 比top更优，交互更好，同时可以直观看到资源占用情况<br>基本命令与top一致<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-04-15333484387393.jpg" width="70%" height="50%"></p><hr><p>6️⃣top：动态查看系统运行状态<br>    -u 指定用户名<br>    -p 指定进程</p><p>7️⃣nvidia-smi 查看显卡状态<br>watch nvidia-smi 实时查看显卡状态，定时刷新</p><hr><p>8️⃣tail 显示指定文件的末尾若干行<br>    -f 显示文件最新追加的内容<br>    -n 显示文件尾部n行内容<br>    -c 显示文件尾部最后c个字符</p><p>如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tail file 显示最后10行</span><br><span class="line">tail -n +20 file 显示从第20行至末尾</span><br><span class="line">tail -c 10 file 显示文件file的最后10个字符</span><br></pre></td></tr></table></figure><pre><code>-------</code></pre><p>9️⃣echo 用于打印指定的字符串<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-04-15333497266470.jpg" width="50%" height="50%"></p><hr><p>🔟which 用于查找并显示给定命令的绝对路径，which指令会在环境变量$PATH设置的目录里查找符合条件的文件。使用which命令，可以看到某个系统命令是否存在，以及执行的是哪个位置的命令。如：</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-04-15333500837235.jpg" width="50%" height="50%"></p><hr><p>1️⃣1️⃣nohup 将程序以忽略挂起信号的方式运行，经常用于在服务器跑代码<br>如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python xxx.py &gt;output.txt 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p><p>即，将输出重定向到output.txt ；最后一个<code>&amp;</code>表示后台挂起</p><hr><p>1️⃣2️⃣cp 复制文件   cp [文件] [目标文件夹]<br>    -r 递归复制，用于目录的复制</p><hr><p>1️⃣3️⃣mv 移动文件、目录或更名  mv [文件/文件夹] [文件夹]<br>    -f 强制，当目标文件存在，直接覆盖<br>    -i 会询问</p><hr><p>1️⃣4️⃣rm 删除文件或目录<br>    -f 强制删除<br>    -r 递归删除，用于目录删除</p><hr><p>1️⃣5️⃣file 用于判断文件的基本数据<br>如：</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-08-05-15334547949248.jpg" width="80%" height="50%"></p><hr><p>1️⃣6️⃣tar 对文件打包/压缩<br>    -t 查看打包文件的内容含有哪些文件名<br>    -x 解压缩<br>    -c 新建打包文件<br>    -C 指定压缩/解压目录<br>    -v 解压/压缩过程中将处理的文件名显示出来<br>常用的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">压缩：tar -jcv -f filename.tar.bz2 要被处理的文件或目录名称</span><br><span class="line">查询：tar -jtv -f filename.tar.bz2</span><br><span class="line">解压：tar -jxv -f filename.tar.bz2 -C 欲解压缩的目录</span><br></pre></td></tr></table></figure><hr><p>1️⃣7️⃣wc word count 统计文件内容信息，如行数、字符数<br>    -l 显示文件行数<br>    -c 显示字节数<br>    -m 显示字符数<br>    -w 显示字数  字被定义为由空白、跳格、换行字符分隔的字符串<br>    -L 显示最长行的长度<br>    不加参数，所有的都显示，依次是行数、单词数、字节数、文件名</p><hr><p>1️⃣8️⃣df 显示磁盘相关信息<br>    -h 以可读性较高的方式显示信息</p><hr><p>1️⃣9️⃣scp 服务器之间的文件复制<br>    如:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r /test1 zhlin@123.12.1.12:/home/zhlin</span><br></pre></td></tr></table></figure><h3 id="✨快捷键"><a href="#✨快捷键" class="headerlink" title="✨快捷键"></a>✨快捷键</h3><p>Ctrl+a 跳到行首<br>Ctrl+c 退出当前进程<br>Ctrl+e 跳到页尾<br>Ctrl+k 删除当前光标后面的文字<br>Ctrl+l 清屏，等价于clear<br>Ctrl+r 搜索之前打过的命令<br>Ctrl+u 删除当前光标前面的文字<br>✨Ctrl+左右键 单词之间跳转 在Mac上可以使用option+左右键<br>Ctrl+y 进行恢复删除<br>Ctrl+z 将当前进程转到后台，使用fg恢复</p><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://blog.csdn.net/leo_618/article/details/53003111" target="_blank" rel="noopener">https://blog.csdn.net/leo_618/article/details/53003111</a></p><p>———-持续更新———-</p>]]></content>
      
      
      
        <tags>
            
            <tag> 技巧 </tag>
            
            <tag> Linux </tag>
            
            <tag> 碎片知识 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>一个关于yield的重新认识</title>
      <link href="/2018/07/31/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E4%B8%80%E4%B8%AA%E5%85%B3%E4%BA%8Eyield%E7%9A%84%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86/"/>
      <url>/2018/07/31/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E4%B8%80%E4%B8%AA%E5%85%B3%E4%BA%8Eyield%E7%9A%84%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<p>今天遇到了一个神奇的”bug”，让我对yield的理解更深一步。</p><p>这是一个函数，我本来打算试着print一下line内部的格式和内容。<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-31-15330184801079.jpg" width="40%" height="40%"></p><p>这是调用的主函数：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-31-15330185414299.jpg" width="50%" height="50%"></p><p>结果跑出的结果是：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-31-15330185762441.jpg" width="90%" height="90%"></p><p>？？？<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-31-15330186003254.jpg" alt=""></p><p>我尝试在函数的开头添加print：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-31-15330186689312.jpg" width="40%" height="50%"></p><p>结果仍然没有任何的输出。</p><p>我试着在main函数添加print：<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-31-15330187249603.jpg" width="50%" height="50%"></p><p>结果：</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-31-15330187445810.jpg" width="50%" height="50%"></p><p>也就是说，根本没有进入到get_dataset_from_txt函数啊。</p><p>我以为是pycharm的问题还重启了一遍，然而并没有任何作用。问了其他人，他们也觉得很神奇。最后一个同学看了一下函数，发现了问题所在：<strong>yield</strong></p><p>我突然想起来，<strong>yield返回的是一个generator，只有在对generator进行遍历时，才会开始运行</strong>…</p><p>于是，我试着这么写，试着对generator遍历：</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-31-15330189280379.jpg" width="50%" height="50%"></p><p>虽然报错了，但函数终于是进去了…</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-31-15330189613535.jpg" width="70%" height="50%"></p><p><strong>结论：有yield的函数会返回一个generator，当对其进行遍历时，函数才会开始运行。</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Python </tag>
            
            <tag> yield </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录2</title>
      <link href="/2018/07/29/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB2/"/>
      <url>/2018/07/29/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB2/</url>
      
        <content type="html"><![CDATA[<p>本周主要看了<a href="https://github.com/allenai/bilm-tf" target="_blank" rel="noopener">AllenNLP/ELMO</a>的代码，但并没有找到很多可复用的代码。本周也没有比较有意义的代码。</p><hr><h3 id="1️⃣get-time-diff"><a href="#1️⃣get-time-diff" class="headerlink" title="1️⃣get_time_diff"></a>1️⃣get_time_diff</h3><p>获取已使用的时间<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta</span><br><span class="line"></span><br><span class="line">start_time=time.time()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time_dif</span><span class="params">(start_time)</span>:</span></span><br><span class="line">    <span class="string">"""获取已使用时间"""</span></span><br><span class="line">    end_time = time.time()</span><br><span class="line">    time_dif = end_time - start_time</span><br><span class="line">    <span class="keyword">return</span> timedelta(seconds=int(round(time_dif)))</span><br></pre></td></tr></table></figure></p><hr><h3 id="2️⃣parser使用"><a href="#2️⃣parser使用" class="headerlink" title="2️⃣parser使用"></a>2️⃣parser使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--save_dir'</span>, help=<span class="string">'Location of checkpoint files'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--vocab_file'</span>, help=<span class="string">'Vocabulary file'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--train_prefix'</span>, help=<span class="string">'Prefix for train files'</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line">    </span><br><span class="line">main(args)   <span class="comment">#使用</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识1</title>
      <link href="/2018/07/29/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%861/"/>
      <url>/2018/07/29/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%861/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[Python]<br>assert用法：</p><p><code>assert expression</code><br>等价于<br><code>if not expression: raise AssertionError</code></p><hr><p>2️⃣[Pytorch]<br>Pytorch view：<br>创建一个新的tensor，但他们的<strong>data是共享的</strong>。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-29-15328360404485.jpg" width="50%" height="50%"></p><hr><p>3️⃣[Pytorch]<br>在Pytorch中，embedding的index是不能requires_grad=True的，否则会出错。<br><a href="https://github.com/pytorch/pytorch/issues/7021" target="_blank" rel="noopener">https://github.com/pytorch/pytorch/issues/7021</a></p><p>之前看过一份代码，设置volatile=false但没有出错，是因为在Pytorch0.4之后volatile已经被弃用了，因此volatile=false不起作用，而默认requires_grad=false</p><hr><p>4️⃣[Pytorch]<br>在Pytorch中，<code>nn.Linear(self.hidden_dim,self.vocab_size)</code>的维度是vocab_size<em>hidden_dim，之前居然没有注意到这个问题。<br>因为nn.Linear的<em>*第一个参数表示输入维度，第二个参数表示输出维度</em></em></p><hr><p>5️⃣[Pytorch]<br>Pytorch中，使用view一般来说必须要用 .contiguous()。也即：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch.view(batch_size, <span class="number">-1</span>).t().contiguous()</span><br></pre></td></tr></table></figure><p>contiguous()的官方解释：<br><a href="https://discuss.pytorch.org/t/runtimeerror-input-is-not-contiguous/930" target="_blank" rel="noopener">https://discuss.pytorch.org/t/runtimeerror-input-is-not-contiguous/930</a></p><blockquote><p>It means that your tensor is not a single block of memory, but a block with holes. view can be only used with contiguous tensors, so if you need to use it here, just call .contiguous() before.</p></blockquote><p>也就是说，contiguous会将数据存到一个连续的空间内（block）。</p><hr><p>6️⃣[Pytorch]<br>调用Cross_entropy时，Pytorch会帮助你加log和softmax。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-29-15328370301774.jpg" alt=""></p><hr><p>7️⃣[Paper]<br><a href="https://arxiv.org/abs/1807.02291" target="_blank" rel="noopener">Sliced_RNN</a></p><p>将RNN分块以提高并行性，甚至每层的RNN都可以不一样，达到抽取不同程度的抽象语义信息的目的。实验证明，在不同任务上都有一定的提升，但速度的提升很大。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-29-15328372609264.jpg" width="50%" height="50%"></p><hr><p>8️⃣[Tf-idf]<br>计算词语对于句子的重要程度</p><p><a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/Tf-idf</a></p><p>tf是词频，idf是逆向文件频率。也即如果词在该句出现的次数越多，在所有文本的出现次数越少，则词对于句子的重要程度越高。</p><hr><p>9️⃣[Numpy]<br>在Numpy中，一个列表虽然是横着表示的，但它是列向量。我之前居然没有注意到这个问题。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-29-15328375536418.jpg" width="50%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Paper </tag>
            
            <tag> Tf-idf </tag>
            
            <tag> Numpy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Mac配置复旦有线网</title>
      <link href="/2018/07/29/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Mac%E9%85%8D%E7%BD%AE%E5%A4%8D%E6%97%A6%E6%9C%89%E7%BA%BF%E7%BD%91/"/>
      <url>/2018/07/29/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Mac%E9%85%8D%E7%BD%AE%E5%A4%8D%E6%97%A6%E6%9C%89%E7%BA%BF%E7%BD%91/</url>
      
        <content type="html"><![CDATA[<h3 id="配置ip、子网掩码、DNS、路由器"><a href="#配置ip、子网掩码、DNS、路由器" class="headerlink" title="配置ip、子网掩码、DNS、路由器"></a>配置ip、子网掩码、DNS、路由器</h3><p>有线似乎不支持DHCP，因此只好自己设置。<br>首先连接上有线，将配置iPv4选为手动。问实验室的学长具体的ip地址、子网掩码、路由器、DNS服务器。其中ip地址最后三位要自己设定，只要不和其他人冲突就好。</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-29-15328333841584.jpg" width="50%" height="50%"></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-29-15328335236981.jpg" width="50%" height="50%"></p><h3 id="手动认证"><a href="#手动认证" class="headerlink" title="手动认证"></a>手动认证</h3><p>到认证平台，下载Mac客户端，其实就是一个.sh文件：<br><a href="http://10.108.255.249/srun_portal_pc.php?ac_id=1&amp;&amp;phone=1" target="_blank" rel="noopener">http://10.108.255.249/srun_portal_pc.php?ac_id=1&amp;&amp;phone=1</a></p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-29-15328336395029.jpg" width="30%" height="30%"></p><p>然后，打开文件配置用户名密码，注意到等号后面要有双引号：</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-29-15328337135244.jpg" width="50%" height="50%"></p><p>保存并放入终端运行，接下来就可以使用有线网了。</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>似乎，每次重新连接都要这样配置，我没有试过不清楚；<br>有线网好像也没有比无线网快多少，但应该会稳定一些。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 网络 </tag>
            
            <tag> 配置 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ssh快速登录配置</title>
      <link href="/2018/07/29/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/ssh%E5%BF%AB%E9%80%9F%E7%99%BB%E5%BD%95%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/07/29/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/ssh%E5%BF%AB%E9%80%9F%E7%99%BB%E5%BD%95%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>分配了服务器之后，每次要ssh进入都很麻烦：<code>ssh user_name@ip_address</code> 然后还要输入密码。</p><p>特别是如果分配了多个服务器，那有时候还容易忘记ip地址。因此如果能够一条命令就进入服务器能够减少麻烦。<br>主要有三点：</p><ol><li>创建rsa key</li><li>上传public key到服务器</li><li>设置alias</li></ol><h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><h2 id="创建rsa-key"><a href="#创建rsa-key" class="headerlink" title="创建rsa key"></a>创建rsa key</h2><p>在终端输入命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p>当然如果以前有创建过的可以不用。</p><p>结果：</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-29-Xnip2018-07-29_10-43-15.jpg" width="50%" height="50%"></p><h2 id="上传public-key到服务器"><a href="#上传public-key到服务器" class="headerlink" title="上传public key到服务器"></a>上传public key到服务器</h2><p>使用命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub user@127.0.0.1</span><br></pre></td></tr></table></figure></p><p>输入密码即可</p><p>结果：</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-29-15328324683354.jpg" width="60%" height="60%"></p><h2 id="设置alias"><a href="#设置alias" class="headerlink" title="设置alias"></a>设置alias</h2><p>完成以上步骤就可以不输入密码登录，但还是需要输入ip地址和用户名，为了更简化操作，给命令起个别名。需要配置 .bash_profile文件。<br>输入命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure><p>在文件后面添加以下文字：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># alias </span><br><span class="line">alias ssh×××=&quot;ssh user_name@ip_address&quot;</span><br><span class="line">alias ssh×××=&quot;ssh user_name@ip_address&quot;</span><br></pre></td></tr></table></figure><p>其中 ×××是你自己起的名字，可以是服务器的名字，user_name和ip_address是自己服务器的用户名和地址。保存更改退出。</p><p>然后还要使其生效:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><p>这样，输入别名，就可以直接登录了：</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-29-15328329815456.jpg" width="50%" height="50%"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.jianshu.com/p/66d658c7cb9e" target="_blank" rel="noopener">https://www.jianshu.com/p/66d658c7cb9e</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 配置 </tag>
            
            <tag> ssh </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于困惑度</title>
      <link href="/2018/07/29/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E5%85%B3%E4%BA%8E%E5%9B%B0%E6%83%91%E5%BA%A6/"/>
      <url>/2018/07/29/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%E7%82%B9/%E5%85%B3%E4%BA%8E%E5%9B%B0%E6%83%91%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<p>前几天在写新手任务<a href="https://github.com/FudanNLP/nlp-beginner" target="_blank" rel="noopener">task3</a>的时候，参考了Pytorch官方example的word language model，官方example在训练过程中计算困惑度是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">math.exp(cur_loss)</span><br></pre></td></tr></table></figure><p>其中，cur_loss表示交叉熵的loss，即 $-P(\hat{x})logP(x)$，$\hat{x}$表示ground truth。</p><p>然而，在查阅了困惑度相关资料后，我发现，困惑度的定义是这样的：</p><script type="math/tex; mode=display">\begin{aligned}PP(S)= &{P(w_{1}w_{2}...w_{N})}^{-\frac{1}{N}} \\= &\sqrt[N]{\frac{1}{p(w_1 w_2 ... w_N)}} \\= & \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} }\end{aligned}</script><p>这是另一种形式:</p><script type="math/tex; mode=display">\begin{aligned}Perplexity (W)=& 2^{H(W)} \\= & {P(w_{1}w_{2}...w_{N})}^{-\frac{1}{N}} \\= & \sqrt[N]{\frac{1}{p(w_1 w_2 ... w_N)}} \\= & \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} }\end{aligned}</script><p>可以看到，二者本质是一样的。</p><p><strong>那么，为什么在代码中以e为底去计算困惑度，而不是2呢?</strong></p><p>实际上，是因为在上述公式中，log是以2为底的，但在Pytorch中，log默认是以e为底的。因此在代码中，需要用e作为指数的底来还原成困惑度的原本形式： </p><script type="math/tex; mode=display">\begin{aligned}\sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} }\end{aligned}</script><p>最后这是perplexity的数学推导：<br><a href="https://www.zhihu.com/question/58482430" target="_blank" rel="noopener">https://www.zhihu.com/question/58482430</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 困惑度 </tag>
            
            <tag> perplexity </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词2</title>
      <link href="/2018/07/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D2/"/>
      <url>/2018/07/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D2/</url>
      
        <content type="html"><![CDATA[<p>本周的诗词有两篇是已经背过的，权当是复习了一遍。</p><hr><p>1️⃣</p><h3 id="下终南山过斛斯山人宿置酒"><a href="#下终南山过斛斯山人宿置酒" class="headerlink" title="下终南山过斛斯山人宿置酒"></a>下终南山过斛斯山人宿置酒</h3><p>[唐] 李白<br>暮从碧山下，山月随人归。<br>却顾所来径，苍苍横翠微。<br>相携及田家，童稚开荆扉。<br>绿竹入幽径，青萝拂行衣。<br>欢言得所憩，美酒聊共挥。<br>长歌吟松风，曲尽河星稀。<br>我醉君复乐，陶然共忘机。</p><p><a href="http://m.xichuangzhu.com/work/57b900307db2a20054269a2a" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b900307db2a20054269a2a</a></p><hr><p>2️⃣</p><h3 id="逢入京使"><a href="#逢入京使" class="headerlink" title="逢入京使"></a>逢入京使</h3><p>[唐] 岑参<br>故园东望路漫漫，双袖龙钟泪不乾。<br><strong>马上相逢无纸笔，凭君传语报平安。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b92218df0eea006335f923" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b92218df0eea006335f923</a></p><hr><p>3️⃣</p><h3 id="念奴娇·赤壁怀古"><a href="#念奴娇·赤壁怀古" class="headerlink" title="念奴娇·赤壁怀古"></a>念奴娇·赤壁怀古</h3><p>[宋] 苏轼<br>大江东去，浪淘尽、千古风流人物。故垒西边，人道是、三国周郎赤壁。乱石穿空，惊涛拍岸，卷起千堆雪。江山如画，一时多少豪杰。<br>遥想公瑾当年，小乔初嫁了，雄姿英发。羽扇纶巾，谈笑间，樯橹灰飞烟灭。故国神游，多情应笑我，早生华发。<strong>人生如梦，一尊还酹江月。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b8bda2df0eea006333ecd2" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8bda2df0eea006333ecd2</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录1</title>
      <link href="/2018/07/23/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB1/"/>
      <url>/2018/07/23/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB1/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-get-batch"><a href="#1️⃣-get-batch" class="headerlink" title="1️⃣ get_batch"></a>1️⃣ get_batch</h3><p>注意到shuffle的标准做法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">(self,data,batch_size=<span class="number">32</span>,is_shuffle)</span>:</span></span><br><span class="line">  N=len(data)  <span class="comment">#获得数据的长度</span></span><br><span class="line">  <span class="keyword">if</span> is_shuffle <span class="keyword">is</span> <span class="keyword">True</span>:</span><br><span class="line">    r=random.Random()</span><br><span class="line">    r.seed()</span><br><span class="line">    r.shuffle(data) <span class="comment">#如果is_shuffle为真则打乱</span></span><br><span class="line">  <span class="comment">#开始获得batch，使用[ for in ]</span></span><br><span class="line">  batch=[data[k:k+batch_size] <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>,N,batch_size)]</span><br><span class="line">  <span class="keyword">if</span> N%batch_size!=<span class="number">0</span>:  <span class="comment">#处理不整除问题，如果有显式要求丢掉则不需要处理，这里默认处理</span></span><br><span class="line">    remainder=N-N%batch_size  <span class="comment">#剩下的部分</span></span><br><span class="line">    batch.append(data[temp:N])</span><br><span class="line">  <span class="keyword">return</span> batch</span><br></pre></td></tr></table></figure><hr><h3 id="2️⃣使用gensim将GloVe读入"><a href="#2️⃣使用gensim将GloVe读入" class="headerlink" title="2️⃣使用gensim将GloVe读入"></a>2️⃣使用gensim将GloVe读入</h3><p>实际上这份代码有点问题，在使用过程中，发现glove文件需要放在gensim的文件夹下才能被读到(7.20 updated,应该使用绝对地址)，并不好。</p><p>教程地址：<a href="https://radimrehurek.com/gensim/scripts/glove2word2vec.html" target="_blank" rel="noopener">gensim: scripts.glove2word2vec – Convert glove format to word2vec</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1. 使用gensim读入word2vec</span></span><br><span class="line"></span><br><span class="line">model = gensim.models.KeyedVectors.load_word2vec_format(</span><br><span class="line">        fname=<span class="string">'GoogleNews-vectors-negative300-SLIM.bin'</span>, binary=<span class="keyword">True</span>)</span><br><span class="line">words = model.vocab  <span class="comment">#获得词表</span></span><br><span class="line">vector= model[word]  <span class="comment">#word是words里面的元素</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2. 使用gensim读入glove</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line"><span class="keyword">from</span> gensim.test.utils <span class="keyword">import</span> datapath, get_tmpfile</span><br><span class="line"><span class="keyword">from</span> gensim.scripts.glove2word2vec <span class="keyword">import</span> glove2word2vec</span><br><span class="line">glove_file=datapath(<span class="string">'glove.txt'</span>)  <span class="comment">#最好使用绝对地址</span></span><br><span class="line">tmp_file=get_tmpfile(<span class="string">'word2vec.txt'</span>)</span><br><span class="line">glove2word2vec(glove_file,tmp_file)</span><br><span class="line">model=KeyedVectors.load_word2vec_format(tmp_file)</span><br><span class="line"><span class="comment">#接下来使用的方法是一样的</span></span><br></pre></td></tr></table></figure></p><hr><h3 id="3️⃣data-split方法"><a href="#3️⃣data-split方法" class="headerlink" title="3️⃣data_split方法"></a>3️⃣data_split方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_split</span><span class="params">(seed=<span class="number">1</span>, proportion=<span class="number">0.7</span>)</span>:</span> </span><br><span class="line">    data = list(iter_corpus())</span><br><span class="line">    ids = list(range(len(data)))</span><br><span class="line"></span><br><span class="line">    N = int(len(ids) * proportion)  <span class="comment"># number of training data</span></span><br><span class="line"></span><br><span class="line">    rng = random.Random(seed)</span><br><span class="line">    rng.shuffle(ids)</span><br><span class="line">    test_ids = set(ids[N:])</span><br><span class="line">    train_data = []</span><br><span class="line">    test_data = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> x[<span class="number">1</span>] <span class="keyword">in</span> test_ids:  <span class="comment"># x[1]: sentence id</span></span><br><span class="line">            test_data.append(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train_data.append(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_data, test_data</span><br></pre></td></tr></table></figure><hr><h3 id="4️⃣对string预处理"><a href="#4️⃣对string预处理" class="headerlink" title="4️⃣对string预处理"></a>4️⃣对string预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_str</span><span class="params">(string)</span>:</span></span><br><span class="line">    string = re.sub(<span class="string">r"[^A-Za-z0-9()!?\'\`]"</span>, <span class="string">""</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'s"</span>, <span class="string">" \'s"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'m"</span>, <span class="string">" \'m"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'ve"</span>, <span class="string">" \'ve"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"n\'t"</span>, <span class="string">" n\'t"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'re"</span>, <span class="string">" \'re"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'d"</span>, <span class="string">" \'d"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'ll"</span>, <span class="string">" \'ll"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r","</span>, <span class="string">" , "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"!"</span>, <span class="string">" ! "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\("</span>, <span class="string">" \( "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\)"</span>, <span class="string">" \) "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\?"</span>, <span class="string">" \? "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\s&#123;2,&#125;"</span>, <span class="string">" "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\@.*?[\s\n]"</span>, <span class="string">""</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"https*://.+[\s]"</span>, <span class="string">""</span>, string)</span><br><span class="line">    <span class="keyword">return</span> string.strip().lower()</span><br></pre></td></tr></table></figure><hr><h3 id="5️⃣collate-fn-batch）"><a href="#5️⃣collate-fn-batch）" class="headerlink" title="5️⃣collate_fn(batch）"></a>5️⃣collate_fn(batch）</h3><p>重写collate_fn组建mini-batch，在NLP中常用，句子的不等长性<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(batch)</span>:</span>  <span class="comment"># rewrite collate_fn to form a mini-batch</span></span><br><span class="line">    lengths = np.array([len(data[<span class="string">'sentence'</span>]) <span class="keyword">for</span> data <span class="keyword">in</span> batch])</span><br><span class="line">    sorted_index = np.argsort(-lengths)</span><br><span class="line">    lengths = lengths[sorted_index]  <span class="comment"># descend order</span></span><br><span class="line"></span><br><span class="line">    max_length = lengths[<span class="number">0</span>]</span><br><span class="line">    batch_size = len(batch)</span><br><span class="line">    sentence_tensor = torch.LongTensor(batch_size, int(max_length)).zero_()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, index <span class="keyword">in</span> enumerate(sorted_index):</span><br><span class="line">        sentence_tensor[i][:lengths[i]] = torch.LongTensor(batch[index][<span class="string">'sentence'</span>][:max_length])</span><br><span class="line"></span><br><span class="line">    sentiments = torch.autograd.Variable(torch.LongTensor([batch[i][<span class="string">'sentiment'</span>] <span class="keyword">for</span> i <span class="keyword">in</span> sorted_index]))</span><br><span class="line">    <span class="keyword">if</span> config.use_cuda:</span><br><span class="line">        packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(Variable(sentence_tensor.t()).cuda(), lengths)  <span class="comment">#remember to transpose</span></span><br><span class="line">        sentiments = sentiments.cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(Variable(sentence_tensor.t()),lengths)  <span class="comment"># remember to transpose</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'sentence'</span>: packed_sequences, <span class="string">'sentiment'</span>: sentiments&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">## 重写collate_fn(batch)以用于dataloader使用，使用方法如下：</span></span><br><span class="line"></span><br><span class="line">train_dataloader=DataLoader(train_data,batch_size=<span class="number">32</span>,shuffle=<span class="keyword">True</span>,collate_fn=collate_fn)</span><br><span class="line">​</span><br><span class="line"><span class="comment">## 其中，train_dataloader可循环遍历​​。</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p><hr><h3 id="6️⃣使用yield获得数据的generator"><a href="#6️⃣使用yield获得数据的generator" class="headerlink" title="6️⃣使用yield获得数据的generator"></a>6️⃣使用yield获得数据的generator</h3><p>yield的用法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span><span class="params">(txt_file)</span>:</span>     <span class="comment"># return generator</span></span><br><span class="line">    <span class="keyword">with</span> open(txt_file,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="keyword">if</span> len(line.strip())==<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            sentence=list(line.strip())+[<span class="string">'&lt;eos&gt;'</span>]</span><br><span class="line">            <span class="keyword">yield</span> sentence</span><br><span class="line">            </span><br><span class="line"><span class="comment">#在使用的时候：</span></span><br><span class="line">dataset=get_dataset(txt_file)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> dataset:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果需要还可以改成list形式</span></span><br><span class="line">dataset=list(get_dataset(txt_file))</span><br></pre></td></tr></table></figure></p><hr><h3 id="7️⃣动态创建RNN实例"><a href="#7️⃣动态创建RNN实例" class="headerlink" title="7️⃣动态创建RNN实例"></a>7️⃣动态创建RNN实例</h3><p>根据rnn_type动态创建对象实例，使用了getattr<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rnn in ['GRU','LSTM','RNN']</span></span><br><span class="line"></span><br><span class="line">self.rnn = getattr(nn, self.rnn_type)(self.embedding_dim, self.hidden_dim, self.num_layers, dropout=self.dropout)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词1</title>
      <link href="/2018/07/23/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D1/"/>
      <url>/2018/07/23/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D1/</url>
      
        <content type="html"><![CDATA[<p>本周背了四篇。</p><hr><p>1️⃣</p><h3 id="临江仙·夜归临皋"><a href="#临江仙·夜归临皋" class="headerlink" title="临江仙·夜归临皋"></a>临江仙·夜归临皋</h3><p>[宋] 苏轼<br>夜饮东坡醒复醉，归来彷彿三更。家童鼻息已雷鸣，敲门都不应，倚杖听江声。<br><strong>长恨此身非我有，何时忘却营营？</strong>夜阑风静縠纹平，小舟从此逝，江海寄馀生。</p><p>縠（hú）纹<br>皋（gao）<br><a href="http://m.xichuangzhu.com/work/57ae79400a2b580063150e39" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57ae79400a2b580063150e39</a></p><hr><p>2️⃣</p><h3 id="蝶恋花·阅尽天涯离别苦"><a href="#蝶恋花·阅尽天涯离别苦" class="headerlink" title="蝶恋花·阅尽天涯离别苦"></a>蝶恋花·阅尽天涯离别苦</h3><p>[清] 王国维<br>阅尽天涯离别苦。不道归来，零落花如许。花底相看无一语，绿窗春与天俱莫。<br>待把相思灯下诉。一缕新欢，旧恨千千缕。<strong>最是人间留不住，朱颜辞镜花辞树。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b8ef70128fe10054c91d17" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8ef70128fe10054c91d17</a></p><hr><p>3️⃣</p><h3 id="送友人"><a href="#送友人" class="headerlink" title="送友人"></a>送友人</h3><p>[唐] 李白<br>青山横北郭，白水绕东城。<br>此地一为别，孤蓬万里征。<br><strong>浮云游子意，落日故人情。</strong><br>挥手自兹去，萧萧班马鸣。</p><p><a href="http://m.xichuangzhu.com/work/57b8facfd342d3005ac6ffb4" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8facfd342d3005ac6ffb4</a></p><hr><p>4️⃣</p><h3 id="黄鹤楼送孟浩然之广陵"><a href="#黄鹤楼送孟浩然之广陵" class="headerlink" title="黄鹤楼送孟浩然之广陵"></a>黄鹤楼送孟浩然之广陵</h3><p>[唐] 李白<br>故人西辞黄鹤楼，烟花三月下扬州。<br>孤帆远影碧空尽，唯见长江天际流。</p><p><a href="http://m.xichuangzhu.com/work/57b8f306128fe10054c92fb8" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8f306128fe10054c92fb8</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>安装conda错误</title>
      <link href="/2018/07/23/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/%E5%AE%89%E8%A3%85conda%E9%94%99%E8%AF%AF/"/>
      <url>/2018/07/23/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/%E5%AE%89%E8%A3%85conda%E9%94%99%E8%AF%AF/</url>
      
        <content type="html"><![CDATA[<p>在服务器上安装conda的时候，一开始使用了pip安装<br><code>pip install conda</code><br>在安装好conda之后想要使用conda命令，出现：</p><p>ERROR: The install method you used for conda—probably either <code>pip install conda</code> or <code>easy_install conda</code>—is not compatible with using conda as an application. If your intention is to install conda as a standalone application, currently supported install methods include the Anaconda installer and the miniconda installer. You can download the miniconda installer from <a href="https://conda.io/miniconda.html" target="_blank" rel="noopener">https://conda.io/miniconda.html</a>.</p><p><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-23-15323331261104.jpg" alt=""></p><p>然后到官网下载.sh文件并bash安装，仍然没有解决该问题；接着尝试pip uninstall conda，出现<br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-23-15323337042406.jpg" alt=""></p><p>最后在查阅了网上之后，使用 <code>which conda</code>找到conda的地址，并删除<code>rm ×××</code><br><img src="http://pcaqhp90s.bkt.clouddn.com/2018-07-23-15323337894186.jpg" alt=""></p><p>最后重新bash安装即可。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 杂七杂八 </tag>
            
            <tag> conda </tag>
            
            <tag> 遇到的问题 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
