<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†12]]></title>
    <url>%2F2018%2F11%2F19%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8612%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Transformer]å¯¹Transformeræ–°ç†è§£ï¼š å¯ä»¥å°†Transformerç†è§£æˆä¸€å¼ å…¨è¿æ¥å›¾ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹ä¸å…¶ä»–èŠ‚ç‚¹çš„å…³ç³»é€šè¿‡attentionæƒé‡è¡¨ç°ã€‚å›¾å…³ç³»æ˜¯åºåˆ—å…³ç³»æˆ–è€…æ ‘å…³ç³»çš„ä¸€èˆ¬åŒ–ã€‚ ä¸ºä»€ä¹ˆè¦æœ‰multi-headï¼Ÿä¸ä»…ä»…æ˜¯è®ºæ–‡çš„è§£é‡Šï¼Œæˆ–è®¸è¿˜å¯ä»¥ç†è§£æˆï¼Œå¯¹ä¸€ä¸ªå‘é‡çš„ä¸åŒéƒ¨åˆ†ï¼ˆå¦‚ç¬¬1ç»´åˆ°20ç»´ï¼Œç¬¬21ç»´åˆ°40ç»´ç­‰ï¼‰æ–½ä»¥ä¸åŒçš„attentionæƒé‡ï¼Œå¦‚æœä¸ä½¿ç”¨multi-headï¼Œé‚£ä¹ˆå¯¹äºä¸€ä¸ªqueryï¼Œå°±åªä¼šæœ‰ä¸€ä¸ªæƒé‡ï¼Œè€Œä¸åŒçš„ç»´åº¦æœ‰ä¸åŒçš„é‡è¦æ€§ã€‚ 2ï¸âƒ£[attention&amp;capsule]attentionæ˜¯æ”¶ä¿¡æ¯ï¼Œqueryä»valueæŒ‰æƒé‡è·å–ä¿¡æ¯ï¼Œå…¶ä¸­æ‰€æœ‰valueçš„æƒé‡å’Œæ˜¯1ã€‚capsuleæ˜¯å‘ä¿¡æ¯ï¼Œå¯¹äº$l-1$å±‚çš„ä¸€ä¸ªcapsuleæ¥è¯´ï¼Œåœ¨ä¼ å…¥åˆ°$l$å±‚çš„kä¸ªcapsuleçš„ä¿¡æ¯ï¼Œå…¶æƒé‡å’Œä¸º1ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡6]]></title>
    <url>%2F2018%2F11%2F19%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%876%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[A STRUCTURED SELF ATTENTIVE SENTENCE EMBEDDING]ä»‹ç»äº†ä¸€ç§ç”Ÿæˆsentence embeddingçš„æ–¹æ³•ã€‚ä¸å…¶ä»–sentence embeddingä¸åŒçš„åœ°æ–¹åœ¨äºï¼Œç”Ÿæˆçš„æ˜¯ä¸€ä¸ªçŸ©é˜µè€Œä¸æ˜¯ä¸€ä¸ªå‘é‡ã€‚é€šè¿‡çŸ©é˜µçš„å½¢å¼ï¼Œèƒ½å¤Ÿå…³æ³¨ä¸åŒéƒ¨åˆ†çš„è¯­ä¹‰è¡¨ç¤ºï¼Œç±»ä¼¼äºTransformerçš„multi-headã€‚ Contribution: å°†sentence embeddingæ‰©å±•ä¸ºçŸ©é˜µå½¢å¼ï¼Œèƒ½å¤Ÿè·å¾—æ›´å¤šçš„ä¿¡æ¯ã€‚ å¼•å…¥æ­£åˆ™åŒ–ï¼Œä½¿å¾—sentence matrixå…·æœ‰æ›´ä¸°å¯Œçš„å¤šæ ·æ€§ã€‚ æ–¹æ³•åŒå‘LSTM+self-attentionã€‚ åŒå‘çš„LSTMè·å¾—ä¸Šä¸‹æ–‡çš„è¡¨ç¤ºï¼š å› æ­¤å¯ä»¥è·å¾—attentionæƒé‡å‘é‡ï¼š å…¶ä¸­$H:n\times2u,W_{s1}:d\times2u ,w_{s2}:d_a$ ï¼Œ$d_a$æ˜¯è¶…å‚ã€‚ ç°å°†å‘é‡$w_{s2}$æ‰©å±•ä¸ºçŸ©é˜µï¼Œäº¦å³æœ‰Multi-hop attentionï¼š $W_{s2}$ç»´åº¦ä¸º$r\times d_a$ï¼Œ$r$ä»£è¡¨äº†headçš„ä¸ªæ•°ã€‚ å› æ­¤æœ€ç»ˆçš„sentence embeddingçŸ©é˜µä¸ºï¼š æ­£åˆ™åŒ–ä¸ºäº†è®©Aå°½å¯èƒ½æœ‰å¤šæ ·æ€§ï¼ˆå› ä¸ºå¦‚æœéƒ½æ˜¯ç›¸ä¼¼çš„ï¼Œé‚£ä¹ˆåˆ™ä¼šæœ‰å†—ä½™æ€§ï¼‰ï¼Œå¼•å…¥å¦‚ä¸‹çš„æ­£åˆ™åŒ–ï¼š åŸå› ï¼šå¯¹äºä¸åŒçš„head $a^i$ä¸$a^j$ï¼Œ$A A^T$æœ‰ï¼š å¦‚æœ$a^i$ä¸$a^j$å¾ˆç›¸ä¼¼é‚£ä¹ˆå°±ä¼šæ¥è¿‘äº1ï¼Œå¦‚æœéå¸¸ä¸ç›¸ä¼¼(no overlay)åˆ™ä¼šæ¥è¿‘äº0ã€‚å› æ­¤æ•´ä¸ªå¼å­å°±æ˜¯:å¸Œæœ›å¯¹è§’çº¿éƒ¨åˆ†æ¥è¿‘äº0ï¼ˆå› ä¸ºå‡äº†å•ä½é˜µï¼‰ï¼Œè¿™å°±ç›¸å½“äºå°½å¯èƒ½focuså°éƒ¨åˆ†çš„è¯ï¼›åŒæ—¶å…¶ä»–éƒ¨åˆ†å°½å¯èƒ½æ¥è¿‘äº0ï¼Œä¹Ÿå³ä¸åŒçš„headä¹‹é—´æ²¡æœ‰overlapã€‚ å¦‚ä½•ä½¿ç”¨æ–‡ç« æåˆ°ï¼Œåœ¨åšåˆ†ç±»çš„æ—¶å€™å¯ä»¥ç›´æ¥å°†çŸ©é˜µMå±•å¼€ï¼Œè¿‡å…¨è¿æ¥å±‚å³å¯ã€‚ 2ï¸âƒ£[Attention-over-Attention Neural Networks for Reading Comprehension]åœ¨å®Œå½¢å¡«ç©ºä»»åŠ¡(Cloze-style Reading Comprehension)ä¸Šæå‡ºä¸€ç§æ–°çš„attentionï¼Œå³nested-attentionã€‚ ä»»åŠ¡æè¿°ä¸‰å…ƒç»„$D,Q,A$ï¼Œdocumentï¼Œquestionï¼Œanswerã€‚å…¶ä¸­answerä¸€èˆ¬æ˜¯documentçš„ä¸€ä¸ªè¯ã€‚ æ–¹æ³•æœ¬æ–‡æå‡ºçš„attentionæœºåˆ¶ï¼Œæ˜¯é€šè¿‡ä¸€ä¸ªæ–°çš„attentionå»æŒ‡ç¤ºå¦ä¸€ä¸ªattentionçš„é‡è¦ç¨‹åº¦ã€‚ é¦–å…ˆé€šè¿‡ä¸€å±‚å…±äº«çš„embeddingå±‚ï¼Œå°†documentå’Œqueryéƒ½encodeæˆword embeddingï¼Œç„¶åé€šè¿‡åŒå‘çš„GRUï¼Œå°†éšå±‚æ‹¼æ¥èµ·æ¥æˆä¸ºæ–°çš„è¡¨ç¤ºã€‚ æ¥ç€è·å¾—pair-wise matching matrixï¼š å…¶ä¸­$h$ä»£è¡¨ä¸Šè¿°æåˆ°çš„æ‹¼æ¥èµ·æ¥çš„è¡¨ç¤ºï¼Œ$M(i,j)$ä»£è¡¨äº†documentçš„è¯$i$å’Œquestionçš„è¯$j$ä¹‹é—´çš„åŒ¹é…ç¨‹åº¦ã€‚ æ¥ç€å¯¹columnåšsoftmaxï¼šå…¶ä»£è¡¨çš„æ„ä¹‰å³query-to-document attentionï¼Œäº¦å³å¯¹äºä¸€ä¸ªqueryå†…çš„è¯ï¼Œdocumentçš„æ¯ä¸ªè¯ä¸å…¶åŒ¹é…çš„æƒé‡ã€‚ æ¥ä¸‹æ¥ï¼Œå¯¹rowè¿›è¡Œsoftmaxæ“ä½œï¼šä»£è¡¨çš„æ˜¯ç»™å®šä¸€ä¸ªdocumentçš„è¯ï¼Œqueryçš„å“ªä¸ªè¯æ›´ä¸ºé‡è¦ã€‚ æ¥ä¸‹æ¥æˆ‘ä»¬å°†Î²å¹³å‡èµ·æ¥ï¼Œè·å¾—ä¸€ä¸ªå‘é‡ï¼šè¿™ä¸ªå‘é‡ä»æœ‰attentionçš„æ€§è´¨ï¼Œå³æ‰€æœ‰å…ƒç´ åŠ å’Œä¸º1ã€‚ä»£è¡¨çš„æ˜¯ä»å¹³å‡æ¥çœ‹ï¼Œqueryè¯çš„é‡è¦æ€§ã€‚ æœ€åï¼Œæˆ‘ä»¬å¯¹Î±å’ŒÎ²åšç‚¹ç§¯ä»¥è·å¾—attended document-level attentionï¼š å…¶ä¸­$s$çš„ç»´åº¦æ˜¯$D\times 1$ã€‚sä»£è¡¨çš„æ„ä¹‰å³â€œa weighted sum of each individual document-level attention Î±(t) when looking at query word at time tâ€ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹Î±è¿›è¡ŒåŠ æƒï¼Œä»£è¡¨query wordçš„å¹³å‡é‡è¦ç¨‹åº¦ã€‚ æœ€ç»ˆåœ¨åšå®Œå‹å¡«ç©ºçš„é¢„æµ‹æ—¶ï¼š ä¸ªäººè§‰å¾—è¿™ç§attention-over-attentionçš„æƒ³æ³•è¿˜æ˜¯æŒºæœ‰åˆ›æ–°çš„ã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>attention</tag>
        <tag>sentence embedding</tag>
        <tag>nested attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç½‘ç»œä¼˜åŒ–ä¸æ­£åˆ™åŒ–æ€»ç»“]]></title>
    <url>%2F2018%2F11%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[å¤§é‡å‚è€ƒè‡ªã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹ ä¼˜åŒ–ç®—æ³•å¯¹äºæ ‡å‡†çš„SGDï¼Œå¸¸è§çš„æ”¹è¿›ç®—æ³•ä»ä¸¤ä¸ªæ–¹é¢è¿›è¡Œï¼šå­¦ä¹ ç‡è¡°å‡&amp;æ¢¯åº¦æ–¹å‘ä¼˜åŒ–ã€‚è®°$g_t$ä¸ºtæ—¶åˆ»çš„å¯¼æ•°ï¼š å­¦ä¹ ç‡è¡°å‡AdaGradç®—æ³•é€šè¿‡è®¡ç®—å†æ¬¡çš„æ¢¯åº¦å¹³æ–¹ç´¯è®¡å€¼è¿›è¡Œå­¦ä¹ ç‡è¡°å‡ã€‚$G_t$æ˜¯ç´¯è®¡å€¼ï¼š æ›´æ–°å€¼åˆ™ä¸ºï¼š ç¼ºç‚¹ï¼šéšç€è¿­ä»£æ¬¡æ•°çš„å¢åŠ å­¦ä¹ ç‡é€’å‡ã€‚åœ¨ç»è¿‡ä¸€å®šæ¬¡æ•°çš„è¿­ä»£ä¾ç„¶æ²¡æœ‰æ‰¾åˆ°æœ€ä¼˜ç‚¹æ—¶ï¼Œç”±äºè¿™æ—¶çš„å­¦ä¹ ç‡å·²ç»éå¸¸å°ï¼Œå¾ˆéš¾å†ç»§ç»­æ‰¾åˆ°æœ€ä¼˜ç‚¹ã€‚ RMSpropç®—æ³•å¯¹AdaGradçš„æ”¹è¿›ï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äº$G_t$çš„è®¡ç®—ï¼Œå°†å†å²ä¿¡æ¯å’Œå½“å‰ä¿¡æ¯è¿›è¡Œçº¿æ€§åŠ æƒï¼Œä½¿å¾—å­¦ä¹ ç‡å¯ä»¥åŠ¨æ€æ”¹å˜è€Œä¸æ˜¯å•è°ƒé€’å‡ï¼š Î²ä¸ºè¡°å‡ç‡ï¼Œé€šå¸¸å–0.9ã€‚ä¹Ÿå³å†å²ä¿¡æ¯å ä¸»å¯¼ã€‚ AdaDeltaç®—æ³•åŒæ ·æ˜¯å¯¹AdaGradçš„æ”¹è¿›ã€‚æ¯æ¬¡è®¡ç®—ï¼š ä¹Ÿå³å†å²æ›´æ–°å·®å’Œä¸Šä¸€æ—¶åˆ»çš„æ›´æ–°å·®çš„åŠ æƒï¼ˆRMSpropæ˜¯å†å²æ¢¯åº¦å’Œå½“å‰æ¢¯åº¦ï¼‰ã€‚ æœ€ç»ˆæ›´æ–°å·®å€¼ä¸ºï¼š å…¶ä¸­$G_t$è®¡ç®—æ–¹æ³•å’ŒRMSpropä¸€è‡´ã€‚ æ¢¯åº¦æ–¹å‘ä¼˜åŒ–åˆ©ç”¨å†å²çš„æ¢¯åº¦ï¼ˆæ–¹å‘ï¼‰è°ƒæ•´å½“å‰æ—¶åˆ»çš„æ¢¯åº¦ã€‚ åŠ¨é‡ï¼ˆMomentumï¼‰æ³•åŠ¨é‡æ³•ï¼ˆMomentum Methodï¼‰æ˜¯ç”¨ä¹‹å‰ç§¯ç´¯åŠ¨é‡æ¥æ›¿ä»£çœŸæ­£çš„æ¢¯åº¦ã€‚æ¯æ¬¡è¿­ä»£çš„æ¢¯åº¦å¯ä»¥çœ‹ä½œæ˜¯åŠ é€Ÿåº¦ã€‚ ä¹Ÿå³ä¸Šä¸€æ—¶åˆ»çš„æ›´æ–°å·®å€¼å’Œå½“å‰æ¢¯åº¦å…±åŒå†³å®šå½“å‰çš„æ›´æ–°å·®å€¼ã€‚$Ï$ä¸ºåŠ¨é‡å› å­ï¼Œé€šå¸¸ä¸º0.9ã€‚ä¹Ÿå³åŠ¨é‡å äº†ä¸»å¯¼ã€‚ å½“æŸä¸ªå‚æ•°åœ¨æœ€è¿‘ä¸€æ®µæ—¶é—´å†…çš„æ¢¯åº¦æ–¹å‘ä¸ä¸€è‡´æ—¶ï¼Œå…¶çœŸå®çš„å‚æ•°æ›´æ–°å¹…åº¦å˜å°ï¼›ç›¸åï¼Œå½“åœ¨æœ€è¿‘ä¸€æ®µæ—¶é—´å†…çš„æ¢¯åº¦æ–¹å‘éƒ½ä¸€è‡´æ—¶ï¼Œå…¶çœŸå®çš„å‚æ•°æ›´æ–°å¹…åº¦å˜å¤§ï¼Œèµ·åˆ°åŠ é€Ÿä½œç”¨ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œåœ¨è¿­ä»£åˆæœŸï¼Œæ¢¯åº¦æ–¹æ³•éƒ½æ¯”è¾ƒä¸€è‡´ï¼ŒåŠ¨é‡æ³•ä¼šèµ·åˆ°åŠ é€Ÿä½œç”¨ï¼Œå¯ä»¥æ›´å¿«åœ°åˆ°è¾¾æœ€ä¼˜ç‚¹ã€‚åœ¨è¿­ä»£åæœŸï¼Œæ¢¯åº¦æ–¹æ³•ä¼šå–å†³ä¸ä¸€è‡´ï¼Œåœ¨æ”¶æ•›å€¼é™„è¿‘éœ‡è¡ï¼ŒåŠ¨é‡æ³•ä¼šèµ·åˆ°å‡é€Ÿä½œç”¨ï¼Œå¢åŠ ç¨³å®šæ€§ã€‚ NesterovåŠ é€Ÿæ¢¯åº¦åŠ¨é‡æ³•çš„æ”¹è¿›ç‰ˆæœ¬ã€‚ å‰é¢æåˆ°çš„åŠ¨é‡æ³•ï¼Œæ˜¯ä¸Šä¸€æ­¥çš„æ›´æ–°æ–¹å‘$\Delta \theta_{t-1}$ä¸å½“å‰æ¢¯åº¦$-g_t$çš„åŠ å’Œã€‚å› æ­¤å¯ä»¥ç†è§£æˆï¼Œå…ˆæ ¹æ®$âˆ†Î¸_{tâˆ’1}$æ›´æ–°ä¸€æ¬¡å¾—åˆ°å‚æ•°Î¸ï¼Œå†ç”¨$g_t$è¿›è¡Œæ›´æ–°ã€‚äº¦å³ï¼šä¸Šå¼çš„ç¬¬äºŒæ­¥ä¸­ï¼Œ$g_t$æ˜¯åœ¨$\theta_{t-1}$ä¸Šçš„æ¢¯åº¦ã€‚æˆ‘ä»¬å°†è¯¥æ­¥æ”¹ä¸ºåœ¨$_theta{t}$çš„æ¢¯åº¦ã€‚å› æ­¤ï¼Œæœ‰ï¼š å’ŒåŠ¨é‡æ³•ç›¸æ¯”ï¼Œç›¸å½“äºæå‰èµ°äº†ä¸€æ­¥ã€‚ Adam&amp;NadamAdamä¸€æ–¹é¢è®¡ç®—æ¢¯åº¦å¹³æ–¹çš„åŠ æƒï¼ŒåŒæ—¶è¿˜è®¡ç®—æ¢¯åº¦çš„åŠ æƒï¼šé€šå¸¸$Î²_1=0.9$,$Î²_2=0.99$ä¹Ÿå³å†å²ä¿¡æ¯å äº†ä¸»å¯¼ã€‚ åœ¨åˆæœŸ$M_t$ä¸$G_t$ä¼šæ¯”çœŸå®å‡å€¼å’Œæ–¹å·®è¦å°ï¼ˆæƒ³è±¡$M_0=0,G_0=0$æ—¶ï¼‰ã€‚å› æ­¤å¯¹å…¶è¿›è¡Œä¿®æ­£ï¼Œå³ï¼šå› æ­¤æœ€ç»ˆæœ‰ï¼š åŒç†æœ‰Nadamã€‚ Adam = Momentum + RMSpropNadam = Nesterov + RMSprop æ¢¯åº¦æˆªæ–­ gradient clippingåˆ†ä¸ºæŒ‰å€¼æˆªæ–­ä¸æŒ‰æ¨¡æˆªæ–­ã€‚ å‚æ•°åˆå§‹åŒ–åˆå§‹å€¼é€‰å–å¾ˆå…³é”®ã€‚å‡è®¾å…¨éƒ¨åˆå§‹åŒ–ä¸º0ï¼Œåˆ™åç»­æ›´æ–°å¯¼è‡´æ‰€æœ‰çš„æ¿€æ´»å€¼ç›¸åŒï¼Œä¹Ÿå³å¯¹ç§°æƒé‡ç°è±¡ã€‚ åŸåˆ™ï¼šä¸èƒ½è¿‡å¤§ï¼Œå¦åˆ™æ¿€æ´»å€¼ä¼šå˜å¾—é¥±å’Œï¼Œå¦‚sigmoidï¼›ä¸èƒ½è¿‡å°ï¼Œå¦åˆ™ç»è¿‡å¤šå±‚ä¿¡å·ä¼šé€æ¸æ¶ˆå¤±ï¼Œå¹¶ä¸”å¯¼è‡´sigmoidä¸¢å¤±éçº¿æ€§çš„èƒ½åŠ›ï¼ˆåœ¨0é™„è¿‘åŸºæœ¬è¿‘ä¼¼çº¿æ€§ï¼‰ã€‚å¦‚æœä¸€ä¸ªç¥ç»å…ƒçš„è¾“å…¥è¿æ¥å¾ˆå¤šï¼Œå®ƒçš„æ¯ä¸ªè¾“å…¥è¿æ¥ä¸Šçš„æƒé‡å°±åº”è¯¥å°ä¸€äº›ï¼Œè¿™æ˜¯ä¸ºäº†é¿å…è¾“å‡ºè¿‡å¤§ã€‚ Gaussianåˆ†å¸ƒåˆå§‹åŒ–åŒæ—¶è€ƒè™‘è¾“å…¥è¾“å‡ºï¼Œå¯ä»¥æŒ‰$N(0,\sqrt{\frac{2}{n_{in} + n_{out}}})$é«˜æ–¯åˆ†å¸ƒæ¥åˆå§‹åŒ–ã€‚ å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–åœ¨$[-r,r]$åŒºé—´å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–ï¼Œå…¶ä¸­rå¯ä»¥æŒ‰ç…§ç¥ç»å…ƒæ•°é‡è‡ªé€‚åº”è°ƒæ•´ã€‚ Xavieråˆå§‹åŒ–æ–¹æ³•è‡ªåŠ¨è®¡ç®—è¶…å‚rã€‚rçš„å…¬å¼ä¸ºï¼šå…¶ä¸­$n^l$ä»£è¡¨ç¬¬$l$å±‚çš„ç¥ç»å…ƒä¸ªæ•°ã€‚ ä¸ºä»€ä¹ˆæ˜¯è¿™ä¸ªå¼å­ï¼ˆæ¨å¯¼è§å‚è€ƒèµ„æ–™ï¼‰ï¼šç»¼åˆè€ƒè™‘äº†â‘ è¾“å…¥è¾“å‡ºçš„æ–¹å·®è¦ä¸€è‡´ï¼›â‘¡åå‘ä¼ æ’­ä¸­è¯¯å·®ä¿¡å·çš„æ–¹å·®ä¸è¢«æ”¾å¤§æˆ–ç¼©å°ã€‚ å½’ä¸€åŒ–å°†æ•°æ®åˆ†å¸ƒå½’ä¸€åŒ–ï¼Œä½¿å¾—åˆ†å¸ƒä¿æŒç¨³å®šã€‚å‡è®¾æ•°æ®æœ‰å››ç»´(N,C,H,W)ã€‚Nä»£è¡¨batchï¼›Cä»£è¡¨channelï¼›H,Wä»£è¡¨heightå’Œwidthã€‚ Batch Normalizationæ²¿ç€é€šé“è¿›è¡Œå½’ä¸€åŒ–ï¼Œäº¦å³æ¯ä¸ªé€šé“éƒ½æœ‰è‡ªå·±çš„å‡å€¼å’Œæ–¹å·®ã€‚å…¶ä¸­ç¼©æ”¾å¹³ç§»å˜é‡æ˜¯å¯å­¦ä¹ çš„ã€‚ ç¼ºç‚¹ï¼šâ‘ å¯¹batch sizeæ•æ„Ÿï¼Œbatch sizeå¤ªå°åˆ™æ–¹å·®å‡å€¼ä¸è¶³ä»¥ä»£è¡¨æ•°æ®åˆ†å¸ƒâ‘¡å¯¹äºä¸ç­‰é•¿çš„è¾“å…¥å¦‚RNNæ¥è¯´ï¼Œæ¯ä¸€ä¸ªtimestepéƒ½éœ€è¦ä¿å­˜ä¸åŒçš„ç‰¹å¾ã€‚ Layer Normalizationå¯¹ä¸€ä¸ªè¾“å…¥è¿›è¡Œæ­£åˆ™åŒ–ï¼Œäº¦å³æ¯ä¸ªè¾“å…¥éƒ½æœ‰è‡ªå·±çš„æ–¹å·®ã€å‡å€¼ã€‚è¿™æ ·ä¸ä¾èµ–äºbatchå¤§å°å’Œè¾“å…¥sequenceçš„æ·±åº¦ã€‚ å¯¹RNNæ•ˆæœæ¯”è¾ƒæ˜æ˜¾ï¼Œä½†CNNä¸­ä¸å¦‚BN Instance Normalizationå¯¹HWè¿›è¡Œå½’ä¸€åŒ– Group Normalizationå°†channelåˆ†ä¸ºå¤šä¸ªgroupï¼Œæ¯ä¸ªgroupå†…åšå½’ä¸€åŒ– Referenceã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹https://blog.csdn.net/liuxiao214/article/details/81037416]]></content>
      <tags>
        <tag>æ·±åº¦å­¦ä¹ ğŸ¤–</tag>
        <tag>ä¼˜åŒ–ç®—æ³•</tag>
        <tag>å‚æ•°åˆå§‹åŒ–</tag>
        <tag>Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•10]]></title>
    <url>%2F2018%2F11%2F11%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9510%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[get_sinusoid_encoding_table]Transformerç»å¯¹ä½ç½®ã€‚ 12345678910111213141516def get_sinusoid_encoding_table(n_position, d_hid, padding_idx=None): def cal_angle(position, hid_idx): return position / np.power(10000, 2 * (hid_idx // 2) / d_hid) def get_posi_angle_vec(position): return [cal_angle(position, hid_j) for hid_j in range(d_hid)] sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)]) sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) sinusoid_table[:, 0::2] = np.cos(sinusoid_table[:, 0::2]) if padding_idx is not None: sinusoid_table[padding_idx] = 0. return torch.FloatTensor(sinusoid_table) # n_position,embed_dim]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†11]]></title>
    <url>%2F2018%2F11%2F11%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8611%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Optimizer]https://zhuanlan.zhihu.com/p/32262540https://zhuanlan.zhihu.com/p/32338983 Adamç­‰è‡ªé€‚åº”å­¦ä¹ ç‡ç®—æ³•å¯¹äºç¨€ç–æ•°æ®å…·æœ‰ä¼˜åŠ¿ï¼Œä¸”æ”¶æ•›é€Ÿåº¦å¾ˆå¿«ï¼›ä½†ç²¾è°ƒå‚æ•°çš„SGDï¼ˆ+Momentumï¼‰å¾€å¾€èƒ½å¤Ÿå–å¾—æ›´å¥½çš„æœ€ç»ˆç»“æœã€‚ å»ºè®®ï¼šå‰æœŸç”¨Adamï¼Œäº«å—Adamå¿«é€Ÿæ”¶æ•›çš„ä¼˜åŠ¿ï¼›åæœŸåˆ‡æ¢åˆ°SGDï¼Œæ…¢æ…¢å¯»æ‰¾æœ€ä¼˜è§£ã€‚ä»€ä¹ˆæ—¶å€™ä»Adamåˆ‡æ¢åˆ°SGDï¼Ÿå½“SGDçš„ç›¸åº”å­¦ä¹ ç‡çš„ç§»åŠ¨å¹³å‡å€¼åŸºæœ¬ä¸å˜çš„æ—¶å€™ã€‚ 2ï¸âƒ£[Pytorch]LongTensoré™¤ä»¥æ•´æ•°ï¼Œä¼šå¯¹é™¤æ•°è¿›è¡Œå–æ•´ï¼Œå†åšé™¤æ³•ã€‚ 3ï¸âƒ£[Pytorch]ä½¿ç”¨Pytorchçš„DataParallel 1234567891011121314device = torch.device('cuda:' + str( config.CUDA_VISIBLE_DEVICES[0]) if config.use_cuda else 'cpu') # æŒ‡å®šç¬¬ä¸€ä¸ªè®¾å¤‡model = ClassifyModel( vocab_size=len(vocab), max_seq_len=config.max_sent_len, embed_dim=config.embed_dim, n_layers=config.n_layers, n_head=config.n_head, d_k=config.d_k, d_v=config.d_v, d_model=config.d_model, d_inner=config.d_inner_hid, n_label=config.n_label, dropout=config.dropout).to(device)model = DataParallel(model, device_ids=config.CUDA_VISIBLE_DEVICES) # æ˜¾å¼å®šä¹‰device_ids æ³¨æ„åˆ°ï¼šdevice_idsçš„èµ·å§‹ç¼–å·è¦ä¸ä¹‹å‰å®šä¹‰çš„deviceä¸­çš„â€œcuda:0â€ç›¸ä¸€è‡´ï¼Œä¸ç„¶ä¼šæŠ¥é”™ã€‚ å¦‚æœä¸æ˜¾å¼åœ¨ä»£ç ä¸­çš„DataParallelæŒ‡å®šè®¾å¤‡ï¼Œé‚£ä¹ˆéœ€è¦åœ¨å‘½ä»¤è¡Œå†…æŒ‡å®šã€‚å¦‚æœæ˜¯åœ¨å‘½ä»¤è¡Œé‡Œé¢è¿è¡Œçš„ï¼Œä¸”deviceä¸æ˜¯ä»0å¼€å§‹ï¼Œåº”å½“æ˜¾å¼è®¾ç½®GPU_idï¼Œå¦åˆ™ä¼šå‡ºé”™â€˜AssertionError: Invalid device idâ€™ï¼Œæ­£ç¡®çš„å‘½ä»¤ï¼š 1CUDA_VISIBLE_DEVICES=4,5 python -u classify_main.py --gpu_id 0,1]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>Optimizer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³äºsparse gradient]]></title>
    <url>%2F2018%2F11%2F11%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8Esparse%20gradient%2F</url>
    <content type="text"><![CDATA[å‰å‡ å¤©åœ¨çœ‹AllenAIåœ¨EMNLPçš„pptæ—¶ï¼Œæœ‰ä¸€é¡µå†™é“ï¼š ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§æƒ…å†µï¼Ÿ Embeddingæ˜¯ä¸€ä¸ªå¾ˆå¤§çš„çŸ©é˜µï¼Œæ¯ä¸€æ¬¡å…¶å®éƒ½åªæœ‰ä¸€ä¸ªå°éƒ¨åˆ†è¿›è¡Œäº†æ›´æ–°ï¼Œå¯¹äºä¸€äº›è¯æ¥è¯´ï¼Œå‡ºç°çš„é¢‘ç‡ä¸é«˜ï¼Œæˆ–è€…è¯´ï¼Œå…¶å®å¤§éƒ¨åˆ†çš„è¯åœ¨ä¸€ä¸ªloop/epochä¸­ï¼Œè¢«æ›´æ–°çš„æ¬¡æ•°æ˜¯è¾ƒå°‘çš„ã€‚ä½†æ˜¯ï¼Œæ³¨æ„åˆ°ä¸€èˆ¬çš„optimizerç®—æ³•ï¼Œæ˜¯ä»¥matrixä¸ºå•ä½è¿›è¡Œæ›´æ–°çš„ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸€æ¬¡éƒ½æ˜¯$W^{t+1}=W^{t}-\eta \frac{\partial L}{\partial{W}}$ è€ŒAdamç®—æ³•ï¼š åŠ¨é‡å äº†ä¸»å¯¼ã€‚ä½†è¿™æ ·ï¼Œæ¯æ¬¡batchæ›´æ–°ï¼Œé‚£äº›æ²¡è¢«æ›´æ–°çš„è¯ï¼ˆä¹Ÿå³gradient=0ï¼‰çš„åŠ¨é‡ä»ç„¶ä¼šè¢«è¡°å‡ï¼Œæ‰€ä»¥è¿™æ ·å½“åˆ°è¿™ä¸ªè¯æ›´æ–°çš„æ—¶å€™ï¼Œä»–çš„åŠ¨é‡å·²ç»è¢«è¡°å‡å®Œäº†ï¼Œæ‰€ä»¥æ›´æ–°çš„gradientå°±å¾ˆå°ã€‚ è§£å†³æ–¹æ¡ˆï¼š â‘ åœ¨PyTorchä¸­ï¼ŒEmbeddingçš„APIï¼štorch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False, _weight=None) å…¶ä¸­sparse (bool, optional) â€“ if True, gradient w.r.t. weight matrix will be a sparse tensor. å°†sparseè®¾ä¸ºTrueå³å¯ã€‚ â‘¡é’ˆå¯¹sparseçŸ©é˜µï¼Œä½¿ç”¨ä¸åŒçš„optimizerï¼Œå¦‚torch.optim.SparseAdamï¼š Implements lazy version of Adam algorithm suitable for sparse tensors.In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.]]></content>
      <tags>
        <tag>sparse gradient</tag>
        <tag>ä»£ç å®è·µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡5]]></title>
    <url>%2F2018%2F11%2F10%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%875%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Neural Turing Machine]é€šè¿‡æ¨¡ä»¿å†¯è¯ºä¾æ›¼æœºï¼Œå¼•å…¥å¤–éƒ¨å†…å­˜(externel memory)ã€‚ å’Œæ™®é€šç¥ç»ç½‘ç»œä¸€æ ·ï¼Œä¸å¤–ç•Œäº¤äº’ï¼Œè·å¾—ä¸€ä¸ªè¾“å…¥ï¼Œäº§ç”Ÿä¸€ä¸ªè¾“å‡ºã€‚ä½†ä¸åŒçš„æ˜¯ï¼Œå†…éƒ¨è¿˜æœ‰ä¸€ä¸ªmemoryè¿›è¡Œè¯»å†™ã€‚å‡è®¾memoryæ˜¯ä¸€ä¸ªN Ã— Mçš„çŸ©é˜µï¼ŒNæ˜¯å†…å­˜çš„ä½ç½®æ•°é‡ã€‚ è¯»å†™memoryâ‘ è¯»å…¶ä¸­è¯»çš„æ—¶å€™å¯¹å„å†…å­˜ä½ç½®çº¿æ€§åŠ æƒã€‚wæ˜¯å½’ä¸€åŒ–æƒé‡ã€‚ â‘¡å†™$e_t$æ˜¯æ“¦é™¤å‘é‡ï¼ˆerase vectorï¼‰ $a_t$æ˜¯åŠ å’Œå‘é‡(add vector) å…·ä½“å¦‚ä½•è·å¾—æƒé‡å°±ä¸è¯´äº†ã€‚ Controller networkä¸­é—´çš„controller networkå¯ä»¥æ˜¯ä¸€ä¸ªæ™®é€šçš„feed forwardæˆ–è€…RNNã€‚ åœ¨å®é™…ä¸­NTMç”¨å¾—å¹¶ä¸å¤šã€‚ 2ï¸âƒ£[Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling]ELMoçš„ç²¾ç®€ç‰ˆï¼Œé€šè¿‡å³æ’å³ç”¨çš„æ–¹æ³•æ¥å‹ç¼©è¯­è¨€æ¨¡å‹ï¼Œå¯¹ç‰¹å®šä»»åŠ¡å‰ªæä¸åŒçš„å±‚ï¼Œä½¿å¾—èƒ½å¤Ÿå‡å°‘inferenceçš„æ—¶é—´ã€‚è¿™ç¯‡çš„ideaæŒºæœ‰åˆ›æ–°çš„ï¼Œä½†ä¼¼ä¹æœ‰äº›trivialçš„æ„Ÿè§‰ã€‚ RNN and Dense Connectivityæ¯ä¸€å±‚çš„è¾“å‡ºéƒ½ä¼šä¼ åˆ°æ‰€æœ‰å±‚ä½œä¸ºè¾“å…¥ï¼Œå› æ­¤å¯¹äºLå±‚çš„è¾“å…¥ï¼š è¿™æ ·æˆ‘ä»¬å°±èƒ½å¤Ÿéšæ„åœ°å»æ‰ä»»æ„ä¸­é—´å±‚äº†ã€‚åŒæ—¶ä¸€äº›è¯­è¨€ä¿¡æ¯ä¹Ÿåˆ†æ•£åˆ°å„ä¸ªå±‚ï¼Œå³ä½¿å»æ‰æŸäº›å±‚ä¹Ÿæ²¡æœ‰å…³ç³»ã€‚ åˆ™æœ€ç»ˆçš„outputä¸ºï¼š æœ€ç»ˆä½œprojectionåˆ°æ­£å¸¸ç»´åº¦ï¼ˆåœ¨æ¯å±‚éƒ½ä¼šè¿™ä¹ˆåšï¼Œå°†è¾“å…¥é™ç»´åˆ°æ­£å¸¸ç»´åº¦å†è¾“å…¥ï¼‰ï¼š å†åšä¸€ä¸ªsoftmaxï¼š ç”±äº $h^{â€»}$ ç”¨äºsoftmaxï¼Œæ‰€ä»¥å¯èƒ½å’Œtarget wordï¼Œä¹Ÿå³ä¸‹ä¸€ä¸ªè¯æ¯”è¾ƒç›¸ä¼¼ï¼Œå› æ­¤å¯èƒ½æ²¡æœ‰å¾ˆå¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ æ‰€ä»¥æœ€ç»ˆæˆ‘ä»¬ä½¿ç”¨$h_t$ï¼Œä»¥åŠåå‘çš„$h_t^r$ï¼Œå†è¿‡ä¸€å±‚çº¿æ€§å±‚è·å¾—æœ€ç»ˆçš„embeddingï¼ˆå’ŒELMoæœ‰äº›ä¸åŒï¼ŒELMoæ˜¯ç›´æ¥æ‹¼èµ·æ¥ï¼‰ï¼š Layer Selectionæˆ‘ä»¬åœ¨æ¯å±‚çš„outputéƒ½åŠ ä¸€ä¸ªæƒé‡ç³»æ•°ã€‚ æˆ‘ä»¬å¸Œæœ›åœ¨target taskä¸Šç”¨çš„æ—¶å€™ï¼Œéƒ¨åˆ†zèƒ½å¤Ÿå˜æˆ0ï¼Œè¾¾åˆ°layer selectionçš„æ•ˆæœï¼ŒåŠ å¿«inferenceçš„é€Ÿåº¦ã€‚ äº¦å³ï¼š ä¸€ç§ç†æƒ³çš„æ–¹æ³•æ˜¯L0æ­£åˆ™åŒ–ï¼š ä½†ç”±äºæ²¡åŠæ³•æ±‚å¯¼ï¼Œå› æ­¤ï¼Œé‡‡ç”¨L1æ­£åˆ™åŒ–ï¼šä½†ä½¿ç”¨L1æ­£åˆ™åŒ–æœ‰ä¸€å®šçš„é£é™©ï¼Œå› ä¸ºå¦‚æœè®©æ‰€æœ‰zéƒ½è¿œç¦»1ï¼Œé‚£ä¹ˆä¼šå½±å“performanceã€‚ å¼•å…¥æ–°çš„æ­£åˆ™åŒ–æ–¹æ³•$R_2 =\delta(|z|_0&gt;\lambda_1) |z|_1$äº¦å³ï¼Œåªæœ‰åœ¨éé›¶zçš„ä¸ªæ•°å¤§äºæŸä¸ªé˜ˆå€¼æ—¶ï¼Œæ‰èƒ½æœ‰æ­£åˆ™åŒ–æ•ˆæœï¼Œä¿è¯éé›¶çš„ä¸ªæ•°ã€‚â€™it can be â€œturned-offâ€ after achieving a satisfying sparsityâ€™. è¿›ä¸€æ­¥å¼•å…¥$R_3=\delta(|z|_0&gt;\lambda_1) |z|_1 + |z(1-z)|_1$å…¶ä¸­ç¬¬äºŒé¡¹ä¸ºäº†é¼“åŠ±zå‘0æˆ–1èµ°ã€‚ Layer-wise Dropoutéšæœºåˆ é™¤éƒ¨åˆ†layerï¼Œè¿™äº›layerçš„è¾“å‡ºä¸ä¼šä¼ å…¥ä¹‹åçš„å±‚ï¼Œä½†ä»ç„¶ä¼šå‚ä¸æœ€åçš„representationè®¡ç®—ã€‚ è¿™ç§dropoutä¼šè®©perplexityæ›´é«˜ï¼Œä½†å¯¹ç”Ÿæˆæ›´å¥½çš„representationæœ‰å¸®åŠ©ã€‚ 3ï¸âƒ£[Constituency Parsing with a Self-Attentive Encoder]å…¶ä¸­çš„positional encodingæˆ‘æ¯”è¾ƒæ„Ÿå…´è¶£ã€‚åŸç‰ˆçš„positional encodingæ˜¯ç›´æ¥å’Œembeddingç›¸åŠ çš„ã€‚äº¦å³ï¼šé‚£ä¹ˆåœ¨selt-attentionæ—¶ï¼Œæœ‰ï¼šè¿™æ ·ä¼šæœ‰äº¤å‰é¡¹ï¼šè¯¥é¡¹æ²¡æœ‰ä»€ä¹ˆæ„ä¹‰ï¼Œä¸”å¯èƒ½ä¼šå¸¦æ¥è¿‡æ‹Ÿåˆã€‚ å› æ­¤åœ¨è¿™è¾¹å°†positional encodingå’Œembeddingæ‹¼èµ·æ¥ï¼Œäº¦å³ï¼š å¹¶ä¸”ï¼Œåœ¨è¿›å…¥multi-headæ—¶çš„çº¿æ€§å±‚ä¹Ÿåšæ”¹å˜ï¼š è¿™æ ·åœ¨ç›¸ä¹˜çš„æ—¶å€™å°±ä¸ä¼šæœ‰äº¤å‰é¡¹äº†ã€‚ å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æœ‰ä¸€å®šçš„æå‡ã€‚ 4ï¸âƒ£[DropBlock: A regularization method for convolutional networks]å¤§è‡´ç¿»äº†ä¸€ä¸‹ã€‚Motivation:åœ¨CNNä¸­ï¼Œdropoutå¯¹convolutional layerçš„ä½œç”¨ä¸å¤§ï¼Œä¸€èˆ¬éƒ½åªç”¨åœ¨å…¨è¿æ¥å±‚ã€‚ä½œè€…æ¨æµ‹ï¼Œå› ä¸ºæ¯ä¸ªfeature mapéƒ½æœ‰ä¸€ä¸ªæ„Ÿå—é‡èŒƒå›´ï¼Œä»…ä»…å¯¹å•ä¸ªåƒç´ è¿›è¡Œdropoutå¹¶ä¸èƒ½é™ä½feature mapå­¦ä¹ çš„ç‰¹å¾èŒƒå›´ï¼Œäº¦å³ç½‘ç»œä»å¯ä»¥é€šè¿‡è¯¥ä½ç½®çš„ç›¸é‚»ä½ç½®å…ƒç´ å»å­¦ä¹ å¯¹åº”çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä¹Ÿå°±ä¸ä¼šä¿ƒä½¿ç½‘ç»œå»å­¦ä¹ æ›´åŠ é²æ£’çš„ç‰¹å¾ã€‚ å› æ­¤ä½œè€…çš„åšæ³•æ˜¯ï¼Œdropoutä¸€æ•´å—ä½ç½®ã€‚ 5ï¸âƒ£[Accelerating Neural Transformer via an Average Attention Network]æå‡ºäº†AAN(average attention network)ï¼Œå¯¹transformerç¿»è¯‘æ¨¡å‹çš„decodeéƒ¨åˆ†è¿›è¡Œæ”¹è¿›ï¼ŒåŠ é€Ÿäº†è¿‡ç¨‹ã€‚ ç”±äºTransformeråœ¨decodeé˜¶æ®µéœ€è¦ç”¨åˆ°å‰é¢æ‰€æœ‰çš„yï¼Œä¹Ÿå³è‡ªå›å½’(auto-regressive)çš„æ€§è´¨ï¼Œæ‰€ä»¥æ— æ³•å¹¶è¡Œï¼š è¿‡ç¨‹ç»™å®šyï¼š é¦–å…ˆå°†ä»–ä»¬åŠ èµ·æ¥ï¼Œè¿‡ä¸€å±‚å…¨è¿æ¥ï¼šè¿™ä¹Ÿç›¸å½“äºå°±æ˜¯è®©æ‰€æœ‰çš„yæœ‰ç›¸åŒçš„æƒé‡ï¼Œæ­¤æ—¶gå°±æ˜¯ä¸Šä¸‹æ–‡ç›¸å…³çš„è¡¨ç¤ºã€‚ æ¥ä¸‹æ¥æ·»åŠ ä¸€ä¸ªgatingï¼šæ§åˆ¶äº†ä»è¿‡å»ä¿å­˜å¤šå°‘ä¿¡æ¯å’Œè·å–å¤šå°‘æ–°çš„ä¿¡æ¯ã€‚ å’ŒTransformeråŸç‰ˆè®ºæ–‡ä¸€æ ·ï¼Œæ·»åŠ ä¸€ä¸ªresidual connectionï¼š å¦‚å›¾æ•´ä¸ªè¿‡ç¨‹ï¼š æ€»ç»“ï¼šAAN=average layer+gating layer åŠ é€Ÿâ‘ è€ƒè™‘åˆ°åŠ å’Œæ“ä½œæ˜¯åºåˆ—åŒ–çš„ï¼Œåªèƒ½ä¸€ä¸ªä¸€ä¸ªæ¥ï¼Œä¸èƒ½å¹¶è¡Œï¼Œåœ¨è¿™é‡Œä½¿ç”¨ä¸€ä¸ªmaskçš„trickï¼Œä½¿å¾—åœ¨è®­ç»ƒæ—¶ä¹Ÿèƒ½å¤Ÿå¹¶è¡Œï¼š â‘¡åœ¨inferenceæ—¶çš„åŠ é€Ÿï¼š è¿™æ ·Transformerå°±èƒ½å¤Ÿç±»ä¼¼RNNï¼Œåªè€ƒè™‘å‰ä¸€ä¸ªçš„stateï¼Œè€Œä¸æ˜¯å‰é¢æ‰€æœ‰çš„stateã€‚ æœ€ç»ˆçš„æ¨¡å‹ï¼š]]></content>
      <tags>
        <tag>ELMo</tag>
        <tag>Paper</tag>
        <tag>dropout</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>Transformer</tag>
        <tag>NTM</tag>
        <tag>self-attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯15]]></title>
    <url>%2F2018%2F11%2F10%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D15%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£èœ€ç›¸[å”] æœç”«ä¸ç›¸ç¥ å ‚ä½•å¤„å¯»ï¼Œé”¦å®˜åŸå¤–æŸæ£®æ£®ã€‚æ˜ é˜¶ç¢§è‰è‡ªæ˜¥è‰²ï¼Œéš”å¶é»„é¹‚ç©ºå¥½éŸ³ã€‚ä¸‰é¡¾é¢‘çƒ¦å¤©ä¸‹è®¡ï¼Œä¸¤æœå¼€æµè€è‡£å¿ƒã€‚å‡ºå¸ˆæœªæ·èº«å…ˆæ­»ï¼Œé•¿ä½¿è‹±é›„æ³ªæ»¡è¥Ÿã€‚ http://m.xichuangzhu.com/work/57b93410a633bd00665efd4a]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡4]]></title>
    <url>%2F2018%2F11%2F04%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%874%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Character-Level Language Modeling with Deeper Self-Attention]å°†transformerç”¨äºcharacter-levelçš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œé€šè¿‡æ·»åŠ å¤šä¸ªlossæ¥æé«˜å…¶è¡¨ç°ä»¥åŠåŠ å¿«æ‹Ÿåˆé€Ÿåº¦ï¼ŒåŒæ—¶åŠ æ·±transformerçš„å±‚æ•°ï¼Œæå¤§æå‡è¡¨ç°ï¼Œ12å±‚çš„transformer layerèƒ½è¾¾åˆ°SOTAï¼Œè€Œ64å±‚åˆ™æœ‰æ›´å¤šçš„æå‡ã€‚ æ™®é€šRNNç”¨äºcharacter-level language modelï¼šå°†å¥å­æŒ‰characterä¸ºå•ä½ç»„æˆå¤šä¸ªbatchï¼Œæ¯ä¸ªbatché¢„æµ‹æœ€åä¸€ä¸ªè¯ï¼Œç„¶åå°†è¯¥batchçš„éšçŠ¶æ€ä¼ å…¥ä¸‹ä¸€ä¸ªbatchã€‚ä¹Ÿå³â€œtruncated backpropagation through timeâ€ (TBTT)ã€‚ å¦‚æœç”¨åœ¨Transformerï¼Œå¦‚ä¸‹å›¾ï¼Œæˆ‘ä»¬åªé¢„æµ‹$t_4$ã€‚ æœ¬æ–‡çš„ä¸€å¤§è´¡çŒ®æ˜¯å¤šåŠ äº†ä¸‰ç§lossï¼Œå¹¶ä¸”æœ‰äº›lossçš„æƒå€¼ä¼šéšç€è®­ç»ƒçš„è¿‡ç¨‹è€Œé€æ¸å‡å°ï¼Œæ¯ä¸ªlosséƒ½ä¼šè‡ªå·±çš„scheduleã€‚è¿™äº›lossåŠ å¿«äº†æ‹Ÿåˆé€Ÿåº¦ï¼ŒåŒæ—¶ä¹Ÿæå‡äº†è¡¨ç°ã€‚ LossMultiple Positionså¯¹äºbatchå†…è€Œè¨€ï¼Œæ¯ä¸ªæ—¶é—´æ­¥téƒ½è¦é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚ Intermediate Layer Lossesè¦æ±‚ä¸­é—´å±‚ä¹Ÿåšå‡ºé¢„æµ‹ï¼š åœ¨è¿™é‡Œï¼Œè¶Šåº•å±‚çš„layerå…¶lossæƒå€¼è¶Šä½ã€‚ Multiple Targetsæ¯ä¸€ä¸ªpositionï¼Œä¸ä»…ä»…è¦é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œè¿˜è¦é¢„æµ‹ä¸‹å‡ ä¸ªè¯ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯å’Œé¢„æµ‹ä¸‹å‡ ä¸ªè¯çš„åˆ†ç±»å™¨æ˜¯ç‹¬ç«‹çš„ã€‚ Positional embeddingæ¯ä¸€å±‚çš„éƒ½æ·»åŠ ä¸€ä¸ªä¸å…±äº«çš„å¯å­¦ä¹ çš„positional embeddingã€‚ 2ï¸âƒ£[Self-Attention with Relative Position Representations]æå‡ºä½¿ç”¨ç›¸å¯¹ä½ç½®æ›¿ä»£Transformerçš„ç»å¯¹ä½ç½®ä¿¡æ¯ï¼Œå¹¶åœ¨NMTä¸Šæœ‰ä¸€å®šçš„æå‡ã€‚ åˆ†è§£ï¼šåœ¨åŸå…ˆçš„self-attentionä¸­ï¼Œè¾“å‡ºä¸ºï¼š å…¶ä¸­ï¼š ç°åœ¨æˆ‘ä»¬è€ƒè™‘æ·»åŠ ç›¸å¯¹ä½ç½®ï¼Œå…¶ä¸­ç›¸å¯¹ä½ç½®ä¿¡æ¯åœ¨å„å±‚éƒ½æ˜¯å…±äº«çš„ï¼š $a_{ij}^K$çš„å…·ä½“å½¢å¼ï¼šä¸Šå¼ä¸ºäº†é™ä½å¤æ‚åº¦ï¼Œä¸è€ƒè™‘é•¿äºkçš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚ è€ƒè™‘åˆ°transformerçš„å¹¶è¡Œæ€§ï¼Œä¸ºäº†å¹¶è¡Œæ€§ï¼Œæˆ‘ä»¬è€ƒè™‘å¦‚ä¸‹å¼å­ï¼šå…¶ä¸­ï¼Œç¬¬ä¸€é¡¹å’ŒåŸæ¥çš„Transformerä¸€è‡´ï¼›ç¬¬äºŒé¡¹ï¼Œé€šè¿‡reshapeå¯ä»¥è¾¾åˆ°å¹¶è¡Œçš„æ•ˆæœï¼Œç„¶åä¸¤é¡¹ç›´æ¥åŠ èµ·æ¥ã€‚ å®éªŒè¯æ˜ï¼Œä½¿ç”¨ç›¸å¯¹ä½ç½®æ•ˆæœæ˜¯æœ‰ä¸€å®šçš„æå‡çš„ï¼Œè€ŒåŒæ—¶ä½¿ç”¨ç»å¯¹ä½ç½®å’Œç›¸å¯¹ä½ç½®å¹¶æ²¡æœ‰æå‡ã€‚ 3ï¸âƒ£[WEIGHTED TRANSFORMER NETWORK FOR MACHINE TRANSLATION]è¿™ç¯‡è¢«ICLRæ‹’äº†ï¼Œä½†æœ‰å®¡ç¨¿äººæ‰“äº†9åˆ†çš„é«˜åˆ†ã€‚ å¯¹Transformerè¿›è¡Œæ”¹è¿›ï¼Œæ‹¥æœ‰æ›´å¥½çš„æ•ˆæœå’Œæ›´å°çš„è®¡ç®—ä»£ä»·ã€‚ ä¼ ç»Ÿçš„Transformerï¼š Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})Vhead_i=Attention(QW_i^Q,KW_i^K,VW_i^V)MultiHead(Q,K,V)=Concat_i (head_i)W^OFFN(x)=max(0,xW_1+b_1)W_2 + b_2åœ¨æœ¬æ–‡ä¸­ï¼Œå…ˆå¯¹headè¿›è¡Œå‡ç»´å¹¶ä¹˜ä»¥æƒé‡ï¼Œè¿‡äº†FNNåï¼Œå†ä¹˜ä»¥å¦ä¸€ä¸ªæƒé‡ã€‚å…¶ä¸­æƒé‡$\alpha$ $ \kappa$ä¸ºå¯å­¦ä¹ å‚æ•°ï¼š head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)\overline{head_i}=head_i W^{O_i} \times \kappa_iBranchedAttention(Q,K,V)=\sum_{i=1}^{M} \alpha_i FFN(\overline{head}_i)å…¶ä¸­è¦æ±‚æƒé‡ä¹‹å’Œä¸º1ã€‚å³$\sum_{i=1}^{M}\alpha_i=1$,$\sum_{i=1}^{M}\kappa_i=1$ã€‚ æ–‡ä¸­å¯¹$\kappa$å’Œ$\alpha$ä½œäº†è§£é‡Šã€‚ Îº can be interpreted as a learned concatenation weight and Î± as the learned addition weight é€šè¿‡å®éªŒï¼Œå‘ç°è¯¥æ¨¡å‹ä¼šæœ‰æ›´å¥½çš„æ­£åˆ™åŒ–ç‰¹æ€§ã€‚åŒæ—¶æ•ˆæœä¹Ÿæœ‰ä¸€å®šæå‡ï¼Œæ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼š 4ï¸âƒ£[You May Not Need Attention]ç²—ç•¥åœ°è¿‡äº†ä¸€éï¼Œä¸€äº›ç»†èŠ‚æ²¡æœ‰å¼„æ˜ç™½ã€‚ æå‡ºä¸€ç§å°†encoder-decoderèåˆèµ·æ¥çš„æ¨¡å‹ï¼Œä¹Ÿå³eager translation modelï¼Œä¸éœ€è¦attentionï¼Œèƒ½å¤Ÿå®ç°å³æ—¶çš„ç¿»è¯‘ï¼Œä¹Ÿå³è¯»å…¥ä¸€ä¸ªè¯å°±èƒ½ç¿»è¯‘ä¸€ä¸ªè¯ï¼ŒåŒæ—¶ä¸éœ€è¦è®°å½•encoderçš„æ‰€æœ‰è¾“å‡ºï¼Œå› æ­¤éœ€è¦å¾ˆå°‘çš„å†…å­˜ã€‚ åˆ†ä¸ºä¸‰æ­¥ï¼šâ‘ pre-processingè¿›è¡Œé¢„å¤„ç†ï¼Œä½¿å¾—æºå¥å­å’Œç›®æ ‡å¥å­æ»¡è¶³eager feasible for every aligned pair of words $(s_i , t_j ), i â‰¤ j$ã€‚ é¦–å…ˆé€šè¿‡ç°æˆçš„å·¥å…·è¿›è¡Œå¯¹é½æ“ä½œ(alignment)ï¼Œç„¶åå¯¹äºé‚£äº›ä¸ç¬¦åˆeager feasibleçš„æœ‰å…·ä½“ç®—æ³•ï¼ˆæ²¡è®¤çœŸçœ‹ï¼‰è¿›è¡Œè¡¥paddingã€‚å¦‚å›¾ æˆ‘ä»¬è¿˜å¯ä»¥åœ¨target sentenceçš„å¼€å¤´æ·»åŠ bä¸ªpaddingï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¼€å§‹é¢„æµ‹ä¹‹å‰è·å–æ›´å¤šçš„source sentenceçš„è¯ã€‚ â‘¡æ¨¡å‹ä¸¤å±‚çš„LSTMï¼Œè¾“å…¥æ˜¯ä¸Šä¸€æ¬¡çš„yå’Œå½“å‰çš„xæ‹¼æ¥èµ·æ¥ç›´æ¥ä¼ è¿›å»ã€‚ â‘¢post processingåœ¨æœ€ç»ˆç»“æœä¹‹å‰ï¼Œå°†paddingå»æ‰ã€‚ åœ¨inferenceï¼ˆä¹Ÿå³beam searchï¼‰æ—¶ï¼Œè¿˜æœ‰å‡ ä¸ªæ“ä½œ/trickï¼š Padding limit Source padding injection SPI å®éªŒè¡¨æ˜ï¼Œeager modelåœ¨é•¿çš„å¥å­è¡¨ç°è¶…è¿‡ä¼ ç»Ÿå¸¦attentionçš„NMTï¼Œè€Œé•¿å¥å­çš„å»ºæ¨¡æ­£æ˜¯attention-based çš„æ¨¡å‹çš„ä¸€å¤§æŒ‘æˆ˜ï¼›è€Œåœ¨çŸ­å¥å­ä¸Šå°±ä¸å¦‚attention-basedçš„NMTã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>Transformer</tag>
        <tag>Language Modeling</tag>
        <tag>self-attention</tag>
        <tag>relative position</tag>
        <tag>positional encoding</tag>
        <tag>NMT</tag>
        <tag>eager translation model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯14]]></title>
    <url>%2F2018%2F11%2F04%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D14%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£é¹¤å†²å¤©[å®‹] æŸ³æ°¸é»„é‡‘æ¦œä¸Šï¼Œå¶å¤±é¾™å¤´æœ›ã€‚æ˜ä»£æš‚é—è´¤ï¼Œå¦‚ä½•å‘ï¼Ÿæœªé‚é£äº‘ä¾¿ï¼Œäº‰ä¸æ£æ¸¸ç‹‚è¡ã€‚ä½•é¡»è®ºå¾—ä¸§ï¼Ÿæ‰å­è¯äººï¼Œè‡ªæ˜¯ç™½è¡£å¿ç›¸ã€‚çƒŸèŠ±å··é™Œï¼Œä¾çº¦ä¸¹é‘å±›éšœã€‚å¹¸æœ‰æ„ä¸­äººï¼Œå ªå¯»è®¿ã€‚ä¸”æåçº¢å€šç¿ ï¼Œé£æµäº‹ï¼Œå¹³ç”Ÿç•…ã€‚é‘æ˜¥éƒ½ä¸€é¥·ã€‚å¿æŠŠæµ®åï¼Œæ¢äº†æµ…æ–Ÿä½å”±ï¼ æ£ï¼ˆzÃ¬ï¼‰ï¼šæ”¾çºµï¼Œéšå¿ƒæ‰€æ¬²ã€‚æï¼ˆnÃ¨nï¼‰ï¼šå¦‚æ­¤ã€‚ http://m.xichuangzhu.com/work/57aeff68a633bd0057f7d406]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•9]]></title>
    <url>%2F2018%2F11%2F04%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%959%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[collate_fn]å°†ä¸ç­‰é•¿å¥å­ç»„åˆæˆbatchã€‚ 1234567891011121314151617def collate_fn(insts): ''' Pad the instance to the max seq length in batch ''' max_len = max(len(inst) for inst in insts) batch_seq = np.array([ inst + [Constants.PAD] * (max_len - len(inst)) for inst in insts]) batch_pos = np.array([ [pos_i + 1 if w_i != Constants.PAD else 0 for pos_i, w_i in enumerate(inst)] for inst in batch_seq]) # ä½ç½®ä¿¡æ¯ batch_seq = torch.LongTensor(batch_seq) batch_pos = torch.LongTensor(batch_pos) return batch_seq, batch_pos]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[â€œè±æ–¯æ¯â€æŒ‘æˆ˜èµ›æœ‰æ„Ÿ]]></title>
    <url>%2F2018%2F10%2F30%2F%E8%A7%81%E9%97%BB%26%E6%83%B3%E6%B3%95%2F%E2%80%9C%E8%8E%B1%E6%96%AF%E6%9D%AF%E2%80%9D%E6%8C%91%E6%88%98%E8%B5%9B%E6%9C%89%E6%84%9F%2F</url>
    <content type="text"><![CDATA[å†æ—¶ä¸‰ä¸ªæœˆçš„â€œè±æ–¯æ¯â€å…¨å›½ç¬¬ä¸€å±Šâ€œå†›äº‹æ™ºèƒ½Â·æœºå™¨é˜…è¯»â€æŒ‘æˆ˜èµ›ç»ˆäºè½ä¸‹å¸·å¹•ï¼Œå‰å‡ æ—¥ï¼ˆ10.26-10.28ï¼‰æœ‰å¹¸åœ¨å—äº¬é’æ—…å®¾é¦†å‚ä¸å†³èµ›ï¼Œä½“éªŒå¤šå¤šï¼Œæ”¶è·æ»¡æ»¡ï¼Œå¿ƒä¸­äº¦æœ‰ä¸€äº›æ„Ÿæƒ³ã€‚ ä¸€ä¸ªæ˜¯å—äº¬æ€»å¸¦ç»™æˆ‘ä¸€ç§å›å®¶çš„æ„Ÿè§‰ï¼Œå¯¹å—äº¬çš„äº‹ç‰©æ€»æœ‰äº²åˆ‡æ„Ÿã€‚ç¬¬ä¸€æ¬¡æ¥å—äº¬æ˜¯ä¸€å¹´åŠå‰ï¼Œä¹Ÿæ˜¯æ¥å‚åŠ æ¯”èµ›ã€‚å‘¨äº”æ™šä¸Šçš„å¤œæ¸¸ç§¦æ·®ï¼Œè®©æˆ‘æ„Ÿå—åˆ°è®¸ä¹…æœªæ›¾æ„Ÿå—åˆ°çš„çƒŸç«æ°”æ¯ã€‚ ç¬¬äºŒä¸ªæ˜¯æ­¤æ¬¡ä¸»åŠæ–¹æä¾›çš„é£Ÿå®¿ä»¤äººæƒŠå–œã€‚ä¸€å¼€å§‹å¬åˆ°é’æ—…å®¾é¦†ï¼Œæˆ‘å·²ç»åšå¥½äº†è‰°è‹¦å¥‹æˆ˜çš„å‡†å¤‡äº†ï¼Œç„¶è€Œé…’åº—æ˜¯æ˜Ÿçº§é…’åº—çš„ï¼Œåƒæ–¹é¢ç›´æ¥åˆ°æ¥¼ä¸‹çš„è‡ªåŠ©ã€‚å¯ä»¥çœ‹å‡ºä¸»åŠæ–¹æ­¤æ¬¡ç¡®å®ç”¨å¿ƒåœ¨ä¸¾åŠè¿™æ¬¡æ¯”èµ›ã€‚ ç¬¬ä¸‰ç‚¹æ˜¯å…³äºæ¯”èµ›çš„ï¼Œå…³äºæ¯”èµ›çš„æ•´ä¸ªå†ç¨‹æˆ‘è¿˜æ˜¯é¢‡æœ‰æ„Ÿè§¦ã€‚æˆ‘ä»¬æ˜¯ä»¥ç¬¬9åçš„æˆç»©æŒºè¿›å†³èµ›ï¼Œå…¶å®åœ¨åæœŸæ¯”èµ›ä¸­ï¼Œæˆ‘ä»¬éƒ½æœ‰æ‰€æ‡ˆæ€ äº†ï¼Œå‡ ä¹æ²¡æœ‰èŠ±æ—¶é—´åœ¨è¿™ä¸Šé¢ï¼Œ10æœˆåˆå‘å¸ƒå†³èµ›çš„æ•°æ®é›†ï¼Œè€Œæˆ‘ä»¬åœ¨10æœˆ20æ—¥æ‰å¾—çŸ¥è¿™ä¸€äº‹æƒ…ï¼Œæ­¤æ—¶ç¦»å†³èµ›åªå‰©ä¸€å‘¨æ—¶é—´ã€‚å› æ­¤æˆ‘ä»¬ç¡®å®å‡†å¤‡ä¸è¶³ã€‚å½“ç„¶æˆ‘ä»¬ä¹Ÿæ²¡æœ‰é¢„æ–™åˆ°æˆ‘ä»¬çš„å†³èµ›æˆç»©ä¼šè¿™ä¹ˆé å‰ï¼Œå¦åˆ™æˆ‘ä»¬è‚¯å®šä¼šæ›´åŠ å……åˆ†å»å‡†å¤‡ã€‚è¿™ç¡®å®æ˜¯æˆ‘ä»¬çš„å¤±è¯¯ã€‚ æˆ‘ä»¬åœ¨æ¯”èµ›è¿‡ç¨‹ä¸­ï¼Œä¸€ç›´å°è¯•åœ¨ä½¿ç”¨ELMoï¼Œè¿™æ­£æ˜¯æˆ‘è´Ÿè´£çš„éƒ¨åˆ†ã€‚ä¸€å¼€å§‹ä½¿ç”¨å®˜æ–¹TensorFlowçš„ä»£ç ï¼Œè´¹äº†ä¹ç‰›äºŒè™ä¹‹åŠ›æˆ‘æ‰è·‘é€šä»£ç ï¼Œä½†å› ä¸ºé˜Ÿé•¿ä½¿ç”¨çš„æ˜¯pytorchï¼Œè€ŒäºŒè€…åœ¨cudaç‰ˆæœ¬ä¸Šä¸å…¼å®¹ï¼Œå› æ­¤åœ¨åˆèµ›æˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨ELMoã€‚è€Œåœ¨æœ€åå‡ å¤©ï¼Œæˆ‘å°è¯•ä½¿ç”¨å“ˆå·¥å¤§çš„pytorchè®­ç»ƒä»£ç ï¼Œä½†å› ä¸ºinferenceé€Ÿåº¦å®åœ¨å¤ªæ…¢ï¼Œæˆ‘ä»¬æœ€ç»ˆè¿˜æ˜¯å¼ƒç”¨äº†è¿™ä¸ªæ–¹æ¡ˆã€‚è€Œåœ¨å†³èµ›ç°åœºï¼Œæˆ‘ä»¬å‘ç°ä¹Ÿç¡®å®æ˜¯å› ä¸ºé€Ÿåº¦å’Œèµ„æºçš„åŸå› ï¼Œå¤§å®¶éƒ½æ²¡æœ‰ä½¿ç”¨ELMoï¼Œé™¤äº†ä¸€ç»„ã€‚è¯¥ç»„æ­£æ˜¯å‡­å€Ÿäº†ELMoå¼¯é“è¶…è½¦ä»ç¬¬7å‡åˆ°äº†ç¬¬ä¸€ï¼Œæ‹¿èµ°äº†20ä¸‡å¤§å¥–ã€‚è¿™ä¹Ÿæ˜¯æˆ‘ä»¬éå¸¸é—æ†¾çš„ä¸€ä¸ªåœ°æ–¹ï¼Œæˆ‘ä»¬åœ¨é‡åˆ°å›°éš¾æ—¶æ²¡æœ‰å°è¯•è§£å†³ï¼Œè€Œæ˜¯ç›´æ¥å¼ƒç”¨ï¼Œæœ€ç»ˆæ²¡æœ‰å–å¾—æ›´å¥½çš„æˆç»©ã€‚ æ­¤æ¬¡æˆ‘ä»¬çš„æˆç»©æ’åç¬¬4(ä¸‰ç­‰å¥–)ï¼Œæ˜¯æœ‰ä¸€å®šçš„è¿›æ­¥çš„ï¼Œä½†æœ‰ä¸€ç‚¹é—æ†¾çš„æ˜¯ï¼Œæˆ‘ä»¬ä»…å·®0.18ç™¾åˆ†ç‚¹ï¼Œå°±èƒ½è¶…è¿‡ç¬¬ä¸‰åæ‹¿åˆ°5ä¸‡çš„å¥–é‡‘äº†ã€‚åé¢æˆ‘ä»¬åˆ†æäº†ä¸€ä¸‹ï¼Œè¿˜æ˜¯å› ä¸ºæˆ‘ä»¬å¯¹æ¯”èµ›æ‡ˆæ€ çš„æ€åº¦ï¼Œå…¶ä»–ç»„éƒ½å¯¹æ•°æ®è¿›è¡Œäº†åˆ†æå¹¶æœ‰é’ˆå¯¹æ€§çš„æ”¹è¿›ï¼Œè€Œæˆ‘ä»¬å¹¶æ²¡æœ‰åšè¿™ä¸€æ­¥ã€‚ Anywayï¼Œç¬¬ä¸€æ¬¡ç»„é˜Ÿå‚åŠ æ¯”èµ›å°±æœ‰æ”¶è·ï¼Œå¢é•¿äº†è§è¯†ï¼Œä»äº¤æµä¸­ä¹Ÿè·å¾—äº†è®¸å¤šã€‚è¿™ä¸ªæ¯”èµ›ä¹‹åï¼Œå°±å¾—å¥½å¥½çœ‹paperäº†ã€‚ __(:Ğ·ã€âˆ )_]]></content>
      <tags>
        <tag>æœ‰æ„Ÿ</tag>
        <tag>è±æ–¯æ¯</tag>
        <tag>æ¯”èµ›</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯13]]></title>
    <url>%2F2018%2F10%2F29%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D13%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£è¡Œè·¯éš¾ä¸‰é¦–[å”] æç™½ã€å…¶ä¸€ã€‘é‡‘æ¨½æ¸…é…’æ–—ååƒï¼Œç‰ç›˜çç¾ç›´ä¸‡é’±ã€‚åœæ¯æŠ•ç®¸ä¸èƒ½é£Ÿï¼Œæ‹”å‰‘å››é¡¾å¿ƒèŒ«ç„¶ã€‚æ¬²æ¸¡é»„æ²³å†°å¡å·ï¼Œå°†ç™»å¤ªè¡Œé›ªæ»¡å±±ã€‚é—²æ¥å‚é’“ç¢§æºªä¸Šï¼Œå¿½å¤ä¹˜èˆŸæ¢¦æ—¥è¾¹ã€‚è¡Œè·¯éš¾ï¼Œè¡Œè·¯éš¾ï¼Œå¤šæ­§è·¯ï¼Œä»Šå®‰åœ¨ï¼Ÿé•¿é£ç ´æµªä¼šæœ‰æ—¶ï¼Œç›´æŒ‚äº‘å¸†æµæ²§æµ·ï¼ æ³¨é‡Šï¼šã€Œé—²æ¥å‚é’“ç¢§æºªä¸Šï¼Œå¿½å¤ä¹˜èˆŸæ¢¦æ—¥è¾¹ã€‚ã€å¥ï¼šæš—ç”¨å…¸æ•…ï¼šå§œå¤ªå…¬å•å°šæ›¾åœ¨æ¸­æ°´çš„ç£»æºªä¸Šé’“é±¼ï¼Œå¾—é‡å‘¨æ–‡ç‹ï¼ŒåŠ©å‘¨ç­å•†ï¼›ä¼Šå°¹æ›¾æ¢¦è§è‡ªå·±ä¹˜èˆ¹ä»æ—¥æœˆæ—è¾¹ç»è¿‡ï¼Œåè¢«å•†æ±¤è˜è¯·ï¼ŒåŠ©å•†ç­å¤ã€‚è¿™ä¸¤å¥è¡¨ç¤ºè¯—äººè‡ªå·±å¯¹ä»æ”¿ä»æœ‰æ‰€æœŸå¾…ã€‚ç¢§ï¼Œä¸€ä½œã€Œåã€ã€‚ 2ï¸âƒ£ç™»ç§‘å[å”] å­ŸéƒŠæ˜”æ—¥é¾Œé¾Šä¸è¶³å¤¸ï¼Œä»Šæœæ”¾è¡æ€æ— æ¶¯ã€‚æ˜¥é£å¾—æ„é©¬è¹„ç–¾ï¼Œä¸€æ—¥çœ‹å°½é•¿å®‰èŠ±ã€‚ æ³¨é‡Šï¼šé¾Œé¾Šï¼ˆwÃ² chuÃ²ï¼‰ï¼šåŸæ„æ˜¯è‚®è„ï¼Œè¿™é‡ŒæŒ‡ä¸å¦‚æ„çš„å¤„å¢ƒã€‚ä¸è¶³å¤¸ï¼šä¸å€¼å¾—æèµ·ã€‚æ”¾è¡ï¼ˆdÃ ngï¼‰ï¼šè‡ªç”±è‡ªåœ¨ï¼Œä¸å—çº¦æŸã€‚æ€æ— æ¶¯ï¼šå…´è‡´é«˜æ¶¨ã€‚ http://m.xichuangzhu.com/work/57add198a633bd0057eefa8a]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡3]]></title>
    <url>%2F2018%2F10%2F29%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%873%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[A Neural Probabilistic Language Model]ç¬¬ä¸€ç¯‡ä½¿ç”¨ç¥ç»ç½‘ç»œè·å¾—è¯å‘é‡çš„paperã€‚ é€šè¿‡å¯¹language modelå»ºæ¨¡ï¼Œå°†è¯æ˜ å°„åˆ°ä½ç»´è¡¨ç¤ºï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒæ—¶è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥åŠæ¯ä¸ªè¯çš„è¯å‘é‡ã€‚ å°†ä¸­å¿ƒè¯çš„å‰nä¸ªæ‹¼æ¥èµ·æ¥ $x=(C(w_{t-1},C(w_{t-2}),â€¦,C(w_{t-n+1}))$å°†$x$é€å…¥ç¥ç»ç½‘ç»œä¸­è·å¾—$y=b+Wx+Utanh(d+Hx)$ï¼Œæœ€ååšä¸€ä¸ªsoftmaxå³å¯ã€‚ 2ï¸âƒ£[Adaptive Computation Time for Recurrent Neural Networks]ä¸€ç§å…è®¸RNNåŠ¨æ€å †å å±‚æ•°çš„ç®—æ³•ã€‚ Motivationè¯æ®è¯æ˜ï¼ŒRNNçš„å †å å±‚æ•°å¤šï¼Œæ•ˆæœä¼šæœ‰æå‡ã€‚ä½†æ˜¯ï¼Œå¯¹äºä¸åŒçš„ä»»åŠ¡ï¼Œè¦æ±‚ä¸åŒçš„è®¡ç®—å¤æ‚åº¦ã€‚æˆ‘ä»¬éœ€è¦å…ˆéªŒæ¥å†³å®šç‰¹å®šä»»åŠ¡çš„è®¡ç®—å¤æ‚åº¦ã€‚å½“ç„¶æˆ‘ä»¬å¯ä»¥ç²—æš´åœ°ç›´æ¥å †å æ·±å±‚çš„ç½‘ç»œã€‚ACT(Adaptive Computation Time)èƒ½å¤ŸåŠ¨æ€å†³å®šæ¯ä¸ªè¾“å…¥tæ‰€éœ€çš„è®¡ç®—æ¬¡æ•°ã€‚ æ–¹æ³•å°†RNNæ¯ä¸€æ­¥çš„è¾“å‡ºè¿‡ä¸€ä¸ªç½‘ç»œ+sigmoidå±‚ï¼Œè·å¾—ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œä¹Ÿå³ä»€ä¹ˆæ—¶å€™åº”å½“åœæ­¢ä¸å†ç»§ç»­å¾€ä¸Šå †å ï¼Œç›´åˆ°æ¦‚ç‡åŠ å’Œä¸º1ã€‚åŒæ—¶ä¸ºäº†å°½å¯èƒ½æŠ‘åˆ¶å±‚æ•°çš„æ— é™å¢é•¿ï¼Œåœ¨lossæ·»åŠ ä¸€é¡¹æƒ©ç½šã€‚ æ¨¡å‹å¯¹äºæ™®é€šçš„RNNï¼š sæ˜¯éšè—å±‚ï¼›yæ˜¯è¾“å‡ºã€‚ å¯¹äºACTçš„RNNï¼Œæœ‰ï¼š ä¸Šæ ‡næ˜¯æŒ‡çš„tæ—¶åˆ»çš„å±‚æ•°ï¼›å…¶ä¸­ï¼š $Î´$æ˜¯flatï¼ŒæŒ‡ç¤ºxæ˜¯ç¬¬å‡ æ¬¡è¾“å…¥ã€‚ å¼•å…¥æ–°çš„ç½‘ç»œï¼Œè¾“å…¥æ—¶éšçŠ¶æ€ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼š é‚£ä¹ˆæ¯ä¸€å±‚çš„æ¦‚ç‡æ˜¯ï¼š å…¶ä¸­$R(t)$æ˜¯åœ¨æ¯ä¸€å±‚æ¦‚ç‡æ±‚å’Œè¶…è¿‡1æ—¶çš„å‰©ä½™æ¦‚ç‡ï¼ˆä¸ºäº†ä¿è¯æ¦‚ç‡å’Œä¸º1ï¼Œå¯ä»¥è¯•ç€ä¸¾ä¸€ä¸ªä¾‹å­æ¥è¯æ˜ï¼‰ Îµæ˜¯ä¸ºäº†è§£å†³ç¬¬ä¸€æ¬¡è¾“å‡ºæ—¶å°±è¶…è¿‡1-Îµçš„æƒ…å†µï¼ŒÎµä¸€èˆ¬å–å¾ˆå°ã€‚ æœ€ç»ˆï¼ŒåŠ æƒæ±‚å’Œï¼Œä½œä¸ºæœ€ç»ˆçš„ç»“æœï¼Œä¼ å…¥ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ï¼š æ™®é€šRNNä¸ACTçš„RNNå¯¹æ¯”ï¼š æŸå¤±å‡½æ•°ä¸ºäº†é˜²æ­¢æ¨¡å‹å±‚æ•°æ— é™å¢é•¿ï¼Œæ·»åŠ ä¸€é¡¹æƒ©ç½šé¡¹ä»¥æŠ‘åˆ¶ã€‚ è®°æ¯ä¸€æ­¥çš„æƒ©ç½šé¡¹ä¸ºï¼š æ€»çš„æƒ©ç½šé¡¹åˆ™ä¸ºï¼š Loss functionåˆ™ä¸ºï¼š å› ä¸ºN(t)æ˜¯ä¸å¯å¯¼çš„ï¼Œæˆ‘ä»¬åœ¨å®é™…è¿‡ç¨‹ä¸­åªå»æœ€å°åŒ–R(t) ï¼ˆæˆ‘è§‰å¾—ä¸ç”šåˆç†ï¼Œä¸€ç§è§£è¯»æ˜¯å¦‚æœæˆ‘ä»¬ä¸æ–­æœ€å°åŒ–R(t)ç›´åˆ°å˜æˆ0ï¼Œé‚£ä¹ˆç›¸å½“äºN(t)å°‘äº†ä¸€å±‚ï¼Œæ¥ç€R(t)å°±ä¼šå˜å¾—å¾ˆå¤§ï¼Œç„¶ååˆç»§ç»­æœ€å°åŒ–R(t)â€¦ï¼‰ 3ï¸âƒ£[Universal Transformers]æå‡ºä¸€ç§æ–°å‹é€šç”¨çš„transformerã€‚ MotivationTransformerçš„é—®é¢˜ï¼šRNNçš„å½’çº³åç½®(inductive bias)åœ¨ä¸€äº›ä»»åŠ¡ä¸Šå¾ˆé‡è¦ï¼Œä¹Ÿå³RNNçš„å¾ªç¯å­¦ä¹ çš„è¿‡ç¨‹ï¼›Transformeråœ¨ä¸€äº›é—®é¢˜ä¸Šè¡¨ç°ä¸å¥½ï¼Œå¯èƒ½æ˜¯å½’çº³åç½®çš„åŸå› ã€‚ Notably, however, the Transformer foregoes the RNNâ€™s inductive bias towards learning iterative or recursive transformations.Our experiments indicate that this inductive bias may be crucial for several algorithmic and language understanding tasks of varying complexity: in contrast to models such as the Neural Turing Machine [13], the Neural GPU [17] or Stack RNNs [16], the Transformer does not generalize well to input lengths not encountered during training. å› æ­¤åœ¨Transformerå†…å¼•å…¥å½’çº³åç½® ç‰¹ç‚¹ æ¯ä¸€å±‚çš„æƒé‡æ˜¯å…±äº«çš„ï¼Œä¹Ÿå³multi-headä¸Šçš„æƒé‡ä»¥åŠtransition functionåœ¨æ¯ä¸€å±‚æ˜¯ä¸€è‡´çš„ã€‚è¿™ä¸€ç‚¹å’ŒRNNã€CNNä¸€è‡´ã€‚ åŠ¨æ€å±‚æ•°ï¼ˆACT mechanism ï¼‰ï¼šå¯¹äºæ¯ä¸ªè¯éƒ½ä¼šæœ‰ä¸åŒçš„å¾ªç¯æ¬¡æ•°ï¼›ä¹Ÿå³æœ‰äº›è¯éœ€è¦æ›´å¤šçš„refineï¼›è€Œæœ‰äº›è¯ä¸éœ€è¦ã€‚å’Œå›ºå®šå±‚æ•°çš„transformerç›¸æ¯”ï¼Œä¼šæœ‰æ›´å¥½çš„é€šç”¨æ€§ã€‚ æ¨¡å‹æ€»ä½“æ¶æ„ è¿‡ç¨‹ï¼š å’Œæ™®é€šTransformerä¸åŒçš„åœ°æ–¹åœ¨äºï¼š åŠ äº†ä¸€å±‚Transitionå±‚ï¼ŒTransitionå¯ä»¥æ˜¯depth-wise separable convolutionï¼ˆæ˜¯ä»€ä¹ˆï¼Ÿï¼‰æˆ–è€…å…¨è¿æ¥å±‚ã€‚ æ¯å±‚éƒ½æ·»åŠ äº†position embeddingï¼›ä»¥åŠtimestep embeddingï¼Œç”¨ä»¥æŒ‡ç¤ºå±‚æ•°ã€‚ ACTç”±äºä¸€ä¸ªå¥å­ä¸­é—´ï¼Œæœ‰äº›è¯æ¯”å…¶ä»–è¯æ›´éš¾å­¦ä¼šï¼Œéœ€è¦æ›´å¤šè®¡ç®—é‡ï¼Œä½†å †å å¤ªå¤šå±‚ä¼šå¤§å¤§å¢åŠ è®¡ç®—é‡ï¼Œä¸ºäº†èŠ‚çœè®¡ç®—é‡ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥ACTæ¥åŠ¨æ€åˆ†é…è®¡ç®—é‡ã€‚ ACTåŸæ¥ç”¨äºRNNï¼Œåœ¨Transformerä¸­ï¼Œå½“halting unitæŒ‡ç¤ºè¯tåº”å½“åœæ­¢æ—¶ï¼Œç›´æ¥è®²è¯¥è¯çš„çŠ¶æ€å¤åˆ¶åˆ°ä¸‹ä¸€ä¸ªtime stepï¼Œç›´åˆ°æ‰€æœ‰çš„è¯éƒ½åœæ­¢ã€‚]]></content>
      <tags>
        <tag>Embedding</tag>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>Transformer</tag>
        <tag>ACT</tag>
        <tag>Language Modeling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯12]]></title>
    <url>%2F2018%2F10%2F21%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D12%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£æœ›æµ·æ½®[å®‹] æŸ³æ°¸ä¸œå—å½¢èƒœï¼Œä¸‰å´éƒ½ä¼šï¼Œé’±å¡˜è‡ªå¤ç¹åã€‚çƒŸæŸ³ç”»æ¡¥ï¼Œé£å¸˜ç¿ å¹•ï¼Œå‚å·®åä¸‡äººå®¶ã€‚äº‘æ ‘ç»•å ¤æ²™ï¼Œæ€’æ¶›å·éœœé›ªï¼Œå¤©å ‘æ— æ¶¯ã€‚å¸‚åˆ—ç ç‘ï¼Œæˆ·ç›ˆç½—ç»®ï¼Œç«è±ªå¥¢ã€‚é‡æ¹–å å·˜æ¸…å˜‰ï¼Œæœ‰ä¸‰ç§‹æ¡‚å­ï¼Œåé‡Œè·èŠ±ã€‚ç¾Œç®¡å¼„æ™´ï¼Œè±æ­Œæ³›å¤œï¼Œå¬‰å¬‰é’“åŸè²å¨ƒã€‚åƒéª‘æ‹¥é«˜ç‰™ï¼Œä¹˜é†‰å¬ç®«é¼“ï¼ŒåŸèµçƒŸéœã€‚å¼‚æ—¥å›¾å°†å¥½æ™¯ï¼Œå½’å»å‡¤æ± å¤¸ã€‚ å å·˜ï¼ˆyÇnï¼‰ï¼šå±‚å±‚å å çš„å±±å³¦ã€‚ http://m.xichuangzhu.com/work/57b318228ac247005f2223db]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•8]]></title>
    <url>%2F2018%2F10%2F21%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%958%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[batchify]å¿«é€Ÿå°†æ•°æ®åˆ†æˆbatchã€‚ 12345678def batchify(data, bsz): # Work out how cleanly we can divide the dataset into bsz parts. nbatch = data.size(0) // bsz # Trim off any extra elements that wouldn't cleanly fit (remainders). data = data.narrow(0, 0, nbatch * bsz) # Evenly divide the data across the bsz batches. data = data.view(bsz, -1).t().contiguous() return data.to(device)]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRMLç¬¬å››ç«  åˆ†ç±»çš„çº¿æ€§æ¨¡å‹]]></title>
    <url>%2F2018%2F10%2F21%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FPRML%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%88%86%E7%B1%BB%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[åˆ¤åˆ«å‡½æ•° â€”-æœªå®Œâ€”-]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡2]]></title>
    <url>%2F2018%2F10%2F20%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%872%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling]æœ¬æ–‡è´¡çŒ®ï¼šæå‡ºä¸€ç§æ–°çš„æ¨¡å‹TCNï¼ˆTemporal Convolutional Networksï¼‰è¿›è¡Œlanguage modelå»ºæ¨¡ã€‚ Dilated convolutionæ¯ä¸€å±‚çš„æ„Ÿå—é‡éƒ½å¯ä»¥æ˜¯ä¸åŒçš„ï¼Œä¹Ÿå³ï¼ŒåŒæ ·çš„kernel sizeï¼Œé«˜å±‚çš„å¯ä»¥è·³ç€çœ‹ã€‚ æ¯å±‚çš„dé€æ¸å¢å¤§ï¼ˆä¹Ÿå³è·³çš„æ­¥æ•°ï¼‰ï¼Œä¸€èˆ¬æŒ‰æŒ‡æ•°å¢å¤§ã€‚ï¼ˆæˆ‘è§‰å¾—è¿™æ ·å¾ˆæœ‰é“ç†ï¼Œå¦‚æœæ¯ä¸€å±‚çš„déƒ½æ˜¯ä¸€æ ·çš„ï¼Œé‚£captureåˆ°çš„ä¿¡æ¯å°±ä¼šæœ‰é‡å¤ï¼Œèƒ½çœ‹åˆ°çš„è§†é‡ä¹Ÿä¸å¦‚é€æ¸å¢å¤§çš„å¤šï¼‰ Residual block è¿™è¾¹çš„residual blockæ¯”è¾ƒå¤æ‚ï¼›ä¸€ä¸ªå€¼å¾—ä¸»æ„çš„ç»†èŠ‚æ˜¯ï¼Œå› ä¸ºæ„Ÿå—é‡çš„ä¸åŒï¼Œä¸Šå±‚çš„æ„Ÿå—é‡æ€»æ˜¯æ¯”ä¸‹å±‚çš„å¤§å¾ˆå¤šï¼Œå› æ­¤ä¸åº”è¯¥ç›´æ¥å°†ä¸‹å±‚çš„åŠ åˆ°ä¸Šå±‚ï¼Œè€Œæ˜¯å¯ä»¥ä½¿ç”¨ä¸€ä¸ª1*1çš„convolutionå¯¹ä¸‹å±‚çš„xè¿›è¡Œå·ç§¯ï¼Œè¿™å°±ç±»ä¼¼scaleå¯¹è¾“å…¥è¿›è¡Œæ”¾ç¼©ã€‚ 2ï¸âƒ£[Dissecting Contextual Word Embeddingsï¼š Architecture and Representation]ä¸€ç¯‡åˆ†æçš„æ–‡ç« ã€‚ELMoä½œè€…çš„åˆä¸€ç¯‡æ–‡ç« ã€‚ å¯¹æ¯”ä¸‰ç§ä¸åŒçš„å»ºæ¨¡æ–¹å¼ï¼ˆLSTM/GCNN/Transformerï¼‰è·å¾—çš„è¯å‘é‡ï¼Œä»¥åŠåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼›ä»¥åŠä¸åŒå±‚è·å¾—çš„ä¸åŒä¿¡æ¯â€¦è·å¾—äº†ä¸åŒçš„ç»“è®ºã€‚ â‘ biLM ä¸“æ³¨äºword morphologyè¯çš„å½¢æ€ï¼›åº•å±‚çš„LMå…³æ³¨local syntaxï¼›è€Œé«˜å±‚çš„LMå…³æ³¨semantic contentï¼› â‘¡ä¸åŒçš„ä»»åŠ¡ä¼šæœ‰ä¸åŒçš„æ­£åˆ™åŒ–sçš„å€¾å‘ã€‚ 3ï¸âƒ£[Transformer-XL: Language modeling with longer-term dependency]åˆ©ç”¨Transformerè¿›è¡Œlanguage modelï¼Œä¸æ™®é€šçš„Transformerå»ºæ¨¡ä¸åŒçš„æ˜¯ï¼ŒTransformer-XLæ·»åŠ äº†å†å²ä¿¡æ¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è¡¨ç°ã€‚è¿™ç¯‡è¿˜åœ¨ICLR2019å®¡ç¨¿ä¸­ã€‚ è´¡çŒ®ï¼šæœ¬æ–‡æå‡ºäº†èƒ½å¤Ÿè¿›è¡Œé•¿ç¨‹ä¾èµ–çš„åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ Transformer-XLï¼›å¼•å…¥ç›¸å¯¹ä½ç½®çš„positional encodingã€‚ ç»“æ„åŸå…ˆçš„transformer language modelæ˜¯å°†å¥å­åˆ†ä¸ºä¸€ä¸ªä¸€ä¸ªsegmentã€‚segmentä¹‹é—´æ˜¯æ²¡æœ‰è”ç³»çš„ã€‚ï¼ˆä¸ºä»€ä¹ˆä¸ç›´æ¥æŒ‰åŸç‰ˆçš„Transformeré‚£æ ·æ‰€æœ‰çš„è¯éƒ½ç›¸äº’åšself-attentionï¼Ÿå› ä¸ºè€ƒè™‘åˆ°æ•ˆç‡é—®é¢˜ï¼Œå¥å­é•¿åº¦å¯èƒ½ä¼šå¾ˆé•¿ï¼‰ è®­ç»ƒé˜¶æ®µï¼š è€Œåœ¨æµ‹è¯•é˜¶æ®µï¼Œæ¯æ¬¡å‘å³æ»‘åŠ¨ä¸€æ ¼ï¼šè¿™æ ·æ¯ä¸€ä¸ªæ—¶é—´æ­¥éƒ½è¦é‡æ–°è®¡ç®—ä¸€éï¼Œå†å²ä¿¡æ¯æ²¡æœ‰åˆ©ç”¨åˆ°ã€‚æ˜¾ç„¶é€Ÿåº¦å¾ˆæ…¢ã€‚ åœ¨Transformerå¼•å…¥recurrenceï¼Œä¹Ÿå³å¼•å…¥å†å²ä¿¡æ¯ã€‚åŸºäºè¿™æ ·çš„æƒ³æ³•ï¼Œæå‡ºçš„æ–°æ¨¡å‹Transformer-XLã€‚åœ¨ç»“æ„ä¸ŠåŒæ ·åˆ†ä¸ºæ¯ä¸ªsegmentï¼Œä½†åœ¨æ¯ä¸ªé˜¶æ®µéƒ½æ¥æ”¶ä¸Šä¸€ä¸ªï¼ˆç”šè‡³ä¸ŠLä¸ªï¼‰å†å²ä¿¡æ¯ã€‚ è®­ç»ƒé˜¶æ®µï¼š è€Œåœ¨æµ‹è¯•é˜¶æ®µï¼ŒåŒæ ·åˆ†ä¸ºsegmentï¼Œä½†å› ä¸ºæ¥æ”¶äº†å†å²ä¿¡æ¯ï¼Œä¸éœ€è¦æ¯æ¬¡æ»‘åŠ¨ä¸€æ ¼ä¹Ÿèƒ½è·å¾—å¤§é‡ä¿¡æ¯ã€‚ å…·ä½“æ¥è¯´ï¼šSGä»£è¡¨stop gradientï¼Œå’Œè¯¥é˜¶æ®µçš„hidden stateè¿›è¡Œæ‹¼æ¥ã€‚ RELATIVE POSITIONAL ENCODINGSå¦‚æœæˆ‘ä»¬ä½¿ç”¨äº†absolute positional encodingsï¼ˆä¹Ÿå³åŸç‰ˆçš„positional encodingsï¼‰é‚£ä¹ˆä¼šå‡ºç°è¿™ç§æƒ…å†µ åœ¨åŒä¸€å±‚ä¹‹é—´çš„å‰ä¸€ä¸ªsegmentå’Œåä¸€ä¸ªsegmentä½¿ç”¨äº†åŒæ ·çš„ç»å¯¹ä½ç½®ä¿¡æ¯ï¼Œå¯¹äºå½“å‰segmentçš„é«˜å±‚ï¼Œå¯¹äºåŒä¸€ä¸ªä½ç½®iï¼Œæ— æ³•åŒºåˆ†è¯¥ä½ç½®ä¿¡æ¯æ˜¯æ¥è‡ªå½“å‰segmentçš„è¿˜æ˜¯ä¸Šä¸€ä¸ªsegmentçš„ï¼ˆå› ä¸ºéƒ½æ˜¯åŒæ ·çš„ç»å¯¹ä½ç½®ï¼‰ã€‚ å› æ­¤æˆ‘ä»¬å¼•å…¥ç›¸å¯¹ä½ç½®ä¿¡æ¯Rï¼Œå…¶ä¸­ç¬¬iè¡Œä»£è¡¨ç›¸å¯¹è·ç¦»içš„encodingã€‚ å…·ä½“æ¥è¯´ï¼š é¦–å…ˆæˆ‘ä»¬åœ¨ä¼ ç»Ÿçš„è®¡ç®—$query_i$å’Œ$key_j$çš„attentionåˆ†æ•°æ—¶ï¼Œå¯ä»¥æ‹†è§£æˆï¼š ï¼ˆå› ä¸ºquery=(embedding E +positional embedding Uï¼‰ï¼Œkeyä¹Ÿä¸€æ ·ï¼Œå°†å¼å­æ‹†å¼€å°±èƒ½è·å¾—ä¸Šè¿°å¼å­) æˆ‘ä»¬å°†è¯¥å¼å­è¿›è¡Œä¿®æ”¹ï¼š ç¬¬ä¸€ï¼Œå°†å‡ºç°äº†absolute positional embedding $U$çš„åœ°æ–¹ï¼Œç»Ÿç»Ÿæ”¹æˆ$R_{i-j}$ï¼Œä¹Ÿå³åœ¨bå’Œdé¡¹ã€‚å…¶ä¸­è¿™é‡Œçš„Rå’ŒåŸç‰ˆçš„Transformerçš„ä½ç½®è®¡ç®—å…¬å¼ç›¸åŒã€‚ ç¬¬äºŒï¼Œåœ¨cé¡¹ä¸­ï¼Œä½¿ç”¨ä¸€ä¸ª$u$æ›¿ä»£äº†$U_i W_q$ï¼Œè¿™ä¸€é¡¹åŸæœ¬çš„æ„ä¹‰åœ¨äºï¼Œ$query_i$çš„positional encodingå¯¹$key_j$çš„embeddingè¿›è¡Œattentionï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¯¥é¡¹è¡¨ç°äº†$query_i$ä½ç½®å¯¹å“ªäº›$key_j$çš„å†…å®¹æœ‰å…´è¶£ï¼Œä½œè€…è®¤ä¸ºqueryä¸ç®¡åœ¨å“ªä¸ªä½ç½®ä¸Šéƒ½æ˜¯ä¸€æ ·çš„ï¼Œä¹Ÿå°±æ˜¯è¯´queryçš„ä½ç½®ä¿¡æ¯åº”å½“æ²¡å½±å“ï¼Œæ‰€ä»¥ç»Ÿç»Ÿæ›¿æ¢æˆä¸€ä¸ªå¯å­¦ä¹ çš„$u$ã€‚åŸºäºç±»ä¼¼çš„ç†ç”±dé¡¹æ¢æˆäº†$v$ã€‚ ç¬¬ä¸‰ï¼Œå°†$W_k$ç»†åˆ†æˆäº†ä¸¤ä¸ª$W_{k,E}$å’Œ$W_{k,R}$ã€‚è¿™æ˜¯æ ¹æ®queryæ˜¯Embeddingè¿˜æ˜¯positional encodingæ¥åŒºåˆ†çš„ã€‚for producing the content-based key vectors and location-based key vectors respectively æ¯ä¸€é¡¹ç°åœ¨éƒ½æœ‰äº†ä¸åŒçš„æ„ä¹‰ï¼š Under the new parameterization, each term has an intuitive meaning: term (a) represents contentbased addressing, term (b) captures a content-dependent positional bias, term (c) governs a global content bias, and (d) encodes a global positional bias. æœ€åæ€»ç»“ä¸€ä¸‹æ•´ä¸ªç»“æ„ï¼š ä¸åŸç‰ˆTransformerä¸åŒçš„æ˜¯ï¼ŒTransformer-XLåœ¨æ¯ä¸€å±‚éƒ½æ·»åŠ äº†ä½ç½®ä¿¡æ¯ã€‚ 4ï¸âƒ£[Trellis Networks for Sequence Modeling]ä¸€ç§ç»“åˆRNNå’ŒCNNçš„è¯­è¨€å»ºæ¨¡æ–¹å¼ã€‚ æœ€å°çš„å•å…ƒç»“æ„ï¼š ä¹Ÿå³ï¼š æ¥ä¸‹æ¥å†å¤„ç†éçº¿æ€§ï¼š å› ä¸ºæ¯å±‚éƒ½è¦è¾“å…¥xï¼Œä¸”Wæ˜¯å…±äº«çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æå‰è®¡ç®—å¥½è¿™ä¸€é¡¹ï¼Œåé¢ç›´æ¥ç”¨å³å¯ã€‚ æœ€ç»ˆåœ¨å®ç°çš„æ—¶å€™æ˜¯ï¼š æ€»ä½“æ¡†æ¶ï¼š ä¸TCNï¼ˆtemporal convolution networkï¼‰ä¸åŒä¹‹å¤„ï¼šâ‘ filter weightä¸ä»…åœ¨time stepä¹‹é—´å…±äº«ï¼Œåœ¨ä¸åŒå±‚ä¹‹é—´ä¹Ÿå…±äº«ï¼›â‘¡åœ¨æ¯ä¸€å±‚éƒ½æ·»åŠ äº†è¾“å…¥ ä¼˜ç‚¹ï¼šå…±äº«äº†Wï¼Œæ˜¾è‘—å‡å°‘äº†å‚æ•°ï¼›â€˜Weight tying can be viewed as a form of regularization that can stabilize trainingâ€™ æˆ‘ä»¬è¿˜å¯ä»¥æ‰©å±•è¯¥ç½‘ç»œï¼Œå¼•å…¥gateï¼š 5ï¸âƒ£[Towards Decoding as Continuous Optimisation in Neural Machine Translation]ä¸€ç¯‡å¾ˆæœ‰æ„æ€çš„paperã€‚ç”¨äºNMT decodeçš„inferenceé˜¶æ®µã€‚è¿™ç¯‡æœ‰ä¸€å®šçš„éš¾åº¦ï¼Œä»¥ä¸‹åªæ˜¯æˆ‘çš„ç†è§£ã€‚ æ€æƒ³Motivationï¼šNMTä¸­çš„decode inferenceé˜¶æ®µï¼Œé€šå¸¸éƒ½æ˜¯ä»å·¦åˆ°å³çš„ï¼Œè¿™æ ·æœ‰ä¸ªç¼ºç‚¹ï¼Œå°±æ˜¯æ•´ä½“çš„targetä¹‹é—´çš„ä¾èµ–æ˜¯æ²¡æœ‰è¢«å……åˆ†åˆ©ç”¨åˆ°çš„ï¼Œæ¯”å¦‚è¯´ç”Ÿæˆçš„è¯çš„å³è¾¹æ˜¯æ²¡æœ‰ç”¨åˆ°çš„ã€‚é‚£ä¹ˆæˆ‘ä»¬ä¸ºä»€ä¹ˆä¸ç›´æ¥å…¨éƒ¨ç”Ÿæˆå‘¢ï¼Ÿç„¶åä¸æ–­æ›´æ–°ã€‚ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬å°†ç¦»æ•£ï¼ˆdiscreteï¼‰çš„decodeè¿‡ç¨‹å˜æˆä¸€ä¸ªè¿ç»­çš„è¿‡ç¨‹ï¼ˆcontinuous optimizationï¼‰ã€‚ å‡è®¾æˆ‘ä»¬å·²ç»è®­ç»ƒå¥½æ¨¡å‹ï¼Œç»™å®šä¸€ä¸ªå¥å­ï¼Œæˆ‘ä»¬è¦ç¿»è¯‘æˆç›®æ ‡å¥å­ï¼Œä¸”å‡è®¾æˆ‘ä»¬å·²çŸ¥è¦ç”Ÿæˆçš„å¥å­é•¿åº¦æ˜¯lï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ‰ï¼šæˆ‘ä»¬è¦æ‰¾åˆ°ä¸€ä¸ªæœ€ä¼˜çš„åºåˆ—$y$ï¼Œä½¿å¾—$-log$æœ€å°ã€‚ ç­‰ä»·äºï¼šå…¶ä¸­$\widetilde{y}_i$æ˜¯one-hotã€‚å…¶å®è¿™é‡Œå°±æ˜¯å‡è®¾æœ‰è¿™ä¹ˆä¸€ä¸ªground truthï¼Œä½†å®é™…ä¸Šæ˜¯æ²¡æœ‰çš„ã€‚ æˆ‘ä»¬å°†$\widetilde{y}_i$æ˜¯one-hotè¿™ä¸ªæ¡ä»¶æ”¾å®½ä¸€äº›ï¼Œå˜æˆæ˜¯ä¸€ä¸ªæ¦‚ç‡å•çº¯å‹ï¼ˆå…¶å®å°±æ˜¯æ‰€æœ‰å…ƒç´ åŠ èµ·æ¥æ˜¯1ï¼Œä¸”éƒ½å¤§äºç­‰äº0ï¼‰ã€‚ é‚£ä¹ˆå°±å˜æˆäº†ï¼š è¿™ä¸ªæ”¹å˜çš„æœ¬è´¨æ˜¯ï¼š å°±æ˜¯è¯´åŸæ¥one-hotçš„$\widetilde{y}_i$ç”Ÿæˆåä¸¢åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œå–äº†ä¸€ä¸ªè¯å‘é‡ï¼Œæ¥ç€è®¡ç®—ã€‚ç°åœ¨æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ$\hat{y}_i$ä¸¢è¿›æ¥ï¼Œå°±ç›¸å½“äºå–äº†å¤šä¸ªè¯å‘é‡çš„åŠ æƒæ±‚å’Œã€‚ åœ¨åˆ©ç”¨ä¸‹è¿°çš„æ›´æ–°ç®—æ³•æ›´æ–°å®Œ$\hat{y}_i$ä¹‹åï¼Œå¯¹äºæ¯ä¸ªæ—¶é—´æ­¥tï¼Œæˆ‘ä»¬æ‰¾$\hat{y}_i$ä¸­å…ƒç´ æœ€å¤§çš„å€¼å¯¹åº”çš„è¯ä½œä¸ºç”Ÿæˆçš„è¯ã€‚ æœ‰ä¸¤ç§æ–¹æ³•Exponentiated Gradient å’Œ SGDã€‚å®é™…ä¸Šæ–¹æ³•å€’åœ¨å…¶æ¬¡äº†ï¼Œä¸»è¦æ˜¯å‰é¢æ‰€è¿°çš„continuous optimizationè¿™ç§æ€æƒ³ã€‚ ç®—æ³•Exponentiated Gradientå…·ä½“è§è®ºæ–‡ SGDå› ä¸ºæˆ‘ä»¬è¦ä¿è¯å•çº¯å½¢çš„çº¦æŸä¸å˜ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥ä¸€ä¸ªrï¼Œç„¶ååšä¸€ä¸ªsoftmax åº”ç”¨è¿™ç§è¿ç»­decodeå¯ä»¥ç”¨åœ¨å“ªï¼Ÿ Bidirectional Ensembleå¯ä»¥å¾ˆæ–¹ä¾¿åœ°è¿›è¡ŒåŒå‘çš„ç”Ÿæˆï¼š è€Œåœ¨ä¼ ç»Ÿçš„æ–¹æ³•ä¸­æ²¡åŠæ³•ï¼ˆå¾ˆéš¾ï¼‰åšåˆ° Bilingual Ensembleæˆ‘ä»¬å¸Œæœ›æºè¯­è¨€åˆ°ç›®æ ‡è¯­è¨€å’Œç›®æ ‡åˆ°æºè¯­è¨€éƒ½ç”Ÿæˆå¾—å¥½ é—®é¢˜$\hat{y}_i$çš„åˆå§‹åŒ–å¾ˆé‡è¦ï¼Œä¸€ä¸å°å¿ƒå°±ä¼šé™·å…¥local minimaï¼›ç”Ÿæˆçš„é€Ÿåº¦æ…¢ 6ï¸âƒ£[Universal Language Model Fine-tuning for Text Classiï¬cation]å’ŒELMoã€OpenAI GPTä¸€æ ·ï¼Œéƒ½æ˜¯é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè¿ç§»åˆ°å…¶ä»–ä»»åŠ¡ä¸Šï¼ˆè¿™é‡Œæ˜¯åˆ†ç±»ä»»åŠ¡ï¼‰ã€‚å¯ä»¥åœ¨éå¸¸å°çš„æ•°æ®é›†ä¸Šæœ‰å¾ˆå¥½çš„æ•ˆæœã€‚ è´¡çŒ®ï¼š è¿ç§»å­¦ä¹ æ¨¡å‹ULMFiT æå‡ºå‡ ç§trickï¼šdiscriminative ï¬ne-tuning, slanted triangular learning rates,gradual unfreezing ï¼Œæœ€å¤§ä¿è¯çŸ¥è¯†çš„ä¿ç•™ã€‚ æ¨¡å‹ ä¸‰éƒ¨æ›²ï¼š é€šç”¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ ç›®æ ‡ä»»åŠ¡çš„è¯­è¨€æ¨¡å‹fine-tuning ç›®æ ‡ä»»åŠ¡çš„åˆ†ç±»fine-tuning trickDiscriminative ï¬ne-tuningMotivationï¼šä¸åŒå±‚æœ‰ä¸åŒçš„ä¿¡æ¯ï¼›åº”å½“fine-tune ä¸åŒç¨‹åº¦ï¼Œä¹Ÿå³ä½¿ç”¨ä¸åŒçš„learning rateã€‚ ä½œè€…å‘ç°ä¸Šä¸€å±‚çš„å­¦ä¹ ç‡æ˜¯ä¸‹ä¸€å±‚çš„2.6å€æ—¶æ•ˆæœæ¯”è¾ƒå¥½ã€‚ Slanted triangular learning rates (STLR) å…·ä½“å…¬å¼ï¼š Gradual unfreezingä»é¡¶å±‚åˆ°åº•å±‚ï¼Œä¸€æ­¥ä¸€æ­¥unfreezeï¼Œä¹Ÿå³ä»ä¸Šåˆ°ä¸‹fine-tuneã€‚è¿™æ˜¯å› ä¸ºæœ€ä¸Šä¸€å±‚æœ‰æœ€å°‘çš„general knowledgeã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>TCN</tag>
        <tag>Transformer-XL</tag>
        <tag>Trellis Networks</tag>
        <tag>continuous decoding</tag>
        <tag>ULMFiT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡1]]></title>
    <url>%2F2018%2F10%2F14%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%871%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Learned in Translation: Contextualized Word Vectors]CoVeæ˜¯ç¬¬ä¸€ä¸ªå¼•å…¥åŠ¨æ€è¯å‘é‡çš„æ¨¡å‹ã€‚Motivationï¼šç¿»è¯‘æ¨¡å‹èƒ½å¤Ÿä¿å­˜æœ€å¤šçš„ä¿¡æ¯ï¼Œå› ä¸ºå¦‚æœä¿å­˜ä¿¡æ¯ä¸å¤Ÿå¤šï¼Œdecoderæ¥æ”¶åˆ°çš„ä¿¡æ¯ä¸è¶³ï¼Œç¿»è¯‘æ•ˆæœå°±ä¸ä¼šå¥½ã€‚ï¼ˆä½†å®é™…ä¸Šï¼Œæˆ‘ä¸ªäººè®¤ä¸ºï¼Œdecoderçš„è¡¨ç°è¿˜å’Œlanguage modelæœ‰å…³ï¼Œå¦‚æœdecoderæ˜¯ä¸€ä¸ªå¥½çš„language modelï¼Œä¹Ÿæœ‰å¯èƒ½ç¿»è¯‘å‡ºä¸é”™çš„ç»“æœï¼‰ åšæ³•ï¼šä½¿ç”¨ä¼ ç»ŸNMTçš„encoder-decoderçš„åšæ³•ç¿»è¯‘æ¨¡å‹ï¼Œåªæ˜¯å°†(bi)LSTMæ‰€å¾—åˆ°çš„éšå±‚çŠ¶æ€è¡¨ç¤ºå–å‡ºæ¥å’Œembeddingæ‹¼æ¥èµ·æ¥ï¼Œä½œä¸ºä¸€ä¸ªè¯çš„è¡¨ç¤ºï¼š w=[GloVe(w); CoVe(w)] 2ï¸âƒ£[Language Modeling with Gated Convolutional Networks]ä½¿ç”¨CNNå¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå»ºæ¨¡ï¼Œæé«˜å¹¶è¡Œæ€§ã€‚ è´¡çŒ®ï¼šä½¿ç”¨äº†CNNè¿›è¡Œlanguage modelå»ºæ¨¡ï¼›æå‡ºäº†ç®€åŒ–ç‰ˆçš„gateæœºåˆ¶åº”ç”¨åœ¨CNNä¸­ã€‚ åšæ³•ï¼š å®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªè¾“å…¥ä¸¤ä¸ªfilterï¼Œå·ç§¯å‡ºæ¥çš„åšä¸€ä¸ªgateçš„æ“ä½œ$H_0 = AâŠ—Ïƒ(B)$ï¼Œæ§åˆ¶æµå‘ä¸‹ä¸€å±‚çš„æ•°æ®ã€‚ ä¸€ä¸ªå°ç»†èŠ‚æ˜¯ï¼Œä¸ºäº†ä¸è®©language modelçœ‹åˆ°ä¸‹ä¸€ä¸ªè¯ï¼Œæ¯ä¸€å±‚åœ¨å¼€å§‹å·ç§¯çš„æ—¶å€™ä¼šåœ¨å·¦è¾¹æ·»åŠ kernel_size-1ä¸ªpaddingã€‚ æ‰©å±•ï¼šå› ä¸ºCNNçš„å¹¶è¡Œæ€§é«˜ï¼Œå¯ä»¥ä½¿ç”¨CNNæ¥å¯¹language modelå»ºæ¨¡æ›¿ä»£ELMoï¼ŒåŒæ ·å¯ä»¥è·å¾—åŠ¨æ€è¯å‘é‡ã€‚è¿™ä¸ªæƒ³æ³•å·²ç»ç”±æå‡ºELMoçš„å›¢é˜Ÿåšå‡ºæ¥å¹¶è¿›è¡Œå¯¹æ¯”äº†ã€‚è®ºæ–‡ï¼šDissecting Contextual Word Embeddings: Architecture and Representation ç›®å‰æ­£åœ¨å¤ç°è¯¥è®ºæ–‡ ã€‚ 3ï¸âƒ£[Attention is All you need]éå¸¸ç»å…¸çš„è®ºæ–‡ã€‚æå‡ºäº†Transformerã€‚ä¸ºäº†è¯»BERTé‡æ¸©äº†ä¸€éã€‚ 4ï¸âƒ£[Improving Language Understanding by Generative Pre-Training]BERTå°±æ˜¯followè¿™ç¯‡æ–‡ç« çš„å·¥ä½œã€‚ä½¿ç”¨Transformeré¢„è®­ç»ƒä¸€ä¸ªlanguage modelè¿›è¡Œè¿ç§»å­¦ä¹ ã€‚ è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤æ­¥ï¼šâ‘ ä½¿ç”¨æœªæ ‡è®°æ•°æ®è®­ç»ƒlanguage modelï¼›â‘¡ä½¿ç”¨æœ‰æ ‡è®°æ•°æ®è¿›è¡Œfine-tune Motivationï¼šELMoæ˜¯è®­ç»ƒå¥½language modelï¼Œç„¶åè·å¾—åŠ¨æ€è¯å‘é‡å†ç”¨åˆ°å…¶ä»–ä»»åŠ¡ä¸Šï¼Œè¿™æ ·å°±ä¼šå¤šäº†å¾ˆå¤šå‚æ•°ã€‚å’ŒELMoä¸åŒçš„æ˜¯ï¼Œè¿™é‡Œä½¿ç”¨ä¸€ä¸ªTransformeræ¨¡å‹è§£å†³å¤šç§ä»»åŠ¡ï¼ˆåˆ©ç”¨è¿ç§»å­¦ä¹ ï¼‰ã€‚ è´¡çŒ®ï¼šä½¿ç”¨Transformerè¿›è¡Œlanguage modelå»ºæ¨¡ï¼›å°è¯•åˆ©ç”¨language modelè¿›è¡Œè¿ç§»å­¦ä¹ è€Œä¸æ˜¯å¦ä¸€ç§æ€è·¯ï¼ˆELMoï¼‰åªæå–è¯å‘é‡ã€‚ â‘ æ— ç›‘ç£å­¦ä¹ language model å…·ä½“åˆ°Transformerå°±æ˜¯ï¼š â‘¡ç›‘ç£å­¦ä¹ ï¼ˆfine-tuneï¼‰æ ¹æ®è¾“å…¥é¢„æµ‹æ ‡ç­¾ å…·ä½“å°±æ˜¯ï¼š å°†ä¸¤ä¸ªä»»åŠ¡ä¸€èµ·è®­ç»ƒï¼Œåˆ™æœ‰ï¼š å¯¹äºä¸åŒä»»åŠ¡ï¼Œå¯¹è¾“å…¥è¿›è¡Œä¸€å®šçš„æ”¹åŠ¨ä»¥é€‚åº”Transformerç»“æ„ï¼š 5ï¸âƒ£[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]åˆ·çˆ†å„æ¦œå•çš„ä¸€ç¯‡ç¥æ–‡ã€‚ä½¿ç”¨Transformeré¢„è®­ç»ƒä¸€ä¸ªlanguage modelè¿›è¡Œè¿ç§»å­¦ä¹ ã€‚ Motivationï¼šä¹‹å‰çš„language modelåªèƒ½æ ¹æ®å‰é¢çš„è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªï¼ˆå³ä½¿ELMoæ˜¯åŒå‘çš„LSTMï¼Œä¹Ÿæ˜¯åˆ†åˆ«è®­ç»ƒä¸€ä¸ªå‰å‘å’Œä¸€ä¸ªåå‘çš„ï¼‰ï¼Œé™åˆ¶äº†åŒå‘çš„contextï¼›å› æ­¤æå‡ºäº†åŒå‘çš„language modelã€‚ åšæ³•ï¼šæ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šâ‘ masked LMï¼šå› ä¸ºä½¿ç”¨äº†ä¸¤è¾¹çš„contextï¼Œè€Œlanguage modelçš„ç›®çš„æ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œè¿™æ ·æ¨¡å‹ä¼šæå‰çœ‹åˆ°ä¸‹ä¸€ä¸ªè¯ï¼Œä¸ºäº†è§£å†³è¯¥é—®é¢˜ï¼Œè®­ç»ƒçš„æ—¶å€™è®²éƒ¨åˆ†è¯maskæ‰ï¼Œæœ€ç»ˆåªé¢„æµ‹è¢«maskæ‰çš„è¯ã€‚ â‘¡Next Sentence Predictionï¼šéšæœº50%ç”Ÿæˆä¸¤ä¸ªå¥å­æ˜¯æœ‰ä¸Šä¸‹å¥å…³ç³»çš„ï¼Œ50%ä¸¤ä¸ªå¥å­æ˜¯æ²¡æœ‰å…³ç³»çš„ï¼Œç„¶ååšåˆ†ç±»ï¼›å…·ä½“æ¥è¯´æ˜¯æ‹¿ç¬¬ä¸€ä¸ªè¯[CLS]ï¼ˆè¿™æ˜¯æ‰‹åŠ¨æ·»åŠ çš„ï¼‰çš„è¡¨ç¤ºï¼Œè¿‡ä¸€ä¸ªsoftmaxå±‚å¾—åˆ°ã€‚ è”åˆè®­ç»ƒè¿™ä¸¤ä¸ªä»»åŠ¡ã€‚ æ¥ä¸‹æ¥æ˜¯é€šè¿‡å…·ä½“çš„ä»»åŠ¡è¿›è¡Œfine-tuneã€‚ä¸€ä¸ªæ¨¡å‹è§£å†³å¤šç§é—®é¢˜ï¼š æœ¬æ–‡è´¡çŒ®ï¼šä½¿ç”¨Transformerè¿›è¡ŒåŒå‘çš„language modelå»ºæ¨¡ã€‚è®ºæ–‡æåˆ°çš„ä¸€äº›ç»†èŠ‚/trickséå¸¸å€¼å¾—è®¨è®ºï¼Œæ¯”å¦‚å¯¹token embeddingæ·»åŠ äº†è®¸å¤šä¿¡æ¯ï¼Œéå¸¸ç®€å•ç²—æš´ã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>CoVe</tag>
        <tag>GCNN</tag>
        <tag>Transformer</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 18:Deep Reinforcement Learning]]></title>
    <url>%2F2018%2F10%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2018%3A%20Deep%20Reinforcement%20Learning%2F</url>
    <content type="text"><![CDATA[è®°å·ï¼š $a$æ˜¯actionï¼Œ$s$å³å¤–éƒ¨çŠ¶æ€stateï¼Œ$\pi_{\theta}(s)$ä¹Ÿå³ä»$s$æ˜ å°„åˆ°$a$çš„å‡½æ•°ï¼›$r$æ˜¯rewardï¼Œæ¯é‡‡å–ä¸€ä¸ªåŠ¨ä½œï¼Œä¼šæœ‰ä¸€ä¸ªrewardï¼Œåˆ™æ€»çš„rewardä¸º R_\theta = \sum_{t=1}^{T} r_tæˆ‘ä»¬ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ‹Ÿåˆ$\pi$ï¼Œä¸€ä¸ªeposide $\tau$æ˜¯ä¸€ä¸ªæµç¨‹ä¸‹æ¥çš„çš„æ‰€æœ‰stateã€actionå’Œrewardçš„é›†åˆã€‚ \tau = \{s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T \}å¦‚æœæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„actorè¿è¡Œnæ¬¡ï¼Œåˆ™æ¯ä¸ª$\tau$ä¼šæœ‰ä¸€å®šçš„æ¦‚ç‡è¢«é‡‡æ ·åˆ°ï¼Œé‡‡æ ·æ¦‚ç‡è®°ä¸º$P(\tau|\theta)$ï¼Œåˆ™æˆ‘ä»¬å¯ä»¥é€šè¿‡é‡‡æ ·çš„æ–¹å¼æ¥å¯¹æœŸæœ›rewardè¿›è¡Œä¼°è®¡ï¼š \overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta) â‰ˆ \frac{1}{N} \sum_{n=1}^{N} R(\tau^n)é‚£ä¹ˆæˆ‘ä»¬æ¥ä¸‹æ¥çš„ç›®æ ‡å°±æ˜¯æœ€å¤§åŒ–æœŸæœ›rewardï¼Œå…¶ä¸­æœŸæœ›rewardæ˜¯ï¼š \overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta)æˆ‘ä»¬åŒæ ·ä½¿ç”¨æ¢¯åº¦ä¸Šå‡ï¼šå…¶ä¸­ä¸$Î¸$ç›¸å…³çš„æ˜¯$P$ï¼Œåˆ™å¯ä»¥å†™æˆï¼š \nabla \overline{R}_\theta = \sum_\tau R(\tau) \nabla P(\tau|\theta)= \sum_\tau R(\tau) P(\tau|\theta) \frac{\nabla P(\tau|\theta)}{P(\tau|\theta)}ç”±äº$\dfrac {d\log \left( f\left( x\right) \right) }{dx}=\dfrac {1}{f\left( x\right) }\dfrac {df(x)}{dx}$ï¼Œåˆ™å‰å¼å¯å†™æˆï¼š \nabla \overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta) \nabla log P(\tau | \theta) â‰ˆ \frac{1}{N} \sum_{n=1}^{N} R(\tau^n) log P(\tau ^n| \theta)å¦‚ä½•æ±‚æ¢¯åº¦ï¼Ÿç”±äºï¼š P(\tau | \theta)=p(s_1)p(a_1|s_1,\theta)p(r_1,s_2|s_1,a_1)p(a_2|s_2,\theta)p(r_2,s_3|s_2,a_2)... \\=p(s_1)\prod_{t=1}^{T}p(a_t|s_t,\theta)p(r_t , s_{t+1}| s_t,a_t)å®é™…ä¸Šï¼Œå…¶ä¸­ä¸æ¢¯åº¦ç›¸å…³çš„åªæœ‰ä¸­é—´é¡¹$p(a_t|s_t,\theta)$ï¼Œè¯¥é¡¹ä¹Ÿå³$Ï€$å‡½æ•°ï¼Œä»stateåˆ°actionçš„æ˜ å°„ã€‚å–logå¹¶æ±‚å¯¼ï¼Œæœ‰ï¼š \nabla log P(\tau | \theta)= \sum_{t=1}^{T} \nabla log p(a_t|s_t,\theta)ä»£å›ï¼Œå› æ­¤æœ€ç»ˆ$\overline{R}_\theta$çš„æ¢¯åº¦ä¸ºï¼š \nabla \overline{R}_\theta = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^n) \nabla log p(a_{t}^n | s_t^n,\theta)æ³¨æ„åˆ°è¯¥å¼å­å‘Šè¯‰æˆ‘ä»¬ï¼Œåº”è€ƒè™‘æ•´ä½“çš„rewardè€Œä¸åº”è¯¥åªè€ƒè™‘æ¯ä¸€æ­¥çš„rewardï¼›å¹¶ä¸”å–logçš„åŸå› å¯ä»¥ç†è§£æˆæ˜¯å¯¹actionå–å½’ä¸€åŒ–ï¼Œå› ä¸ºï¼š \frac{\nabla p(a_t^n | s_t^n,\theta)}{p(a_t^n | s_t^n,\theta)}ä¹Ÿå°±æ˜¯è¯´å¯¹äºé‚£äº›å‡ºç°æ¬¡æ•°è¾ƒå¤šçš„actionï¼Œè¦è¡¡é‡ä»–ä»¬å¯¹rewardçš„çœŸæ­£å½±å“ï¼Œåº”å½“å¯¹ä»–ä»¬å½’ä¸€åŒ–ã€‚ ä¸ºäº†è®©é‚£äº›å‡ºç°å¯èƒ½æ€§è¾ƒä½çš„actionä¸ä¼šå› ä¸ºæ²¡è¢«sampleåˆ°è€Œåœ¨æ›´æ–°åè¢«é™ä½ä»–ä»¬çš„æ¦‚ç‡ï¼Œå¯ä»¥æ·»åŠ ä¸€ä¸ªbaselineï¼Œåªæœ‰è¶…è¿‡$b$çš„rewardæ‰ä¼šå¢åŠ ä»–ä»¬å‡ºç°çš„æ¦‚ç‡ã€‚ \nabla \overline{R}_\theta = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} (R(\tau^n)-b) \nabla log p(a_{t}^n | s_t^n,\theta)]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Deep Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 17:Ensemble]]></title>
    <url>%2F2018%2F10%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2017%3A%20Ensemble%2F</url>
    <content type="text"><![CDATA[Baggingå¯¹äºå¤æ‚æ¨¡å‹ï¼Œå¾€å¾€varianceä¼šå¤§ï¼Œé€šè¿‡å¯¹å¤šä¸ªæ¨¡å‹çš„å¹³å‡ï¼Œèƒ½å¤Ÿå‡å°varianceï¼š baggingçš„æ€æƒ³æ˜¯å¤šæ¬¡æœ‰æ”¾å›åœ°é‡‡æ ·Nâ€™ä¸ªç‚¹ï¼ˆé€šå¸¸Nâ€™=Nï¼‰ï¼Œç„¶åå¯¹é‡‡æ ·çš„å‡ ä¸ªæ•°æ®é›†åˆ†åˆ«è®­ç»ƒä¸€ä¸ªæ¨¡å‹ æµ‹è¯•çš„æ—¶å€™å†å¯¹å‡ ä¸ªæ¨¡å‹è¿›è¡Œå¹³å‡æˆ–æŠ•ç¥¨ BoostingåŸºæœ¬æ€æƒ³æ˜¯å¯¹å‡ ä¸ªå¼±åˆ†ç±»å™¨çº¿æ€§åŠ æƒï¼Œå¾—åˆ°å¼ºåˆ†ç±»å™¨ã€‚åˆ†ç±»å™¨æŒ‰å…ˆåé¡ºåºè®­ç»ƒï¼Œæ¯æ¬¡è®­ç»ƒå®Œï¼Œå¯¹æ–°æ¨¡å‹åˆ†ç±»é”™è¯¯çš„æ•°æ®è¿›è¡Œè°ƒé«˜æƒé‡ï¼Œè€Œæ­£ç¡®çš„æ•°æ®åˆ™é™ä½æƒé‡ã€‚ å¯ä»¥ä¿è¯ï¼šåªè¦åˆ†ç±»å™¨çš„é”™è¯¯ç‡å°äº50%ï¼Œåœ¨boostingåèƒ½å¤Ÿæœ‰100%çš„æ­£ç¡®ç‡ï¼ˆåœ¨è®­ç»ƒé›†ï¼‰ã€‚ è¯æ˜è¿‡ç¨‹ç•¥ã€‚ Ensemble: StackingåŸºæœ¬æ€æƒ³ï¼šä½¿ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒå¤šä¸ªåˆçº§åˆ†ç±»å™¨ï¼Œå°†åˆçº§åˆ†ç±»å™¨çš„è¾“å‡ºä½œä¸ºæ¬¡çº§åˆ†ç±»å™¨çš„è¾“å…¥ï¼Œè·å¾—æœ€ç»ˆçš„è¾“å‡ºã€‚æˆ‘ä»¬åº”å½“ä½¿ç”¨ä¸åŒçš„è®­ç»ƒæ•°æ®æ¥è®­ç»ƒæ¬¡çº§åˆ†ç±»å™¨]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Ensemble</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æµ…è°ˆmaskçŸ©é˜µ]]></title>
    <url>%2F2018%2F10%2F12%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%B5%85%E8%B0%88mask%E7%9F%A9%E9%98%B5%2F</url>
    <content type="text"><![CDATA[ä¸ªäººç›®å‰å¯¹maskçŸ©é˜µçš„ä¸€ç‚¹ç†è§£ã€‚ æ˜¯ä»€ä¹ˆmaskçŸ©é˜µæ˜¯ä»€ä¹ˆï¼Ÿæ˜¯ä¸€ä¸ªç”±0å’Œ1ç»„æˆçš„çŸ©é˜µã€‚ä¸€ä¸ªä¾‹å­æ˜¯ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸­ï¼Œå¥å­çš„é•¿åº¦æ˜¯ä¸ç­‰é•¿çš„ï¼Œä½†å› ä¸ºæˆ‘ä»¬ç»å¸¸å°†å¥å­ç»„æˆmini-batchç”¨ä»¥è®­ç»ƒï¼Œå› æ­¤é‚£äº›é•¿åº¦è¾ƒçŸ­çš„å¥å­éƒ½ä¼šåœ¨å¥å°¾è¿›è¡Œå¡«å……0ï¼Œä¹Ÿå³paddingçš„æ“ä½œã€‚ä¸€ä¸ªmaskçŸ©é˜µå³ç”¨ä»¥æŒ‡ç¤ºå“ªäº›æ˜¯çœŸæ­£çš„æ•°æ®ï¼Œå“ªäº›æ˜¯paddingã€‚å¦‚ï¼šå›¾ç‰‡æ¥æºï¼šTheanoï¼šLSTMæºç è§£æ å…¶ä¸­maskçŸ©é˜µä¸­1ä»£è¡¨çœŸå®æ•°æ®ï¼›0ä»£è¡¨paddingæ•°æ®ã€‚ ä¸ºä»€ä¹ˆä¸ºä»€ä¹ˆè¦ä½¿ç”¨maskçŸ©é˜µï¼Ÿä½¿ç”¨maskçŸ©é˜µæ˜¯ä¸ºäº†è®©é‚£äº›è¢«maskæ‰çš„tensorä¸ä¼šè¢«æ›´æ–°ã€‚è€ƒè™‘ä¸€ä¸ªtensor Tçš„size(a,b)ï¼ŒåŒæ ·å¤§å°çš„maskçŸ©é˜µMï¼Œç›¸ä¹˜åï¼Œåœ¨åå‘å›ä¼ çš„æ—¶å€™åœ¨Tå¯¹åº”maskä¸º0çš„åœ°æ–¹ï¼Œ0çš„æ¢¯åº¦ä»ä¸º0ã€‚å› æ­¤ä¸ä¼šè¢«æ›´æ–°ã€‚ æ€ä¹ˆåšæ¥ä¸‹æ¥ä»‹ç»å‡ ç§ï¼ˆå¯èƒ½ä¸å…¨ï¼‰ä½¿ç”¨maskçš„åœºæ™¯ã€‚ å¯¹è¾“å…¥è¿›è¡Œmaskè€ƒè™‘NLPä¸­å¸¸è§çš„å¥å­ä¸ç­‰é•¿çš„æƒ…å†µã€‚è®¾æˆ‘ä»¬çš„è¾“å…¥çš„batch I:(batch_size,max_seqlen)ï¼Œæˆ‘ä»¬åœ¨è¿‡ä¸€å±‚Embeddingå±‚ä¹‹å‰ï¼Œåœ¨è¿‡äº†ä¸€å±‚Embeddingå±‚ï¼Œåˆ™æœ‰ E:(batch_size,max_seqlen,embed_dim)ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›Embeddingæ˜¯æ›´æ–°çš„(æ¯”å¦‚æˆ‘ä»¬çš„Embeddingæ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œé‚£å½“ç„¶Embeddingéœ€è¦æ›´æ–°)ï¼Œä½†æˆ‘ä»¬åˆä¸å¸Œæœ›paddingæ›´æ–°ã€‚ä¸€ç§æ–¹æ³•å³ä»¤Eä¸Mç›¸ä¹˜ã€‚å…¶ä¸­Mæ˜¯maskçŸ©é˜µ(batch_size,max_seqlen,1) (1æ˜¯å› ä¸ºè¦broadcastï¼‰ï¼Œè¿™æ ·åœ¨Embeddingæ›´æ–°æ¢¯åº¦æ—¶ï¼Œå› ä¸ºmaskçŸ©é˜µçš„å…³ç³»ï¼Œpaddingä½ç½®ä¸Šçš„æ¢¯åº¦å°±æ˜¯0ã€‚å½“ç„¶åœ¨Pytorchä¸­è¿˜å¯ä»¥ç›´æ¥æ˜¾å¼åœ°å†™ï¼š1self.embedding = nn.Embedding(vocab_size, embed_dim,padding_idx=0) è€Œæ­¤æ—¶åº”å½“å°†paddingæ˜¾å¼æ·»åŠ åˆ°è¯å…¸çš„ç¬¬ä¸€ä¸ªã€‚ å¯¹æ¨¡å‹ä¸­é—´è¿›è¡Œmaskä¸€ä¸ªå¾ˆç»å…¸çš„åœºæ™¯å°±æ˜¯dropoutã€‚å¯¹äºå‚æ•°çŸ©é˜µW:(h,w)ï¼ŒåŒæ ·å¤§å°çš„maskçŸ©é˜µMï¼Œåœ¨å‰å‘ä¼ æ’­æ—¶ä»¤Wâ€™=W*Mï¼Œåˆ™åœ¨åå‘ä¼ æ’­æ—¶ï¼ŒMä¸­ä¸º0çš„éƒ¨åˆ†ä¸è¢«æ›´æ–°ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è°ƒç”¨PyTorchä¸­çš„åŒ…nn.Dropout() 123m = nn.Dropout(p=0.2)input = torch.randn(20, 16)output = m(input) å¯¹lossè¿›è¡Œmaskè€ƒè™‘NLPä¸­çš„language modelï¼Œæ¯ä¸ªè¯éƒ½éœ€è¦é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œåœ¨ä¸€ä¸ªbatchä¸­å¥å­æ€»æ˜¯æœ‰é•¿æœ‰çŸ­ï¼Œå¯¹äºä¸€ä¸ªçŸ­å¥ï¼Œæ­¤æ—¶åœ¨è®¡ç®—lossçš„æ—¶å€™ï¼Œä¼šå‡ºç°è¿™æ ·çš„åœºæ™¯ï¼š&lt;pad&gt;è¯è¦é¢„æµ‹ä¸‹ä¸€ä¸ª&lt;pad&gt;è¯ã€‚ä¸¾ä¸ªä¾‹å­ï¼šä¸‰ä¸ªå¥å­[a,b,c,d],[e,f,g],[h,i]ï¼Œåœ¨ç»„æˆbatchåï¼Œä¼šå˜æˆXï¼š a b c d e f g &lt;pad&gt; h i &lt;pad&gt; &lt;pad&gt; Yï¼š b c d &lt;pad&gt; f g &lt;eos&gt; &lt;pad&gt; i &lt;eos&gt; &lt;pad&gt; &lt;pad&gt; Xæ˜¯è¾“å…¥ï¼ŒYæ˜¯é¢„æµ‹ã€‚é‚£ä¹ˆä»ç¬¬ä¸‰è¡Œå¯ä»¥çœ‹å‡ºï¼Œ&lt;pad&gt;åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ª&lt;pad&gt;ã€‚è¿™æ˜¾ç„¶æ˜¯æœ‰é—®é¢˜çš„ã€‚ä¸€ç§è§£å†³æ–¹æ¡ˆå°±æ˜¯ä½¿ç”¨maskçŸ©é˜µï¼Œåœ¨lossçš„è®¡ç®—æ—¶ï¼Œå°†é‚£äº›æœ¬ä¸åº”è¯¥è®¡ç®—çš„maskæ‰ï¼Œä½¿å¾—å…¶lossä¸º0ï¼Œè¿™æ ·å°±ä¸ä¼šåå‘å›ä¼ äº†ã€‚å…·ä½“å®è·µï¼šåœ¨PyTorchä¸­ï¼Œä»¥CrossEntropyä¸ºä¾‹ï¼š 12class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=â€™elementwise_meanâ€™ å¦‚æœreduction=Noneåˆ™ä¼šè¿”å›ä¸€ä¸ªä¸è¾“å…¥åŒæ ·å¤§å°çš„çŸ©é˜µã€‚åœ¨ä¸maskçŸ©é˜µç›¸ä¹˜åï¼Œå†å¯¹æ–°çŸ©é˜µè¿›è¡Œmeanæ“ä½œã€‚åœ¨PyTorchå®è·µä¸Šè¿˜å¯ä»¥å¯ä»¥è¿™ä¹ˆå†™ï¼š 123masked_outputs = torch.masked_select(dec_outputs, mask)masked_targets = torch.masked_select(targets, mask)loss = my_criterion(masked_outputs, masked_targets) å¦ä¸€ç§æ›´ä¸ºç®€å•çš„è§£å†³æ–¹æ¡ˆæ˜¯ï¼Œç›´æ¥åœ¨CrossEntropyä¸­è®¾ignore_index=0ï¼Œè¿™æ ·ï¼Œåœ¨è®¡ç®—lossçš„æ—¶å€™ï¼Œå‘ç°target=0æ—¶ï¼Œä¼šè‡ªåŠ¨ä¸å¯¹å…¶è¿›è¡Œlossçš„è®¡ç®—ã€‚å…¶æœ¬è´¨å’ŒmaskçŸ©é˜µæ˜¯ä¸€è‡´çš„ã€‚ æ€»ç»“maskçŸ©é˜µå¯ä»¥ç”¨åœ¨ä»»ä½•åœ°æ–¹ï¼Œåªè¦å¸Œæœ›ä¸ä¹‹ç›¸ä¹˜çš„tensorç›¸å¯¹åº”çš„åœ°æ–¹ä¸æ›´æ–°å°±å¯ä»¥è¿›è¡Œmaskæ“ä½œã€‚]]></content>
      <tags>
        <tag>ä»£ç å®è·µ</tag>
        <tag>maskçŸ©é˜µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ·±åº¦ç‚¼ä¸¹tricksåˆé›†]]></title>
    <url>%2F2018%2F10%2F07%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%B7%B1%E5%BA%A6%E7%82%BC%E4%B8%B9tricks%E5%90%88%E9%9B%86%2F</url>
    <content type="text"><![CDATA[é•¿æœŸæ›´æ–°ã€‚å—çŸ¥ä¹æ·±åº¦ç‚¼ä¸¹çš„å¯å‘ï¼Œä»¥åŠä¸ªäººåœ¨å®è·µè¿‡ç¨‹å’Œé˜…è¯»ä¸­ä¼šæ¥è§¦åˆ°ä¸€äº›tricksï¼Œè®¤ä¸ºæœ‰å¿…è¦åšä¸€ä¸ªåˆé›†ï¼Œå°†ä¸€äº›å¯èƒ½æœ‰ç”¨çš„tricksåšè®°å½•ã€‚æœ‰å®è·µè¿‡çš„ä¼šç‰¹åˆ«æ ‡æ³¨ã€‚ è°ƒå‚æŠ€å·§æ•°æ®å¢å¼ºé¢„å¤„ç†1ï¸âƒ£zero-center[9]å°†æ•°æ®ä¸­å¿ƒåŒ– åˆå§‹åŒ–1ï¸âƒ£Xavier initialization[7]æ–¹æ³•é€‚ç”¨[9]äºæ™®é€šæ¿€æ´»å‡½æ•°(tanh,sigmoid)ï¼šscale = np.sqrt(3/n) 2ï¸âƒ£He initialization[8]æ–¹æ³•é€‚ç”¨[9]äºReLUï¼šscale = np.sqrt(6/n) 3ï¸âƒ£Batch normalization[10]4ï¸âƒ£RNN/LSTM init hidden stateHinton[3]æåˆ°å°†RNN/LSTMçš„åˆå§‹hidden stateè®¾ç½®ä¸ºå¯å­¦ä¹ çš„weight è®­ç»ƒæŠ€å·§1ï¸âƒ£Gradient Clipping[5,6]2ï¸âƒ£learning rateåŸåˆ™ï¼šå½“validation losså¼€å§‹ä¸Šå‡æ—¶ï¼Œå‡å°‘å­¦ä¹ ç‡ã€‚[1]Time/Drop-based/Cyclical Learning Rate 3ï¸âƒ£batch size[2]ä¸­è¯¦ç»†è®ºè¿°äº†å¢åŠ batch sizeè€Œä¸æ˜¯å‡å°learning rateèƒ½å¤Ÿæå‡æ¨¡å‹è¡¨ç°ã€‚ä¿æŒå­¦ä¹ ç‡ä¸å˜ï¼Œæé«˜batch sizeï¼Œç›´åˆ°batch size~è®­ç»ƒé›†/10ï¼Œæ¥ä¸‹æ¥å†é‡‡ç”¨å­¦ä¹ ç‡ä¸‹é™çš„ç­–ç•¥ã€‚ Reference[1]How to make your model happy again â€” part 1 [2]Donâ€™t Decay the Learning Rate, Increase the Batch Size [3]CSC2535 2013: Advanced Machine Learning Lecture 10 Recurrent neural networks [4]https://zhuanlan.zhihu.com/p/25110150 [5]On the difficulty of training Recurrent Neural Networks [6]Language Modeling with Gated Convolutional Networks [7]Understanding the difficulty of training deep feedforward neural networks [8]Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification [9]çŸ¥ä¹ï¼šä½ æœ‰å“ªäº›deep learningï¼ˆrnnã€cnnï¼‰è°ƒå‚çš„ç»éªŒï¼Ÿ [10]Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift]]></content>
      <tags>
        <tag>è°ƒå‚</tag>
        <tag>tricks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯11]]></title>
    <url>%2F2018%2F10%2F07%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D11%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£èµ‹å¾—å¤åŸè‰é€åˆ«[å”] ç™½å±…æ˜“ç¦»ç¦»åŸä¸Šè‰ï¼Œä¸€å²ä¸€æ¯è£ã€‚é‡ç«çƒ§ä¸å°½ï¼Œæ˜¥é£å¹åˆç”Ÿã€‚è¿œèŠ³ä¾µå¤é“ï¼Œæ™´ç¿ æ¥è’åŸã€‚åˆé€ç‹å­™å»ï¼Œè‹è‹æ»¡åˆ«æƒ…ã€‚ è‹è‹ï¼ˆqÄ«ï¼‰ï¼šå½¢å®¹è‰æœ¨é•¿å¾—èŒ‚ç››çš„æ ·å­ã€‚ http://m.xichuangzhu.com/work/57b8a5371532bc005b99da51]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRMLç¬¬ä¸‰ç«  å›å½’çš„çº¿æ€§æ¨¡å‹]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FPRML%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%9B%9E%E5%BD%92%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[çº¿æ€§åŸºå‡½æ•°æ¨¡å‹ åç½®-â½…å·®åˆ†è§£ è´å¶æ–¯çº¿æ€§å›å½’ è´å¶æ–¯æ¨¡å‹â½è¾ƒ è¯æ®è¿‘ä¼¼]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 16:SVM]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2016%3A%20SVM%2F</url>
    <content type="text"><![CDATA[Hinge Loss+kernel method = SVM Hinge LossSVMä¸logistic regressionçš„åŒºåˆ«å³åœ¨äºloss functionçš„ä¸åŒï¼Œlogisticæ˜¯cross entropyï¼Œè€ŒSVMæ˜¯hinge loss ä¹Ÿå³å¦‚æœåˆ†ç±»é—´éš”å¤§äº1ï¼Œåˆ™ $L(m_i)=max(0,1âˆ’m_i(w))$ï¼Œåˆ™æŸå¤±ä¸º0ã€‚å› æ­¤SVMæ›´å…·é²æ£’æ€§ï¼Œå› ä¸ºå¯¹ç¦»ç¾¤ç‚¹ä¸æ•æ„Ÿã€‚ å¯¹äºlinear SVMï¼š å®šä¹‰å‡½æ•° $f(x)=\sum_i w_i x_i +b=w^T x$ å®šä¹‰æŸå¤±å‡½æ•° $L(f)=\sum_n l(f(x^n),\hat{y}^n)+\lambda ||w||_2$ï¼Œå…¶ä¸­$l(f(x^n),\hat{y}^n)=max(0,1-\hat{y}^n f(x))$ æ¢¯åº¦ä¸‹é™æ±‚è§£ï¼ˆçœç•¥äº†æ­£åˆ™åŒ–ï¼‰ \frac{\partial{l(f(x^n),\hat{y}^n})}{\partial{w_i}}= \frac{\partial{l(f(x^n),\hat{y}^n})}{\partial{f(x^n)}} \frac{\partial{f(x^n)}}{\partial{w_i}} x_i^n è€Œ f(x^n)=w^T \cdot x^n \frac{\partial{max(0,1-\hat{y}^n f(x^n)})}{\partial{f(x^n)}}= \left\{ \begin{array}{**lr**} -\hat{y}^n & if \hat{y}^n f(x^n)]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 15:Transfer Learning]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2015%3A%20Transfer%20Learning%2F</url>
    <content type="text"><![CDATA[Model Fine-tuningå‡è®¾æˆ‘ä»¬æœ‰å¾ˆå¤šçš„source data $(x^s,y^s )$ï¼Œä¸ä»»åŠ¡ç›¸å…³çš„target data $(x^t,y^t )$ å¾ˆå°‘ã€‚æˆ‘ä»¬åˆ©ç”¨source dataè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œç„¶åç”¨target dataæ¥fine tuneæ¨¡å‹ã€‚ conservative training æˆ‘ä»¬å¯ä»¥ç”¨source dataè®­ç»ƒå¥½çš„æ¨¡å‹çš„weightä½œä¸ºæ–°çš„æ¨¡å‹çš„weightï¼Œç„¶åè®¾å®šä¸€äº›é™åˆ¶ï¼Œæ¯”å¦‚source dataä½œä¸ºè¾“å…¥çš„outputåº”å’Œtarget dataä½œä¸ºè¾“å…¥çš„outputå°½é‡ç›¸ä¼¼ï¼Œæˆ–è€…å‚æ•°å°½é‡ç›¸ä¼¼ç­‰ã€‚ layer transferä¹Ÿå°±æ˜¯æ–°æ¨¡å‹æœ‰å‡ å±‚æ˜¯ç›´æ¥copyæ—§æ¨¡å‹çš„ï¼Œåªè®­ç»ƒå…¶å®ƒå±‚ã€‚æ³¨æ„åˆ°ä¸åŒä»»åŠ¡æ‰€åº”copyçš„å±‚æ˜¯ä¸åŒçš„ï¼Œè¯­éŸ³ä»»åŠ¡æœ€åå‡ å±‚æ•ˆæœå¥½ï¼Œå›¾åƒè¯†åˆ«å‰é¢å‡ å±‚æ•ˆæœå¥½ Multitask Learningä¸åŒä»»åŠ¡ä¹‹é—´å…±äº«ç›¸åŒçš„ä¸­é—´å±‚ï¼Œå¦‚ï¼š è¿˜æœ‰ä¸€ç§progressive neural networksï¼šé¦–å…ˆè®­ç»ƒå¥½ç¬¬ä¸€ä¸ªä»»åŠ¡çš„æ¨¡å‹ï¼Œç„¶ååœ¨è®­ç»ƒç¬¬äºŒä¸ªæ¨¡å‹çš„æ—¶å€™å°†ç¬¬ä¸€ä¸ªæ¨¡å‹çš„éšå±‚åŠ å…¥åˆ°ç¬¬äºŒä¸ªæ¨¡å‹çš„éšå±‚ä¸­ï¼›è®­ç»ƒç¬¬ä¸‰ä¸ªæ¨¡å‹åˆ™å°†ç¬¬äºŒä¸ªå’Œç¬¬ä¸€ä¸ªæ¨¡å‹çš„éšå±‚åŠ å…¥åˆ°ç¬¬ä¸‰ä¸ªæ¨¡å‹çš„éšå±‚ä¸­ï¼Œä»¥æ­¤ç±»æ¨ Domain-adversarial trainingsource dataæ˜¯æœ‰æ ‡ç­¾çš„ï¼Œè€Œtarget dataæ˜¯æ— æ ‡ç­¾çš„ï¼Œéƒ½å±äºåŒä¸€ä¸ªä»»åŠ¡ï¼Œä½†æ•°æ®æ˜¯mismatchçš„ï¼Œå¦‚ï¼š å› ä¸ºNNçš„éšå±‚å¯ä»¥ç†è§£æˆæ˜¯åœ¨æŠ½å–å›¾åƒçš„ç‰¹å¾ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿåœ¨è®­ç»ƒNNçš„è¿‡ç¨‹ä¸­å»æ‰source dataçš„ä¸€äº›domain specificçš„ç‰¹æ€§ï¼Œè¿™æ ·å°±å¯ä»¥ç”¨åœ¨target dataä¸Šäº†ã€‚å› æ­¤æˆ‘ä»¬åœ¨feature exactoråé¢è¿æ¥ä¸¤ä¸ªæ¨¡å—ï¼š ä¸€æ–¹é¢æˆ‘ä»¬å¸Œæœ›æŠ½å–çš„ç‰¹å¾èƒ½å¤Ÿä½¿å¾—åˆ†ç±»å™¨æ­£ç¡®åœ°åˆ†ç±»ï¼Œå¦ä¸€æ–¹é¢æˆ‘ä»¬å¸Œæœ›è¿™äº›ç‰¹å¾èƒ½å¤Ÿè®©domain classifierèƒ½å¤Ÿæ— æ³•è¯†åˆ«ç‰¹å¾æ˜¯ä»å“ªäº›dataæŠ½å–å¾—åˆ°çš„ï¼Œè¿™æ ·å¾—åˆ°çš„ç‰¹å¾å°±æ˜¯è¢«å»æ‰domain specificç‰¹å¾çš„ã€‚ å…·ä½“è®­ç»ƒï¼š Zero-shot Learningsource dataæœ‰æ ‡ç­¾ï¼Œtarget dataæ— æ ‡ç­¾ï¼Œä½†ä»»åŠ¡ä¸åŒï¼Œå¦‚ï¼š Representing each class by its attributesä¸€ç§æ–¹æ³•æ˜¯å°†æ¯ä¸€ä¸ªç±»éƒ½ç”¨ç‰¹å¾è¡¨ç¤ºï¼Œä½†ç‰¹å¾è¦è¶³å¤Ÿä¸°å¯Œï¼š åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œè¾“å…¥æ˜¯å›¾ç‰‡ï¼Œè¾“å‡ºåˆ™æ˜¯è¿™äº›ç‰¹å¾ï¼šè¿™æ ·åœ¨å°†target dataæ”¾å…¥è®­ç»ƒå¥½çš„NNåä¹Ÿä¼šå¾—åˆ°ä¸€ä¸ªè¿™æ ·çš„attributeï¼ŒæŸ¥è¡¨å³å¯æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ç‰¹å¾å¯¹åº”çš„ç±»ã€‚ Attribute embeddingå¦‚æœç‰¹å¾ç»´åº¦å¤ªé«˜ï¼Œä¹Ÿå¯ä»¥å°†ç‰¹å¾å‹ç¼©æˆä¸€ä¸ªå‘é‡è¡¨ç¤ºï¼Œè¿™æ ·åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œè¾“å‡ºåˆ™æ˜¯è¿™æ ·çš„å‘é‡ç‰¹å¾ï¼Œè¾“å…¥target dataï¼Œè¾“å‡ºå‘é‡ç‰¹å¾ï¼Œæ‰¾åˆ°æœ€è¿‘çš„ç‰¹å¾å¯¹åº”çš„ç±»å³å¯ Attribute embedding + word embeddingå¦‚æœæ²¡æœ‰attributeæ•°æ®ï¼Œåˆ©ç”¨word embeddingä¹Ÿå¯ä»¥è¾¾åˆ°ä¸é”™çš„æ•ˆæœã€‚åœ¨zero-shot learningä¸­ï¼Œå…‰æ˜¯è®©ç›¸åŒç±»çš„få’Œgç›¸ä¼¼æ˜¯ä¸å¤Ÿçš„ï¼Œè¿˜åº”è¯¥è®©ä¸åŒçš„få’Œgå°½é‡è¿œã€‚ f^âˆ—,g^âˆ—=arg min_{(f,g)}â¡âˆ‘_nmax(0,kâˆ’f(x^n )\cdot g(y^n )+max_{(mâ‰ n)} â¡f(x^m )\cdot g(x^m ) )]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Transfer Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 14:Unsupervised Learning:Generation]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2014%3A%20Unsupervised%20Learning%3A%20Generation%2F</url>
    <content type="text"><![CDATA[Component-by-componentå¯¹äºå›¾åƒæ¥è¯´ï¼Œæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªpixelï¼šPixelRNN VAEæ¶æ„ï¼š å…¶ä¸­eæ˜¯å™ªå£°ï¼ŒÏƒæ˜¯æ–¹å·®ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–reconstruction errorï¼Œä»¥åŠä¸€ä¸ªé™åˆ¶ã€‚è¯¥é™åˆ¶çš„ç›®çš„å³é˜²æ­¢Ïƒ=0ï¼Œmæ˜¯æ­£åˆ™åŒ–é¡¹ã€‚ ä¸­é—´çš„æ¨å¯¼ä»¥åŠä¸ºä»€ä¹ˆæ˜¯è¿™æ ·çš„æ¶æ„æˆ‘è¿˜ä¸æ˜¯å¾ˆæ‡‚ï¼Œä¹‹åå†æ›´æ–°ã€‚å®é™…ä¸Šå¯ä»¥è¿™ä¹ˆç†è§£ï¼Œæœ‰å‡ ä¸ªè¦ç‚¹ï¼š é¦–å…ˆæˆ‘ä»¬æ˜¯åŸºäºè¿™ä¹ˆä¸€ä¸ªå‡è®¾ï¼šä¸­é—´çš„codeåº”å½“æ˜¯æœä»æ­£æ€åˆ†å¸ƒçš„ï¼Œè€Œencoderçš„ä½œç”¨å³åœ¨äºæ‹Ÿåˆè¯¥æ­£æ€åˆ†å¸ƒçš„å‡å€¼ä¸æ–¹å·®çš„å¯¹æ•°ï¼ˆå› ä¸ºæ–¹å·®åº”å½“æ’ä¸ºæ­£ï¼Œä½†ç¥ç»ç½‘ç»œçš„è¾“å‡ºå¯èƒ½æœ‰æ­£æœ‰è´Ÿï¼‰ å¦‚æœç”Ÿæˆå‡ºæ¥çš„codeä¸ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼Œä¼šæœ‰ä¸€ä¸ªæƒ©ç½šé¡¹ï¼Œä¹Ÿå°±æ˜¯ä¸Šå›¾çš„constraintï¼ˆå¯ä»¥é€šè¿‡KLæ•£åº¦æ¨å¯¼è·å¾—ï¼‰ æŒ‰ç†è¯´ï¼Œåº”å½“æ˜¯åœ¨ç”Ÿæˆäº†å‡å€¼å’Œæ–¹å·®åï¼Œå®šä¹‰å¥½è¯¥æ­£æ€åˆ†å¸ƒï¼Œç„¶åå†ä»ä¸­é‡‡æ ·ï¼Œä½†æ˜¯è¿™æ ·æ²¡åŠæ³•å›ä¼ æ›´æ–°æ¢¯åº¦ï¼Œå› æ­¤è¿™é‡Œä½¿ç”¨é‡å‚æ•°æŠ€å·§(Reparameterization Trick)ï¼Œä¹Ÿå³ä»$N(\mu,\sigma^2)$ä¸­é‡‡æ ·$Z$ï¼Œç›¸å½“äºä»$N(0,I)$ä¸­é‡‡æ ·$\varepsilon$ï¼Œç„¶åè®©$Z=\mu + \varepsilon \times \mu$ Reference:https://www.sohu.com/a/226209674_500659 VAEçš„ä¸»è¦é—®é¢˜åœ¨äºï¼Œç½‘ç»œåªè¯•å›¾å»è®°ä½è§è¿‡çš„å›¾åƒï¼Œä½†æ²¡æ³•çœŸæ­£å»ç”Ÿæˆæ²¡è§è¿‡çš„å›¾åƒã€‚ Generative Adversarial Network (GAN)GANåŒ…å«ä¸€ä¸ªdiscriminatorå’Œä¸€ä¸ªgeneratorï¼Œgeneratorè¯•å›¾ç”Ÿæˆèƒ½å¤Ÿéª—è¿‡discriminatorçš„æ ·æœ¬ï¼Œè€Œgeneratorè¯•å›¾èƒ½å¤Ÿå°†generatorç”Ÿæˆçš„æ ·æœ¬å’ŒçœŸå®çš„æ ·æœ¬åŒºåˆ†ã€‚ ä¹‹åä¼šæœ‰è¯¦ç»†çš„ä»‹ç»ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>Unsupervised Learning</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Generation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 13:Unsupervised Learning:Auto-encoder]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2013%3A%20Unsupervised%20Learning%3A%20Auto-encoder%2F</url>
    <content type="text"><![CDATA[Auto-encoderç”±ä¸€ä¸ªencoderå’Œä¸€ä¸ªdecoderç»„æˆï¼Œencoderè´Ÿè´£å°†è¾“å…¥è½¬æˆä¸€ä¸ªå‘é‡è¡¨ç¤ºï¼ˆç»´åº¦é€šå¸¸å°äºè¾“å…¥ï¼‰ï¼Œdecoderè´Ÿè´£å°†è¿™æ®µå‘é‡è¡¨ç¤ºæ¢å¤æˆåŸæ¥çš„è¾“å…¥ã€‚é‚£ä¹ˆä¸­é—´çš„codeå°±å¯ä»¥ä½œä¸ºè¾“å…¥çš„ä¸€ä¸ªä½ç»´è¡¨ç¤ºï¼š Auto-encoder for CNN Unpoolingæœ‰ä¸¤ç§æ–¹æ³•ï¼Œä¸€ç§åœ¨poolingçš„æ—¶å€™è®°å½•æœ€å¤§å€¼çš„ä½ç½®ï¼Œåœ¨unpoolingæ—¶åœ¨ç›¸å¯¹ä½ç½®å¡«å……æœ€å¤§å€¼ï¼Œå…¶ä»–ä½ç½®å¡«å……0ï¼›å¦ä¸€ç§ä¸è®°å½•æœ€å¤§å€¼ä½ç½®ï¼Œç›´æ¥åœ¨poolingåŒºåŸŸå…¨éƒ¨å¡«å……æœ€å¤§å€¼ã€‚ Deconvolutionå…¶å®æœ¬è´¨å°±æ˜¯convolutionã€‚ è¿™æ˜¯convolution: æˆ‘ä»¬æœŸå¾…çš„convolutionï¼š å®é™…ä¸Šå°±ç­‰ä»·åœ¨ä¸¤è¾¹åšpaddingï¼Œç„¶åç›´æ¥convolutionï¼š Auto-encoderçš„ç”¨å¤„å¯ä»¥é¢„è®­ç»ƒæ¯ä¸€å±‚çš„DNNï¼š åŒç†å…¶å®ƒå±‚ä¹Ÿæ˜¯ä¸€æ ·ï¼Œæ¯æ¬¡fixä½å…¶ä»–å±‚ç„¶ååšAuto-encoderã€‚é‚£ä¹ˆåœ¨bpçš„æ—¶å€™åªéœ€è¦fine-tuneå°±è¡Œã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>Unsupervised Learning</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Auto-encoder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 12:Unsupervised Learning:Neighbor Embedding]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2012%3A%20Unsupervised%20Learning%3A%20Neighbor%20Embedding%2F</url>
    <content type="text"><![CDATA[Locally Linear Embedding (LLE)ä¸€ç§é™ç»´æ–¹æ³•æ€æƒ³ï¼šå‡è®¾æ¯ä¸ªç‚¹å¯ä»¥ç”±å…¶å‘¨å›´çš„ç‚¹æ¥è¡¨ç¤º æˆ‘ä»¬éœ€è¦æ‰¾åˆ°è¿™æ ·çš„$w_{ij}$ï¼Œä½¿å¾—ï¼š âˆ‘_iâ€–x^iâˆ’âˆ‘_j w_{ij} x^j â€–_2è¿™æ ·åœ¨é™ç»´çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä»ç„¶ä¿æŒxä¹‹é—´çš„è¿™æ ·çš„å…³ç³»: Laplacian Eigenmapsä¸€ç§é™ç»´æ–¹æ³•åŸºæœ¬æ€æƒ³ï¼šå¦‚æœ$x^1$ä¸$x^2$åœ¨é«˜ç»´ç©ºé—´ä¸­ç›¸è¿‘ï¼Œåˆ™é™ç»´åä¹Ÿåº”è¯¥æ¥è¿‘ï¼š S=1/2 âˆ‘_{i,j} w_{i,j} (z^iâˆ’z^j )^2å…¶ä¸­ï¼š å¦‚æœå°†zå…¨è®¾ä¸º0ï¼Œæ˜¾ç„¶Sæœ€å°ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ç»™zä¸€ä¸ªé™åˆ¶ï¼šzåº”å½“å……æ»¡ç©ºé—´ï¼Œä¹Ÿå³å‡å¦‚zæ˜¯Mç»´ï¼Œé‚£ä¹ˆ$\{z^1,z^2â€¦,z^N\}$çš„ç§©åº”è¯¥ç­‰äºM T-distributed Stochastic Neighbor Embedding (t-SNE)ä¹Ÿæ˜¯ä¸€ç§é™ç»´æ–¹æ³•å‰é¢æåˆ°çš„æ–¹æ³•æœ‰ä¸€ä¸ªé—®é¢˜ï¼šåŒä¸€ç±»çš„ç‚¹ç¡®å®èšåœ¨ä¸€èµ·ï¼Œä½†ä¸åŒç±»çš„ç‚¹å¹¶æ²¡æœ‰å°½é‡åˆ†å¼€ t-SNEçš„ä¸»è¦æ€æƒ³ï¼šå°†æ•°æ®ç‚¹æ˜ å°„åˆ°æ¦‚ç‡åˆ†å¸ƒï¼Œæˆ‘ä»¬å¸Œæœ›é™ç»´å‰å’Œé™ç»´åï¼Œæ•°æ®åˆ†å¸ƒçš„æ¦‚ç‡åº”å½“å°½å¯èƒ½ä¸€è‡´ã€‚t-SNEæ„å»ºä¸€ä¸ªé«˜ç»´å¯¹è±¡ä¹‹é—´çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä½¿å¾—ç›¸ä¼¼çš„å¯¹è±¡æœ‰æ›´é«˜çš„æ¦‚ç‡è¢«é€‰æ‹©ï¼Œè€Œä¸ç›¸ä¼¼çš„å¯¹è±¡æœ‰è¾ƒä½çš„æ¦‚ç‡è¢«é€‰æ‹©ã€‚t-SNEåœ¨ä½ç»´ç©ºé—´é‡Œåœ¨æ„å»ºè¿™äº›ç‚¹çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä½¿å¾—è¿™ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´å°½å¯èƒ½çš„ç›¸ä¼¼ã€‚ å¦‚ä½•åšï¼Ÿåœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰ï¼š P(x^j |x^i )=\frac{S(x^i,x^j )}{âˆ‘_{kâ‰ i}S(x^i,x^k )}å…¶ä¸­Sè¡¨ç¤ºiä¸jä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚ åœ¨ä½ç»´ç©ºé—´ä¸­ï¼ŒåŒæ ·æœ‰ï¼š Q(z^j |z^i )=\frac{Sâ€²(z^i,z^j )}{âˆ‘_{kâ‰ i}Sâ€²(z^i,z^k )}ä½¿ç”¨KLæ•£åº¦å»è®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼š L=âˆ‘_i KL(P(âˆ—|x^i )||Q(âˆ—|z^i )) =âˆ‘_iâˆ‘_j P(x^j |x^i )\frac{log P(x^j |x^i )}{Q(z^j |z^i )}t-SNEä¸­ï¼Œé«˜ç»´ç©ºé—´å’Œä½ç»´ç©ºé—´è®¡ç®—ç›¸ä¼¼åº¦çš„å…¬å¼ä¸å¤§ä¸€æ ·ï¼š S(x^i,x^j )=exp(âˆ’â€–x^iâˆ’x^j â€–_2 )Sâ€²(z^i,z^j )=\frac{1}{(1+â€–z^iâˆ’z^j â€–_2)}ä¸¤ä¸ªå…¬å¼çš„å›¾ç¤ºï¼š ä¹Ÿå³ä½ç»´ç©ºé—´ä¼šæ‹‰é•¿è·ç¦»ï¼Œä½¿å¾—è·ç¦»è¿œçš„ç‚¹å°½å¯èƒ½è¢«æ‹‰å¼€ã€‚ t-SNEçš„é—®é¢˜åœ¨äºï¼št-SNEæ— æ³•å¯¹æ–°çš„æ•°æ®ç‚¹è¿›è¡Œé™ç»´ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>Unsupervised Learning</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Neighbor Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 11:Unsupervised Learning:Linear Dimension Reduction]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2011%3A%20Unsupervised%20Learning%3A%20Linear%20Dimension%20Reduction%2F</url>
    <content type="text"><![CDATA[ClusteringK-meansç®—æ³•æ­¥éª¤ï¼š è¿­ä»£æ›´æ–°ä½¿å¾—æœ€åèšç±»ä¸­å¿ƒæ”¶æ•›ã€‚ä½†äº‹å…ˆéœ€è¦å®šå¥½æœ‰å¤šå°‘ç±»ã€‚ Hierarchical Agglomerative Clustering (HAC)è‡ªä¸‹è€Œä¸Šï¼Œæ¯æ¬¡é€‰ä¸¤ä¸ªæœ€è¿‘çš„èšä¸ºä¸€ç±»ï¼Œç›´åˆ°æ‰€æœ‰çš„éƒ½åˆ†æˆä¸€ç±»æœ€åé€‰æ‹©ä¸€ä¸ªé˜ˆå€¼åˆ’åˆ†ï¼Œå¦‚è“è‰²ç»¿è‰²å’Œçº¢è‰²çš„çº¿ Dimension Reductionæ‰¾åˆ°ä¸€ä¸ªæ˜ å°„ï¼Œä½¿å¾—xèƒ½å¤Ÿæ˜ å°„åˆ°ä½ç»´z Principle Component Analysis (PCA)ç›®çš„æ˜¯æ‰¾åˆ°ä¸€ä¸ªç»´åº¦ï¼Œä½¿å¾—æŠ•å½±å¾—åˆ°çš„varianceæœ€å¤§ï¼Œä¹Ÿå³æœ€å¤§ç¨‹åº¦ä¿ç•™æ•°æ®çš„å·®å¼‚æ€§ã€‚ å½¢å¼åŒ–å¯ä»¥å†™æˆï¼ˆä¸€ç»´æƒ…å½¢ï¼‰ï¼š Var(z_1 )=\frac{1}{N} âˆ‘_{z_1}(z_1âˆ’\overline{z_1} )^2å…¶ä¸­ï¼š â€–w^1 â€–_2=1z_1=w^1 \cdot x$\overline{z_1}$è¡¨ç¤ºzçš„å‡å€¼ å‡å¦‚æˆ‘ä»¬è¦æŠ•å½±åˆ°å¤šç»´ï¼Œå…¶ä»–ç»´åº¦ä¹Ÿæœ‰åŒæ ·çš„ç›®æ ‡ã€‚å…¶ä¸­æ¯ä¸ªç»´åº¦ä¹‹é—´éƒ½åº”è¯¥æ˜¯ç›¸äº’æ­£äº¤çš„ã€‚ å¦‚ä½•åšï¼Ÿæ‰¾åˆ°$ \frac{1}{N}âˆ‘(xâˆ’\overline{x} ) (xâˆ’\overline{x})^T$çš„å‰kä¸ªæœ€å¤§çš„ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œç»„åˆèµ·æ¥å³æ˜¯æˆ‘ä»¬è¦æ‰¾çš„$W$ è¯æ˜â€”-Warning of Mathâ€”-ç›®çš„ï¼š$Var(z_1 )=\frac{1}{N} âˆ‘_{z_1}(z_1âˆ’\overline{z_1} )^2 $å…¶ä¸­ $\overline{z_1} =\frac{1}{N} âˆ‘{z_1} = \frac{1}{N} âˆ‘ w^1 \cdot x=w^1\cdot \overline{x}$ æ¨å¯¼ï¼šæ”¹å˜ç¬¦å· $S=Cov(x)$ åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•ï¼Œæœ‰ï¼š$Sw^1=Î±w^1$ç­‰å¼ä¸¤è¾¹å„å·¦ä¹˜$(w^1)^T$ï¼Œæœ‰ï¼š$(w^1 )^T Sw^1=Î±(w^1 )^T w^1=Î±$ ä¹Ÿå³ï¼Œ$Î±$æ˜¯$S$çš„ç‰¹å¾å€¼ï¼Œé€‰æ‹©æœ€å¤§çš„ç‰¹å¾å€¼ï¼Œå°±èƒ½å¤Ÿæœ€å¤§åŒ–æˆ‘ä»¬çš„ç›®æ ‡ã€‚ åŒç†ï¼Œæˆ‘ä»¬è¦æ‰¾$w^2$ï¼Œæœ€å¤§åŒ–$(w^2 )^T Sw^2$ï¼Œå…¶ä¸­æœ‰ï¼š$(w^2 )^T w^2=1$$(w^2 )^T w^1=0$ ï¼ˆä¸ç¬¬ä¸€ç»´æ­£äº¤ï¼‰ å› æ­¤åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•ï¼š g(w^2 )= (w^2 )^T Sw^2âˆ’Î±((w^2 )^T w^2âˆ’1)âˆ’Î²((w^2 )^T w^1âˆ’0)æœ€ç»ˆå¾—åˆ°ï¼Œw2å¯¹åº”ç¬¬äºŒå¤§çš„ç‰¹å¾å€¼çš„ç‰¹å¾å‘é‡ã€‚ ä»¥æ­¤ç±»æ¨ï¼Œå…¶ä»–ç»´ä¹ŸåŒç†ã€‚â€”-End of Mathâ€”- PCAçš„å…¶ä»–å®é™…ä¸Šæœ€ç»ˆå¾—åˆ°çš„zï¼Œæ¯ä¸€ç»´ä¹‹é—´çš„åæ–¹å·®éƒ½ä¸º0 è¯æ˜å¦‚ä¸‹ï¼š PCAä¹Ÿå¯ä»¥ç”¨SVDæ¥åšï¼š Uä¸­ä¿å­˜äº†Kä¸ªç‰¹å¾å‘é‡ã€‚ ä»å¦ä¸€ç§è§’åº¦ç†è§£PCAï¼Œä¹Ÿå¯ä»¥è®¤ä¸ºPCAæ˜¯ä¸€ç§autoencoderï¼š PCAçš„é—®é¢˜PCAæ˜¯æ— ç›‘ç£å­¦ä¹ ï¼Œå¦‚æœæœ‰æ ‡ç­¾ï¼Œåˆ™æ— æ³•æŒ‰ç…§ç±»åˆ«æ¥è¿›è¡Œæ­£ç¡®é™ç»´ï¼Œå¦‚ï¼š ç¬¬äºŒå°±æ˜¯PCAæ˜¯çº¿æ€§å˜æ¢ï¼Œå¯¹äºä¸€äº›éœ€è¦éçº¿æ€§å˜æ¢çš„æ— èƒ½ä¸ºåŠ› Matrix Factorizationå®šä¹‰ï¼šçŸ©é˜µåˆ†è§£ï¼Œå°±æ˜¯å°†ä¸€ä¸ªçŸ©é˜µDåˆ†è§£ä¸ºUå’ŒVçš„ä¹˜ç§¯ï¼Œå³å¯¹äºä¸€ä¸ªç‰¹å®šçš„è§„æ¨¡ä¸ºm*nçš„çŸ©é˜µDï¼Œä¼°è®¡å‡ºè§„æ¨¡åˆ†åˆ«ä¸ºm*kå’Œn*kçš„çŸ©é˜µUå’ŒVï¼Œä½¿å¾—$UV^T$çš„å€¼å°½å¯èƒ½é€¼è¿‘çŸ©é˜µDã€‚å¸¸ç”¨äºæ¨èç³»ç»Ÿã€‚ æ€æƒ³ï¼šå‡å¦‚æœ‰ä¸€ä¸ªçŸ©é˜µï¼š å‡è®¾æ¨ªè½´å’Œçºµè½´æ¯ä¸€ç»´éƒ½æœ‰ä¸€ä¸ªå‘é‡ä»£è¡¨è¯¥ç»´ï¼ŒçŸ©é˜µçš„æ¯ä¸ªå…ƒç´ å°±æ˜¯æ¨ªè½´å’Œçºµè½´å¯¹åº”ç»´çš„ç‚¹ç§¯ã€‚æˆ‘ä»¬çš„ç›®çš„æ˜¯å°½å¯èƒ½å‡å°ï¼š L=\sum_{(i,j)} (r^i \cdot r^j -n_{ij})^2å…¶ä¸­$r_i$ $r_j$å°±æ˜¯å‘é‡è¡¨ç¤ºï¼Œ$n_{ij}$å°±æ˜¯çŸ©é˜µçš„å†…å®¹ã€‚ å¯ä»¥ä½¿ç”¨SVDæ±‚è§£ä¸Šå¼ï¼š å®é™…ä¸Šï¼Œè€ƒè™‘æ¯ä¸€è¡Œæˆ–åˆ—æœ¬èº«çš„ç‰¹æ€§ï¼Œæˆ‘ä»¬å¯¹Lossè¿›è¡Œæ‰©å±•ï¼š Minimizing \ \ L=\sum_{(i,j)} (r^i \cdot r^j +b_i+b_j-n_{ij})^2ä½¿ç”¨SGDå¯ä»¥æ±‚è§£ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Linear Dimension Reduction</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¢¯åº¦æ¶ˆå¤±ä¸æ¢¯åº¦çˆ†ç‚¸çš„æ¨å¯¼]]></title>
    <url>%2F2018%2F10%2F07%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[è®°RNNä¸­æ¯ä¸€æ­¥çš„æŸå¤±ä¸º$E_t$ï¼Œåˆ™æŸå¤±å¯¹$h_{t-1}$çš„æƒé‡$W$çš„å¯¼æ•°æœ‰ï¼š \frac{\partial{E_t}}{\partial{W}}=\sum_{k=1}^{t} \frac{\partial{E_t}}{\partial{y_t}} \frac{\partial{y_t}}{\partial{h_t}} \frac{\partial{h_t}}{\partial{h_k}} \frac{\partial{h_k}}{\partial{W}}å…¶ä¸­$\frac{\partial{h_t}}{\partial{h_k}}$ä½¿ç”¨é“¾å¼æ³•åˆ™æœ‰ï¼š \frac{\partial{h_t}}{\partial{h_k}} = \prod_{j=k+1}^{t} \frac{\partial{h_j}}{\partial{h_{j-1}}} = \prod_{j=k+1}^{t} W^T \times diag[f^{\prime}(h_{j-1})]å…¶ä¸­$\frac{\partial{h_j}}{\partial{h_{j-1}}}$ æ˜¯é›…å…‹æ¯”çŸ©é˜µã€‚å¯¹å…¶å–æ¨¡(norm)ï¼Œæœ‰ï¼š \rVert \frac{\partial{h_j}}{\partial{h_{j-1}}}\rVert â‰¤ \rVert W^T \rVert \rVert diag[f^{\prime}(h_{j-1})] \rVert â‰¤ \beta_W \beta_hå½“$f$ä¸ºsigmoidæ—¶ï¼Œ$f^{\prime}(h_{j-1})$æœ€å¤§å€¼ä¸º1ã€‚ æœ€ç»ˆæˆ‘ä»¬æœ‰ï¼š \rVert \frac{\partial{h_t}}{\partial{h_{k}}}\rVert â‰¤ \rVert \prod_{j=k+1}^{t} \frac{\partial{h_j}}{\partial{h_{j-1}}} \rVert â‰¤ (\beta_W \beta_h)^{t-k}ä»ä¸Šå¼å¯ä»¥çœ‹å‡ºï¼Œå½“t-kè¶³å¤Ÿå¤§æ—¶ï¼Œå¦‚æœ$(\beta_W \beta_h)$å°äº1åˆ™$(\beta_W \beta_h)^{t-k}$åˆ™ä¼šå˜å¾—éå¸¸å°ï¼Œç›¸åï¼Œè‹¥$(\beta_W \beta_h)$å¤§äº1åˆ™$(\beta_W \beta_h)^{t-k}$åˆ™ä¼šå˜å¾—éå¸¸å¤§ã€‚ åœ¨è®¡ç®—æœºä¸­ï¼Œå½“æ¢¯åº¦å€¼å¾ˆå¤§æ—¶ï¼Œä¼šé€ æˆä¸Šæº¢(NaN)ï¼Œä¹Ÿå³æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œå½“æ¢¯åº¦å€¼å¾ˆå°æ—¶ï¼Œä¼šå˜æˆ0ï¼Œä¹Ÿå³æ¢¯åº¦æ¶ˆå¤±ã€‚æ³¨æ„åˆ°ï¼Œt-kçš„æŸå¤±å®é™…ä¸Šè¯„ä¼°çš„æ˜¯ä¸€ä¸ªè¾ƒè¿œçš„è¯å¯¹å½“å‰tçš„è´¡çŒ®ï¼Œæ¢¯åº¦æ¶ˆå¤±ä¹Ÿå³æ„å‘³ç€å¯¹å½“å‰çš„è´¡çŒ®æ¶ˆå¤±ã€‚ Reference:CS224d: Deep Learning for NLP Lecture4]]></content>
      <tags>
        <tag>æ¢¯åº¦æ¶ˆå¤±</tag>
        <tag>æ¢¯åº¦çˆ†ç‚¸</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†10]]></title>
    <url>%2F2018%2F10%2F07%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8610%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[æ­£æ€åˆ†å¸ƒ]é«˜ç»´æ­£æ€åˆ†å¸ƒæ˜¯ä»ä¸€ç»´å‘å±•è€Œæ¥çš„ï¼š https://www.zhihu.com/question/36339816 2ï¸âƒ£[RNN]from https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf é€šå¸¸è€Œè¨€ï¼Œæˆ‘ä»¬éƒ½ä¼šå°†RNNçš„initial stateè®¾ä¸ºå…¨0ï¼Œä½†åœ¨Hintonçš„slideä¸­æåˆ°ï¼Œæˆ‘ä»¬å¯ä»¥å°†åˆå§‹çŠ¶æ€ä½œä¸ºå¯å­¦ä¹ çš„å˜é‡ï¼Œå’Œæˆ‘ä»¬åœ¨å­¦ä¹ æƒé‡çŸ©é˜µä¸€æ ·ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>æ­£æ€åˆ†å¸ƒ</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRMLç¬¬äºŒç«  æ¦‚ç‡åˆ†å¸ƒ]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FPRML%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[äºŒå…ƒå˜é‡ å¤šé¡¹å¼åˆ†å¸ƒ é«˜æ–¯åˆ†å¸ƒ æŒ‡æ•°æ—åˆ†å¸ƒ éå‚æ•°ä¼˜åŒ–]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 10:Semi-supervised learning]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2010%3A%20Semi-supervised%2F</url>
    <content type="text"><![CDATA[ä»€ä¹ˆæ˜¯semi-supervised learning ç»™å®šæ•°æ®${(x^r,\hat{y}^r)}_{r=1}^{R},{(x_u)}_{u=R}^{R+U}$ï¼Œå…¶ä¸­æœªæ ‡è®°æ•°æ®è¿œè¿œå¤šäºæ ‡è®°æ•°æ® $U&gt;&gt;R$ ä¸ºä»€ä¹ˆåŠç›‘ç£å­¦ä¹ æœ‰ç”¨ï¼Ÿå› ä¸ºæœªæ ‡è®°æ•°æ®çš„åˆ†å¸ƒå¯èƒ½èƒ½å¤Ÿç»™æˆ‘ä»¬ä¸€äº›ä¿¡æ¯ã€‚ ç”Ÿæˆæ¨¡å‹çš„åŠç›‘ç£å­¦ä¹ ç»™å®šä¸¤ç±»$C_1$ã€$C_2$ï¼Œè¦æ±‚å¾—åˆ°åéªŒæ¦‚ç‡åˆ†å¸ƒ P(C_1 |x)=\frac{P(x|C_1 )P(C_1 )}{(P(x|C_1 )P(C_1 )+P(x|C_2 )P(C_2 ) )}å…¶ä¸­è”åˆæ¦‚ç‡åˆ†å¸ƒæœä»é«˜æ–¯åˆ†å¸ƒã€‚æœªæ ‡è®°æ•°æ®æ­¤æ—¶çš„ä½œç”¨å³å¸®æˆ‘ä»¬é‡æ–°ä¼°è®¡$P(C_1),P(C_2),\mu,\Sigma$ å¦‚ä½•åš?å…ˆåˆå§‹åŒ–$P(C_1),P(C_2),\mu,\Sigma$ï¼Œé€šå¸¸å¯ä»¥å…ˆç”¨æœ‰æ ‡è®°æ•°æ®è¿›è¡Œä¼°è®¡ è®¡ç®—æ¯ä¸ªæœªæ ‡è®°æ•°æ®çš„åéªŒæ¦‚ç‡åˆ†å¸ƒ ä»¥è¯¥æ¦‚ç‡åˆ†å¸ƒæ›´æ–°æ¨¡å‹ä¸æ–­é‡å¤ç›´è‡³æ‹Ÿåˆ åŸå› ï¼šå½“æˆ‘ä»¬åœ¨åšç›‘ç£å­¦ä¹ æ—¶ï¼Œä½¿ç”¨æœ€å¤§ä¼¼ç„¶æ±‚è§£ï¼š logL(Î¸)=âˆ‘_{x^r,\hat{y}^r} logP_Î¸ (x^r |\hat{y}^r )åŠ ä¸Šäº†æœªæ ‡è®°æ•°æ®åï¼ŒåŒæ ·ä¹Ÿè¦åšæœ€å¤§ä¼¼ç„¶ï¼š logL(Î¸)=âˆ‘_{(x^r,\hat{y}^r)} logP_Î¸ (x^r |\hat{y}^r )+âˆ‘_{x^u} logP_Î¸ (x^u)Low-density Separationå‡è®¾ä¸åŒç±»åˆ«ä¹‹é—´æœ‰ä¸€æ¡æ˜æ˜¾çš„åˆ†ç•Œçº¿ï¼Œä¹Ÿå³å­˜åœ¨ä¸€ä¸ªåŒºåŸŸï¼Œå…¶å¯†åº¦æ¯”å…¶ä»–åŒºåŸŸå° Self-trainingå¦‚ä½•åš? å…ˆç”¨æœ‰æ ‡ç­¾æ•°æ®è®­ç»ƒä¸€ä¸ªæ¨¡å‹$f$ï¼› åˆ©ç”¨æ¨¡å‹å¯¹æœªæ ‡è®°æ•°æ®è¿›è¡Œæ ‡è®°ï¼Œè¿™äº›æ ‡ç­¾ç§°ä¸ºä¼ªæ ‡ç­¾ï¼ˆpseudo-labelï¼‰ å°†éƒ¨åˆ†æœ‰ä¼ªæ ‡ç­¾çš„æ•°æ®æ”¾å…¥æœ‰æ ‡ç­¾æ•°æ®ä¸­ï¼Œé‡æ–°è®­ç»ƒé‡å¤ç›´åˆ°æ‹Ÿåˆ è¿™ç§æ–¹å¼å’Œç”Ÿæˆæ¨¡å‹çš„åŒºåˆ«ï¼šè¯¥æ–¹æ³•ä½¿ç”¨çš„æ˜¯hard labelè€Œç”Ÿæˆæ¨¡å‹ä½¿ç”¨çš„æ˜¯soft label Entropy-based Regularizationå°†æœªæ ‡è®°æ•°æ®å……å½“æ­£åˆ™åŒ–çš„æ•ˆæœï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹é¢„æµ‹æ ‡ç­¾çš„æ¦‚ç‡è¾ƒä¸ºé›†ä¸­ï¼Œä¹Ÿå³ç†µåº”è¯¥å°½å¯èƒ½å°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæœªæ ‡è®°æ•°æ®ä½¿å¾—åˆ†ç±»è¾¹ç•Œå°½å¯èƒ½åˆ’åœ¨ä½å¯†åº¦åŒºåŸŸã€‚ Smoothness Assumptionå‡è®¾ï¼šä½äºç¨ å¯†æ•°æ®åŒºåŸŸçš„ä¸¤ä¸ªè·ç¦»å¾ˆè¿‘çš„æ ·ä¾‹çš„ç±»æ ‡ç­¾ç›¸ä¼¼ï¼Œé€šè¿‡high density pathè¿æ¥ã€‚ x1ä¸x2ä¹‹é—´è¾ƒä¸ºç¨ å¯†ï¼Œå› æ­¤x2ä¸x1æ¯”x2ä¸x3æ›´ä¸ºæ¥è¿‘ã€‚ å¦‚ä½•çŸ¥é“x1ä¸x2é€šè¿‡high density pathè¿æ¥ï¼Ÿ åŸºäºå›¾çš„æ–¹æ³•ï¼š å®šä¹‰xiä¸xjä¹‹é—´çš„ç›¸ä¼¼åº¦$s(x^i,x^j)$ æ·»åŠ è¾¹ï¼Œæœ‰ä¸¤ç§é€‰æ‹© k nearest neighbor e-neighborhood è¾¹ä¹‹é—´çš„æƒé‡é€šè¿‡ç›¸ä¼¼åº¦æ¥è¡¡é‡ã€‚å¦‚ï¼š $s(x^i,x^j )=exp(âˆ’Î³â€–x^iâˆ’x^jâ€–^2)$ è¯¥æ–¹æ³•æœ¬è´¨å³åˆ©ç”¨æœ‰æ ‡ç­¾æ•°æ®å»å½±å“æœªæ ‡è®°æ•°æ®ï¼Œé€šè¿‡å›¾çš„ä¼ æ’­ã€‚ä½†ä¸€ä¸ªé—®é¢˜æ˜¯å¦‚æœæ•°æ®ä¸å¤Ÿå¤šï¼Œå°±å¯èƒ½æ²¡åŠæ³•ä¼ æ’­ã€‚å¦‚ï¼š åœ¨å»ºç«‹å¥½å›¾åï¼Œå¦‚ä½•ä½¿ç”¨? å®šä¹‰å›¾çš„å¹³æ»‘ç¨‹åº¦ï¼Œ$y$è¡¨ç¤ºæ ‡ç­¾ã€‚$S$è¶Šå°è¡¨ç¤ºè¶Šå¹³æ»‘ã€‚S=1/2âˆ‘_{i,j} w_{i,j} (y^iâˆ’y^j )^2=y^T Lyy=[â‹¯y^iâ‹¯y^jâ‹¯]^TL=Dâˆ’W Dæ˜¯é‚»æ¥çŸ©é˜µï¼Œç¬¬ijä¸ªå…ƒç´ å³xiä¸xjä¹‹é—´çš„weightï¼ŒWæ˜¯å¯¹è§’çŸ©é˜µï¼Œiiä¸ªå…ƒç´ æ˜¯Dçš„ç¬¬iè¡Œçš„åŠ å’Œï¼›Lç§°ä¸ºGraph Laplacian æˆ‘ä»¬æœ€ç»ˆåœ¨è®¡ç®—Lossçš„æ—¶å€™è¦åŠ ä¸Šè¿™é¡¹æ­£åˆ™é¡¹L=âˆ‘_{x^r}C(y^r,\hat{y}^r ) +Î»S]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Semi-supervised learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 7:Tips for DL]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%207%3A%20Tips%20for%20DL%2F</url>
    <content type="text"><![CDATA[å¤§çº² new activation functionæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼šç”±äºsigmoidä¼šå°†å€¼å‹ç¼©ï¼Œæ‰€ä»¥åœ¨åå‘ä¼ æ’­æ—¶ï¼Œè¶Šåˆ°åé¢å€¼è¶Šå°ã€‚ æ‰€ä»¥åå±‚çš„æ›´æ–°ä¼šæ¯”å‰å±‚çš„æ›´æ–°æ›´å¿«ï¼Œå¯¼è‡´å‰å±‚è¿˜æ²¡convergeï¼Œåå±‚å°±æ ¹æ®å‰å±‚çš„æ•°æ®ï¼ˆrandomï¼‰è¾¾åˆ°convergeäº† ReLUèƒ½å¤Ÿå¿«é€Ÿè®¡ç®—ï¼Œä¸”èƒ½å¤Ÿè§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚ å› ä¸ºä¼šæœ‰éƒ¨åˆ†neuronçš„å€¼æ˜¯0ï¼Œæ‰€ä»¥ç›¸å½“äºæ¯æ¬¡è®­ç»ƒä¸€ä¸ªç˜¦é•¿çš„ç¥ç»ç½‘ç»œã€‚ ReLUçš„å˜ä½“ Maxouté¦–å…ˆå°†å‡ ä¸ªneuronå½’ä¸ºä¸€ç»„ï¼Œç„¶åæ¯æ¬¡å‰å‘ä¼ æ’­æ—¶å–æœ€å¤§çš„ä½œä¸ºè¾“å‡ºã€‚ å®é™…ä¸ŠReLUæ˜¯maxoutçš„ä¸€ç§ç‰¹æ®Šå½¢å¼ï¼š æ›´ä¸€èˆ¬çš„ï¼Œæœ‰ï¼š å› ä¸ºwå’Œbçš„å˜åŒ–ï¼Œæ‰€ä»¥è¯¥activation functionå®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªlearnable activation function è¿™æ ·ä¸€ä¸ªlearnable activation functionæœ‰è¿™æ ·çš„ç‰¹ç‚¹ï¼š Activation function in maxout network can be any piecewise linear convex functionHow many pieces depending on how many elements in a group å¦‚ï¼š maxoutåº”å¦‚ä½•è®­ç»ƒï¼Ÿ å®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªæ™®é€šçš„ç˜¦é•¿networkï¼Œå¸¸è§„è®­ç»ƒå³å¯ã€‚ Adaptive learning rateåœ¨adagradä¸­: è¶Šåˆ°åé¢learning rateè¶Šæ¥è¶Šå°ï¼Œä½†å®é™…ä¸Šåœ¨dlé‡Œé¢ï¼Œerror surfaceæ˜¯éå¸¸å¤æ‚çš„ï¼Œè¶Šæ¥è¶Šå°çš„learning rateå¯èƒ½ä¸é€‚ç”¨äºdlã€‚å¦‚ï¼š RMSprop$Ïƒ^t$æ˜¯å†å²ä¿¡æ¯ï¼Œä¹Ÿå°±æ˜¯è¯´$Ïƒ^t$å‚è€ƒäº†è¿‡å»çš„æ¢¯åº¦å’Œå½“å‰çš„æ¢¯åº¦è·å¾—ä¸€ä¸ªæ–°çš„æ”¾ç¼©å¤§å° Momentumå¼•å…¥æƒ¯æ€§ä½œä¸ºå‚è€ƒï¼Œä¹Ÿå³å‚è€ƒäº†ä¸Šä¸€æ¬¡æ¢¯åº¦çš„æ–¹å‘ã€‚å¼•å…¥æƒ¯æ€§åï¼Œå¯èƒ½æœ‰æœºä¼šè¶Šè¿‡local minimumã€‚æ™®é€šçš„gradient descent:æ¯æ¬¡æœç€æ¢¯åº¦çš„åæ–¹å‘èµ°ã€‚ Momentum: è€ƒè™‘äº†ä¸Šä¸€æ­¥èµ°çš„æ–¹å‘ã€‚ å…·ä½“ç®—æ³•ï¼š Adamç»“åˆäº†RMSpropå’ŒMomentumï¼Œä¹Ÿå³ç»¼åˆè€ƒè™‘äº†å†å²ä¿¡æ¯å†³å®šå½“å‰æ­¥é•¿ï¼›è€ƒè™‘äº†ä¸Šä¸€æ­¥çš„æ–¹å‘å†³å®šå½“å‰èµ°çš„æ–¹å‘ã€‚å…·ä½“ç®—æ³•ï¼š Early Stoppingå°±æ˜¯åœ¨validation setçš„lossä¸å†å‡å°æ—¶åœæ­¢ RegularizationL2æ­£åˆ™åŒ–å…¶ä¸­å› æ­¤æ›´æ–°å…¬å¼ä¸ºï¼š ä¹Ÿå³æ¯æ¬¡ä»¥$1-\eta \lambda$å¯¹wè¿›è¡Œæ”¾ç¼©ï¼Œä½¿wæ›´æ¥è¿‘0æ­£åˆ™åŒ–åœ¨DLä¸­ä¹Ÿç§°ä¸ºweight decay L1æ­£åˆ™åŒ– åˆ™æ›´æ–°å…¬å¼ä¸ºï¼š ä¹Ÿå³æ¯æ¬¡ä»¥$Î·Î»sgn(w)$ ä½¿wå¾€0é ï¼ˆsgnè¡¨ç¤ºç¬¦å·å‡½æ•°ï¼‰ å¯ä»¥çœ‹å‡ºï¼ŒL1æ¯æ¬¡éƒ½åŠ å‡ç›¸åŒçš„å€¼ï¼Œè€ŒL2æŒ‰æ¯”ä¾‹è¿›è¡Œç¼©æ”¾ã€‚å› æ­¤L1æ›´ä¸ºç¨€ç–(sparse)ã€‚ Dropoutè®­ç»ƒçš„æ—¶å€™æ¯ä¸€å±‚é‡‡æ ·p%çš„ç¥ç»å…ƒè®¾ä¸º0ï¼Œè®©å…¶ä¸å·¥ä½œ å®é™…ä¸Šå°±æ˜¯æ¯ä¸ªbatchæ”¹å˜äº†ç½‘ç»œç»“æ„ï¼Œä½¿å¾—ç½‘ç»œæ›´ç»†é•¿ æµ‹è¯•çš„æ—¶å€™æ‰€æœ‰çš„weightéƒ½ä¹˜ä»¥1-p% ä»ensembleçš„è§’åº¦çœ‹å¾…dropoutï¼šåœ¨è®­ç»ƒçš„æ—¶å€™è®­ç»ƒä¸€å †ä¸åŒç»“æ„çš„networkï¼Œæœ€å¤šæœ‰$2^N$ç§ç»„åˆï¼ŒNä¸ºneuronä¸ªæ•°ï¼Œå¯ä»¥ç§°ä¸ºç»ˆæçš„ensembleæ–¹æ³•äº†ã€‚è€Œåœ¨æµ‹è¯•çš„æ—¶å€™å¯¹è¿™äº›ä¸åŒçš„ç½‘ç»œè¿›è¡Œå¹³å‡ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Tips for DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[é‡‡æ ·æµ…æ]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F%E9%87%87%E6%A0%B7%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[æ€»ç»“åœ¨NLPä¸­çš„é‡‡æ ·æ–¹æ³•ï¼ˆæŒç»­æ›´æ–°ï¼‰ã€‚ é‡‡æ ·æ–¹æ³•1ï¸âƒ£é€†å˜æ¢é‡‡æ ·(Inverse Sampling)ç›®çš„ï¼šå·²çŸ¥ä»»æ„æ¦‚ç‡åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°æ—¶ï¼Œç”¨äºä»è¯¥åˆ†å¸ƒä¸­ç”Ÿæˆéšæœºæ ·æœ¬ã€‚ â€”-ä»€ä¹ˆæ˜¯ç´¯ç§¯åˆ†å¸ƒå‡½æ•°(CDF)â€”-æ˜¯æ¦‚ç‡å¯†åº¦å‡½æ•°(PDF)çš„ç§¯åˆ†ï¼Œå®šä¹‰ï¼š F_X(x)=P(Xâ‰¤x)=\int_{-âˆ}^{x}f_X(t)dtâ€”-ENDâ€”- æƒ³è±¡æˆ‘ä»¬çŸ¥é“é«˜æ–¯åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œæˆ‘ä»¬åº”è¯¥å¦‚ä½•é‡‡æ ·ï¼Ÿæœ¬è´¨ä¸Šæˆ‘ä»¬åªèƒ½å¯¹å‡åŒ€åˆ†å¸ƒè¿›è¡Œç›´æ¥é‡‡æ ·ï¼ˆé«˜æ–¯åˆ†å¸ƒæœ‰ç®—æ³•å¯ä»¥ç”Ÿæˆé‡‡æ ·ï¼Œä½†æ— æ³•ä¸€èˆ¬åŒ–ï¼‰ã€‚å¯¹äºè¿™ç§è¿ç»­çš„éšæœºå˜é‡ï¼Œæˆ‘ä»¬åªèƒ½é€šè¿‡é—´æ¥çš„æ–¹æ³•è¿›è¡Œé‡‡æ ·ã€‚ é€†å˜æ¢é‡‡æ ·å³æ˜¯é€šè¿‡ç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„åå‡½æ•°æ¥é‡‡æ ·ã€‚å› ä¸ºç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„å€¼åŸŸä¸º$[0,1]$ï¼Œå› æ­¤æˆ‘ä»¬é€šè¿‡åœ¨$[0,1]$ä¸Šè¿›è¡Œé‡‡æ ·ï¼Œå†æ˜ å°„åˆ°åŸåˆ†å¸ƒã€‚ä¾‹å­:æ˜ å°„å…³ç³»å¦‚å›¾ï¼š 2ï¸âƒ£é‡è¦æ€§é‡‡æ ·(Importance Sampling)ç›®çš„ï¼šå·²çŸ¥æŸä¸ªåˆ†å¸ƒ$P$ï¼Œå¸Œæœ›èƒ½ä¼°è®¡$f(x)$çš„æœŸæœ›ã€‚äº¦å³ï¼š E[f(x)]=\int_{x}f(x)p(x)dxâ‰ˆ\frac{1}{n}\sum_{i=1}^{n}f(x_i)å…¶ä¸­$x\sim p$ã€‚å‡è®¾$p(x)$çš„åˆ†å¸ƒå¤æ‚æˆ–æ ·æœ¬ä¸å¥½ç”Ÿæˆï¼Œå¦ä¸€åˆ†å¸ƒ$q(x)$æ–¹ä¾¿ç”Ÿæˆæ ·æœ¬ã€‚å› æ­¤æˆ‘ä»¬å¼•å…¥$q(x)$å¯¹åŸå…ˆåˆ†å¸ƒè¿›è¡Œä¼°è®¡ã€‚ E[f(x)]=\int_{x}f(x)p(x)dx=\int_{x}f(x)\frac{p(x)}{q(x)}q(x)dxâ‰ˆ\frac{1}{n}\sum_{i=1}^{n}f(x_i)\frac{p(x_i)}{q(x_i)}å…¶ä¸­ï¼Œ$x \sim q$ã€‚$w(x)=\frac{p(x)}{q(x)}$ç§°ä¸ºImportance Weight æ ¹æ®ä¸Šå¼ï¼Œå®é™…ä¸Šå°±æ˜¯æ¯æ¬¡é‡‡æ ·çš„åŠ æƒæ±‚å’Œã€‚ Referenceé€†å˜æ¢é‡‡æ ·https://zh.wikipedia.org/wiki/%E9%80%86%E5%8F%98%E6%8D%A2%E9%87%87%E6%A0%B7 é‡è¦æ€§é‡‡æ ·https://www.youtube.com/watch?v=S3LAOZxGcnk â€”â€”æŒç»­æ›´æ–°â€”â€”]]></content>
      <tags>
        <tag>é‡‡æ ·</tag>
        <tag>sampling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†9]]></title>
    <url>%2F2018%2F09%2F30%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%869%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Pytorch]Pytorchä¸­ä¿å­˜checkpointæ˜¯ä¸€ä¸ªdictå½¢å¼ï¼Œå¯ä»¥ä¿å­˜ä»»æ„å¤šä¸ªæ¨¡å‹åˆ°ä¸€ä¸ªcheckpointä¸­ã€‚1234567import torch#savetorch.save(&#123; 'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': loss, ... &#125;, PATH)#loadmodel = TheModelClass(*args, **kwargs) optimizer = TheOptimizerClass(*args, **kwargs)checkpoint = torch.load(PATH) model.load_state_dict(checkpoint['model_state_dict']) optimizer.load_state_dict(checkpoint['optimizer_state_dict']) epoch = checkpoint['epoch'] loss = checkpoint['loss']model.eval() # - or - model.train() 2ï¸âƒ£[Pytorch]Pytorchå¯ä»¥loadéƒ¨åˆ†æ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯åªloadè¿›æ¥éƒ¨åˆ†æˆ‘ä»¬éœ€è¦çš„å±‚ï¼Œè¿™åœ¨transfer learningä¸­ç”¨åˆ°ã€‚123torch.save(modelA.state_dict(), PATH)modelB = TheModelBClass(*args, **kwargs) modelB.load_state_dict(torch.load(PATH), strict=False)]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[no title]]></title>
    <url>%2F2018%2F09%2F30%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%9C%89%E5%8A%9B%E9%87%8F%E7%9A%84%E6%96%87%E5%AD%97%2F</url>
    <content type="text"><![CDATA[æ¯å½“æˆ‘é‡åˆ°è‡ªå·±ä¸æ•¢ç›´è§†çš„å›°éš¾æ—¶ï¼Œæˆ‘å°±ä¼šé—­ä¸ŠåŒçœ¼ï¼Œæƒ³è±¡è‡ªå·±æ˜¯ä¸€ä¸ª80å²çš„è€äººï¼Œä¸ºäººç”Ÿä¸­æ›¾æ”¾å¼ƒå’Œé€ƒé¿è¿‡çš„æ— æ•°å›°éš¾è€Œæ‡Šæ‚”ä¸å·²ï¼Œæˆ‘ä¼šå¯¹è‡ªå·±è¯´ï¼Œèƒ½å†å¹´è½»ä¸€æ¬¡è¯¥æœ‰å¤šå¥½ï¼Œç„¶åæˆ‘çå¼€çœ¼ç›ï¼šç °ï¼æˆ‘åˆå¹´è½»ä¸€æ¬¡äº†ï¼]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯10]]></title>
    <url>%2F2018%2F09%2F30%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D10%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£æ¬¡åŒ—å›ºå±±ä¸‹[å”] ç‹æ¹¾å®¢è·¯é’å±±å¤–ï¼Œè¡ŒèˆŸç»¿æ°´å‰ã€‚æ½®å¹³ä¸¤å²¸é˜”ï¼Œé£æ­£ä¸€å¸†æ‚¬ã€‚æµ·æ—¥ç”Ÿæ®‹å¤œï¼Œæ±Ÿæ˜¥å…¥æ—§å¹´ã€‚ä¹¡ä¹¦ä½•å¤„è¾¾ï¼Œå½’é›æ´›é˜³è¾¹ã€‚ æ¬¡ï¼šæ—…é€”ä¸­æš‚æ—¶åœå®¿ï¼Œè¿™é‡Œæ˜¯åœæ³Šçš„æ„æ€ã€‚ http://m.xichuangzhu.com/work/57b95de92e958a005fa8919e 2ï¸âƒ£å°†èµ´å´å…´ç™»ä¹æ¸¸åŸ[å”] æœç‰§æ¸…æ—¶æœ‰å‘³æ˜¯æ— èƒ½ï¼Œé—²çˆ±å­¤äº‘é™çˆ±åƒ§ã€‚æ¬²æŠŠä¸€éº¾æ±Ÿæµ·å»ï¼Œä¹æ¸¸åŸä¸Šæœ›æ˜­é™µã€‚ æ— èƒ½ï¼šæ— æ‰€ä½œä¸ºã€‚ http://m.xichuangzhu.com/work/57b99db9165abd005a6da742]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRMLç¬¬ä¸€ç«  ç»ªè®º]]></title>
    <url>%2F2018%2F09%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FPRML%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E7%BB%AA%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[è®°å½•PRMLå­¦ä¹ è¿‡ç¨‹ã€‚ç¬”è®°å…±äº«é“¾æ¥ï¼šhttps://1drv.ms/u/s!Apsp2510NHF6rIRjMclFB16v7B0FWg æ¦‚ç‡è®º å†³ç­–è®º ä¿¡æ¯è®º]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 6:Backpropagation]]></title>
    <url>%2F2018%2F09%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%206%3A%20Backpropagation%2F</url>
    <content type="text"><![CDATA[Chain RuleåŸºæœ¬å…¬å¼ forward passå’Œbackward passå¯ä»¥å°†backpropagationåˆ†ä¸ºä¸¤æ­¥ forward passåœ¨å‰å‘ä¼ æ’­çš„æ—¶å€™æå‰è®¡ç®—/ä¿å­˜å¥½ï¼Œå› ä¸ºè¯¥æ¢¯åº¦å¾ˆç®€å• æ¯”å¦‚zå¯¹w1çš„æ¢¯åº¦å°±æ˜¯x1ï¼Œå°±æ˜¯å’Œw1ç›¸è¿çš„é¡¹ backward passå›ä¼ çš„æ—¶å€™é€å±‚ç›¸ä¹˜ä¸‹å»ï¼Œç±»ä¼¼åŠ¨æ€è§„åˆ’ï¼Œè·å¾—äº†åä¸€å±‚çš„æ¢¯åº¦æ‰èƒ½æ±‚å‡ºå‰ä¸€å±‚çš„æ¢¯åº¦ã€‚ æ€»ç»“ å…ˆå‰å‘ï¼Œæå‰ç®—å‡ºæœ€é‚»è¿‘çš„æ¢¯åº¦ï¼Œç›´åˆ°output layerï¼Œè®¡ç®—å®Œè¯¥æ¢¯åº¦ï¼Œå†ä¸æ–­å›ä¼ é€å±‚ç›¸ä¹˜è·å¾—outputå¯¹å„å±‚çš„æ¢¯åº¦ã€‚ ä»£ç å®ç°ä¾‹å­reluå®ç°forward passå’Œbackward pass1234567891011121314151617181920212223242526272829303132import torchclass MyReLU(torch.autograd.Function): """ We can implement our own custom autograd Functions by subclassing torch.autograd.Function and implementing the forward and backward passes which operate on Tensors. """ @staticmethod def forward(ctx, input): """ In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. """ ctx.save_for_backward(input) #ä¸ºäº†ä¹‹åçš„backwardè®¡ç®— return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): """ In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. """ input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input &lt; 0] = 0 return grad_input]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Backpropagation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 5:Classification:Logistic Regression]]></title>
    <url>%2F2018%2F09%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%205%20Classification%3A%20Logistic%20Regression%2F</url>
    <content type="text"><![CDATA[logistic regressionå¦‚ä½•åšï¼Ÿstep1: å®šä¹‰function set step2: æ›´æ–°ä½¿ç”¨æœ€å¤§ä¼¼ç„¶æ›´æ–° L(w,b)=f_{w,b}(x^1 )f_{w,b}(x^2 )(1âˆ’f_{w,b} (x^3 ))â‹¯f_{w,b} (x^N )æ‰¾åˆ°wï¼Œbä½¿å¾—Læœ€å¤§ å¯¹ä¼¼ç„¶å‡½æ•°å–è´Ÿå¯¹æ•°ï¼Œåˆ™æœ‰ï¼š å°†å¼å­çš„æ¯ä¸ªå…ƒç´ å†™æˆä¼¯åŠªåˆ©åˆ†å¸ƒå½¢å¼ï¼š ä¸Šå¼å°±æ˜¯cross-entropyæŸå¤±å‡½æ•°ã€‚ æ±‚å¯¼è¯¥å¼å­å¯å¾—ï¼šæ›´æ–°å…¬å¼ï¼šå¯ä»¥çœ‹å‡ºä¸Šå¼å¾ˆç›´è§‚ï¼šå’Œç­”æ¡ˆå·®è·è¶Šå¤§ï¼Œæ›´æ–°æ­¥ä¼è¶Šå¤§ã€‚ åŒæ—¶å‘ç°ä¸Šå¼å’Œlinear regressionçš„æ›´æ–°å…¬å¼æ˜¯ä¸€è‡´çš„ã€‚ ä¸ºä»€ä¹ˆä¸åƒlinear regressioné‚£æ ·è®¾lossä¸ºsquareï¼Ÿå‡è®¾æˆ‘ä»¬ä½¿ç”¨square lossï¼Œåˆ™æ±‚å¯¼å¾—åˆ°çš„æ¢¯åº¦ï¼šä¸Šå¼å¯ä»¥çœ‹å‡ºï¼Œå½“æ¥è¿‘targetæ—¶ï¼Œæ¢¯åº¦å°ï¼›è¿œç¦»targetæ—¶ï¼Œæ¢¯åº¦ä¹Ÿå°ã€‚éš¾ä»¥è¾¾åˆ°å…¨å±€æœ€å° ä¸‹å›¾æ˜¯cross entropyå’Œsquare errorçš„å›¾åƒç¤ºæ„ï¼š å¦‚å›¾ï¼Œsquare losséš¾ä»¥åˆ°è¾¾å…¨å±€æœ€å°ã€‚ ç”Ÿæˆå¼æ¨¡å‹ä¸åˆ¤åˆ«å¼æ¨¡å‹çš„åŒºåˆ«ç”Ÿæˆå¼å¯¹è”åˆæ¦‚ç‡åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå†é€šè¿‡è´å¶æ–¯å®šç†è·å¾—åéªŒæ¦‚ç‡ï¼›è€Œåˆ¤åˆ«å¼æ¨¡å‹ç›´æ¥å¯¹åéªŒæ¦‚ç‡å»ºæ¨¡ã€‚äºŒè€…æ‰€å®šä¹‰çš„function setæ˜¯ä¸€è‡´çš„ï¼Œä½†åŒä¸€ç»„æ•°æ®å¯èƒ½ä¼šå¾—åˆ°ä¸åŒçš„wå’Œbã€‚ äºŒè€…ä¼˜åŠ£å¯¹æ¯”ï¼š æ•°æ®é‡å¤šæ—¶ï¼Œä¸€èˆ¬æ¥è¯´åˆ¤åˆ«å¼æ¨¡å‹ä¼šæ›´å¥½ã€‚å› ä¸ºåˆ¤åˆ«å¼æ¨¡å‹æ²¡æœ‰å…ˆéªŒå‡è®¾ï¼Œå®Œå…¨ä¾èµ–äºæ•°æ®ã€‚ä½†å¦‚æœæ•°æ®æœ‰å™ªå£°ï¼Œå®¹æ˜“å—å½±å“ã€‚ ç”Ÿæˆå¼æ¨¡å‹æ˜¯æœ‰ä¸€å®šçš„å‡è®¾çš„ï¼Œå½“å‡è®¾é”™è¯¯ï¼Œä¼šå½±å“åˆ†ç±»æ•ˆæœã€‚ æ­£å› ä¸ºæœ‰ä¸€å®šçš„å…ˆéªŒå‡è®¾ï¼Œå½“æ•°æ®é‡å¾ˆå°‘æ—¶ï¼Œå¯èƒ½æ•ˆæœä¼šä¸é”™ï¼›å¯¹äºå™ªå£°æ›´å…·æœ‰é²æ£’æ€§ã€‚ å…ˆéªŒå¯ä»¥ä»å…¶ä»–æ•°æ®æºè·å¾—æ¥å¸®åŠ©ç‰¹å®šä»»åŠ¡ï¼Œå¦‚è¯­éŸ³è¯†åˆ«é—®é¢˜ã€‚ logisticçš„å±€é™æœ¬è´¨ä»æ˜¯ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨ï¼Œæ²¡åŠæ³•åˆ†ç±»éçº¿æ€§çš„æ•°æ®ã€‚å¦‚ä½•è§£å†³è¯¥é—®é¢˜?å°†logistic regression modelæ‹¼æ¥èµ·æ¥ï¼Œå‰é¢çš„modelå¯¹æ•°æ®è¿›è¡Œfeature transformationï¼Œç„¶åå†å¯¹æ–°çš„featureè¿›è¡Œåˆ†ç±»ã€‚ logisticä¸deep learningçš„è”ç³»ï¼šå¦‚æœå°†logistic regressionçš„ä¸€ä¸ªå•å…ƒç§°ä¸ºneuronï¼Œæ‹¼èµ·æ¥å°±æ˜¯neural networkäº†ï¼ï¼ï¼]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Classification</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†8]]></title>
    <url>%2F2018%2F09%2F23%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%868%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Pytorch]torch.max()æœ‰ä¸¤ç§ä¸åŒå†™æ³•ã€‚torch.max(input) â†’ Tensor è¿”å›å…¶ä¸­æœ€å¤§çš„å…ƒç´ torch.max(input, dim, keepdim=False, out=None) â†’ (Tensor, LongTensor) è¿”å›è¯¥ç»´åº¦ä¸Šæœ€å¤§å€¼ï¼Œä»¥åŠå¯¹åº”çš„index 2ï¸âƒ£[Pytorch]å°†æ¨¡å‹åŒæ—¶éƒ¨ç½²åˆ°å¤šå¼ å¡ä¸Šè®­ç»ƒï¼Œæœ¬è´¨å°±æ˜¯å°†ä¸€ä¸ªbatchçš„æ•°æ®splitï¼Œé€åˆ°å„ä¸ªmodelï¼Œç„¶ååˆå¹¶ç»“æœã€‚ 123456model = nn.DataParallel(model)device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")model.to(device)for data in rand_loader: input = data.to(device) output = model(input) 3ï¸âƒ£[æ±‚å¯¼]æ ‡é‡ã€å‘é‡ã€çŸ©é˜µä¹‹é—´çš„æ±‚å¯¼æœ‰ä¸¤ç§å¸ƒå±€ï¼Œå³åˆ†å­å¸ƒå±€å’Œåˆ†æ¯å¸ƒå±€ã€‚åˆ†å­å¸ƒå±€å’Œåˆ†æ¯å¸ƒå±€åªå·®ä¸€ä¸ªè½¬ç½®ã€‚æˆ‘çš„è®°æ³•ï¼šåœ¨æ±‚å¯¼è¿‡ç¨‹ä¸­ï¼Œå‡è®¾åˆ†æ¯ä¸ºm*nï¼Œåˆ†å­ä¸º k*nï¼Œåˆ™å¯¼æ•°çŸ©é˜µåº”è¯¥ä¸º k*m ã€‚ä¸€äº›ç‰¹æ®Šçš„å¦‚æ ‡é‡å¯¹çŸ©é˜µæ±‚å¯¼ç­‰é™¤å¤–ã€‚å…·ä½“ç›´æ¥æŸ¥è¡¨ï¼šhttps://en.m.wikipedia.org/wiki/Matrix_calculus æŒ‰ä½è®¡ç®—æ±‚å¯¼ï¼šå‡è®¾ä¸€ä¸ªå‡½æ•°$f(x)$çš„è¾“å…¥æ˜¯æ ‡é‡$x$ã€‚å¯¹äºä¸€ç»„Kä¸ªæ ‡é‡$x_1,Â·Â·Â· ,x_K$ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡$f(x)$å¾—åˆ°å¦å¤–ä¸€ç»„Kä¸ªæ ‡é‡$z_1,Â·Â·Â· ,z_K$ï¼Œ$z_k = f(x_k),âˆ€k = 1,Â·Â·Â· ,K$å…¶ä¸­ï¼Œ$f(x)$æ˜¯æŒ‰ä½è¿ç®—çš„ï¼Œå³$[f(x)]_i = f(x_i)$å…¶å¯¼æ•°æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼š Referenceï¼šhttps://en.m.wikipedia.org/wiki/Matrix_calculushttps://blog.csdn.net/uncle_gy/article/details/78879131https://nndl.github.io/chap-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.pdf]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>æ±‚å¯¼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•7]]></title>
    <url>%2F2018%2F09%2F23%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%957%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£softmaxçš„numpyå®ç°123def softmax(x,axis=0): """Compute softmax values for each sets of scores in x.""" return np.exp(x) / np.sum(np.exp(x), axis=axis) 2ï¸âƒ£numpy æ‰‹åŠ¨æ±‚å¯¼relu123456789101112131415161718192021222324252627282930313233343536import numpy as np# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random input and output datax = np.random.randn(N, D_in)y = np.random.randn(N, D_out)# Randomly initialize weightsw1 = np.random.randn(D_in, H)w2 = np.random.randn(H, D_out)learning_rate = 1e-6for t in range(500): # Forward pass: compute predicted y h = x.dot(w1) h_relu = np.maximum(h, 0) y_pred = h_relu.dot(w2) # Compute and print loss loss = np.square(y_pred - y).sum() print(t, loss) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.T.dot(grad_y_pred) grad_h_relu = grad_y_pred.dot(w2.T) grad_h = grad_h_relu.copy() grad_h[h &lt; 0] = 0 grad_w1 = x.T.dot(grad_h) # Update weights w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 3ï¸âƒ£Pytorchå®ç°relu1234567891011121314151617181920212223242526272829303132import torchclass MyReLU(torch.autograd.Function): """ We can implement our own custom autograd Functions by subclassing torch.autograd.Function and implementing the forward and backward passes which operate on Tensors. """ @staticmethod def forward(ctx, input): """ In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. """ ctx.save_for_backward(input) #ä¸ºäº†ä¹‹åçš„backwardè®¡ç®— return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): """ In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. """ input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input &lt; 0] = 0 return grad_input 4ï¸âƒ£Pytorchåœ¨å¤šå¼ å¡ä¸Šéƒ¨ç½²123456model = nn.DataParallel(model)device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")model.to(device)for data in rand_loader: input = data.to(device) output = model(input)]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¬è¾¾è§‚æ¯ç°åœºç­”è¾©æœ‰æ„Ÿ]]></title>
    <url>%2F2018%2F09%2F19%2F%E8%A7%81%E9%97%BB%26%E6%83%B3%E6%B3%95%2F%E5%90%AC%E8%BE%BE%E8%A7%82%E6%9D%AF%E7%8E%B0%E5%9C%BA%E7%AD%94%E8%BE%A9%E6%9C%89%E6%84%9F%2F</url>
    <content type="text"><![CDATA[å‰å‡ æ—¥ï¼ˆå‘¨æ—¥ï¼‰å»äº†è¾¾è§‚æ¯ç­”è¾©ç°åœºå¬äº†å‰10ååšäº†æŠ¥å‘Šï¼Œæœ‰äº†ä¸€äº›æ„Ÿæƒ³ï¼Œä½†ä¸€ç›´æ²¡æœ‰æŠ½å‡ºæ—¶é—´å†™ä¸€ä¸‹è‡ªå·±çš„æ„Ÿæƒ³ï¼ˆæ‡’ï¼‰ã€‚ è‡ªå·±å¤§æ¦‚èŠ±äº†åæ¥å¤©åšäº†ä¸€ä¸‹æ¯”èµ›ï¼Œå®é™…ä¸Šä¹Ÿå°±æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»çš„æ¯”èµ›ï¼Œå› ä¸ºæ²¡æœ‰æ¯”èµ›ç»éªŒçš„ç¼˜æ•…ï¼Œèµ°äº†å¾ˆå¤šå¼¯è·¯ã€‚ä¸è¿‡ä¹Ÿå­¦åˆ°äº†ä¸€äº›ä¸œè¥¿ã€‚ ç°è®°å½•å‰ååçš„ä¸€äº›idea/trickï¼š æ•°æ®å¢å¼º å› ä¸ºç»™çš„å¥å­é•¿åº¦å¾ˆé•¿ï¼Œå› æ­¤åœ¨åšæˆªæ–­çš„æ—¶å€™åé¢çš„å°±æ²¡æ³•è®­ç»ƒåˆ°äº†ï¼Œå¯ä»¥å°†æ–‡æœ¬å€’åºä½œä¸ºæ–°çš„æ•°æ®è®­ç»ƒæ¨¡å‹ã€‚å¯ä»¥å……åˆ†åˆ©ç”¨åˆ°æ•°æ® å°†æ•°æ®æ‰“ä¹±ã€éšæœºåˆ é™¤ï¼Œå®é™…ä¸Šå°±æ˜¯å¯¹ä¸€ä¸ªå¥å­çš„è¯è¿›è¡Œsampleå†ç»„åˆ æ‰“ä¹±è¯åºä»¥å¢åŠ æ•°æ®é‡ ä½¿ç”¨pseudo labelingï¼Œä½†æœ‰çš„é˜Ÿä¼ä½¿ç”¨è¿™ä¸ªåšå‡ºæ•ˆæœäº†ï¼Œä½†æœ‰çš„æ²¡æœ‰ ç‰¹å¾å·¥ç¨‹ å‡è®¾å¼€å¤´ä¸­é—´ç»“å°¾çš„ä¿¡æ¯å¯¹åˆ†ç±»æœ‰å¸®åŠ©ï¼Œå› æ­¤æˆªå–è¯¥éƒ¨åˆ†ä¿¡æ¯åšè®­ç»ƒ æ”¹è¿›baselineçš„tfidfçš„ç‰¹å¾å·¥ç¨‹æ–¹æ³•ï¼Œä½¿ç”¨åŸºäºç†µçš„è¯æƒé‡è®¡ç®— é™ç»´ï¼Œç•™ä¸‹æœ€é‡è¦çš„ç‰¹å¾ã€‚å…ˆç”¨å¡æ–¹åˆ†å¸ƒé™åˆ°20ä¸‡ï¼Œå†ç”¨SVDé™åˆ°8000 å°†word2vecå’ŒGloVeæ‹¼æ¥èµ·æ¥ä½œä¸ºdeep learningæ¨¡å‹çš„è¾“å…¥ å°†æ–‡ç« åˆ†æ®µï¼Œæ¯æ®µå–å‰20å20æ‹¼èµ·æ¥ æ¨¡å‹èåˆ æ‰€æœ‰é˜Ÿä¼éƒ½æ— ä¸€ä¾‹å¤–ä½¿ç”¨äº†æ¨¡å‹èåˆï¼Œstackingæˆ–è€…ç®€å•çš„æŠ•ç¥¨ DL+ML â€”&gt; lgbm model â€”&gt; voting æ·±åº¦æ¨¡å‹+ä¼ ç»Ÿæ¨¡å‹ï¼Œåœ¨æ·±åº¦æ¨¡å‹æœ€åä¸€å±‚åŠ å…¥ä¼ ç»Ÿæ¨¡å‹çš„ä¿¡æ¯/feature åå‘é€‰æ‹©å‰”é™¤å†—ä½™æ¨¡å‹ DL&amp;å…¶ä»– HANï¼Œé€‰æ‹©10ä¸ªattention vector å¯¹æ˜“é”™ç±»å¢åŠ æƒé‡ï¼Œé€šè¿‡æ”¹å˜æŸå¤±å‡½æ•°æ¥å¢åŠ æƒé‡ CNN, [1,2,3,4,5,6]*600 æå‡ºæ–°çš„æ¨¡å‹ï¼ˆç¬¬ä¸€åï¼‰ å…¶å®é™¤äº†ä¸€äº›trickï¼Œæˆ‘è¿˜æ˜¯æœ‰äº›å¤±æœ›çš„ï¼Œå› ä¸ºéƒ½æ˜¯ç”¨æ¨¡å‹èåˆå †å‡ºæ¥çš„ï¼Œè¿™ä¹Ÿè®©æˆ‘å¯¹æ¯”èµ›å¤±å»äº†ä¸€äº›å…´è¶£ã€‚è™½ç„¶èƒ½ç†è§£ç°åœ¨çš„æ¯”èµ›éƒ½æ˜¯è¿™æ ·çš„ï¼Œä½†æ„Ÿè§‰å®åœ¨å¤ªæš´åŠ›äº†ã€‚å½“ç„¶ï¼Œå…¶ä¸­è¿˜æ˜¯æœ‰ä¸€äº›äº®ç‚¹çš„ï¼Œæœ‰ä¸€æ”¯é˜Ÿä¼ç«‹æ„å¾ˆé«˜ï¼Œä»ç†è§£ä¸šåŠ¡çš„è§’åº¦å‡ºå‘è€Œä¸æ˜¯å †æ¨¡å‹ï¼Œä¹Ÿå–å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼›è¿˜æœ‰ä¸€ä¸ªä½¿ç”¨äº†æœ€æ–°è®ºæ–‡ä¸­çš„ç‰¹å¾å·¥ç¨‹æ”¹è¿›æ–¹æ³•ï¼Œä»¤æˆ‘è€³ç›®ä¸€æ–°ï¼›ä»¥åŠç¬¬ä¸€ååœ¨æ¯”èµ›è¿‡ç¨‹ä¸­æå‡ºæ¥ä¸‰ä¸ªæ–°çš„æ¨¡å‹ã€‚ Anywayï¼Œæˆ‘ç›®å‰è¿˜æ˜¯å¤ªèœäº†ï¼Œè¿˜æ˜¯å®‰å¿ƒæç§‘ç ”å§ã€‚_(:Ğ·ã€âˆ )]]></content>
      <tags>
        <tag>æœ‰æ„Ÿ</tag>
        <tag>æ¯”èµ›</tag>
        <tag>è¾¾è§‚æ¯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†7]]></title>
    <url>%2F2018%2F09%2F16%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%867%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Pytorch]åªæœ‰ä¸€ä¸ªå…ƒç´ çš„tensorï¼Œå¯ç”¨.item()æ¥è·å–å…ƒç´  tensor &lt;â€”&gt; numpy ç›¸äº’è½¬åŒ–ä¼šå…±äº«å†…éƒ¨æ•°æ®ï¼Œå› æ­¤æ”¹å˜å…¶ä¸­ä¸€ä¸ªä¼šæ”¹å˜å¦ä¸€ä¸ª å¯ç”¨ä½¿ç”¨ .to æ¥ç§»åŠ¨åˆ°è®¾å¤‡ .detech() detach it from the computation history, and to prevent future computation from being tracked. å°†å…¶ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»ï¼Œå˜ä¸ºå¶å­èŠ‚ç‚¹ï¼Œå¹¶ä¸”requires_grad=False Function è®°å½•äº†è¿™ä¸ªtensoræ˜¯æ€ä¹ˆæ¥çš„ï¼Œæ‰€æœ‰çš„tensoréƒ½æœ‰ï¼Œé™¤éæ˜¯ç”¨æˆ·è‡ªå®šä¹‰çš„ï¼š 2ï¸âƒ£[åæ–¹å·®]å…³äºåæ–¹å·®çš„ç†è§£ï¼Œxä¸yå…³äºæŸä¸ªè‡ªå˜é‡çš„å˜åŒ–ç¨‹åº¦ï¼Œå³åº¦é‡äº†xä¸yä¹‹é—´çš„è”ç³»ã€‚https://www.zhihu.com/question/20852004]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>åæ–¹å·®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 4:Classification:Probabilistic Generative Model]]></title>
    <url>%2F2018%2F09%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%204%20Classification%20%20Probabilistic%20Generative%20Model%2F</url>
    <content type="text"><![CDATA[ä¸ºä»€ä¹ˆä¸ä½¿ç”¨regressionæ¥åˆ†ç±»ï¼Ÿ1ï¸âƒ£å¦‚æœä½¿ç”¨regressionçš„æ€æƒ³æ¥åˆ†ç±»ï¼Œä¼šå¯¹ç¦»è¾¹ç•Œè¾ƒè¿œçš„ç‚¹è¿›è¡Œæƒ©ç½šï¼š 2ï¸âƒ£å¦‚æœå¤šåˆ†ç±»ä½¿ç”¨regressionï¼Œå¦‚class 1, class 2, class 3ï¼›åˆ™éšå¼åœ°å‡è®¾äº†class 1 å’Œ class 2è¾ƒä¸ºæ¥è¿‘ï¼Œå¦‚æœæ²¡æœ‰è¿™ç§æ¥è¿‘å…³ç³»ï¼Œåˆ™åˆ†ç±»ä¼šä¸æ­£ç¡®ã€‚ é—®é¢˜æè¿°ä¸å®šä¹‰ å½“På¤§äº0.5åˆ™æ˜¯C1ç±»ï¼Œåä¹‹æ˜¯C2ç±»å…ˆéªŒP(C1)å’ŒP(C2)éƒ½å¥½è®¡ç®—ï¼Œè®¡ç®—C1å æ€»çš„æ¯”ä¾‹å³å¯å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—çš„å°±æ˜¯p(x|C) è¿™ä¸€æƒ³æ³•ï¼Œæœ¬è´¨æ˜¯å¾—åˆ°äº†ç”Ÿæˆå¼æ¨¡å‹ï¼š åŸç†æ¦‚è¿°ç°å‡è®¾è®­ç»ƒæ•°æ®ç‚¹çš„åˆ†å¸ƒæœä»é«˜æ–¯åˆ†å¸ƒï¼šï¼ˆæ˜¾ç„¶å¯ä»¥è‡ªå·±è®¾ä»»ä½•åˆ†å¸ƒï¼‰å³æ•°æ®ä»é«˜æ–¯åˆ†å¸ƒé‡‡æ ·å¾—åˆ°ï¼š æ ¹æ®æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œå¯ä»¥è·å¾—æ¯ä¸ªç±»åˆ«çš„Î¼å’ŒÎ£ï¼š å¾—åˆ°äº†å‚æ•°åï¼Œå³å¯ä»£å…¥å¾—åˆ°P(C|x) ï¼š åˆšåˆšå‡è®¾$Î£$å¯¹äºä¸åŒç±»åˆ«ä¸åŒï¼Œç°æˆ‘ä»¬ä»¤ä¸åŒç±»åˆ«å…±äº«ç›¸åŒ$Î£$ï¼šï¼ˆå› ä¸ºåæ–¹å·®ä»£è¡¨çš„æ˜¯ä¸åŒfeatureä¹‹é—´çš„è”ç³»ï¼Œå¯ä»¥è®¤ä¸ºæ˜¯å’Œç±»åˆ«æ— å…³çš„ï¼‰ $Î£$çš„è®¡ç®—å…¬å¼æ˜¯åŠ æƒæ±‚å’Œï¼š åœ¨ä½¿ç”¨äº†ç›¸åŒçš„åæ–¹å·®çŸ©é˜µåï¼Œè¾¹ç•Œå°±æ˜¯çº¿æ€§çš„ï¼ˆåé¢ä¼šæåˆ°ä¸ºä»€ä¹ˆæ˜¯è¿™æ ·ï¼‰ï¼š æ€»ç»“ï¼š ä¸‰æ­¥èµ°ï¼Œå®šä¹‰function setï¼Œè®¡ç®—Î¼å’Œåæ–¹å·®çŸ©é˜µï¼Œå¾—åˆ°best functionï¼š æ³¨æ„åˆ°ï¼Œå¦‚æœæˆ‘ä»¬è®¤ä¸ºï¼Œä¸åŒfeatureä¹‹é—´æ²¡æœ‰å…³ç³»ï¼Œæ¯ä¸ªfeatureç¬¦åˆç‰¹å®šçš„é«˜æ–¯åˆ†å¸ƒï¼Œåˆ™è¯¥åˆ†ç±»å™¨åˆ™æ˜¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼š åˆ†ç±»ä¸logistics regressionç°æ¨å¯¼ï¼Œè¯¥åˆ†ç±»é—®é¢˜ä¸logistics regressionä¹‹é—´çš„è”ç³»ï¼šå³ï¼š å‡è®¾æ•°æ®æœä»é«˜æ–¯åˆ†å¸ƒï¼Œå…±äº«$Î£$ æ¨å¯¼â‘ æ€»æ¡†æ¶ï¼š ä»¤ åˆ™æœ‰ï¼š â‘¡zçš„è¿›ä¸€æ­¥æ¨å¯¼ä¸ç®€åŒ–ï¼š å°†zå±•å¼€ï¼š è€Œç¬¬ä¸€éƒ¨åˆ†æœ‰ï¼š ç¬¬ä¸€éƒ¨åˆ†ç›¸é™¤ï¼Œæœ‰ï¼š å†è¿›è¡Œå±•å¼€ï¼Œæœ‰ï¼š æœ€ç»ˆzçš„å…¬å¼ä¸ºï¼š ç”±äºå…±äº«åæ–¹å·®çŸ©é˜µï¼Œåˆ™å¯ä»¥æ¶ˆå»éƒ¨åˆ†ï¼Œå¾—åˆ°ï¼š æ›¿æ¢æˆwå’Œbï¼š â‘¢æœ€ç»ˆï¼Œå°†zå¸¦å›åˆ°åŸå¼ï¼š æ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦å†ä¼°è®¡N1,N2,Î¼å’ŒÎ£ï¼Œç›´æ¥è®¡ç®—wå’Œbå³å¯ã€‚ä¹Ÿå› æ­¤ï¼Œåˆ†ç•Œçº¿æ˜¯çº¿æ€§çš„ã€‚ å…¨è¿‡ç¨‹ï¼š]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Classification</tag>
        <tag>Probabilistic Generative Model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 3:Gradient Descent]]></title>
    <url>%2F2018%2F09%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%203%20Gradient%20Descent%2F</url>
    <content type="text"><![CDATA[Gradient Descent tipstip 1ï¼šAdaptive Learning RatesAdagradåŸºæœ¬æ€æƒ³ å…¶ä¸­Ïƒæ˜¯ä¹‹å‰æ‰€æœ‰çš„æ¢¯åº¦çš„å¹³æ–¹æ ¹ åŒ–ç®€å½¢å¼ï¼š ä¸ºä»€ä¹ˆè¦æ€ä¹ˆåšï¼Ÿè€ƒè™‘ä¸€ä¸ªå¼€å£å‘ä¸Šçš„äºŒæ¬¡å‡½æ•° ä¹Ÿå³ï¼Œæœ€å¥½çš„æ­¥é•¿æ˜¯ä¸€æ¬¡å¯¼é™¤ä»¥äºŒæ¬¡å¯¼ï¼Œä½†äºŒæ¬¡å¯¼è®¡ç®—é‡å¤§ï¼Œå› æ­¤ä½¿ç”¨è¿‘ä¼¼çš„æ–¹å¼ï¼šå¯¹ä¸€æ¬¡å¯¼ä½œå¤šæ¬¡çš„sampleã€‚ä¸‹å›¾æ˜¾ç¤ºï¼Œå¦‚æœäºŒæ¬¡å¯¼å°ï¼Œé‚£ä¹ˆå¤šæ¬¡sampleè·å¾—çš„ä¸€æ¬¡å¯¼ä¹Ÿå°ï¼Œåä¹‹åˆ™å¤§ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€æ¬¡å¯¼åœ¨æŸç§ç¨‹åº¦ä¸Šå¯ä»¥åæ˜ äºŒæ¬¡å¯¼çš„å¤§å°ï¼Œæ‰€ä»¥ç›´æ¥ç”¨ä¸€æ¬¡å¯¼è¿‘ä¼¼ï¼Œå¯ä»¥å‡å°‘è®¡ç®—é‡ã€‚ tip 2ï¼šfeature scaling èƒ½å¤Ÿæ”¹å˜lossçš„åˆ†å¸ƒï¼Œä¸Šå›¾1ä¸­w2å¯¹lossçš„å½±å“è¾ƒå¤§ï¼Œåˆ™è¾ƒé™¡å³­ï¼Œå‚æ•°æ›´æ–°å°±è¾ƒå›°éš¾ï¼Œéœ€è¦adaptive learning rateï¼›å¦‚æœè¿›è¡Œfeature scalingï¼Œèƒ½å¤Ÿæ›´å¥½è¾¾åˆ°local optimal Gradient Descent Theoryå¦ä¸€ç§è§’åº¦çœ‹gradient descentï¼š åŸºæœ¬æ€æƒ³ï¼šæˆ‘ä»¬å¸Œæœ›æ¯ä¸€æ¬¡éƒ½åœ¨å½“å‰ç‚¹é™„è¿‘æ‰¾åˆ°ä¸€ä¸ªæœ€å°çš„ç‚¹ï¼Œå³åœ¨ä¸€ä¸ªèŒƒå›´å†…ï¼š åº”è¯¥å¦‚ä½•æ‰¾åˆ°è¯¥æœ€å°ç‚¹ï¼Ÿ æˆ‘ä»¬çŸ¥é“ï¼Œæ³°å‹’çº§æ•°çš„å½¢å¼ï¼š å½“xæ¥è¿‘x0æ—¶ï¼Œä¼šæœ‰å¦‚ä¸‹è¿‘ä¼¼ï¼š æ¨å¹¿åˆ°å¤šå…ƒæ³°å‹’çº§æ•°åˆ™æœ‰ï¼š é‚£ä¹ˆï¼Œå¦‚å‰æ‰€è¿°ï¼Œxæ¥è¿‘x0ï¼Œå¯¹äºå›¾ä¸­ï¼Œå³åœ†åœˆè¶³å¤Ÿå°æ—¶ï¼š ç®€åŒ–ç¬¦å·ï¼š æ‰€ä»¥å¯ä»¥ç®€å†™æˆï¼š ç”±äºs,u,véƒ½æ˜¯å¸¸æ•°ï¼Œåœ¨åœ†åœˆèŒƒå›´å†…å¯»æ‰¾æœ€å°å€¼å¯¹åº”çš„å‚æ•°å¯ä»¥ç®€åŒ–æˆï¼š å†åº¦ç®€åŒ–ï¼Œå¯ä»¥è¡¨è¾¾æˆï¼š åœ¨å›¾ä¸­å¯ä»¥ç”»ä¸ºä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯ æ˜¾ç„¶ï¼Œå½“åæ–¹å‘æ—¶ï¼Œæœ€å°ï¼š ä¹Ÿå³ï¼š æœ€ç»ˆå®Œæ•´çš„å¼å­ï¼š å› æ­¤ï¼Œå½“learning rateä¸å¤Ÿå°æ—¶ï¼Œæ˜¯ä¸æ»¡è¶³æ³°å‹’çº§æ•°è¿‘ä¼¼çš„ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Gradient Descent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 2:Bias and Variance]]></title>
    <url>%2F2018%2F09%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%202%20Bias%20and%20Variance%2F</url>
    <content type="text"><![CDATA[å¦‚ä½•ç†è§£bias&amp;variancebiasæ˜¯function spaceä¸­å¿ƒç¦»optimal modelçš„å·®è·ï¼Œvarianceæ˜¯æŸæ¬¡å®éªŒæ‰€å¾—æ¨¡å‹ç¦»function spaceä¸­å¿ƒçš„è·ç¦»ã€‚ æ¯”å¦‚è¯´ï¼Œç®€å•åœ°æ¨¡å‹çš„function spaceå°ï¼Œéšæœºæ€§å°ï¼Œå› æ­¤varianceå°ï¼Œä½†ä¹Ÿå› ä¸ºfunction spaceå°ï¼Œè¡¨ç¤ºèƒ½åŠ›æœ‰é™ï¼Œå› æ­¤biaså¤§ã€‚ å¦‚å›¾ï¼šè¯¥å›¾ä¸­è“è‰²åœˆä»£è¡¨æ¨¡å‹æ‰€èƒ½è¡¨è¾¾çš„èŒƒå›´ã€‚ å¦‚ä½•è§£å†³varianceå¤§çš„é—®é¢˜â‘ æ›´å¤šçš„dataâ‘¡regularizationï¼šå¼ºè¿«functionæ›´å¹³æ»‘ï¼Œå› æ­¤å‡å°varianceï¼Œä½†å› ä¸ºè°ƒæ•´äº†function spaceï¼Œå¯èƒ½ä¼šå¢åŠ biasã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>bias&amp;variance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯9]]></title>
    <url>%2F2018%2F09%2F16%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D9%2F</url>
    <content type="text"><![CDATA[ç™½é›ªæ­Œé€æ­¦åˆ¤å®˜å½’äº¬[å”] å²‘å‚åŒ—é£å·åœ°ç™½è‰æŠ˜ï¼Œèƒ¡å¤©å…«æœˆå³é£é›ªã€‚å¿½å¦‚ä¸€å¤œæ˜¥é£æ¥ï¼Œåƒæ ‘ä¸‡æ ‘æ¢¨èŠ±å¼€ã€‚æ•£å…¥ç å¸˜æ¹¿ç½—å¹•ï¼Œç‹è£˜ä¸æš–é”¦è¡¾è–„ã€‚å°†å†›è§’å¼“ä¸å¾—æ§ï¼Œéƒ½æŠ¤é“è¡£å†·éš¾ç€ã€‚ç€šæµ·é˜‘å¹²ç™¾ä¸ˆå†°ï¼Œæ„äº‘æƒ¨æ·¡ä¸‡é‡Œå‡ã€‚ä¸­å†›ç½®é…’é¥®å½’å®¢ï¼Œèƒ¡ç´çµç¶ä¸ç¾Œç¬›ã€‚çº·çº·æš®é›ªä¸‹è¾•é—¨ï¼Œé£æ£çº¢æ——å†»ä¸ç¿»ã€‚è½®å°ä¸œé—¨é€å›å»ï¼Œå»æ—¶é›ªæ»¡å¤©å±±è·¯ã€‚å±±å›è·¯è½¬ä¸è§å›ï¼Œé›ªä¸Šç©ºç•™é©¬è¡Œå¤„ã€‚ http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290 ç»å‘½è¯—è°­å—£åŒæœ›é—¨æŠ•æ­¢æ€å¼ ä¿­ï¼Œå¿æ­»é¡»è‡¾å¾…æœæ ¹ã€‚æˆ‘è‡ªæ¨ªåˆ€å‘å¤©ç¬‘ï¼Œå»ç•™è‚èƒ†ä¸¤æ˜†ä»‘ï¼]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch backward()æµ…æ]]></title>
    <url>%2F2018%2F09%2F16%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPytorch%20backward()%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[æœ€è¿‘åœ¨çœ‹pytorchæ–‡æ¡£çš„æ—¶å€™ï¼Œçœ‹åˆ°backwardå†…æœ‰ä¸€ä¸ªå‚æ•°gradientï¼Œåœ¨ç»è¿‡æŸ¥é˜…äº†ç›¸å…³èµ„æ–™å’Œè¿›è¡Œäº†å®éªŒåï¼Œå¯¹backwardæœ‰äº†æ›´æ·±çš„è®¤è¯†ã€‚ backward1ï¸âƒ£å¦‚æœè°ƒç”¨backwardçš„æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œå¦‚ï¼šloss.backward()åˆ™gradientä¸éœ€è¦æ‰‹åŠ¨ä¼ å…¥ï¼Œä¼šè‡ªåŠ¨æ±‚å¯¼ã€‚ä¾‹å­:$a=[x_1,x_2],b=\frac{x_1+x_2}{2}$åˆ™bå¯¹aæ±‚å¯¼ï¼Œæœ‰ï¼š$\dfrac {\partial b}{\partial x_{1}}=\frac{1}{2}ï¼Œ\dfrac {\partial b}{\partial x_{2}}=\frac{1}{2}$ 123456import torcha=torch.Tensor([2,3])a.requires_grad=Trueb=torch.mean(a) #tensor(2.5000, grad_fn=&lt;MeanBackward1&gt;)b.backward()a.grad #tensor([0.5000, 0.5000]) gradientæ­¤æ—¶åªæ˜¯åœ¨ç¼©æ”¾åŸgradçš„å¤§å°ï¼Œä¹Ÿå³ä¸æŒ‡å®šgradientå’Œgradient=1æ˜¯ç­‰ä»·çš„ å½“ç„¶ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šgradientï¼Œå…¶ä¸­æŒ‡å®šgradientçš„shapeå¿…é¡»å’Œbçš„ç»´åº¦ç›¸åŒ123gradient=torch.tensor(10.0)b.backward(gradient)a.grad #tensor([5., 5.]) 2ï¸âƒ£å¦‚æœè°ƒç”¨backwardçš„æ˜¯ä¸€ä¸ªå‘é‡ä¾‹å­ï¼š$a=[x_1,x_2],b=[b_1,b_2]$, å…¶ä¸­ $b_1=x_1+x_2,b_2=x_1*x_2$bå¯¹aæ±‚å¯¼ï¼Œæœ‰ï¼š$\dfrac {\partial b_1}{\partial x_{1}}=1,\dfrac {\partial b_1}{\partial x_{2}}=1$ $\dfrac {\partial b_2}{\partial x_{1}}=x_2,\dfrac {\partial b_2}{\partial x_{2}}=x_1$ åœ¨backwardçš„æ—¶å€™åˆ™å¿…é¡»æŒ‡å®šgradientã€‚ 1234567891011121314151617181920import torcha=torch.FloatTensor([2,3])a.requires_grad=Trueb=torch.zeros(2)b[0]=a[0]+a[1]b[1]=a[0]*a[1] # b=tensor([5., 6.], grad_fn=&lt;CopySlices&gt;)gradient=torch.tensor([1.0,0.0])b.backward(gradient,retain_graph=True)a.grad #tensor([1., 1.])ï¼Œè¯´æ˜æ˜¯å¯¹b_1è¿›è¡Œæ±‚å¯¼a.grad.zero_() #å°†æ¢¯åº¦æ¸…ç©ºï¼Œå¦åˆ™ä¼šå åŠ #-------------- #gradient=torch.tensor([0.0,1.0])b.backward(gradient,retain_graph=True)a.grad # tensor([3., 2.])ï¼Œè¯´æ˜å¯¹b_2è¿›è¡Œæ±‚å¯¼a.grad.zero_()# ------------- #gradient=torch.tensor([1.0,1.0])b.backward(gradient,retain_graph=True)a.grad # tensor([4., 3.])ï¼Œå³b_1,b_2çš„å¯¼æ•°çš„å åŠ a.grad.zero_() æ³¨æ„åˆ°b.backward()æ—¶éœ€è¦retain_graphè®¾ä¸ºTrueï¼Œå¦åˆ™åœ¨è®¡ç®—å®Œåä¼šè‡ªåŠ¨é‡Šæ”¾è®¡ç®—å›¾çš„å†…å­˜ï¼Œè¿™æ ·å°±æ²¡æ³•è¿›è¡ŒäºŒæ¬¡åå‘ä¼ æ’­äº†ã€‚ Referencehttps://www.pytorchtutorial.com/pytorch-backward/]]></content>
      <tags>
        <tag>Pytorch</tag>
        <tag>backward</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯8]]></title>
    <url>%2F2018%2F09%2F09%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D8%2F</url>
    <content type="text"><![CDATA[æœ›æœˆæ€€è¿œ[å”] å¼ ä¹é¾„æµ·ä¸Šç”Ÿæ˜æœˆï¼Œå¤©æ¶¯å…±æ­¤æ—¶ã€‚æƒ…äººæ€¨é¥å¤œï¼Œç«Ÿå¤•èµ·ç›¸æ€ã€‚ç­çƒ›æ€œå…‰æ»¡ï¼ŒæŠ«è¡£è§‰éœ²æ»‹ã€‚ä¸å ªç›ˆæ‰‹èµ ï¼Œè¿˜å¯æ¢¦ä½³æœŸã€‚ é¥å¤œï¼Œé•¿å¤œã€‚ http://m.xichuangzhu.com/work/57aca120a341310060e2a09f æ— é¢˜è¨é•‡å†°äº”åä¸ƒè½½çŠ¹å¦‚æ¢¦ï¼Œä¸¾å›½æ²¦äº¡ç¼˜æ±‰åŸã€‚é¾™æ¸¸æµ…æ°´å‹¿è‡ªå¼ƒï¼Œç»ˆæœ‰æ‰¬çœ‰åæ°”å¤©ã€‚ 1951å¹´ï¼Œä¸­å›½äººæ°‘å¿—æ„¿å†›åœ¨æŠ—ç¾æ´æœæˆ˜äº‰ç¬¬ä¸‰æ¬¡æˆ˜å½¹åæ‰“è¿›äº†æ±‰åŸï¼Œè¨é•‡å†°å¾—çŸ¥æ­¤äº‹ï¼Œå›æƒ³èµ·57å¹´å‰çš„ç”²åˆæ‚²æ­Œï¼Œå½“å³ä½œè¯—ä¸€é¦–ã€‚ ç™½é›ªæ­Œé€æ­¦åˆ¤å®˜å½’äº¬[å”] å²‘å‚åŒ—é£å·åœ°ç™½è‰æŠ˜ï¼Œèƒ¡å¤©å…«æœˆå³é£é›ªã€‚å¿½å¦‚ä¸€å¤œæ˜¥é£æ¥ï¼Œåƒæ ‘ä¸‡æ ‘æ¢¨èŠ±å¼€ã€‚æ•£å…¥ç å¸˜æ¹¿ç½—å¹•ï¼Œç‹è£˜ä¸æš–é”¦è¡¾è–„ã€‚å°†å†›è§’å¼“ä¸å¾—æ§ï¼Œéƒ½æŠ¤é“è¡£å†·éš¾ç€ã€‚ç€šæµ·é˜‘å¹²ç™¾ä¸ˆå†°ï¼Œæ„äº‘æƒ¨æ·¡ä¸‡é‡Œå‡ã€‚ä¸­å†›ç½®é…’é¥®å½’å®¢ï¼Œèƒ¡ç´çµç¶ä¸ç¾Œç¬›ã€‚çº·çº·æš®é›ªä¸‹è¾•é—¨ï¼Œé£æ£çº¢æ——å†»ä¸ç¿»ã€‚è½®å°ä¸œé—¨é€å›å»ï¼Œå»æ—¶é›ªæ»¡å¤©å±±è·¯ã€‚å±±å›è·¯è½¬ä¸è§å›ï¼Œé›ªä¸Šç©ºç•™é©¬è¡Œå¤„ã€‚ http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯7]]></title>
    <url>%2F2018%2F09%2F02%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D7%2F</url>
    <content type="text"><![CDATA[æ»•ç‹é˜åºé¥è¥Ÿç”«ç•…ï¼Œé€¸å…´é„é£ã€‚çˆ½ç±å‘è€Œæ¸…é£ç”Ÿï¼Œçº¤æ­Œå‡è€Œç™½äº‘éã€‚ç¢å›­ç»¿ç«¹ï¼Œæ°”å‡Œå½­æ³½ä¹‹æ¨½ï¼›é‚ºæ°´æœ±åï¼Œå…‰ç…§ä¸´å·ä¹‹ç¬”ã€‚å››ç¾å…·ï¼ŒäºŒéš¾å¹¶ã€‚ç©·ç‡çœ„äºä¸­å¤©ï¼Œæå¨±æ¸¸äºæš‡æ—¥ã€‚å¤©é«˜åœ°è¿¥ï¼Œè§‰å®‡å®™ä¹‹æ— ç©·ï¼›å…´å°½æ‚²æ¥ï¼Œè¯†ç›ˆè™šä¹‹æœ‰æ•°ã€‚æœ›é•¿å®‰äºæ—¥ä¸‹ï¼Œç›®å´ä¼šäºäº‘é—´ã€‚åœ°åŠ¿æè€Œå—æºŸæ·±ï¼Œå¤©æŸ±é«˜è€ŒåŒ—è¾°è¿œã€‚å…³å±±éš¾è¶Šï¼Œè°æ‚²å¤±è·¯ä¹‹äººï¼›èæ°´ç›¸é€¢ï¼Œå°½æ˜¯ä»–ä¹¡ä¹‹å®¢ã€‚æ€€å¸é˜è€Œä¸è§ï¼Œå¥‰å®£å®¤ä»¥ä½•å¹´ï¼Ÿ æ³¨é‡Šï¼šé¥è¥Ÿç”«ç•…ï¼Œé€¸å…´é„ï¼ˆchuÃ¡nï¼‰é£ï¼šç™»é«˜æœ›è¿œçš„èƒ¸æ€€é¡¿æ—¶èˆ’ç•…ï¼Œé£˜æ¬²è„±ä¿—çš„å…´è‡´æ²¹ç„¶è€Œç”Ÿã€‚ çˆ½ç±ï¼ˆlÃ iï¼‰å‘è€Œæ¸…é£ç”Ÿï¼Œçº¤æ­Œå‡è€Œç™½äº‘éï¼šå®´ä¼šä¸Šï¼Œæ’ç®«å“èµ·ï¼Œå¥½åƒæ¸…é£æ‹‚æ¥ï¼›æŸ”ç¾çš„æ­Œå£°ç¼­ç»•ä¸æ•£ï¼Œéæ­¢äº†ç™½äº‘é£åŠ¨ã€‚çˆ½ï¼šå½¢å®¹ç±çš„å‘éŸ³æ¸…è„†ã€‚ç±ï¼šæ’ç®«ï¼Œä¸€ç§ç”±å¤šæ ¹ç«¹ç®¡ç¼–æ’è€Œæˆçš„ç®¡ä¹å™¨ã€‚ ç¢ï¼ˆsuÄ«ï¼‰å›­ç»¿ç«¹ï¼Œæ°”å‡Œå½­æ³½ä¹‹æ¨½ï¼šä»Šæ—¥çš„å®´ä¼šï¼Œå¥½æ¯”å½“å¹´ç¢å›­ç«¹æ—çš„èšä¼šï¼Œåœ¨åº§çš„æ–‡äººé›…å£«ï¼Œè±ªçˆ½å–„é¥®çš„æ°”æ¦‚è¶…è¿‡äº†é™¶æ¸Šæ˜ã€‚ç¢å›­ï¼šè¥¿æ±‰æ¢å­ç‹åœ¨ç¢æ°´æ—ä¿®å»ºçš„ç«¹å›­ï¼Œä»–å¸¸å’Œä¸€äº›æ–‡äººåœ¨æ­¤é¥®é…’èµ‹è¯—ã€‚ é‚ºï¼ˆyÃ¨ï¼‰æ°´æœ±åï¼Œå…‰ç…§ä¸´å·ä¹‹ç¬”ï¼šè¿™æ˜¯å€Ÿè¯—äººæ›¹æ¤ã€è°¢çµè¿æ¥æ¯”æ‹Ÿå‚åŠ å®´ä¼šçš„æ–‡äººã€‚é‚ºï¼šä»Šæ²³åŒ—ä¸´æ¼³ï¼Œæ˜¯æ›¹é­å…´èµ·çš„åœ°æ–¹ã€‚æ›¹æ¤æ›¾åœ¨è¿™é‡Œä½œè¿‡ã€Šå…¬å®´è¯—ã€‹ï¼Œè¯—ä¸­æœ‰â€œæœ±åå†’ç»¿æ± â€çš„å¥å­ã€‚ä¸´å·ä¹‹ç¬”ï¼šæŒ‡è°¢çµè¿ï¼Œä»–æ›¾ä»»ä¸´å·ï¼ˆä»Šå±æ±Ÿè¥¿ï¼‰å†…å²ã€‚ å››ç¾ï¼šæŒ‡è‰¯è¾°ã€ç¾æ™¯ã€èµå¿ƒã€ä¹äº‹ã€‚ äºŒéš¾ï¼šè´¤ä¸»ã€å˜‰å®¾ã€‚ åœ°åŠ¿æè€Œå—æºŸæ·±ï¼Œå¤©æŸ±é«˜è€ŒåŒ—è¾°è¿œï¼šåœ°åŠ¿åè¿œï¼Œå—æµ·æ·±é‚ƒï¼›å¤©æŸ±é«˜è€¸ï¼ŒåŒ—ææ˜Ÿè¿œæ‚¬ã€‚ å¸é˜ï¼ˆhÅ«nï¼‰ï¼šåŸæŒ‡å¤©å¸çš„å®ˆé—¨è€…ã€‚è¿™é‡ŒæŒ‡çš‡å¸çš„å®«é—¨ã€‚ å¥‰å®£å®¤ä»¥ä½•å¹´ï¼šä»€ä¹ˆæ—¶å€™æ‰èƒ½åƒè´¾è°Šé‚£æ ·å»ä¾å¥‰å›ç‹å‘¢]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†6]]></title>
    <url>%2F2018%2F09%2F02%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%866%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[dropout]dropoutå½¢å¼:RNNçš„å½¢å¼æœ‰å¤šç§ï¼š recurrent dropoutRNN: $h_t=f(W_h âŠ™ [x_t,h_{t-1}]+b_h)$åŠ ä¸Šdropoutçš„RNNï¼š$h_t=f(W_h âŠ™ [x_t,d(h_{t-1})]+b_h)$ï¼Œå…¶ä¸­$d(\cdot)$ä¸ºdropoutå‡½æ•°åŒç†ï¼šLSTM:$c_t=f_t âŠ™c_{t-1} + i_t âŠ™ d(g_t)$GRU:$h_t=(1-z_t)âŠ™c_{t-1}+z_tâŠ™d(g_t)$ å‚ç›´è¿æ¥çš„dropoutdropoutçš„ä½œç”¨å³æ˜¯å¦å…è®¸Lå±‚æŸä¸ªLSTMå•å…ƒçš„éšçŠ¶æ€ä¿¡æ¯æµå…¥L+1å±‚å¯¹åº”å•å…ƒã€‚ Reference:https://blog.csdn.net/falianghuang/article/details/72910161 2ï¸âƒ£[Pytorch]pack_padded_sequenceç”¨äºRNNä¸­ï¼Œå°†paddingçŸ©é˜µå‹ç¼©:è¿™æ ·å°±å¯ä»¥å®ç°åœ¨RNNä¼ è¾“è¿‡ç¨‹ä¸­çŸ­å¥æå‰ç»“æŸã€‚ pad_packed_sequenceæ˜¯pack_padded_sequenceçš„é€†è¿ç®—ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>dropout</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æˆ‘æ²¡æœ‰è¯´è¯]]></title>
    <url>%2F2018%2F08%2F29%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%88%91%E6%B2%A1%E6%9C%89%E8%AF%B4%E8%AF%9D%2F</url>
    <content type="text"><![CDATA[ã€Šæˆ‘æ²¡æœ‰è¯´è¯ã€‹ çº³ç²¹æ€å…±äº§å…šæ—¶ï¼Œæˆ‘æ²¡æœ‰å‡ºå£°â€”â€”å› ä¸ºæˆ‘ä¸æ˜¯å…±äº§å…šå‘˜ï¼›æ¥ç€ä»–ä»¬è¿«å®³çŠ¹å¤ªäººï¼Œæˆ‘æ²¡æœ‰å‡ºå£°â€”â€”å› ä¸ºæˆ‘ä¸æ˜¯çŠ¹å¤ªäººï¼›ç„¶åä»–ä»¬æ€å·¥ä¼šæˆå‘˜ï¼Œæˆ‘æ²¡æœ‰å‡ºå£°â€”â€”å› ä¸ºæˆ‘ä¸æ˜¯å·¥ä¼šæˆå‘˜ï¼›åæ¥ä»–ä»¬è¿«å®³å¤©ä¸»æ•™å¾’ï¼Œæˆ‘æ²¡æœ‰å‡ºå£°â€”â€”å› ä¸ºæˆ‘æ˜¯æ–°æ•™å¾’ï¼›æœ€åå½“ä»–ä»¬å¼€å§‹å¯¹ä»˜æˆ‘çš„æ—¶å€™ï¼Œå·²ç»æ²¡æœ‰äººèƒ½ç«™å‡ºæ¥ä¸ºæˆ‘å‘å£°äº†]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning NLP best practicesç¬”è®°]]></title>
    <url>%2F2018%2F08%2F26%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FDeep%20Learning%20NLP%20best%20practices%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[åšå®¢åœ°å€ï¼šhttp://ruder.io/deep-learning-nlp-best-practices/index.htmlä¸ªäººè§‰å¾—è¿™ç¯‡æ–‡ç« å†™å¾—å¾ˆå¥½ï¼Œæœ‰è®¸å¤šå®è·µå¾—åˆ°çš„ç»éªŒï¼Œé€šè¿‡è¿™ç¯‡å¯ä»¥é¿å…èµ°ä¸€äº›å¼¯è·¯ã€‚ PracticesWord Embedding The optimal dimensionality of word embeddings is mostly task-dependent: a smaller dimensionality works better for more syntactic tasks such as named entity recognition or part-of-speech (POS) tagging, while a larger dimensionality is more useful for more semantic tasks such as sentiment analysis. å¯¹äºåå‘è¯­æ³•çš„ï¼Œä½¿ç”¨ç»´åº¦ä½ä¸€äº›çš„è¯å‘é‡ï¼›è€Œå¯¹äºåå‘è¯­ä¹‰å†…å®¹çš„ï¼Œä½¿ç”¨ç»´åº¦å¤§ä¸€äº›çš„è¯å‘é‡ï¼Œå¦‚æƒ…æ„Ÿåˆ†æã€‚ LSTM Depth performance improvements of making the model deeper than 2 layers are minimal LSTMæ·±åº¦æœ€å¥½ä¸è¦è¶…è¿‡ä¸¤å±‚ã€‚ Optimization It is often thought that Adam clearly outperforms vanilla stochastic gradient descent (SGD). However, while it converges much faster than SGD, it has been observed that SGD with learning rate annealing slightly outperforms Adam. Recent work furthermore shows that SGD with properly tuned momentum outperforms Adam . Adamå¯ä»¥æ›´æ—©æ‹Ÿåˆï¼Œè€ŒSGDæ•ˆæœå¯èƒ½ä¼šæ›´å¥½ä¸€äº›ã€‚ å¯ä»¥é‡‡ç”¨ä¼˜åŒ–ç­–ç•¥ï¼Œæ¯”å¦‚è¯´ä½¿ç”¨Adamè®­ç»ƒç›´åˆ°æ‹Ÿåˆï¼Œç„¶åå°†å­¦ä¹ ç‡å‡åŠï¼Œå¹¶é‡æ–°å¯¼å…¥ä¹‹å‰è®­ç»ƒå¥½çš„æœ€å¥½çš„æ¨¡å‹ã€‚è¿™æ ·Adamèƒ½å¤Ÿå¿˜è®°ä¹‹å‰çš„ä¿¡æ¯å¹¶é‡æ–°å¼€å§‹è®­ç»ƒã€‚ Denkowski &amp; Neubig (2017) show that Adam with 2 restarts and learning rate annealing is faster and performs better than SGD with annealing Ensembling Combining multiple models into an ensemble by averaging their predictions is a proven strategy to improve model performance. Ensemblingå¾ˆé‡è¦çš„ä¸€ç‚¹æ˜¯éœ€è¦ä¿è¯å¤šæ ·æ€§ï¼š Ensembling is an important way to ensure that results are still reliable if the diversity of the evaluated models increases (Denkowski &amp; Neubig, 2017). While ensembling different checkpoints of a model has been shown to be effective (Jean et al., 2015; Sennrich et al., 2016) [51, 52], it comes at the cost of model diversity. Cyclical learning rates can help to mitigate this effect LSTM tricks åœ¨initial stateä¸­æˆ‘ä»¬å¸¸å¸¸ä½¿ç”¨å…¨0å‘é‡ï¼Œå®é™…ä¸Šå¯ä»¥å°†å…¶ä½œä¸ºå‚æ•°å­¦ä¹ ã€‚ Instead of fixing the initial state, we can learn it like any other parameter, which can improve performance å°†inputå’Œoutput embeddingçš„å‚æ•°å…±äº«ï¼Œå¦‚æœæ˜¯åšlanguage modelæˆ–è€…æœºå™¨ç¿»è¯‘ä¹‹ç±»çš„ï¼Œå¯ä»¥è®©ä»–ä»¬å…±äº«ã€‚ Gradient Norm Clipping Rather than clipping each gradient independently, clipping the global norm of the gradient yields more significant improvements è¿™ç‚¹æˆ‘æ²¡çœ‹æ‡‚ã€‚ Classification practiceså…³äºCNN CNN filters:Combining filter sizes near the optimal filter size, e.g. (3,4,5) performs best (Kim, 2014; Kim et al., 2016). The optimal number of feature maps is in the range of 50-600 (Zhang &amp; Wallace, 2015) [59]. Aggregation function:1-max-pooling outperforms average-pooling and k-max pooling (Zhang &amp; Wallace, 2015). è¿™åœ¨æˆ‘ä¹‹å‰çš„å…³äºCNNæ–‡æœ¬åˆ†ç±»æŒ‡å—ä¸­æœ‰æ›´è¯¦å°½çš„åˆ†æã€‚ Conclusionè¿™æ˜¯ä¸€ç¯‡å¹²è´§æ»¡æ»¡çš„åšå®¢ï¼Œå®é™…ä¸Šæˆ‘è¿˜æ˜¯æœ‰è®¸å¤šåœ°æ–¹æ²¡æœ‰è¯»æ‡‚ï¼Œè¿™é€‚åˆå¤šçœ‹å‡ éï¼Œæ…¢æ…¢ç†è§£ã€‚]]></content>
      <tags>
        <tag>æŒ‡å—</tag>
        <tag>è°ƒå‚</tag>
        <tag>NLPğŸ¤–</tag>
        <tag>ç¬”è®°ğŸ“’</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†5]]></title>
    <url>%2F2018%2F08%2F26%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%865%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Paper]Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components åŸºæœ¬æ¡†æ¶å’ŒCBOWä¸€è‡´ï¼Œä¸»è¦è´¡çŒ®åœ¨äºé’ˆå¯¹ä¸­æ–‡è¯å‘é‡æ·»åŠ äº†åæ—ã€å­—çš„ç»„ä»¶ä½œä¸ºè®­ç»ƒä¿¡æ¯ã€‚ 2ï¸âƒ£[Paper]Highway Networks ä¸ºäº†è§£å†³ç¥ç»ç½‘ç»œæ·±åº¦è¿‡æ·±æ—¶å¯¼è‡´çš„åå‘ä¼ æ’­å›°éš¾çš„é—®é¢˜ã€‚å‰å‘ä¼ æ’­çš„å…¬å¼ï¼š y=H(x,W_H)è€Œè®ºæ–‡æ‰€åšçš„æ”¹è¿›ï¼š y=H(x,W_H) \cdot T(x,W_T)+ x \cdot C(x,W_C)å…¶ä¸­$T$æ˜¯transform gateï¼Œ$C$æ˜¯carry gateã€‚æ–¹ä¾¿èµ·è§ï¼Œå¯ä»¥å°† $C=1-T$ï¼Œæœ€ç»ˆæœ‰ï¼š y=H(x,W_H) \cdot T(x,W_T)+ x \cdot (1-T(x,W_T))å¯ä»¥çœ‹å‡ºæ€æƒ³å’ŒLSTMå¾ˆç±»ä¼¼ï¼Œéƒ½æ˜¯gateçš„æ€æƒ³ã€‚ 3ï¸âƒ£[è°ƒå‚æ–¹æ³•]åšå®¢ï¼šhttps://blog.goodaudience.com/how-to-make-your-model-happy-again-part-1-40d94a9ffb41 å­¦ä¹ ç‡ï¼š ä¸€æ¡åŸåˆ™ï¼šå½“validation losså¼€å§‹ä¸Šå‡æ—¶ï¼Œå‡å°‘å­¦ä¹ ç‡ã€‚ å¦‚ä½•å‡å°‘ï¼Ÿ æˆ–è€…ï¼š è®¾å®šä¸€å®šçš„epochä½œä¸ºä¸€ä¸ªstepsizeï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çº¿æ€§å¢åŠ å­¦ä¹ ç‡ï¼Œç„¶ååœ¨åˆ°è¾¾æœ€å¤§å€¼åå†çº¿æ€§å‡å°ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ–¹æ³•å¯ä»¥åœ¨ä¸€åŠçš„epochå†…è¾¾åˆ°ç›¸åŒçš„æ•ˆæœã€‚ batch sizeï¼š ç”±äºbatch sizeå’Œå­¦ä¹ ç‡çš„å¼ºç›¸å…³æ€§ï¼Œç›¸å…³è®ºæ–‡æå‡ºæé«˜batch sizeè€Œä¸æ˜¯é™ä½å­¦ä¹ ç‡çš„æ–¹æ³•æ¥æå‡æ¨¡å‹è¡¨ç°ã€‚ increasing the batch size during training, instead of decaying learning rate. â€” L. Smithhttps://arxiv.org/pdf/1711.00489.pdf ä¸€ä¸ªtrickï¼šä¿æŒå­¦ä¹ ç‡ä¸å˜ï¼Œæé«˜batch sizeï¼Œç›´åˆ°batch size~è®­ç»ƒé›†/10ï¼Œæ¥ä¸‹æ¥å†é‡‡ç”¨å­¦ä¹ ç‡ä¸‹é™çš„ç­–ç•¥ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Paper</tag>
        <tag>è°ƒå‚æ–¹æ³•</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•6]]></title>
    <url>%2F2018%2F08%2F26%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB6%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£å°†æ•°æ®æ•´ç†æˆbatch123456789101112131415161718192021def _iter_batch(paras,labels,batch_size,shuffle=True): ''' :param paras: :param labels: :param batch_size: :param shuffle: :return: ''' assert len(paras)==len(labels) paras_size=len(paras) if shuffle: indices=np.arange(paras_size) np.random.shuffle(indices) for start_idx in range(0,paras_size-batch_size+1,batch_size): if shuffle: excerpt=indices[start_idx:start_idx+batch_size] else: excerpt=slice(start_idx,start_idx+batch_size) yield paras[excerpt],labels[excerpt]]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯6]]></title>
    <url>%2F2018%2F08%2F26%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D6%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£ æˆä¸ºå…­ç»å¥[å”] æœç”«ã€å…¶äºŒã€‘ç‹æ¨å¢éª†å½“æ—¶ä½“ï¼Œè½»è–„ä¸ºæ–‡å“‚æœªä¼‘ã€‚å°”æ›¹èº«ä¸åä¿±ç­ï¼Œä¸åºŸæ±Ÿæ²³ä¸‡å¤æµã€‚ å“‚ï¼ˆshÄ›nï¼‰ï¼šè®¥ç¬‘ã€‚ http://m.xichuangzhu.com/work/57b9658c0a2b58005c95d2a7]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNNæ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—]]></title>
    <url>%2F2018%2F08%2F25%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FCNN%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[æœ€è¿‘å› ä¸ºæ¯”èµ›çš„ç¼˜æ•…å¯¹æ–‡æœ¬åˆ†ç±»æœ‰ä¸€å®šçš„äº†è§£ã€‚å…¶ä¸­ä½¿ç”¨CNNæ–¹æ³•åšæƒ…æ„Ÿåˆ†æä»»åŠ¡å­˜åœ¨ç€è®¸å¤šä¼˜åŠ¿ã€‚è™½ç„¶æ¨¡å‹ç®€å•ï¼Œä½†å¦‚ä½•è®¾ç½®è¶…å‚æœ‰æ—¶å€™å¯¹ç»“æœæœ‰å¾ˆå¤§çš„å½±å“ã€‚æœ¬æ–‡è®°å½•äº†å…³äºCNNæ–‡æœ¬åˆ†ç±»çš„ä¸€äº›å­¦ä¹ å†ç¨‹å’ŒæŒ‡å—ï¼ŒåŸºæœ¬å‚è€ƒäº†è®ºæ–‡ã€‚ åšæ³•åŸºæœ¬ä¸Šç›®å‰è¾ƒä¸ºæµ…å±‚çš„CNNæ–‡æœ¬åˆ†ç±»çš„åšæ³•éƒ½æ˜¯å¦‚ä¸‹å›¾ï¼š å°†è¯å‘é‡å †ç§¯æˆä¸ºäºŒç»´çš„çŸ©é˜µï¼Œé€šè¿‡CNNçš„å·ç§¯å•å…ƒå¯¹çŸ©é˜µè¿›è¡Œå·ç§¯å¤„ç†ï¼ŒåŒæ—¶ä½¿ç”¨poolingï¼ˆé€šå¸¸æ˜¯1max-poolingï¼‰æ“ä½œï¼Œå°†ä¸ç­‰é•¿çš„å·ç§¯ç»“æœå˜ä¸ºç­‰é•¿ï¼Œå¯¹ä¸åŒçš„å·ç§¯å•å…ƒçš„ç»“æœè¿›è¡Œæ‹¼æ¥åç”Ÿæˆå•ä¸ªå‘é‡ï¼Œæœ€åå†é€šè¿‡çº¿æ€§å±‚è½¬åŒ–æˆç±»åˆ«æ¦‚ç‡åˆ†å¸ƒã€‚ å¦ä¸€å¼ å›¾ä¹Ÿè¯´æ˜äº†è¯¥æµç¨‹ã€‚ å»ºè®®ä¸æŒ‡å¯¼è¶…å‚åŠå…¶å¯¹ç»“æœçš„å½±å“æ¥ä¸‹æ¥çš„å†…å®¹å‚è€ƒäº†è®ºæ–‡A Sensitivity Analysis of (and Practitionersâ€™ Guide to) ConvolutionalNeural Networks for Sentence Classification CNNæ–‡æœ¬åˆ†ç±»çš„è¶…å‚ï¼š è¾“å…¥å‘é‡ å·ç§¯å¤§å° è¾“å‡ºé€šé“ï¼ˆfeature mapsï¼‰ æ¿€æ´»å‡½æ•° æ± åŒ–ç­–ç•¥ æ­£åˆ™åŒ– è¾“å…¥å‘é‡çš„å½±å“å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨word2vecå’ŒGloVeä¸åˆ†ä¼¯ä»²ï¼Œä½†å°†word2vecå’ŒGloVeç®€å•æ‹¼æ¥åœ¨ä¸€èµ·å¹¶ä¸èƒ½å¸¦æ¥æå‡ã€‚ unfortunately, simply concatenating these representations does necessarily seem helpful å½“å¥å­é•¿åº¦å¾ˆé•¿ï¼ˆdocument classificationï¼‰æ—¶ï¼Œä½¿ç”¨one-hotå¯èƒ½ä¼šæœ‰æ•ˆæœï¼Œä½†åœ¨å¥å­é•¿åº¦ä¸æ˜¯å¾ˆé•¿æ—¶ï¼Œæ•ˆæœä¸å¥½ã€‚ å»ºè®®å¯¹äºæ–°ä»»åŠ¡ï¼Œå¯ä»¥word2vecæˆ–GloVeæˆ–è€…å…¶ä»–è¯å‘é‡éƒ½è¯•ä¸€ä¸‹ï¼Œå¦‚æœå¥å­é•¿ï¼Œå¯ä»¥è¯•ç€ä½¿ç”¨one-hotã€‚ å·ç§¯å¤§å°ç”±äºå·ç§¯çš„é•¿åº¦æ˜¯å›ºå®šçš„ï¼Œä¹Ÿå°±æ˜¯è¯å‘é‡çš„é•¿åº¦ï¼Œå› æ­¤åªéœ€è®¨è®ºå®½åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œä¸åŒçš„æ•°æ®é›†ä¼šæœ‰ä¸åŒçš„æœ€ä½³å¤§å°ï¼Œä½†ä¼¼ä¹å¯¹äºé•¿åº¦è¶Šé•¿çš„å¥å­ï¼Œæœ€ä½³å¤§å°æœ‰è¶Šå¤§çš„è¶‹åŠ¿ã€‚ However, for datasets comprising longer sentences, such as CR (maximum sentence length is 105, whereas it ranges from 36-56 on the other sentiment datasets used here), the optimal region size may be larger. åŒæ—¶ï¼Œå½“å¢åŠ ä¸åŒå·ç§¯å¤§å°ä½œä¸ºç»„åˆæ—¶ï¼Œå¦‚æœç»„åˆçš„å·ç§¯æ ¸å¤§å°æ¥è¿‘äºæœ€ä½³å¤§å°ï¼ˆoptimal region sizeï¼‰ï¼Œæœ‰åŠ©äºç»“æœçš„æå‡ï¼›ç›¸åï¼Œå¦‚æœå·ç§¯æ ¸å¤§å°ç¦»æœ€ä½³å¤§å°å¾ˆè¿œæ—¶ï¼Œåè€Œä¼šäº§ç”Ÿè´Ÿé¢å½±å“ã€‚ å»ºè®®é¦–å…ˆè¯•ç€æ‰¾åˆ°æœ€ä¼˜çš„å·ç§¯æ ¸å¤§å°ï¼Œç„¶ååœ¨è¿™ä¸ªåŸºç¡€ä¸Šæ·»åŠ å’Œè¯¥å·ç§¯æ ¸å¤§å°ç±»ä¼¼çš„å·ç§¯æ ¸ã€‚ feature mapsä¹Ÿå°±æ˜¯è¾“å‡ºé€šé“ï¼ˆout channelï¼‰ï¼Œè¡¨æ˜è¯¥å·ç§¯æ ¸å¤§å°çš„å·ç§¯æ ¸æœ‰å¤šå°‘ä¸ªã€‚ å®éªŒè¡¨æ˜ï¼Œæœ€ä½³çš„feature mapså’Œæ•°æ®é›†ç›¸å…³ï¼Œä½†ä¸€èˆ¬ä¸è¶…è¿‡600ã€‚ it would seem that increasing the number of maps beyond 600 yields at best very marginal returns, and often hurts performance. å»ºè®®åœ¨600å†…æœç´¢æœ€ä¼˜ï¼Œå¦‚æœåœ¨600çš„è¾¹ç¼˜è¿˜æ²¡æœ‰æ˜æ˜¾çš„æ•ˆæœä¸‹é™ï¼Œé‚£ä¹ˆå¯ä»¥å°è¯•å¤§äº600çš„feature mapsã€‚ æ¿€æ´»å‡½æ•°å®éªŒç»“æœï¼š ç»“æœè¡¨æ˜ï¼Œtanhã€ReLUå’Œä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°æ•ˆæœè¾ƒå¥½ã€‚tanhçš„ä¼˜ç‚¹æ˜¯ä»¥0ä¸ºä¸­å¿ƒï¼ŒReLUèƒ½å¤ŸåŠ é€Ÿæ‹Ÿåˆï¼Œè‡³äºä¸ºä»€ä¹ˆä¸ä½¿ç”¨çš„æ•ˆæœä¼šå¥½ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ¨¡å‹è¾ƒä¸ºç®€å•ï¼š This indicates that on some datasets, a linear transformation is enough to capture thecorrelation between the word embedding and the output label. å»ºè®®ä½¿ç”¨tanhã€ReLUæˆ–è€…å¹²è„†ä¸ä½¿ç”¨ã€‚ä½†å¦‚æœæ¨¡å‹æ›´ä¸ºå¤æ‚ï¼Œæœ‰å¤šå±‚çš„ç»“æ„ï¼Œè¿˜æ˜¯éœ€è¦ä½¿ç”¨æ¿€æ´»å‡½æ•°çš„ã€‚ poolingç­–ç•¥æ‰€æœ‰çš„å®éªŒéƒ½è¡¨æ˜äº†ï¼Œ1-max poolingçš„æ•ˆæœæ¯”å…¶ä»–å¥½ï¼Œå¦‚k-max poolingã€‚åœ¨poolingè¿™ä¸€æ­¥å¯ä»¥ç›´æ¥é€‰æ‹©1-max poolingã€‚ This may be because the location of predictive contexts does not matter, and certain n-grams in the sentence can be more predictive on their own than the entire sentence considered jointly. æ­£åˆ™åŒ–ä¸»è¦æ˜¯dropoutå’Œl2 norm constraintã€‚dropoutå°±æ˜¯éšæœºå°†ä¸€äº›ç¥ç»å…ƒç½®ä¸º0ï¼Œl2 norm constraintæ˜¯å¯¹å‚æ•°çŸ©é˜µWè¿›è¡Œæ•´ä½“ç¼©æ”¾ï¼Œä½¿å…¶ä¸è¶…è¿‡ä¸€å®šé˜ˆå€¼ã€‚ï¼ˆä¸é€šå¸¸çš„l2 regularizationä¸åŒï¼Œæœ€æ—©å¯è¿½æº¯åˆ°Hintonçš„Improving neural networks by preventingco-adaptation of feature detectorsï¼‰ the l2 norm of a weight vector is linearly scaled to a constraint c when it exceeds this threshold, so a smaller c implies stronger regularization å®éªŒè¡¨æ˜ï¼Œdropoutèµ·çš„ä½œç”¨å¾ˆå°ï¼Œl2 normæ²¡æœ‰æå‡ç”šè‡³è¿˜ä¼šå¯¼è‡´ä¸‹é™ã€‚å¯èƒ½æ˜¯å› ä¸ºæ¨¡å‹å‚æ•°ä¸å¤šï¼Œå› æ­¤è¿‡æ‹Ÿåˆçš„å¯èƒ½æ€§è¾ƒä½ã€‚ å»ºè®®è®¾ç½®è¾ƒå°çš„dropoutå’Œè¾ƒå¤§çš„l2 normï¼Œå½“feature mapså¢å¤§æ—¶ï¼Œå¯ä»¥è¯•ç€è°ƒèŠ‚è¾ƒå¤§çš„dropoutä»¥é¿å…è¿‡æ‹Ÿåˆã€‚ å»ºè®®åŠç»“è®º åˆšå¼€å§‹çš„ä½¿ç”¨ä½¿ç”¨word2vecæˆ–è€…GloVeï¼Œå¦‚æœæ•°æ®é‡å¤Ÿå¤§ï¼Œå¯ä»¥å°è¯•one-hot çº¿æ€§æœç´¢æœ€ä½³çš„å·ç§¯æ ¸å¤§å°ï¼Œå¦‚æœå¥å­å¤Ÿé•¿ï¼Œé‚£ä¹ˆå¯ä»¥æ‰©å¤§æœç´¢èŒƒå›´ã€‚ä¸€æ—¦ç¡®å®šäº†æœ€ä½³å·ç§¯æ ¸å¤§å°ï¼Œå°è¯•åœ¨è¯¥å·ç§¯æ ¸å¤§å°çš„é™„è¿‘è¿›è¡Œç»„åˆï¼Œå¦‚æœ€ä½³å·ç§¯æ ¸å®½åº¦æ˜¯5ï¼Œé‚£ä¹ˆå°è¯•[3,4,5]æˆ–è€…[2,3,4,5]ç­‰ ä½¿ç”¨è¾ƒå°çš„dropoutå’Œè¾ƒå¤§çš„max norm constraintï¼Œç„¶ååœ¨[100,600]èŒƒå›´å†…æœç´¢feature mapsï¼Œå¦‚æœæœ€ä½³çš„feature mapsåœ¨600é™„è¿‘ï¼Œå¯ä»¥è¯•ç€é€‰æ‹©æ¯”600æ›´å¤§çš„èŒƒå›´ å°è¯•ä¸åŒçš„æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸tanhå’ŒReLUæ˜¯è¾ƒå¥½çš„ï¼Œä½†ä¹Ÿå¯ä»¥å°è¯•ä»€ä¹ˆéƒ½ä¸åŠ ã€‚ ä½¿ç”¨1-max poolingã€‚ å¦‚æœæ¨¡å‹å¤æ‚ï¼Œæ¯”å¦‚feature mapså¾ˆå¤§ï¼Œé‚£ä¹ˆå¯ä»¥å°è¯•æ›´ä¸ºä¸¥æ ¼çš„æ­£åˆ™åŒ–ï¼Œå¦‚æ›´å¤§çš„dropout rateå’Œè¾ƒå°çš„max norm constraintã€‚ ReferenceConvolutional Neural Networks for Sentence Classification A Sensitivity Analysis of (and Practitionersâ€™ Guide to) ConvolutionalNeural Networks for Sentence Classification]]></content>
      <tags>
        <tag>CNN</tag>
        <tag>æƒ…æ„Ÿåˆ†æ</tag>
        <tag>æŒ‡å—</tag>
        <tag>è°ƒå‚</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³äºPytorchä¸­çš„inplaceçš„æ“ä½œ]]></title>
    <url>%2F2018%2F08%2F20%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADin-place%E7%9A%84%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[æœ€è¿‘åœ¨å†™Hierarchical attention networkçš„æ—¶å€™é‡åˆ°äº†å¦‚ä¸‹çš„bugï¼š one of the variables needed for gradient computation has been modified by an inplace operation åœ¨æŸ¥é˜…äº†æ–‡æ¡£å’Œè¯·æ•™äº†å…¶ä»–äººä¹‹åï¼Œæœ€ç»ˆæ‰¾åˆ°äº†bugã€‚ 1234for i in range(seq_len): h_i = rnn_outputs[i] # batch,hidden*2 a_i = attn_weights[i].unsqueeze_(1) # take in-place opt may cause an error a_i = a_i.expand_as(h_i) # batch,hidden*2 è¿™æ˜¯æˆ‘åŸæ¥çš„é€»è¾‘ï¼Œæˆ‘åœ¨æ— æ„ä¸­åšäº†inplaceæ“ä½œï¼Œå¯¼è‡´äº†bugçš„å‘ç”Ÿã€‚æ­£ç¡®çš„åšæ³•åº”è¯¥æ˜¯è¿™æ ·çš„ï¼š 12345for i in range(seq_len): h_i = rnn_outputs[i] # batch,hidden*2 # a_i = attn_weights[i].unsqueeze_(1) # take in-place opt may cause an error a_i = attn_weights[i].unsqueeze(1) # batch,1 a_i = a_i.expand_as(h_i) # batch,hidden*2 å®é™…ä¸Šï¼Œåœ¨å®è·µè¿‡ç¨‹ä¸­åº”å½“å°½é‡é¿å…inplaceæ“ä½œï¼Œåœ¨å®˜æ–¹æ–‡æ¡£ä¸­ä¹Ÿæåˆ°äº†ï¼ˆå­˜ç–‘ï¼‰è¿™ç‚¹ï¼Œè™½ç„¶æä¾›äº†inplaceæ“ä½œï¼Œä½†å¹¶ä¸æ¨èä½¿ç”¨ã€‚ å…·ä½“çš„åŸå› æ˜¯ï¼Œåœ¨Pytorchæ„å»ºè®¡ç®—å›¾çš„è¿‡ç¨‹ä¸­ï¼Œä¼šè®°å½•æ¯ä¸ªèŠ‚ç‚¹æ˜¯æ€ä¹ˆæ¥çš„ï¼Œä½†inplaceä¼šç ´åè¿™ç§å…³ç³»ï¼Œä½¿å¾—åœ¨å›ä¼ çš„æ—¶å€™æ²¡æ³•æ­£å¸¸æ±‚å¯¼ã€‚ ç‰¹åˆ«åœ°ï¼Œæœ‰ä¸¤ç§æƒ…å†µä¸åº”è¯¥ä½¿ç”¨inplaceæ“ä½œï¼ˆæ‘˜è‡ªçŸ¥ä¹ï¼‰ï¼š å¯¹äºrequires_grad=Trueçš„å¶å­å¼ é‡(leaf tensor)ä¸èƒ½ä½¿ç”¨inplace operation å¯¹äºåœ¨æ±‚æ¢¯åº¦é˜¶æ®µéœ€è¦ç”¨åˆ°çš„å¼ é‡ä¸èƒ½ä½¿ç”¨inplace operation Reference:https://zhuanlan.zhihu.com/p/38475183]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>é‡åˆ°çš„é—®é¢˜</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ„¿ä¸­å›½é’å¹´éƒ½æ‘†è„±å†·æ°”]]></title>
    <url>%2F2018%2F08%2F19%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%84%BF%E4%B8%AD%E5%9B%BD%E9%9D%92%E5%B9%B4%E9%83%BD%E6%91%86%E8%84%B1%E5%86%B7%E6%B0%94%2F</url>
    <content type="text"><![CDATA[è¿‘æœŸçš„æ–°é—»å¸¸è®©äººæ„Ÿåˆ°æ„¤æ€’ä»¥è‡´ç»æœ›â€¦ æ„¿ä¸­å›½é’å¹´éƒ½æ‘†è„±å†·æ°”ï¼Œåªæ˜¯å‘ä¸Šèµ°ï¼Œä¸å¿…å¬è‡ªæš´è‡ªå¼ƒè€…æµçš„è¯ã€‚èƒ½åšäº‹çš„åšäº‹ï¼Œèƒ½å‘å£°çš„å‘å£°ã€‚æœ‰ä¸€åˆ†çƒ­ï¼Œå‘ä¸€åˆ†å…‰ã€‚å°±ä»¤è¤ç«ä¸€èˆ¬ï¼Œä¹Ÿå¯ä»¥åœ¨é»‘æš—é‡Œå‘ä¸€ç‚¹å…‰ï¼Œä¸å¿…ç­‰å€™ç‚¬ç«ã€‚ â€”é²è¿…ã€Šçƒ­é£ã€‹]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•5]]></title>
    <url>%2F2018%2F08%2F19%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB5%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£sklearnæ¨¡å‹çš„ä¿å­˜ä¸æ¢å¤123456789from sklearn import svmX = [[0, 0], [1, 1]]y = [0, 1]clf = svm.SVC()clf.fit(X, y) clf.fit(train_X,train_y)joblib.dump(clf, "train_model.m")clf = joblib.load("train_model.m")clf.predit(test_X) 2ï¸âƒ£Dictionaryç±»åœ¨æ„é€ å­—å…¸æ—¶éœ€è¦ç”¨åˆ°1234567891011121314151617181920212223242526class Dictionary(): def __init__(self): self.word2idx = &#123;&#125; self.idx2word = [] self.__vocab_size = 0 self.add_word('&lt;pad&gt;') self.add_word('&lt;UNK&gt;') def add_word(self, word): if word not in self.word2idx: self.idx2word.append(word) self.word2idx[word] = self.__vocab_size self.__vocab_size += 1 def __len__(self): return self.__vocab_size def get_index(self, word): if word in self.word2idx: return self.word2idx[word] else: return self.word2idx['&lt;UNK&gt;'] def get_word(self, idx): return self.idx2word[idx] 3ï¸âƒ£å¯¹dictæŒ‰å…ƒç´ æ’åºçš„ä¸‰ç§æ–¹æ³•12345678910111213d=&#123;'apple':10,'orange':20,'banana':5,'watermelon':1&#125;#æ³•1print(sorted(d.items(),key=lambda x:x[1])) #[('watermelon', 1), ('banana', 5), ('apple', 10), ('orange', 20)]#æ³•2from operator import itemgetterprint(sorted(d.items(),key=itemgetter(1))) #[('watermelon', 1), ('banana', 5), ('apple', 10), ('orange', 20)]#æ³•3print(sorted(d,key=d.get)) #['watermelon', 'banana', 'apple', 'orange'] æ²¡æœ‰valueäº† 4ï¸âƒ£åˆå¹¶dictçš„ä¸‰ç§æ–¹æ³•1234567891011121314151617&gt;&gt;&gt; d1=&#123;'a':1&#125;&gt;&gt;&gt; d2=&#123;'b':2&#125;#æ³•1&gt;&gt;&gt; d=&#123;**d1,**d2&#125;&gt;&gt;&gt; d&#123;'a': 1, 'b': 2&#125;#æ³•2&gt;&gt;&gt; dd=dict(d1.items()|d2.items())&gt;&gt;&gt; dd&#123;'a': 1, 'b': 2&#125;#æ³•3&gt;&gt;&gt; d1.update(d2)&gt;&gt;&gt; d1&#123;'a': 1, 'b': 2&#125; 5ï¸âƒ£æ‰¾åˆ°listæœ€å¤§æœ€å°å€¼çš„index12345678lst = [40, 10, 20, 30]def minIndex(lst): return min(range(len(lst)),key=lst.__getitem__)def maxIndex(lst): return max(range(len(lst)),key=lst.__getitem__) print(minIndex(lst))print(maxIndex(lst))]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³äºPytorchä¸­çš„Embedding padding]]></title>
    <url>%2F2018%2F08%2F19%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADEmbedding%E7%9A%84padding%2F</url>
    <content type="text"><![CDATA[åœ¨Pytorchä¸­ï¼Œnn.Embedding()ä»£è¡¨embeddingçŸ©é˜µï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªå‚æ•°padding_idxæŒ‡å®šç”¨ä»¥paddingçš„ç´¢å¼•ä½ç½®ã€‚æ‰€è°“paddingï¼Œå°±æ˜¯åœ¨å°†ä¸ç­‰é•¿çš„å¥å­ç»„æˆä¸€ä¸ªbatchæ—¶ï¼Œå¯¹é‚£äº›ç©ºç¼ºçš„ä½ç½®è¡¥0ï¼Œä»¥å½¢æˆä¸€ä¸ªç»Ÿä¸€çš„çŸ©é˜µã€‚ ç”¨æ³•ï¼š1self.embedding = nn.Embedding(vocab_size, embed_dim,padding_idx=0) #ä¹Ÿå¯ä»¥æ˜¯åˆ«çš„æ•°å€¼ åœ¨æ˜¾å¼è®¾å®špadding_idx=0åï¼Œåœ¨è‡ªå®šä¹‰çš„è¯å…¸å†…ä¹Ÿåº”å½“åœ¨ç›¸åº”ä½ç½®æ·»åŠ &lt;pad&gt;ä½œä¸ºä¸€ä¸ªè¯ã€‚å¦‚ï¼š 1234567class Dictionary(): def __init__(self): self.word2idx = &#123;&#125; self.idx2word = [] self.__vocab_size = 0 self.add_word('&lt;pad&gt;') # should add &lt;pad&gt; first self.add_word('&lt;UNK&gt;') é‚£ä¹ˆå¯¹äºpadding_idxï¼Œå†…éƒ¨æ˜¯å¦‚ä½•æ“ä½œçš„å‘¢ï¼Ÿ åœ¨æŸ¥çœ‹äº†Embeddingçš„æºç åï¼Œå‘ç°è®¾ç½®äº†padding_idxï¼Œç±»å†…éƒ¨ä¼šæœ‰å¦‚ä¸‹æ“ä½œï¼š 12345678910#-----Embedding __init__ å†…éƒ¨--------------if _weight is None: self.weight = Parameter(torch.Tensor(num_embeddings, embedding_dim)) self.reset_parameters() #---------reset_parameters()--------def reset_parameters(self): self.weight.data.normal_(0, 1) if self.padding_idx is not None: self.weight.data[self.padding_idx].fill_(0) ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“Embeddingæ˜¯éšæœºåˆå§‹åŒ–çš„çŸ©é˜µæ—¶ï¼Œä¼šå¯¹padding_idxæ‰€åœ¨çš„è¡Œè¿›è¡Œå¡«0ã€‚ä¿è¯äº†paddingè¡Œä¸ºçš„æ­£ç¡®æ€§ã€‚ é‚£ä¹ˆï¼Œè¿˜éœ€è¦ä¿è¯ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯åœ¨åå‘å›ä¼ çš„æ—¶å€™ï¼Œpadding_idxæ˜¯ä¸ä¼šæ›´æ–°çš„. åœ¨æŸ¥çœ‹äº†æºç åå‘ç°åœ¨Embeddingç±»å†…æœ‰å¦‚ä¸‹æ³¨é‡Šï¼š .. note:: With :attr:padding_idx set, the embedding vector at :attr:padding_idx is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from :class:~torch.nn.Embedding is always zero. å¹¶ä¸”åœ¨æŸ¥é˜…äº†å…¶ä»–èµ„æ–™åï¼Œå‘ç°è¯¥è¡Œç¡®å®ä¼šä¸æ›´æ–°ã€‚æœ‰æ„æ€çš„æ˜¯ï¼ŒæŸ¥é˜…æºç å¹¶æ²¡æœ‰æ‰¾åˆ°å¦‚ä½•ä½¿å…¶ä¸æ›´æ–°çš„æœºåˆ¶ï¼Œå› ä¸ºåœ¨F.embeddingå‡½æ•°ä¸­ï¼Œè¿”å›ï¼š 1return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) ä½†æˆ‘å¹¶ä¸èƒ½è·³è½¬åˆ°torch.embeddingä¸­ï¼Œå¤§æ¦‚æ˜¯å› ä¸ºè¿™éƒ¨åˆ†è¢«éšè—äº†å§ã€‚æˆ‘ä¹Ÿæ²¡æœ‰å†æ·±ç©¶ä¸‹å»ã€‚æˆ‘çŒœæµ‹æœ‰å¯èƒ½æ˜¯åœ¨autogradå†…éƒ¨æœ‰å¯¹è¯¥éƒ¨åˆ†è¿›è¡Œå•ç‹¬çš„å¤„ç†ï¼Œç”¨maskå±è”½è¿™éƒ¨åˆ†çš„æ›´æ–°ï¼›æˆ–è€…ä¸€ä¸ªæ›´ç®€å•çš„æ–¹æ³•ï¼Œå°±æ˜¯ä»»å…¶æ›´æ–°ï¼Œä½†æ¯ä¸€æ¬¡éƒ½resetï¼Œå°†ç¬¬ä¸€è¡Œæ‰‹åŠ¨è®¾ä¸ºå…¨0ã€‚ é™„è®°ï¼š å‡å¦‚è¯´æ²¡æœ‰æ˜¾å¼è®¾ç½®è¯¥è¡Œï¼Œæ˜¯å¦paddingå°±æ²¡æœ‰æ•ˆæœå‘¢ï¼Ÿæˆ‘è®¤ä¸ºæ˜¯çš„ã€‚ ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬éƒ½æ˜¯ä»¥0ä½œä¸ºpaddingçš„å¡«å……ï¼Œå¦‚ï¼š 12 44 22 67 85 12 13 534 31 0 87 23 0 0 0 æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªå¥å­ï¼Œå…¶ä¸­0ä½œä¸ºå¡«å……ã€‚ç„¶åå°†è¯¥çŸ©é˜µé€å…¥åˆ°embedding_lookupä¸­ï¼Œè·å¾—ä¸‰ç»´çš„tensorï¼Œé‚£ä¹ˆ0å¡«å……çš„éƒ¨åˆ†ï¼Œæ‰€è·å¾—çš„embeddingè¡¨ç¤ºåº”å½“æ˜¯è¦å…¨0ã€‚ å‡å¦‚ä¸æ˜¾å¼è®¾ç½®padding_idx=0ï¼Œå°±å¯èƒ½ä¼šå‡ºç°ä¸¤ä¸ªç»“æœï¼ˆä¸ªäººæ¨æµ‹)ï¼š â‘ æœ¬åº”è¯¥å…¨0çš„åœ°æ–¹ï¼Œè¢«è¯å…¸ä¸­ç¬¬ä¸€ä¸ªè¯çš„è¯å‘é‡è¡¨ç¤ºç»™æ›¿ä»£äº†ï¼Œå› ä¸ºå°†0ä½œä¸ºç´¢å¼•å»embeddingçŸ©é˜µè·å–åˆ°çš„è¯å‘é‡ï¼Œå°±æ˜¯ç¬¬ä¸€ä¸ªè¯çš„è¯å‘é‡ï¼Œè€Œè¯¥è¯å¹¶ä¸å…¨0ã€‚ â‘¡è¯å…¸çš„æœ€åä¸€ä¸ªè¯è¢«å…¨0è¦†ç›–ã€‚F.embeddingä¸­æœ‰å¦‚ä¸‹ç‰‡æ®µï¼š 12345678if padding_idx is not None: if padding_idx &gt; 0: assert padding_idx &lt; weight.size(0), 'Padding_idx must be within num_embeddings' elif padding_idx &lt; 0: assert padding_idx &gt;= -weight.size(0), 'Padding_idx must be within num_embeddings' padding_idx = weight.size(0) + padding_idxelif padding_idx is None: padding_idx = -1 ä¸Šé¢ç‰‡æ®µæ˜¾ç¤ºï¼Œpadding_idxè¢«è®¾ç½®ä¸º-1ï¼Œä¹Ÿå°±æ˜¯æœ€åä¸€ä¸ªå•è¯ã€‚åšå®Œè¿™æ­¥ç´§æ¥ç€å°±è¿”å›ï¼š 1return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) è¿˜æ˜¯ç”±äºtorch.embeddingæ— æ³•æŸ¥çœ‹çš„åŸå› ï¼Œæˆ‘ä¸çŸ¥é“å†…éƒ¨æ˜¯å¦‚ä½•å®ç°çš„ï¼Œä½†åº”è¯¥æ¥è¯´ï¼Œæœ€åä¸€ä¸ªè¯å°±æ˜¯è¢«è¦†ç›–äº†ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>Embedding</tag>
        <tag>padding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Tricks[è½¬]]]></title>
    <url>%2F2018%2F08%2F19%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPython%20Tricks%5B%E8%BD%AC%5D%2F</url>
    <content type="text"><![CDATA[åŸæ–‡åœ°å€:https://hackernoon.com/python-tricks-101-2836251922e0 æˆ‘è§‰å¾—è¿™ä¸ªä»‹ç»Pythonä¸€äº›tricksçš„æ–‡ç« å¾ˆå¥½ï¼Œèƒ½å¤Ÿæ›´åŠ ç†Ÿæ‚‰Pythonçš„ä¸€äº›éå¸¸æ–¹ä¾¿çš„ç”¨æ³•ã€‚ä»¥ä¸‹æ˜¯æˆ‘è§‰å¾—æœ‰ç”¨çš„å‡ ä¸ªç‚¹ã€‚ 1ï¸âƒ£Reverse a String/List [::-1]è§£é‡Šï¼š[:]è¡¨ç¤ºå–æ‰€æœ‰çš„å…ƒç´ ï¼Œ-1è¡¨ç¤ºæ­¥è¿›ã€‚[1:5:2]è¡¨ç¤ºçš„å°±æ˜¯ä»å…ƒç´ 1åˆ°å…ƒç´ 5ï¼Œæ¯2ä¸ªè·ç¦»å–ä¸€ä¸ªã€‚ 2ï¸âƒ£transpose 2d array zip()ç›¸å½“äºå‹ç¼©ï¼Œzip(*)ç›¸å½“äºè§£å‹ã€‚ 3ï¸âƒ£Chained function call éå¸¸ç®€æ´çš„å†™æ³•ã€‚ 4ï¸âƒ£Copy List ä¹‹å‰è°ˆè¿‡çš„Pythonçš„èµ‹å€¼ã€æµ…æ‹·è´ã€æ·±æ‹·è´ã€‚ 5ï¸âƒ£Dictionary get é¿å…äº†dictä¸å­˜åœ¨è¯¥å…ƒç´ çš„é—®é¢˜ã€‚ 6ï¸âƒ£âœ¨Sort Dictionary by Value å…¶ä¸­ç¬¬ä¸‰ç§è¿”å›çš„æ˜¯[â€˜watermelonâ€™, â€˜bananaâ€™, â€˜appleâ€™, â€˜orangeâ€™]ï¼Œæ²¡æœ‰valueäº†ã€‚ 7ï¸âƒ£Forâ€¦else æ³¨æ„åˆ°å¦‚æœforåœ¨ä¸­é€”breakäº†ï¼Œå°±ä¸ä¼šè¿›å…¥åˆ°elseäº†ï¼›åªæœ‰é¡ºåˆ©å¾ªç¯å®Œæ‰ä¼šè¿›å…¥åˆ°elseã€‚ 1234567891011121314151617&gt;&gt;&gt; a=[1,2,0]&gt;&gt;&gt; for e in a:... if e==0:... break... else:... print('hello')... #ä»€ä¹ˆéƒ½æ²¡æœ‰print&gt;&gt;&gt; for e in a:... print(e)... else:... print('hello')... 120hello 8ï¸âƒ£Merge dictâ€™s åˆå¹¶dictçš„æ–¹æ³•ã€‚ 9ï¸âƒ£Min and Max index in List]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Python</tag>
        <tag>Python tricks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†4]]></title>
    <url>%2F2018%2F08%2F19%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%864%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[æ¦‚ç‡æ ¡å‡†(Probability Calibration)]ä¸€ç§å¯¹æœºå™¨å­¦ä¹ ç®—æ³•è¾“å‡ºç»“æœçš„æ ¡å‡†ï¼Œé€šè¿‡å‡ ä¸ªå®éªŒå¯ä»¥å‘ç°ï¼Œæ¦‚ç‡æ ¡å‡†èƒ½å¤Ÿä¸€å®šç¨‹åº¦æé«˜è¡¨ç°ã€‚å‡ ä¸ªå‚è€ƒèµ„æ–™ï¼šç›´è§‚ç†è§£: http://www.bubuko.com/infodetail-2133893.htmlSVCçš„æ¦‚ç‡æ ¡å‡†åœ¨sklearnä¸Šçš„åº”ç”¨: https://blog.csdn.net/ericcchen/article/details/79337716âœ¨å®Œå…¨æ‰‹å†Œ: Calibration of Machine Learning Models 2ï¸âƒ£[Paper]Hierarchical Attention Networks for Document Classification äº®ç‚¹åœ¨ä½¿ç”¨å±‚æ¬¡çš„RNNç»“æ„ï¼Œä»¥åŠä½¿ç”¨äº†attentionæ–¹æ³•ã€‚ å‚è€ƒäº†å…¶ä»–äººçš„ä»£ç è‡ªå·±ä¹Ÿè¯•ç€å®ç°äº†ä¸€ä¸ªï¼ŒGitHubåœ°å€ï¼šhttps://github.com/linzehui/pytorch-hierarchical-attention-network 3ï¸âƒ£[XGBoost]kaggleç¥å™¨XGBoostï¼Œä¸€ç¯‡åŸç†çš„è¯¦ç»†ä»‹ç»ï¼šhttp://www.cnblogs.com/willnote/p/6801496.htmlè™½ç„¶è¿˜æ˜¯æœ‰å¥½äº›åœ°æ–¹æ²¡ææ‡‚ï¼Œæœ‰å¿…è¦ä»å¤´å­¦èµ·ã€‚ 4ï¸âƒ£[Python]å…³äºå‡½æ•°åˆ—è¡¨ä¸­å•æ˜Ÿå·(*)å’ŒåŒæ˜Ÿå·(**)å•æ˜Ÿå·ï¼š ä»£è¡¨æ¥æ”¶ä»»æ„å¤šä¸ªéå…³é”®å­—å‚æ•°ï¼Œå°†å…¶è½¬æ¢æˆå…ƒç»„ï¼š 1234def one(a,*b): """aæ˜¯ä¸€ä¸ªæ™®é€šä¼ å…¥å‚æ•°ï¼Œ*bæ˜¯ä¸€ä¸ªéå…³é”®å­—æ˜Ÿå·å‚æ•°""" print(b)one(1,2,3,4,5,6) #è¾“å‡ºï¼š(2, 3, 4, 5, 6) å¯¹ä¸€ä¸ªæ™®é€šå˜é‡ä½¿ç”¨å•æ˜Ÿå·ï¼Œè¡¨ç¤ºå¯¹è¯¥å˜é‡æ‹†åˆ†æˆå•ä¸ªå…ƒç´  1234def fun(a,b): print(a,b)l=[1,2]fun(*l) #è¾“å‡º 1,2 åŒæ˜Ÿå·ï¼š è·å¾—å­—å…¸å€¼ 1234def two(a=1,**b): """aæ˜¯ä¸€ä¸ªæ™®é€šå…³é”®å­—å‚æ•°ï¼Œ**bæ˜¯ä¸€ä¸ªå…³é”®å­—åŒæ˜Ÿå·å‚æ•°""" print(b)two(a=1,b=2,c=3,d=4,e=5,f=6) #è¾“å‡º&#123;'b': 2, 'c': 3, 'e': 5, 'f': 6, 'd': 4&#125; 5ï¸âƒ£[Pytorch]åœ¨Pytorchä¸­ï¼Œåªè¦ä¸€ä¸ªtensorçš„requires_gradæ˜¯trueï¼Œé‚£ä¹ˆä¸¤ä¸ªtensorçš„åŠ å‡ä¹˜é™¤åçš„ç»“æœçš„requires_gradä¹Ÿä¼šæ˜¯trueã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Python</tag>
        <tag>Pytorch</tag>
        <tag>Paper</tag>
        <tag>æ¦‚ç‡æ ¡å‡†</tag>
        <tag>Probability Calibration</tag>
        <tag>HAN</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯5]]></title>
    <url>%2F2018%2F08%2F19%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D5%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨å¤ªå¿™äº†ï¼Œæ²¡èƒŒä»€ä¹ˆè¯—è¯ï¼ŒåªèƒŒï¼ˆå¤ä¹ ï¼‰äº†éƒ¨åˆ†çš„ã€Šæ»•ç‹é˜åºã€‹ã€‚ 1ï¸âƒ£ æ»•ç‹é˜åºå—Ÿä¹ï¼æ—¶è¿ä¸é½ï¼Œå‘½é€”å¤šèˆ›ã€‚å†¯å”æ˜“è€ï¼Œæå¹¿éš¾å°ã€‚å±ˆè´¾è°Šäºé•¿æ²™ï¼Œéæ— åœ£ä¸»ï¼›çªœæ¢é¸¿äºæµ·æ›²ï¼Œå²‚ä¹æ˜æ—¶ï¼Ÿæ‰€èµ–å›å­è§æœºï¼Œè¾¾äººçŸ¥å‘½ã€‚è€å½“ç›Šå£®ï¼Œå®ç§»ç™½é¦–ä¹‹å¿ƒï¼Ÿç©·ä¸”ç›Šåšï¼Œä¸å é’äº‘ä¹‹å¿—ã€‚é…Œè´ªæ³‰è€Œè§‰çˆ½ï¼Œå¤„æ¶¸è¾™ä»¥çŠ¹æ¬¢ã€‚åŒ—æµ·è™½èµŠï¼Œæ‰¶æ‘‡å¯æ¥ï¼›ä¸œéš…å·²é€ï¼Œæ¡‘æ¦†éæ™šã€‚å­Ÿå°é«˜æ´ï¼Œç©ºé¦€æŠ¥å›½ä¹‹æƒ…ï¼›é˜®ç±çŒ–ç‹‚ï¼Œå²‚æ•ˆç©·é€”ä¹‹å“­ï¼ å‹ƒï¼Œä¸‰å°ºå¾®å‘½ï¼Œä¸€ä»‹ä¹¦ç”Ÿã€‚æ— è·¯è¯·ç¼¨ï¼Œç­‰ç»ˆå†›ä¹‹å¼±å† ï¼›æœ‰æ€€æŠ•ç¬”ï¼Œæ…•å®—æ…¤ä¹‹é•¿é£ã€‚èˆç°ªç¬äºç™¾é¾„ï¼Œå¥‰æ™¨æ˜äºä¸‡é‡Œã€‚éè°¢å®¶ä¹‹å®æ ‘ï¼Œæ¥å­Ÿæ°ä¹‹èŠ³é‚»ã€‚ä»–æ—¥è¶‹åº­ï¼Œå¨é™ªé²¤å¯¹ï¼›ä»Šå…¹æ§è¢‚ï¼Œå–œæ‰˜é¾™é—¨ã€‚æ¨æ„ä¸é€¢ï¼ŒæŠšå‡Œäº‘è€Œè‡ªæƒœï¼›é”ºæœŸæ—¢é‡ï¼Œå¥æµæ°´ä»¥ä½•æƒ­ï¼Ÿ æ³¨é‡Šï¼šå†¯å”ï¼šè¥¿æ±‰äººï¼Œæœ‰æ‰èƒ½å´ä¸€ç›´ä¸å—é‡ç”¨ã€‚æ±‰æ­¦å¸æ—¶é€‰æ±‚è´¤è‰¯ï¼Œæœ‰äººä¸¾èå†¯å”ï¼Œå¯æ˜¯ä»–å·²ä¹åå¤šå²ï¼Œéš¾å†åšå®˜äº†ã€‚æå¹¿ï¼šæ±‰æ­¦å¸æ—¶çš„åå°†ï¼Œå¤šå¹´æŠ—å‡»åŒˆå¥´ï¼Œå†›åŠŸå¾ˆå¤§ï¼Œå´ç»ˆèº«æ²¡æœ‰å°ä¾¯ã€‚ è´¾è°Šï¼šæ±‰æ–‡å¸æœ¬æƒ³ä»»è´¾è°Šä¸ºå…¬å¿ï¼Œä½†å› æœä¸­æƒè´µåå¯¹ï¼Œå°±ç–è¿œäº†è´¾è°Šï¼Œä»»ä»–ä¸ºé•¿æ²™ç‹å¤ªå‚…ã€‚æ¢é¸¿ï¼šä¸œæ±‰äººï¼Œå› ä½œè¯—è®½åˆºå›ç‹ï¼Œå¾—ç½ªäº†æ±‰ç« å¸ï¼Œè¢«è¿«é€ƒåˆ°é½é²ä¸€å¸¦èº²é¿ã€‚ é…Œï¼ˆzhuÃ³ï¼‰è´ªæ³‰è€Œè§‰çˆ½ï¼šå–ä¸‹è´ªæ³‰çš„æ°´ï¼Œä»è§‰å¾—å¿ƒå¢ƒæ¸…çˆ½ã€‚å¤ä»£ä¼ è¯´å¹¿å·æœ‰æ°´åè´ªæ³‰ï¼Œäººå–äº†è¿™é‡Œçš„æ°´å°±ä¼šå˜å¾—è´ªå©ªã€‚è¿™å¥æ˜¯è¯´æœ‰å¾·è¡Œçš„äººåœ¨æ±¡æµŠçš„ç¯å¢ƒä¸­ä¹Ÿèƒ½ä¿æŒçº¯æ­£ï¼Œä¸è¢«æ±¡æŸ“ã€‚å¤„æ¶¸è¾™ä»¥çŠ¹æ¬¢ï¼šå¤„åœ¨å¥„å¥„å¾…æ¯™çš„æ—¶å€™ï¼Œä»ç„¶ä¹è§‚å¼€æœ—ã€‚å¤„æ²³è¾™ï¼šåŸæŒ‡é²‹é±¼å¤„åœ¨å¹²æ¶¸çš„è½¦è¾™æ—¦ã€‚æ¯”å–»äººé™·å…¥å±æ€¥ä¹‹ä¸­ã€‚ å­Ÿå°ï¼šä¸œæ±‰äººï¼Œä¸ºå®˜æ¸…æ­£è´¤èƒ½ï¼Œä½†ä¸è¢«é‡ç”¨ï¼Œåæ¥å½’ç”°ã€‚é˜®ç±ï¼šä¸‰å›½é­è¯—äººï¼Œä»–æœ‰æ—¶ç‹¬è‡ªé©¾è½¦å‡ºè¡Œï¼Œåˆ°æ— è·¯å¤„ä¾¿æ¸å“­è€Œè¿”ï¼Œå€Ÿæ­¤å®£æ³„ä¸æ»¡äºç°å®çš„è‹¦é—·å¿ƒæƒ…ã€‚ ç»ˆå†›ï¼šã€Šæ±‰ä¹¦Â·ç»ˆå†›ä¼ ã€‹è®°è½½ï¼Œæ±‰æ­¦å¸æƒ³è®©å—è¶Šï¼ˆä»Šå¹¿ä¸œã€å¹¿è¥¿ä¸€å¸¦ï¼‰ç‹å½’é¡ºï¼Œæ´¾ç»ˆå†›å‰å¾€åŠè¯´ï¼Œç»ˆå†›è¯·æ±‚ç»™ä»–é•¿ç¼¨ï¼Œå¿…ç¼šä½å—è¶Šç‹ï¼Œå¸¦å›åˆ°çš‡å®«é—¨å‰ï¼ˆæ„æ€æ˜¯ä¸€å®šå®Œæˆä½¿å‘½ï¼‰ã€‚åæ¥ç”¨â€œè¯·ç¼¨â€æŒ‡æŠ•å†›æŠ¥å›½ã€‚ å®—æ‚«ï¼ˆquÃ¨ï¼‰ï¼šå—æœå®‹äººï¼Œå°‘å¹´æ—¶å¾ˆæœ‰æŠ±è´Ÿï¼Œè¯´â€œæ„¿ä¹˜é•¿é£ç ´ä¸‡é‡Œæµªâ€ã€‚ ç°ªï¼ˆzÄnï¼‰ç¬ï¼ˆhÃ¹ï¼‰ï¼šè¿™é‡Œä»£æŒ‡å®˜èŒã€‚æ™¨æ˜ï¼šæ™¨æ˜å®šçœï¼Œå‡ºè‡ª ã€Šç¤¼è®°Â·æ›²ç¤¼ä¸Šã€‹ï¼Œé‡Šä¹‰ä¸ºæ—§æ—¶ä¾å¥‰çˆ¶æ¯çš„æ—¥å¸¸ç¤¼èŠ‚ã€‚ éè°¢å®¶ä¹‹å®æ ‘ï¼Œæ¥å­Ÿæ°ä¹‹èŠ³é‚»ï¼šè‡ªå·±å¹¶ä¸æ˜¯åƒè°¢ç„é‚£æ ·å‡ºè‰²çš„äººæ‰ï¼Œå´èƒ½åœ¨ä»Šæ—¥çš„å®´ä¼šä¸Šç»“è¯†å„ä½åå£«ã€‚è°¢å®¶ä¹‹å®æ ‘ï¼šæŒ‡è°¢ç„ã€‚ã€Šæ™‹ä¹¦Â·è°¢ç„ä¼ ã€‹è®°è½½ï¼Œæ™‹æœè°¢å®‰æ›¾é—®å­ä¾„ä»¬ï¼šä¸ºä»€ä¹ˆäººä»¬æ€»å¸Œæœ›è‡ªå·±çš„å­å¼Ÿå¥½ï¼Ÿä¾„å­è°¢ç„å›ç­”ï¼šâ€œè­¬å¦‚èŠå…°ç‰æ ‘ï¼Œæ¬²ä½¿å…¶ç”Ÿäºåº­é˜¶è€³ã€‚â€åæ¥å°±ç§°è°¢ç„ä¸ºè°¢å®¶å®æ ‘ã€‚å­Ÿæ°ä¹‹èŠ³é‚»ï¼šè¿™é‡Œå€Ÿå­Ÿå­çš„æ¯äº²ä¸ºå¯»æ‰¾é‚»å±…è€Œä¸‰æ¬¡æ¬å®¶çš„æ•…äº‹ï¼Œæ¥æŒ‡èµ´å®´çš„å˜‰å®¾ã€‚ ä»–æ—¥è¶‹åº­ï¼Œå¨é™ªé²¤å¯¹ï¼šè¿‡äº›æ—¶å€™è‡ªå·±å°†åˆ°çˆ¶äº²é‚£é‡Œé™ªä¾å’Œè†å¬æ•™è¯²ã€‚è¶‹åº­ï¼šå¿«æ­¥èµ°è¿‡åº­é™¢ï¼Œè¿™æ˜¯è¡¨ç¤ºå¯¹é•¿è¾ˆçš„æ­æ•¬ã€‚å¨ï¼šæƒ­æ„§åœ°æ‰¿å—ï¼Œè¡¨ç¤ºè‡ªè°¦ã€‚é²¤å¯¹ï¼šå­”é²¤æ˜¯å­”å­çš„å„¿å­ï¼Œé²¤å¯¹æŒ‡æ¥å—çˆ¶äº²æ•™è¯²ã€‚äº‹è§ã€Šè®ºè¯­Â·å­£æ°ã€‹ï¼šï¼ˆå­”å­ï¼‰å°ç‹¬ç«‹ï¼Œï¼ˆå­”ï¼‰é²¤è¶‹è€Œè¿‡åº­ã€‚ï¼ˆå­ï¼‰æ›°ï¼šâ€œå­¦è¯—ä¹ï¼Ÿâ€å¯¹æ›°ï¼šâ€œæœªä¹Ÿã€‚â€â€œä¸å­¦è¯—ï¼Œæ— ä»¥è¨€ã€‚â€é²¤é€€è€Œå­¦è¯—ã€‚ä»–æ—¥ï¼Œåˆç‹¬ç«‹ï¼Œé²¤è¶‹è€Œè¿‡åº­ã€‚ï¼ˆå­ï¼‰æ›°ï¼šâ€œå­¦ç¤¼ä¹ï¼Ÿâ€å¯¹æ›°ï¼šâ€˜æœªä¹Ÿã€‚â€â€œä¸å­¦ç¤¼ï¼Œæ— ä»¥ç«‹ã€‚â€é²¤é€€è€Œå­¦ç¤¼ã€‚ æ§è¢‚ï¼ˆmÃ¨iï¼‰ï¼šä¸¾èµ·åŒè¢–ä½œæ–ï¼ŒæŒ‡è°’è§é˜å…¬ã€‚å–œæ‰˜é¾™é—¨ï¼šï¼ˆå—åˆ°é˜å…¬çš„æ¥å¾…ï¼‰ååˆ†é«˜å…´ï¼Œå¥½åƒç™»ä¸Šé¾™é—¨ä¸€æ ·ã€‚ æ¨æ„ï¼šå³èœ€äººæ¨å¾—æ„ï¼Œä»»æŒç®¡å¤©å­çŒçŠ¬çš„å®˜ï¼Œè¥¿æ±‰è¾èµ‹å®¶å¸é©¬ç›¸å¦‚æ˜¯ç”±ä»–æ¨èç»™æ±‰æ­¦å¸çš„ã€‚å‡Œäº‘ï¼šè¿™é‡ŒæŒ‡å¸é©¬ç›¸å¦‚çš„èµ‹ï¼Œã€Šå²è®°Â·å¸é©¬ç›¸å¦‚ä¼ ã€‹è¯´ï¼Œç›¸å¦‚çŒ®ã€Šå¤§äººèµ‹ã€‹ï¼Œâ€œå¤©å­å¤§æ‚¦ï¼Œé£˜é£˜æœ‰å‡Œäº‘ä¹‹æ°”ï¼Œä¼¼æ¸¸å¤©åœ°ä¹‹é—´â€ã€‚é’ŸæœŸï¼šå³é’Ÿå­æœŸã€‚]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pythonä¸­çš„æ‹·è´]]></title>
    <url>%2F2018%2F08%2F18%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPython%E4%B8%AD%E7%9A%84%E6%8B%B7%E8%B4%9D%2F</url>
    <content type="text"><![CDATA[Pythonçš„æ‹·è´å’ŒC/C++çš„å·®åˆ«å¾ˆå¤§ï¼Œå¾ˆç»å¸¸å°±å®¹æ˜“ææ··ï¼Œå› æ­¤è®°å½•ä¸€ä¸‹ã€‚ èµ‹å€¼ã€æ‹·è´ èµ‹å€¼ï¼šå®é™…ä¸Šå°±æ˜¯å¯¹è±¡çš„å¼•ç”¨ï¼Œæ²¡æœ‰å¼€è¾Ÿæ–°çš„å†…å­˜ç©ºé—´12lst=[1,2,3]l=lst æµ…æ‹·è´:åˆ›å»ºäº†æ–°å¯¹è±¡ï¼Œä½†æ˜¯å†…å®¹æ˜¯å¯¹åŸå¯¹è±¡çš„å¼•ç”¨ï¼Œæœ‰ä¸‰ç§å½¢å¼ åˆ‡ç‰‡ 12l=lst[:]l=[i for i in lst] å·¥å‚ 1l=list(lst) copy 12import copyl=copy.copy(lst) æ·±æ‹·è´:copyä¸­çš„deepcopyï¼Œç”Ÿæˆä¸€ä¸ªå…¨æ–°çš„å¯¹è±¡ï¼Œä¸åŸæ¥çš„å¯¹è±¡æ— å…³ 12import copyl=copy.deepcopy(lst) ä¾‹å­12345678910111213141516171819202122232425262728293031### å¼•ç”¨https://www.cnblogs.com/huangbiquan/p/7795152.html çš„ä¾‹å­###&gt;&gt;&gt; import copy&gt;&gt;&gt; a = [1,2,3,4,['a','b']] #å®šä¹‰ä¸€ä¸ªåˆ—è¡¨a&gt;&gt;&gt; b = a #èµ‹å€¼&gt;&gt;&gt; c = copy.copy(a) #æµ…æ‹·è´&gt;&gt;&gt; d = copy.deepcopy(a) #æ·±æ‹·è´&gt;&gt;&gt; a.append(5)&gt;&gt;&gt; print(a)[1, 2, 3, 4, ['a', 'b'], 5] #aæ·»åŠ ä¸€ä¸ªå…ƒç´ 5&gt;&gt;&gt; print(b) [1, 2, 3, 4, ['a', 'b'], 5] #bè·Ÿç€æ·»åŠ ä¸€ä¸ªå…ƒç´ 5 &gt;&gt;&gt; print(c)[1, 2, 3, 4, ['a', 'b']] #cä¿æŒä¸å˜&gt;&gt;&gt; print(d)[1, 2, 3, 4, ['a', 'b']] #dä¿æŒä¸å˜&gt;&gt;&gt; a[4].append('c') &gt;&gt;&gt; print(a)[1, 2, 3, 4, ['a', 'b', 'c'], 5] #aä¸­çš„list(å³a[4])æ·»åŠ ä¸€ä¸ªå…ƒç´ c&gt;&gt;&gt; print(b)[1, 2, 3, 4, ['a', 'b', 'c'], 5] #bè·Ÿç€æ·»åŠ ä¸€ä¸ªå…ƒç´ c&gt;&gt;&gt; print(c)[1, 2, 3, 4, ['a', 'b', 'c']] #cè·Ÿç€æ·»åŠ ä¸€ä¸ªå…ƒç´ c&gt;&gt;&gt; print(d)[1, 2, 3, 4, ['a', 'b']] #dä¿æŒä¸å˜#è¯´æ˜å¦‚ä¸‹ï¼š#1.å¤–å±‚æ·»åŠ å…ƒç´ æ—¶ï¼Œ æµ…æ‹·è´cä¸ä¼šéšåŸåˆ—è¡¨aå˜åŒ–è€Œå˜åŒ–ï¼›å†…å±‚listæ·»åŠ å…ƒç´ æ—¶ï¼Œæµ…æ‹·è´cæ‰ä¼šå˜åŒ–ã€‚#2.æ— è®ºåŸåˆ—è¡¨aå¦‚ä½•å˜åŒ–ï¼Œæ·±æ‹·è´déƒ½ä¿æŒä¸å˜ã€‚#3.èµ‹å€¼å¯¹è±¡éšç€åŸåˆ—è¡¨ä¸€èµ·å˜åŒ– Referencehttps://www.cnblogs.com/huangbiquan/p/7795152.htmlhttps://www.cnblogs.com/xueli/p/4952063.html]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Python</tag>
        <tag>æ‹·è´</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¦‚ä½•å°†ELMoè¯å‘é‡ç”¨äºä¸­æ–‡]]></title>
    <url>%2F2018%2F08%2F12%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%A6%82%E4%BD%95%E5%B0%86ELMo%E8%AF%8D%E5%90%91%E9%87%8F%E7%94%A8%E4%BA%8E%E4%B8%AD%E6%96%87%2F</url>
    <content type="text"><![CDATA[10.10æ›´æ–°ï¼šELMoå·²ç»ç”±å“ˆå·¥å¤§ç»„ç”¨PyTorché‡å†™äº†ï¼Œå¹¶ä¸”æä¾›äº†ä¸­æ–‡çš„é¢„è®­ç»ƒå¥½çš„language modelï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚ ELMoäºä»Šå¹´äºŒæœˆç”±AllenNLPæå‡ºï¼Œä¸word2vecæˆ–GloVeä¸åŒçš„æ˜¯å…¶åŠ¨æ€è¯å‘é‡çš„æ€æƒ³ï¼Œå…¶æœ¬è´¨å³é€šè¿‡è®­ç»ƒlanguage modelï¼Œå¯¹äºä¸€å¥è¯è¿›å…¥åˆ°language modelè·å¾—ä¸åŒçš„è¯å‘é‡ã€‚æ ¹æ®å®éªŒå¯å¾—ï¼Œä½¿ç”¨äº†Elmoè¯å‘é‡ä¹‹åï¼Œè®¸å¤šNLPä»»åŠ¡éƒ½æœ‰äº†å¤§å¹…çš„æé«˜ã€‚ è®ºæ–‡:Deep contextualized word representations AllenNLPä¸€å…±releaseäº†ä¸¤ä»½ELMoçš„ä»£ç ï¼Œä¸€ä»½æ˜¯Pytorchç‰ˆæœ¬çš„ï¼Œå¦ä¸€ä»½æ˜¯Tensorflowç‰ˆæœ¬çš„ã€‚Pytorchç‰ˆæœ¬çš„åªå¼€æ”¾äº†ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„è¯å‘é‡çš„æ¥å£ï¼Œä½†æ²¡æœ‰ç»™å‡ºè‡ªå·±è®­ç»ƒçš„æ¥å£ï¼Œå› æ­¤æ— æ³•ä½¿ç”¨åˆ°ä¸­æ–‡è¯­æ–™ä¸­ã€‚Tensorflowç‰ˆæœ¬æœ‰æä¾›è®­ç»ƒçš„ä»£ç ï¼Œå› æ­¤æœ¬æ–‡è®°å½•å¦‚ä½•å°†ELMoç”¨äºä¸­æ–‡è¯­æ–™ä¸­ï¼Œä½†æœ¬æ–‡åªè®°å½•ä½¿ç”¨åˆ°çš„éƒ¨åˆ†ï¼Œè€Œä¸ä¼šåˆ†æå…¨éƒ¨çš„ä»£ç ã€‚ éœ€æ±‚:ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„è¯å‘é‡ä½œä¸ºå¥å­è¡¨ç¤ºç›´æ¥ä¼ å…¥åˆ°RNNä¸­(ä¹Ÿå°±æ˜¯ä¸ä½¿ç”¨ä»£ç ä¸­é»˜è®¤çš„å…ˆè¿‡CNN)ï¼Œåœ¨è®­ç»ƒå®Œåï¼Œå°†æ¨¡å‹ä¿å­˜ï¼Œåœ¨éœ€è¦ç”¨çš„æ—¶å€™loadè¿›æ¥ï¼Œå¯¹äºä¸€ä¸ªç‰¹å®šçš„å¥å­ï¼Œé¦–å…ˆå°†å…¶è½¬æ¢æˆé¢„è®­ç»ƒçš„è¯å‘é‡ï¼Œä¼ å…¥language modelä¹‹åæœ€ç»ˆå¾—åˆ°ELMoè¯å‘é‡ã€‚ å‡†å¤‡å·¥ä½œ: å°†ä¸­æ–‡è¯­æ–™åˆ†è¯ è®­ç»ƒå¥½GloVeè¯å‘é‡æˆ–è€…word2vec ä¸‹è½½bilm-tfä»£ç  ç”Ÿæˆè¯è¡¨ vocab_file ï¼ˆè®­ç»ƒçš„æ—¶å€™è¦ç”¨åˆ°ï¼‰ optional:é˜…è¯»Readme optional:é€šè¯»bilm-tfçš„ä»£ç ï¼Œå¯¹ä»£ç ç»“æ„æœ‰ä¸€å®šçš„è®¤è¯† æ€è·¯: å°†é¢„è®­ç»ƒçš„è¯å‘é‡è¯»å…¥ ä¿®æ”¹bilm-tfä»£ç  optionéƒ¨åˆ† æ·»åŠ ç»™embedding weightèµ‹åˆå€¼ æ·»åŠ ä¿å­˜embedding weightçš„ä»£ç  å¼€å§‹è®­ç»ƒï¼Œè·å¾—checkpointå’Œoptionæ–‡ä»¶ è¿è¡Œè„šæœ¬ï¼Œè·å¾—language modelçš„weightæ–‡ä»¶ å°†embedding weightä¿å­˜ä¸ºhdf5æ–‡ä»¶å½¢å¼ è¿è¡Œè„šæœ¬ï¼Œå°†è¯­æ–™è½¬åŒ–æˆELMo embeddingã€‚ è®­ç»ƒGloVeæˆ–word2vecå¯å‚è§æˆ‘ä»¥å‰çš„åšå®¢æˆ–è€…ç½‘ä¸Šçš„æ•™ç¨‹ã€‚æ³¨æ„åˆ°ï¼Œå¦‚æœè¦ç”¨gensimå¯¼å…¥GloVeè®­å¥½çš„è¯å‘é‡ï¼Œéœ€è¦åœ¨å¼€å¤´æ·»åŠ num_word embedding_dimã€‚ å¦‚ï¼š è·å¾—vocabè¯è¡¨æ–‡ä»¶æ³¨æ„åˆ°ï¼Œè¯è¡¨æ–‡ä»¶çš„å¼€å¤´å¿…é¡»è¦æœ‰&lt;S&gt; &lt;/S&gt; &lt;UNK&gt;ï¼Œä¸”å¤§å°å†™æ•æ„Ÿã€‚å¹¶ä¸”åº”å½“æŒ‰ç…§å•è¯çš„è¯é¢‘é™åºæ’åˆ—ã€‚å¯ä»¥é€šè¿‡æ‰‹åŠ¨æ·»åŠ è¿™ä¸‰ä¸ªç‰¹æ®Šç¬¦å·ã€‚å¦‚ï¼š ä»£ç ï¼š1234567891011121314model=gensim.models.KeyedVectors.load_word2vec_format( fname='/home/zhlin/GloVe/vectors.txt',binary=False)words=model.vocabwith open('vocab.txt','w') as f: f.write('&lt;S&gt;') f.write('\n'ï¼‰ f.write('&lt;/S&gt;') f.write('\n') f.write('&lt;UNK&gt;') f.write('\n') # bilm-tf è¦æ±‚vocabæœ‰è¿™ä¸‰ä¸ªç¬¦å·ï¼Œå¹¶ä¸”åœ¨æœ€å‰é¢ for word in words: f.write(word) f.write('\n') ä¿®æ”¹bilm-tfä»£ç æ³¨æ„åˆ°ï¼Œåœ¨ä½¿ç”¨è¯¥ä»£ç ä¹‹å‰ï¼Œéœ€è¦å®‰è£…å¥½ç›¸åº”çš„ç¯å¢ƒã€‚ å¦‚æœä½¿ç”¨çš„æ˜¯condaä½œä¸ºé»˜è®¤çš„Pythonè§£é‡Šå™¨ï¼Œå¼ºçƒˆå»ºè®®ä½¿ç”¨condaå®‰è£…ï¼Œå¦åˆ™å¯èƒ½ä¼šå‡ºç°ä¸€äº›è«åçš„é”™è¯¯ã€‚123conda install tensorflow-gpu=1.4conda install h5pypython setup.py install #åº”åœ¨bilm-tfçš„æ–‡ä»¶å¤¹ä¸‹æ‰§è¡Œè¯¥æŒ‡ä»¤ ç„¶åå†è¿è¡Œæµ‹è¯•ä»£ç ï¼Œé€šè¿‡è¯´æ˜å®‰è£…æˆåŠŸã€‚ ä¿®æ”¹train_elmo.pybinæ–‡ä»¶å¤¹ä¸‹çš„train_elmo.pyæ˜¯ç¨‹åºçš„å…¥å£ã€‚ä¸»è¦ä¿®æ”¹çš„åœ°æ–¹ï¼š load_vocabçš„ç¬¬äºŒä¸ªå‚æ•°åº”è¯¥æ”¹ä¸ºNone n_gpus CUDA_VISIBLE_DEVICES æ ¹æ®è‡ªå·±éœ€æ±‚æ”¹ n_train_tokens å¯æ”¹å¯ä¸æ”¹ï¼Œå½±å“çš„æ˜¯è¾“å‡ºä¿¡æ¯ã€‚è¦æŸ¥çœ‹è‡ªå·±è¯­æ–™çš„è¡Œæ•°ï¼Œå¯ä»¥é€šè¿‡wc -l corpus.txt æŸ¥çœ‹ã€‚ optionçš„ä¿®æ”¹ï¼Œå°†char_cnnéƒ¨åˆ†éƒ½æ³¨é‡Šæ‰ï¼Œå…¶ä»–æ ¹æ®è‡ªå·±éœ€æ±‚ä¿®æ”¹ å¦‚ï¼š ä¿®æ”¹LanguageModelç±»ç”±äºæˆ‘éœ€è¦ä¼ å…¥é¢„è®­ç»ƒå¥½çš„GloVe embeddingï¼Œé‚£ä¹ˆè¿˜éœ€è¦ä¿®æ”¹embeddingéƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†åœ¨bilmæ–‡ä»¶å¤¹ä¸‹çš„training.pyï¼Œè¿›å…¥åˆ°LanguageModelç±»ä¸­_build_word_embeddingså‡½æ•°ä¸­ã€‚æ³¨æ„åˆ°ï¼Œç”±äºå‰ä¸‰ä¸ªæ˜¯&lt;S&gt; &lt;/S&gt; &lt;UNK&gt;ï¼Œè€Œè¿™ä¸‰ä¸ªå­—ç¬¦åœ¨GloVeé‡Œé¢æ˜¯æ²¡æœ‰çš„ï¼Œå› æ­¤è¿™ä¸‰ä¸ªå­—ç¬¦çš„embeddingåº”å½“åœ¨è®­ç»ƒçš„æ—¶å€™é€æ¸å­¦ä¹ åˆ°ï¼Œè€Œæ­£å› æ­¤ embedding_weightsçš„trainableåº”å½“è®¾ä¸ºTrue å¦‚: ä¿®æ”¹trainå‡½æ•°æ·»åŠ ä»£ç ï¼Œä½¿å¾—åœ¨trainå‡½æ•°çš„æœ€åä¿å­˜embeddingæ–‡ä»¶ã€‚ è®­ç»ƒå¹¶è·å¾—weightsæ–‡ä»¶è®­ç»ƒéœ€è¦è¯­æ–™æ–‡ä»¶corpus.txtï¼Œè¯è¡¨æ–‡ä»¶vocab.txtã€‚ è®­ç»ƒcdåˆ°bilm-tfæ–‡ä»¶å¤¹ä¸‹ï¼Œè¿è¡Œ12345export CUDA_VISIBLE_DEVICES=4nohup python -u bin/train_elmo.py \--train_prefix='/home/zhlin/bilm-tf/corpus.txt' \--vocab_file /home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab.txt \--save_dir /home/zhlin/bilm-tf/try &gt;bilm_out.txt 2&gt;&amp;1 &amp; æ ¹æ®å®é™…æƒ…å†µè®¾å®šä¸åŒçš„å€¼å’Œè·¯å¾„ã€‚ è¿è¡Œæƒ…å†µï¼š PS:è¿è¡Œè¿‡ç¨‹ä¸­å¯èƒ½ä¼šæœ‰warning: â€˜listâ€™ object has no attribute â€˜nameâ€™WARNING:tensorflow:Error encountered when serializing lstm_output_embeddings.Type is unsupported, or the types of the items donâ€™t match field type in CollectionDef. åº”è¯¥ä¸ç”¨æ‹…å¿ƒï¼Œè¿˜æ˜¯èƒ½å¤Ÿç»§ç»­è¿è¡Œçš„ï¼Œåé¢ä¹Ÿä¸å—å½±å“ã€‚ åœ¨ç­‰å¾…äº†ç›¸å½“é•¿çš„æ—¶é—´åï¼Œåœ¨save_diræ–‡ä»¶å¤¹å†…ç”Ÿæˆäº†å‡ ä¸ªæ–‡ä»¶ï¼Œå…¶ä¸­checkpointå’Œoptionsæ˜¯å…³é”®ï¼Œcheckpointèƒ½å¤Ÿè¿›ä¸€æ­¥ç”Ÿæˆlanguage modelçš„weightsæ–‡ä»¶ï¼Œè€Œoptionsè®°å½•language modelçš„å‚æ•°ã€‚ è·å¾—language modelçš„weightsæ¥ä¸‹æ¥è¿è¡Œbin/dump_weights.pyå°†checkpointè½¬æ¢æˆhdf5æ–‡ä»¶ã€‚ 123nohup python -u /home/zhlin/bilm-tf/bin/dump_weights.py \--save_dir /home/zhlin/bilm-tf/try \--outfile /home/zhlin/bilm-tf/try/weights.hdf5 &gt;outfile.txt 2&gt;&amp;1 &amp; å…¶ä¸­save_diræ˜¯checkpointå’Œoptionæ–‡ä»¶ä¿å­˜çš„åœ°å€ã€‚ æ¥ä¸‹æ¥ç­‰å¾…ç¨‹åºè¿è¡Œï¼š æœ€ç»ˆè·å¾—äº†æƒ³è¦çš„weightså’Œoptionï¼š å°†è¯­æ–™è½¬åŒ–æˆELMo embeddingç”±äºæˆ‘ä»¬æœ‰äº†vocab_fileã€ä¸vocab_fileä¸€ä¸€å¯¹åº”çš„embedding h5pyæ–‡ä»¶ã€ä»¥åŠlanguage modelçš„weights.hdf5å’Œoptions.jsonã€‚æ¥ä¸‹æ¥å‚è€ƒusage_token.pyå°†ä¸€å¥è¯è½¬åŒ–æˆELMo embeddingã€‚ å‚è€ƒä»£ç ï¼š12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import tensorflow as tfimport osfrom bilm import TokenBatcher, BidirectionalLanguageModel, weight_layers, \ dump_token_embeddings# Our small dataset.raw_context = [ 'è¿™ æ˜¯ æµ‹è¯• .', 'å¥½çš„ .']tokenized_context = [sentence.split() for sentence in raw_context]tokenized_question = [ ['è¿™', 'æ˜¯', 'ä»€ä¹ˆ'],]vocab_file='/home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab.txt'options_file='/home/zhlin/bilm-tf/try/options.json'weight_file='/home/zhlin/bilm-tf/try/weights.hdf5'token_embedding_file='/home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab_embedding.hdf5'## Now we can do inference.# Create a TokenBatcher to map text to token ids.batcher = TokenBatcher(vocab_file)# Input placeholders to the biLM.context_token_ids = tf.placeholder('int32', shape=(None, None))question_token_ids = tf.placeholder('int32', shape=(None, None))# Build the biLM graph.bilm = BidirectionalLanguageModel( options_file, weight_file, use_character_inputs=False, embedding_weight_file=token_embedding_file)# Get ops to compute the LM embeddings.context_embeddings_op = bilm(context_token_ids)question_embeddings_op = bilm(question_token_ids)elmo_context_input = weight_layers('input', context_embeddings_op, l2_coef=0.0)with tf.variable_scope('', reuse=True): # the reuse=True scope reuses weights from the context for the question elmo_question_input = weight_layers( 'input', question_embeddings_op, l2_coef=0.0 )elmo_context_output = weight_layers( 'output', context_embeddings_op, l2_coef=0.0)with tf.variable_scope('', reuse=True): # the reuse=True scope reuses weights from the context for the question elmo_question_output = weight_layers( 'output', question_embeddings_op, l2_coef=0.0 )with tf.Session() as sess: # It is necessary to initialize variables once before running inference. sess.run(tf.global_variables_initializer()) # Create batches of data. context_ids = batcher.batch_sentences(tokenized_context) question_ids = batcher.batch_sentences(tokenized_question) # Compute ELMo representations (here for the input only, for simplicity). elmo_context_input_, elmo_question_input_ = sess.run( [elmo_context_input['weighted_op'], elmo_question_input['weighted_op']], feed_dict=&#123;context_token_ids: context_ids, question_token_ids: question_ids&#125; )print(elmo_context_input_,elmo_context_input_) å¯ä»¥ä¿®æ”¹ä»£ç ä»¥é€‚åº”è‡ªå·±çš„éœ€æ±‚ã€‚ Referencehttps://github.com/allenai/bilm-tf]]></content>
      <tags>
        <tag>ELMo</tag>
        <tag>æ•™ç¨‹</tag>
        <tag>è¯å‘é‡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•4]]></title>
    <url>%2F2018%2F08%2F12%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB4%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨æ²¡æœ‰ä»€ä¹ˆä»£ç è¦è®°å½•çš„ã€‚ 1ï¸âƒ£sklearnä¹‹Pipelineä¾‹å­ç”¨æœºå™¨å­¦ä¹ è§£å†³é—®é¢˜çš„æµç¨‹ï¼š(å»æ‰éƒ¨åˆ†æ•°æ®ï¼‰â€”&gt; è·å–featureï¼ˆTf-idfç­‰ï¼‰ â€”&gt; ï¼ˆfeature selectionï¼Œchi2ã€äº’ä¿¡æ¯ç­‰ï¼‰ â€”&gt; ï¼ˆç¼©æ”¾/æ­£åˆ™åŒ–ï¼‰ â€”&gt; åˆ†ç±»å™¨ â€”&gt; GridSearch/RandomizedSearchè°ƒå‚ 123456789101112131415161718192021222324252627282930pipe=Pipeline([ #å»ºç«‹pipeline ('vect',TfidfVectorizer()), ('select',SelectKBest(chi2), ('norm',MaxAbsScaler()), ('svm',svm.LinearSVC())])parameters=&#123; 'vect__ngram_range':[(1,1),(1,2),(1,3),(2,3)], 'vect__max_df':[0.6,0.7,0.8,0.9], 'vect__min_df':[1,3,5,7,9], 'vect__norm':['l1','l2'], 'svm__penalty':['l1','l2'], 'svm__loss':['squared_hinge'], 'svm__dual':[False,True], 'svm__tol':[1e-5,1e-4], 'svm__C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1], 'svm__class_weight':[None,'balanced'], 'svm__max_iter':[1000,5000]&#125;grid_search_model=GridSearchCV(pipe,parameters,error_score=0,n_jobs=5)grid_search_model.fit(train[column],train['class'])for para_name in sorted(parameters.keys()): print(para_name,grid_search_model.best_params_[para_name])print("cv_result:")print(grid_search_model.cv_results_)]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†3]]></title>
    <url>%2F2018%2F08%2F12%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%863%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Python]åœ¨æœåŠ¡å™¨ä¸Šè·‘ä»£ç æ—¶ï¼Œå¦‚ python project/folder1/a.pyï¼Œå¦‚æœa.pyå¼•ç”¨äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„æ¨¡å—ä½†åˆä¸åœ¨folder1å†…ï¼Œæ­¤æ—¶interpreterå°±ä¼šæŠ¥é”™ï¼Œæç¤ºæ‰¾ä¸åˆ°è¯¥æ¨¡å—ã€‚è¿™æ˜¯å› ä¸ºè§£é‡Šå™¨é»˜è®¤åªä¼šåœ¨åŒä¸€ä¸ªfolderä¸‹æŸ¥æ‰¾ã€‚è§£å†³æ–¹æ¡ˆæ˜¯åœ¨è¿è¡Œå‰æ˜¾å¼æ·»åŠ æŸ¥æ‰¾èŒƒå›´ã€‚å¦‚ï¼š1export PYTHONPATH=/home/zhlin/bilm-tf:$PYTHONPATH é‚£ä¹ˆpythonè§£é‡Šå™¨å°±ä¼šåˆ°è¯¥ç›®å½•ä¸‹å»æ‰¾ã€‚ 2ï¸âƒ£[åº¦é‡æ ‡å‡†] å‡†ç¡®ç‡(accuracy): $ACC=\frac{TP+TN}{TP+TN+FP+FN}$ è¡¡é‡çš„æ˜¯åˆ†ç±»å™¨é¢„æµ‹å‡†ç¡®çš„æ¯”ä¾‹ å¬å›ç‡(recall): $Recall=\frac{TP}{TP+FN}$ æ­£ä¾‹ä¸­è¢«åˆ†å¯¹çš„æ¯”ä¾‹ï¼Œè¡¡é‡äº†åˆ†ç±»å™¨å¯¹æ­£ä¾‹çš„è¯†åˆ«èƒ½åŠ›ã€‚ ç²¾ç¡®ç‡(Precision): $P=\frac{TP}{TP+FP}$åº¦é‡äº†è¢«åˆ†ä¸ºæ­£ä¾‹çš„ç¤ºä¾‹ä¸­å®é™…ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ã€‚ F-Measure: $F=\frac{(\alpha^2 +1)P*R}{\alpha^2 (P+R)}$ å…¶ä¸­Pæ˜¯Precision,Ræ˜¯Recallã€‚ç»¼åˆè€ƒé‡äº†ä¸¤ç§åº¦é‡ã€‚ å½“$\alpha=1$æ—¶ï¼Œç§°ä¸ºF1å€¼ $F1=\frac{2PR}{P+R}$ 3ï¸âƒ£[è°ƒå‚æŠ€å·§]åœ¨googleå‘å¸ƒçš„ä¸€ä»½å…³äºtext-classificationçš„guideä¸­ï¼Œæåˆ°äº†å‡ ä¸ªè°ƒå‚çš„trickã€‚ åœ¨feature selectionæ­¥éª¤ä¸­ï¼Œå¡æ–¹æ£€éªŒchi2å’Œæ–¹å·®åˆ†æçš„Få€¼ f_classifçš„è¡¨ç°ç›¸å½“ï¼Œåœ¨å¤§çº¦é€‰æ‹©20kçš„featureæ—¶ï¼Œå‡†ç¡®ç‡è¾¾åˆ°é¡¶å³°ï¼Œå½“featureè¶Šå¤šï¼Œæ•ˆæœå¹¶æ²¡æœ‰æå‡ç”šè‡³ä¼šä¸‹é™ã€‚ åœ¨æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œä¼¼ä¹ä½¿ç”¨normalizationå¹¶æ²¡æœ‰å¤šå°‘ç”¨å¤„ï¼Œå»ºè®®è·³è¿‡ã€‚ Normalization converts all feature/sample values to small and similar values. This simplifies gradient descent convergence in learning algorithms. From what we have seen, normalization during data preprocessing does not seem to add much value in text classification problems; we recommend skipping this step. å®é™…ä¸Šæˆ‘ä¹Ÿæµ‹è¯•è¿‡ï¼Œå‘ç°ç¡®å®normalizationå¯¹äºå‡†ç¡®ç‡çš„æé«˜æ²¡ä»€ä¹ˆå¸®åŠ©ï¼Œç”šè‡³è¿˜æœ‰ä¸€ç‚¹ä¸‹é™ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Python</tag>
        <tag>åº¦é‡æ ‡å‡†</tag>
        <tag>è°ƒå‚æŠ€å·§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯4]]></title>
    <url>%2F2018%2F08%2F12%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D4%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£ çä¸Šç§‹å±…[å”] é©¬æˆ´çåŸé£é›¨å®šï¼Œæ™šè§é›è¡Œé¢‘ã€‚è½å¶ä»–ä¹¡æ ‘ï¼Œå¯’ç¯ç‹¬å¤œäººã€‚ç©ºå›­ç™½éœ²æ»´ï¼Œå­¤å£é‡åƒ§é‚»ã€‚å¯„å§éƒŠæ‰‰ä¹…ï¼Œä½•å¹´è‡´æ­¤èº«ã€‚ http://m.xichuangzhu.com/work/57b8a4080a2b58005c9108d9 2ï¸âƒ£ å”å¤šä»¤[å®‹] åˆ˜è¿‡èŠ¦å¶æ»¡æ±€æ´²ï¼Œå¯’æ²™å¸¦æµ…æµã€‚äºŒåå¹´é‡è¿‡å—æ¥¼ã€‚æŸ³ä¸‹ç³»èˆ¹çŠ¹æœªç¨³ï¼Œèƒ½å‡ æ—¥ï¼Œåˆä¸­ç§‹ã€‚é»„é¹¤æ–­çŸ¶å¤´ï¼Œæ•…äººä»Šåœ¨å¦ï¼Ÿæ—§æ±Ÿå±±æµ‘æ˜¯æ–°æ„ã€‚æ¬²ä¹°æ¡‚èŠ±åŒè½½é…’ï¼Œç»ˆä¸ä¼¼ã€å°‘å¹´æ¸¸ã€‚ http://m.xichuangzhu.com/work/57b922e7c4c9710055904842]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vimå¸¸ç”¨å¿«æ·é”®]]></title>
    <url>%2F2018%2F08%2F10%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2FVim%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[åœ¨æœåŠ¡å™¨ç»å¸¸è¦ç”¨åˆ°Vimï¼Œå› æ­¤è®°å½•å¸¸ç”¨çš„å¿«æ·é”®å¹¶ç†Ÿæ‚‰ä¹‹ã€‚ é€€å‡º:q é€€å‡º:wq å†™å…¥å¹¶é€€å‡º:q! é€€å‡ºå¹¶å¿½ç•¥æ‰€æœ‰æ›´æ”¹:e! æ”¾å¼ƒä¿®æ”¹å¹¶æ‰“å¼€åŸæ¥çš„æ–‡ä»¶ æ’å…¥i åœ¨å½“å‰ä½ç½®å‰æ’å…¥a åœ¨å½“å‰ä½ç½®åæ’å…¥ æ’¤é”€:u æ’¤é”€:U æ’¤é”€æ•´è¡Œæ“ä½œCtrl+r é‡åš åˆ é™¤:md åˆ é™¤ç¬¬mè¡Œnd åˆ é™¤å½“å‰è¡Œå¼€å§‹çš„nè¡Œ(ä¸€å…±n+1è¡Œ)dd åˆ é™¤å½“å‰è¡ŒD åˆ é™¤å½“å‰å­—ç¬¦è‡³è¡Œå°¾:m,nd åˆ é™¤ä»måˆ°nè¡Œçš„å†…å®¹ï¼Œå¦‚: :100,10000d:m,$d åˆ é™¤mè¡ŒåŠä»¥åæ‰€æœ‰çš„è¡Œ:10d ç§»åŠ¨:n è·³è½¬åˆ°è¡Œå· å¦‚ï¼Œ :100gg è·³åˆ°è¡Œé¦–G(shift+g)ç§»åŠ¨åˆ°æ–‡ä»¶å°¾ æœç´¢/text æœç´¢textï¼Œnæœç´¢ä¸‹ä¸€ä¸ªï¼ŒNæœç´¢ä¸Šä¸€ä¸ª?text åå‘æŸ¥æ‰¾:set ignorecase å¿½ç•¥å¤§å°å†™æŸ¥æ‰¾:set noignorecase ä¸å¿½ç•¥å¤§å°å†™æŸ¥æ‰¾*æˆ–# å¯¹å…‰æ ‡å¤„çš„å•è¯æœç´¢ å¤åˆ¶ç²˜è´´v ä»å½“å‰ä½ç½®å¼€å§‹ï¼Œå…‰æ ‡ç»è¿‡çš„åœ°æ–¹è¢«é€‰ä¸­ï¼Œå†æŒ‰ä¸€ä¸‹vç»“æŸ ç¯å¢ƒè®¾ç½®:set nu æ˜¾ç¤ºè¡Œå·:set nonu éšè—è¡Œå·:set hlsearch è®¾ç½®æœç´¢ç»“æœé«˜äº® Referencehttps://www.cnblogs.com/wangrx/p/5907013.htmlhttps://www.cnblogs.com/yangjig/p/6014198.html]]></content>
      <tags>
        <tag>æŠ€å·§</tag>
        <tag>æ‚ä¸ƒæ‚å…«</tag>
        <tag>å¿«æ·é”®</tag>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pycharmå¸¸ç”¨æŠ€å·§]]></title>
    <url>%2F2018%2F08%2F10%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2FPycharm%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[è®°å½•Pycharmçš„ä¸€äº›æŠ€å·§ï¼Œè®©Pycharmæ›´é¡ºæ‰‹ å¿«æ·é”®0ï¸âƒ£Double Shift ä¸‡èƒ½æœç´¢å¯ä»¥æœç´¢æ–‡ä»¶åã€ç±»åã€æ–¹æ³•åã€ç›®å½•åï¼ˆåœ¨å…³é”®å­—å‰é¢åŠ / ï¼‰ï¼Œå¹¶ä¸èƒ½ç”¨æ¥æœç´¢ä»»æ„å…³é”®å­— 1ï¸âƒ£ Command+F åœ¨é¡µé¢æœç´¢ 2ï¸âƒ£ Ctrl+Shift+F Find in Path åœ¨è·¯å¾„ä¸‹æœç´¢ 3ï¸âƒ£âœ¨Command+E å¿«é€ŸæŸ¥æ‰¾æ–‡ä»¶æ˜¾ç¤ºæœ€è¿‘æ‰“å¼€çš„æ–‡ä»¶ 4ï¸âƒ£ Shift+Enter ä»»æ„ä½ç½®æ¢è¡Œæ— è®ºå…‰æ ‡åœ¨ä½•å¤„éƒ½å¯ä»¥ç›´æ¥å¦èµ·ä¸€è¡Œ 5ï¸âƒ£ Option+Enter è‡ªåŠ¨å¯¼å…¥æ¨¡å—ï¼›ä¸‡èƒ½æç¤ºé”®è‡ªåŠ¨å¯¼å…¥å¦‚ä½•è®¾ç½®è§å°æŠ€å·§#0ï¸âƒ£ 6ï¸âƒ£ Ctrl+F10 è¿è¡Œæˆ‘å·²ç»æ·»åŠ äº†Ctrl+Rä½œä¸ºå¦ä¸€å¯¹è¿è¡Œå¿«æ·é”® 7ï¸âƒ£ Command+Shift+ +/- å±•å¼€/æ”¶ç¼©ä»£ç  8ï¸âƒ£ Option+F åœ¨Dashä¸­æœç´¢ å°æŠ€å·§0ï¸âƒ£ Pycharmè‡ªåŠ¨å¯¼å…¥æ¨¡å—https://blog.csdn.net/lantian_123/article/details/78094148 1ï¸âƒ£ âœ¨è¿œç¨‹éƒ¨ç½²å·¥ç¨‹ å¼ºçƒˆæ¨èä¸¤æ­¥èµ°ï¼šé…ç½®æœåŠ¡å™¨æ˜ å°„+é…ç½®æœåŠ¡å™¨è§£é‡Šå™¨ 2ï¸âƒ£è·³è½¬åå¦‚ä½•å›é€€å¼€å¯toolbarå³å¯https://segmentfault.com/a/1190000010205945 Referencehttps://foofish.net/pycharm-tips.htmlhttps://blog.csdn.net/lantian_123/article/details/78094148https://segmentfault.com/a/1190000010205945]]></content>
      <tags>
        <tag>æŠ€å·§</tag>
        <tag>Pycharm</tag>
        <tag>æ‚ä¸ƒæ‚å…«</tag>
        <tag>å¿«æ·é”®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ— é¢˜]]></title>
    <url>%2F2018%2F08%2F06%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%97%A0%E9%A2%98%2F</url>
    <content type="text"><![CDATA[äººè¿™è¾ˆå­ä¸€å…±ä¼šæ­»ä¸‰æ¬¡ã€‚ ç¬¬ä¸€æ¬¡æ˜¯ä½ çš„å¿ƒè„åœæ­¢è·³åŠ¨ï¼Œé‚£ä¹ˆä»ç”Ÿç‰©çš„è§’åº¦æ¥è¯´ï¼Œä½ æ­»äº†ï¼› ç¬¬äºŒæ¬¡æ˜¯åœ¨è‘¬ç¤¼ä¸Šï¼Œè®¤è¯†ä½ çš„äººéƒ½æ¥ç¥­å¥ ï¼Œé‚£ä¹ˆä½ åœ¨ç¤¾ä¼šå…³ç³»ä¸Šçš„äº‹å®å­˜åœ¨å°±æ­»äº†ï¼› ç¬¬ä¸‰æ¬¡æ˜¯åœ¨æœ€åä¸€ä¸ªè®°å¾—ä½ çš„äººæ­»åï¼Œé‚£ä½ å°±çœŸçš„æ­»äº†ã€‚]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[äººå¯ä»¥å‘å¾®å¦‚å°˜åœŸ,ä¸å¯æ‰­æ›²å¦‚è›†è™«]]></title>
    <url>%2F2018%2F08%2F05%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E4%BA%BA%E5%8F%AF%E4%BB%A5%E5%8D%91%E5%BE%AE%E5%A6%82%E5%B0%98%E5%9C%9F%2C%E4%B8%8D%E5%8F%AF%E6%89%AD%E6%9B%B2%E5%A6%82%E8%9B%86%E8%99%AB%2F</url>
    <content type="text"><![CDATA[å¦‚æœå¤©æ€»ä¹Ÿä¸äº®ï¼Œé‚£å°±æ‘¸é»‘è¿‡ç”Ÿæ´»; å¦‚æœå‘å‡ºå£°éŸ³æ˜¯å±é™©çš„ï¼Œé‚£å°±ä¿æŒæ²‰é»˜; å¦‚æœè‡ªè§‰æ— åŠ›å‘å…‰ï¼Œé‚£å°±åˆ«å»ç…§äº®åˆ«äººã€‚ ä½†æ˜¯â€”â€”ä¸è¦ä¹ æƒ¯äº†é»‘æš—å°±ä¸ºé»‘æš—è¾©æŠ¤; ä¸è¦ä¸ºè‡ªå·±çš„è‹Ÿä¸”è€Œå¾—æ„æ´‹æ´‹; ä¸è¦å˜²è®½é‚£äº›æ¯”è‡ªå·±æ›´å‹‡æ•¢ã€æ›´æœ‰çƒ­é‡çš„äººä»¬ã€‚ å¯ä»¥å‘å¾®å¦‚å°˜åœŸï¼Œä¸å¯æ‰­æ›²å¦‚è›†è™«ã€‚]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pythonæƒ¯ä¾‹[è½¬]]]></title>
    <url>%2F2018%2F08%2F05%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPython%E6%83%AF%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[fork from https://github.com/jackfrued/Python-100-Days/blob/master/Pythonæƒ¯ä¾‹.md Pythonæƒ¯ä¾‹â€œæƒ¯ä¾‹â€è¿™ä¸ªè¯æŒ‡çš„æ˜¯â€œä¹ æƒ¯çš„åšæ³•ï¼Œå¸¸è§„çš„åŠæ³•ï¼Œä¸€è´¯çš„åšæ³•â€ï¼Œä¸è¿™ä¸ªè¯å¯¹åº”çš„è‹±æ–‡å•è¯å«â€œidiomâ€ã€‚ç”±äºPythonè·Ÿå…¶ä»–å¾ˆå¤šç¼–ç¨‹è¯­è¨€åœ¨è¯­æ³•å’Œä½¿ç”¨ä¸Šè¿˜æ˜¯æœ‰æ¯”è¾ƒæ˜¾è‘—çš„å·®åˆ«ï¼Œå› æ­¤ä½œä¸ºä¸€ä¸ªPythonå¼€å‘è€…å¦‚æœä¸èƒ½æŒæ¡è¿™äº›æƒ¯ä¾‹ï¼Œå°±æ— æ³•å†™å‡ºâ€œPythonicâ€çš„ä»£ç ã€‚ä¸‹é¢æˆ‘ä»¬æ€»ç»“äº†ä¸€äº›åœ¨Pythonå¼€å‘ä¸­çš„æƒ¯ç”¨çš„ä»£ç ã€‚ è®©ä»£ç æ—¢å¯ä»¥è¢«å¯¼å…¥åˆå¯ä»¥è¢«æ‰§è¡Œã€‚ 1if __name__ == '__main__': ç”¨ä¸‹é¢çš„æ–¹å¼åˆ¤æ–­é€»è¾‘â€œçœŸâ€æˆ–â€œå‡â€ã€‚ 12if x:if not x: å¥½çš„ä»£ç ï¼š 12345name = 'jackfrued'fruits = ['apple', 'orange', 'grape']owners = &#123;'1001': 'éª†æ˜Š', '1002': 'ç‹å¤§é”¤'&#125;if name and fruits and owners: print('I love fruits!') ä¸å¥½çš„ä»£ç ï¼š 12345name = 'jackfrued'fruits = ['apple', 'orange', 'grape']owners = &#123;'1001': 'éª†æ˜Š', '1002': 'ç‹å¤§é”¤'&#125;if name != '' and len(fruits) &gt; 0 and owners != &#123;&#125;: print('I love fruits!') å–„äºä½¿ç”¨inè¿ç®—ç¬¦ã€‚ 12if x in items: # åŒ…å«for x in items: # è¿­ä»£ å¥½çš„ä»£ç ï¼š 123name = 'Hao LUO'if 'L' in name: print('The name has an L in it.') ä¸å¥½çš„ä»£ç ï¼š 123name = 'Hao LUO'if name.find('L') != -1: print('This name has an L in it!') ä¸ä½¿ç”¨ä¸´æ—¶å˜é‡äº¤æ¢ä¸¤ä¸ªå€¼ã€‚ 1a, b = b, a ç”¨åºåˆ—æ„å»ºå­—ç¬¦ä¸²ã€‚ å¥½çš„ä»£ç ï¼š 123chars = ['j', 'a', 'c', 'k', 'f', 'r', 'u', 'e', 'd']name = ''.join(chars)print(name) # jackfrued ä¸å¥½çš„ä»£ç ï¼š 12345chars = ['j', 'a', 'c', 'k', 'f', 'r', 'u', 'e', 'd']name = ''for char in chars: name += charprint(name) # jackfrued EAFPä¼˜äºLBYLã€‚ EAFP - Easier to Ask Forgiveness than Permission. LBYL - Look Before You Leap. å¥½çš„ä»£ç ï¼š 123456d = &#123;'x': '5'&#125;try: value = int(d['x']) print(value)except (KeyError, TypeError, ValueError): value = None ä¸å¥½çš„ä»£ç ï¼š 1234567d = &#123;'x': '5'&#125;if 'x' in d and isinstance(d['x'], str) \ and d['x'].isdigit(): value = int(d['x']) print(value)else: value = None ä½¿ç”¨enumerateè¿›è¡Œè¿­ä»£ã€‚ å¥½çš„ä»£ç ï¼š 123fruits = ['orange', 'grape', 'pitaya', 'blueberry']for index, fruit in enumerate(fruits): print(index, ':', fruit) ä¸å¥½çš„ä»£ç ï¼š 12345fruits = ['orange', 'grape', 'pitaya', 'blueberry']index = 0for fruit in fruits: print(index, ':', fruit) index += 1 ç”¨ç”Ÿæˆå¼ç”Ÿæˆåˆ—è¡¨ã€‚ å¥½çš„ä»£ç ï¼š 123data = [7, 20, 3, 15, 11]result = [num * 3 for num in data if num &gt; 10]print(result) # [60, 45, 33] ä¸å¥½çš„ä»£ç ï¼š 123456data = [7, 20, 3, 15, 11]result = []for i in data: if i &gt; 10: result.append(i * 3)print(result) # [60, 45, 33] ç”¨zipç»„åˆé”®å’Œå€¼æ¥åˆ›å»ºå­—å…¸ã€‚ å¥½çš„ä»£ç ï¼š 1234keys = ['1001', '1002', '1003']values = ['éª†æ˜Š', 'ç‹å¤§é”¤', 'ç™½å…ƒèŠ³']d = dict(zip(keys, values))print(d) ä¸å¥½çš„ä»£ç ï¼š 123456keys = ['1001', '1002', '1003']values = ['éª†æ˜Š', 'ç‹å¤§é”¤', 'ç™½å…ƒèŠ³']d = &#123;&#125;for i, key in enumerate(keys): d[key] = values[i]print(d) è¯´æ˜ï¼šè¿™ç¯‡æ–‡ç« çš„å†…å®¹æ¥è‡ªäºç½‘ç»œï¼Œæœ‰å…´è¶£çš„è¯»è€…å¯ä»¥é˜…è¯»åŸæ–‡ã€‚ æ³¨ï¼šè®¸å¤šåŸåˆ™æˆ‘è®¤ä¸ºéå¸¸æœ‰æ„ä¹‰ï¼Œèƒ½å¤Ÿæ‘†è„±C/C++çš„é£æ ¼ï¼ŒçœŸæ­£å†™å‡ºPythonicçš„ä»£ç ã€‚è®©æˆ‘æœ‰å¾ˆå¤§æ„Ÿè§¦çš„æ˜¯1ã€3ã€8ï¼Œèƒ½å¤Ÿå†™å‡ºéå¸¸ç®€æ´ä¼˜é›…çš„ä»£ç ã€‚åŒæ—¶6æˆ‘ä¹‹å‰ä»æ²¡æ³¨æ„è¿‡ï¼Œä¹ æƒ¯äº†C/C++é£æ ¼ä¹‹åæ€»æ˜¯ä¼šåœ¨æ‰§è¡Œä¹‹å‰è€ƒè™‘æ‰€æœ‰æƒ…å†µï¼Œä½†ç¡®å®ä¸å¤Ÿä¼˜é›…ï¼Œä»Šåå¯ä»¥å°è¯•EAFPé£æ ¼ï¼ˆä»€ä¹ˆæ˜¯EAFPï¼‰ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>æ‚ä¸ƒæ‚å…«</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¦‚ä½•è®­ç»ƒGloVeä¸­æ–‡è¯å‘é‡]]></title>
    <url>%2F2018%2F08%2F05%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83GloVe%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F%2F</url>
    <content type="text"><![CDATA[å‡†å¤‡è¯­æ–™å‡†å¤‡å¥½è‡ªå·±çš„è¯­æ–™ï¼Œä¿å­˜ä¸ºtxtï¼Œæ¯è¡Œä¸€ä¸ªå¥å­æˆ–ä¸€æ®µè¯ï¼Œæ³¨æ„è¦åˆ†å¥½è¯ã€‚ å‡†å¤‡æºç ä»GitHubä¸‹è½½ä»£ç ï¼Œhttps://github.com/stanfordnlp/GloVeå°†è¯­æ–™corpus.txtæ”¾å…¥åˆ°Gloveçš„ä¸»æ–‡ä»¶å¤¹ä¸‹ã€‚ ä¿®æ”¹bashæ‰“å¼€demo.shï¼Œä¿®æ”¹ç›¸åº”çš„å†…å®¹ å› ä¸ºdemoé»˜è®¤æ˜¯ä¸‹è½½ç½‘ä¸Šçš„è¯­æ–™æ¥è®­ç»ƒçš„ï¼Œå› æ­¤å¦‚æœè¦è®­ç»ƒè‡ªå·±çš„è¯­æ–™ï¼Œéœ€è¦æ³¨é‡Šæ‰ ä¿®æ”¹å‚æ•°è®¾ç½®ï¼Œå°†CORPUSè®¾ç½®æˆè¯­æ–™çš„åå­— æ‰§è¡Œbashæ–‡ä»¶è¿›å…¥åˆ°ä¸»æ–‡ä»¶å¤¹ä¸‹ make bash demo.sh æ³¨æ„ï¼Œå¦‚æœè®­ç»ƒæ•°æ®è¾ƒå¤§ï¼Œåˆ™è®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œé‚£ä¹ˆå»ºè®®ä½¿ç”¨nohupæ¥è¿è¡Œç¨‹åº 1nohup bash demo.sh &gt;output.txt 2&gt;&amp;1 &amp; åç­‰è®­ç»ƒï¼Œæœ€åä¼šå¾—åˆ°vectors.txt ä»¥åŠå…¶ä»–çš„ç›¸åº”çš„æ–‡ä»¶ã€‚å¦‚æœè¦ç”¨gensimçš„word2ve loadè¿›æ¥ï¼Œé‚£ä¹ˆéœ€è¦åœ¨vectors.txtçš„ç¬¬ä¸€è¡ŒåŠ ä¸Švacob_size vector_sizeï¼Œç¬¬ä¸€ä¸ªæ•°æŒ‡æ˜ä¸€å…±æœ‰å¤šå°‘ä¸ªå‘é‡ï¼Œç¬¬äºŒä¸ªæ•°æŒ‡æ˜æ¯ä¸ªå‘é‡æœ‰å¤šå°‘ç»´ã€‚ å‚è€ƒhttps://www.cnblogs.com/echo-cheng/p/8561171.html]]></content>
      <tags>
        <tag>æ•™ç¨‹</tag>
        <tag>GloVe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†2]]></title>
    <url>%2F2018%2F08%2F05%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%862%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Pytorch]é¿å…å†™å‡ºï¼š 1x = Variable(torch.zeros(...), requires_grad=True).cuda() è€Œæ˜¯åº”è¯¥è¦ï¼š 1x = Variable(torch.zeros(...).cuda(), requires_grad=True) Reference:https://discuss.pytorch.org/t/variable-grad-is-always-none-when-extending-autograd/12187 2ï¸âƒ£[Tf-idf]æœ¬å‘¨å› ä¸ºæ¯”èµ›çš„åŸå› äº†è§£äº†ä¸€ä¸‹å„ç§æ–‡æœ¬å»ºæ¨¡çš„æ–¹æ³•ã€‚Tf-idfèƒ½å¤Ÿå–å¾—ä¸é”™çš„æˆç»©ï¼Œä½†æœ‰ä¸€å®šçš„ç¼ºé™·ã€‚ TF-IDFç”¨äºå‘é‡ç©ºé—´æ¨¡å‹ï¼Œè¿›è¡Œæ–‡æ¡£ç›¸ä¼¼åº¦è®¡ç®—æ˜¯ç›¸å½“æœ‰æ•ˆçš„ã€‚ä½†åœ¨æ–‡æœ¬åˆ†ç±»ä¸­å•çº¯ä½¿ç”¨TF-IDFæ¥åˆ¤æ–­ä¸€ä¸ªç‰¹å¾æ˜¯å¦æœ‰åŒºåˆ†åº¦æ˜¯ä¸å¤Ÿçš„ã€‚ å®ƒä»…ä»…ç»¼åˆè€ƒè™‘äº†è¯¥è¯åœ¨æ–‡æ¡£ä¸­çš„é‡è¦ç¨‹åº¦å’Œæ–‡æ¡£åŒºåˆ†åº¦ã€‚ å®ƒæ²¡æœ‰è€ƒè™‘ç‰¹å¾è¯åœ¨ç±»é—´çš„åˆ†å¸ƒã€‚ç‰¹å¾é€‰æ‹©æ‰€é€‰æ‹©çš„ç‰¹å¾åº”è¯¥åœ¨æŸç±»å‡ºç°å¤šï¼Œè€Œå…¶å®ƒç±»å‡ºç°å°‘ï¼Œå³è€ƒå¯Ÿå„ç±»çš„æ–‡æ¡£é¢‘ç‡çš„å·®å¼‚ã€‚å¦‚æœä¸€ä¸ªç‰¹å¾è¯ï¼Œåœ¨å„ä¸ªç±»é—´åˆ†å¸ƒæ¯”è¾ƒå‡åŒ€ï¼Œè¿™æ ·çš„è¯å¯¹åˆ†ç±»åŸºæœ¬æ²¡æœ‰è´¡çŒ®ï¼›ä½†æ˜¯å¦‚æœä¸€ä¸ªç‰¹å¾è¯æ¯”è¾ƒé›†ä¸­çš„åˆ†å¸ƒåœ¨æŸä¸ªç±»ä¸­ï¼Œè€Œåœ¨å…¶å®ƒç±»ä¸­å‡ ä¹ä¸å‡ºç°ï¼Œè¿™æ ·çš„è¯å´èƒ½å¤Ÿå¾ˆå¥½ä»£è¡¨è¿™ä¸ªç±»çš„ç‰¹å¾ï¼Œè€ŒTF-IDFä¸èƒ½åŒºåˆ†è¿™ä¸¤ç§æƒ…å†µã€‚ å®ƒæ²¡æœ‰è€ƒè™‘ç‰¹å¾è¯åœ¨ç±»å†…éƒ¨æ–‡æ¡£ä¸­çš„åˆ†å¸ƒæƒ…å†µã€‚åœ¨ç±»å†…éƒ¨çš„æ–‡æ¡£ä¸­ï¼Œå¦‚æœç‰¹å¾è¯å‡åŒ€åˆ†å¸ƒåœ¨å…¶ä¸­ï¼Œåˆ™è¿™ä¸ªç‰¹å¾è¯èƒ½å¤Ÿå¾ˆå¥½çš„ä»£è¡¨è¿™ä¸ªç±»çš„ç‰¹å¾ï¼Œå¦‚æœåªåœ¨å‡ ç¯‡æ–‡æ¡£ä¸­å‡ºç°ï¼Œè€Œåœ¨æ­¤ç±»çš„å…¶å®ƒæ–‡æ¡£ä¸­ä¸å‡ºç°ï¼Œæ˜¾ç„¶è¿™æ ·çš„ç‰¹å¾è¯ä¸èƒ½å¤Ÿä»£è¡¨è¿™ä¸ªç±»çš„ç‰¹å¾ã€‚ Reference:https://blog.csdn.net/mmc2015/article/details/46771791 3ï¸âƒ£[å¡æ–¹æ£€éªŒCHI]åœ¨æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œç”¨äºé€‰æ‹©æœ€ç›¸å…³çš„ç‰¹å¾ã€‚ Reference:https://blog.csdn.net/blockheadls/article/details/49977361 4ï¸âƒ£[æ–‡æœ¬åˆ†ç±»]å„ç§æ–‡æœ¬åˆ†ç±»æ–¹æ³•çš„ç®€å•ä»‹ç»ã€‚ Reference:https://github.com/wangjiang0624/Note/blob/master/MachineLearning/æ–‡æœ¬åˆ†ç±».md 5ï¸âƒ£[Python]collectionsçš„ä¸¤ä¸ªæœ‰ç”¨çš„ç±» named_tupleï¼šå¿«é€Ÿå»ºç«‹ä¸€ä¸ªç±»ï¼Œä½¿å¾—å¯ä»¥ä½¿ç”¨å±æ€§æ¥è®¿é—®è€Œéç´¢å¼•ï¼Œæé«˜äº†ä»£ç å¯è¯»æ€§ 12345from collections import namedtuplePoint = namedtuple('Point',['x','y'])p = Point(1,2)print(p.x) # 1print(p.y) # 2 Counterï¼šç»Ÿè®¡å­—ç¬¦å‡ºç°çš„æ¬¡æ•° 1234567from collections import Countercount = Counter([...]).most_commom() #ä¼šæŒ‰ç…§å‡ºç°çš„æ¬¡æ•°æ’åºï¼Œé€šå¸¸å¯ç”¨äºæ„å»ºè¯å…¸for c in count: # cæ˜¯ä¸€ä¸ªtupleï¼Œc[0]æ˜¯è¯ï¼Œc[1]æ˜¯é¢‘ç‡ if c[1]&gt;= threshold: vocab.add_word(c[0]) else: break Counterç”¨æ³•ï¼šhttps://blog.csdn.net/u014755493/article/details/69812244 6ï¸âƒ£[nohup]æœ¬å‘¨åœ¨æœåŠ¡å™¨ä¸Šè·‘ä»£ç çš„æ—¶å€™é‡åˆ°ä¸€ä¸ªé—®é¢˜ï¼Œä½¿ç”¨nohupæ‰§è¡Œpythonç¨‹åºæ—¶ï¼Œå‘ç°è¾“å‡ºæ–‡ä»¶æ²¡æœ‰æ˜¾ç¤ºã€‚ä»¥ä¸ºæ˜¯ä»£ç çš„é—®é¢˜ï¼Œä½†ç»è¿‡æ’æŸ¥å¹¶éæ˜¯ä»£ç çš„é—®é¢˜ã€‚é€šè¿‡æŸ¥é˜…èµ„æ–™ï¼Œå‘ç°é—®é¢˜æ‰€åœ¨ï¼šå› ä¸ºpythonè¾“å‡ºæœ‰ç¼“å†²ï¼Œå¯¼è‡´outputä¸èƒ½é©¬ä¸Šçœ‹åˆ°è¾“å‡ºã€‚å®é™…ä¸Šï¼Œåœ¨ç­‰å¾…äº†ä¸€æ®µæ—¶é—´åï¼Œè¾“å‡ºæ–‡ä»¶ç»ˆäºæ˜¾ç¤ºå‡ºæ¥äº†ã€‚ è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨pythonçš„å‚æ•° -u ä½¿å¾—pythonä¸å¯ç”¨ç¼“å†²ã€‚ 1nohup python -u test.py &gt; nohup.out 2&gt;&amp;1 &amp; Reference:https://blog.csdn.net/sunlylorn/article/details/19127107 7ï¸âƒ£[hexoé…ç½®] mathjaxé…ç½®: https://www.jianshu.com/p/7ab21c7f0674 é…ç½®åŸŸå:https://www.zhihu.com/question/31377141 é…ç½®sitemap:http://www.yuan-ji.me/Hexo-ä¼˜åŒ–ï¼šæäº¤sitemapåŠè§£å†³ç™¾åº¦çˆ¬è™«æŠ“å–-GitHub-Pages-é—®é¢˜/ 8ï¸âƒ£[Paper]Learning Chinese Word Representations From Glyphs Of Characters ä½¿ç”¨å›¾åƒçš„å·ç§¯æ¥ç”Ÿæˆè¯å‘é‡:]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>Paper</tag>
        <tag>Tf-idf</tag>
        <tag>æ–‡æœ¬åˆ†ç±»</tag>
        <tag>hexo</tag>
        <tag>nohup</tag>
        <tag>CHI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•3]]></title>
    <url>%2F2018%2F08%2F05%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB3%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨åªæœ‰ç®€å•çš„ä»£ç ã€‚ 1ï¸âƒ£ä½¿ç”¨gensimè®­ç»ƒword2vec12345678910111213141516#encoding=utf-8from gensim.models import word2vecsentences=word2vec.Text8Corpus(u'åˆ†è¯åçš„çˆ½è‚¤æ°´è¯„è®º.txt') #sentence:[ [ a b ],[c d]... ]model=word2vec.Word2Vec(sentences, size=50) #size:dim y2=model.similarity(u"å¥½", u"è¿˜è¡Œ") #è®¡ç®—ç›¸ä¼¼åº¦print(y2)for i in model.most_similar(u"æ»‹æ¶¦"): print i[0],i[1] #ä¿å­˜model.save('/model/word2vec_model')new_model=gensim.models.Word2Vec.load('/model/word2vec_model') 2ï¸âƒ£ä½¿ç”¨Counterå»ºç«‹è¯è¡¨123456789def build_dict(dataset,min_freq=5): dictionary=Dictionary() count=Counter(flat(dataset)).most_common() for c in count: if c[1]&gt;=min_freq: dictionary.add_word(c[0]) else: break return dictionary]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯3]]></title>
    <url>%2F2018%2F08%2F05%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D3%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨èƒŒçš„éƒ½æ˜¯æ¯”è¾ƒç®€å•çš„ã€‚ 1ï¸âƒ£ æŠŠé…’é—®æœˆ[å”] æç™½é’å¤©æœ‰æœˆæ¥å‡ æ—¶ï¼Ÿæˆ‘ä»Šåœæ¯ä¸€é—®ä¹‹ã€‚äººæ”€æ˜æœˆä¸å¯å¾—ï¼Œæœˆè¡Œå´ä¸äººç›¸éšã€‚çšå¦‚é£é•œä¸´ä¸¹é˜™ï¼Œç»¿çƒŸç­å°½æ¸…è¾‰å‘ã€‚ä½†è§å®µä»æµ·ä¸Šæ¥ï¼Œå®çŸ¥æ™“å‘äº‘é—´æ²¡ã€‚ç™½å…”æ£è¯ç§‹å¤æ˜¥ï¼Œå«¦å¨¥å­¤æ –ä¸è°é‚»ï¼Ÿä»Šäººä¸è§å¤æ—¶æœˆï¼Œä»Šæœˆæ›¾ç»ç…§å¤äººã€‚å¤äººä»Šäººè‹¥æµæ°´ï¼Œå…±çœ‹æ˜æœˆçš†å¦‚æ­¤ã€‚å”¯æ„¿å½“æ­Œå¯¹é…’æ—¶ï¼Œæœˆå…‰é•¿ç…§é‡‘æ¨½é‡Œã€‚ http://m.xichuangzhu.com/work/57b8f6f4165abd0054bf8c13 2ï¸âƒ£ é‡‘ç¼•è¡£[å”] æœç§‹å¨˜åŠå›è«æƒœé‡‘ç¼•è¡£ï¼ŒåŠå›æƒœå–å°‘å¹´æ—¶ã€‚èŠ±å¼€å ªæŠ˜ç›´é¡»æŠ˜ï¼Œè«å¾…æ— èŠ±ç©ºæŠ˜æã€‚ http://m.xichuangzhu.com/work/57b92bdca633bd00665eb99e 3ï¸âƒ£ åŒ—é’è[å”] æå•†éšæ®‹é˜³è¥¿å…¥å´¦ï¼ŒèŒ…å±‹è®¿å­¤åƒ§ã€‚è½å¶äººä½•åœ¨ï¼Œå¯’äº‘è·¯å‡ å±‚ã€‚ç‹¬æ•²åˆå¤œç£¬ï¼Œé—²å€šä¸€æè—¤ã€‚ä¸–ç•Œå¾®å°˜é‡Œï¼Œå¾å®çˆ±ä¸æ†ã€‚ å´¦ï¼ˆyÄnï¼‰ï¼šå³â€œå´¦åµ«ï¼ˆzÄ«ï¼‰â€ï¼Œå±±åï¼Œåœ¨ç”˜è‚ƒã€‚å¤æ—¶å¸¸ç”¨æ¥æŒ‡å¤ªé˜³è½å±±çš„åœ°æ–¹ã€‚ç£¬ï¼ˆqÃ¬ngï¼‰ï¼šå¤ä»£æ‰“å‡»ä¹å™¨ï¼Œå½¢çŠ¶åƒæ›²å°ºï¼Œç”¨ç‰ã€çŸ³åˆ¶æˆï¼Œå¯æ‚¬æŒ‚ã€‚ http://m.xichuangzhu.com/work/57b8ee240a2b58005c91c99e 4ï¸âƒ£ å¤æ—¥ç»å¥[å®‹] ææ¸…ç…§ç”Ÿå½“ä½œäººæ°ï¼Œæ­»äº¦ä¸ºé¬¼é›„ã€‚è‡³ä»Šæ€é¡¹ç¾½ï¼Œä¸è‚¯è¿‡æ±Ÿä¸œã€‚ http://m.xichuangzhu.com/work/57b911dac4c97100558fb30e 5ï¸âƒ£ é›¨éœ–é“ƒ[å®‹] æŸ³æ°¸å¯’è‰å‡„åˆ‡ï¼Œå¯¹é•¿äº­æ™šï¼Œéª¤é›¨åˆæ­‡ã€‚éƒ½é—¨å¸é¥®æ— ç»ªï¼Œç•™æ‹å¤„ï¼Œå…°èˆŸå‚¬å‘ã€‚æ‰§æ‰‹ç›¸çœ‹æ³ªçœ¼ï¼Œç«Ÿæ— è¯­å‡å™ã€‚å¿µå»å»ï¼Œåƒé‡ŒçƒŸæ³¢ï¼Œæš®éœ­æ²‰æ²‰æ¥šå¤©é˜”ã€‚å¤šæƒ…è‡ªå¤ä¼¤ç¦»åˆ«ï¼Œæ›´é‚£å ªã€å†·è½æ¸…ç§‹èŠ‚ã€‚ä»Šå®µé…’é†’ä½•å¤„ï¼Ÿæ¨æŸ³å²¸ï¼Œæ™“é£æ®‹æœˆã€‚æ­¤å»ç»å¹´ï¼Œåº”æ˜¯è‰¯è¾°å¥½æ™¯è™šè®¾ã€‚ä¾¿çºµæœ‰åƒç§é£æƒ…ï¼Œæ›´ä¸ä½•äººè¯´ï¼Ÿ http://m.xichuangzhu.com/work/57ad742f5bbb500062bc7c9c]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³äºPytorchä¸­gradçš„ç†è§£]]></title>
    <url>%2F2018%2F08%2F03%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADgrad%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[äº‹æƒ…èµ·æºäºæˆ‘å†™äº†ä¸€ä¸ªCNNç”¨äºæ–‡æœ¬åˆ†ç±»ï¼Œä½†lossä¸€ç›´æ²¡é™ï¼Œå› æ­¤æˆ‘å°è¯•print(loss.grad)çš„gradï¼Œå‘ç°ç¥å¥‡çš„æ˜¯loss gradæ˜¾ç¤ºä¸ºNoneï¼Œæ¥ç€å°è¯•print(y_pred.grad)ï¼ŒåŒæ ·æ˜¯Noneï¼Œä½†å†print losså’Œy_predçš„requires_gradå‘ç°æ˜¯æ­£å¸¸çš„Trueã€‚ åœ¨æŸ¥é˜…äº†èµ„æ–™ï¼Œä»¥åŠé—®äº†å­¦é•¿ä¹‹åå‘ç°åŸæ¥å¹¶ä¸æ˜¯bugï¼Œè€Œæ˜¯å› ä¸ºï¼ŒPytorché»˜è®¤ä¸ä¼šä¿å­˜ä¸­é—´èŠ‚ç‚¹(intermediate variable)çš„gradï¼Œæ­¤ä¸¾æ˜¯ä¸ºäº†èŠ‚çœå†…å­˜ã€‚ By default, gradients are only retained for leaf variables. non-leaf variablesâ€™ gradients are not retained to be inspected later. This was done by design, to save memory. https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94 å®é™…ä¸Šå¯ä»¥é€šè¿‡retain_grad()æˆ–è€…hookæ¥æŸ¥çœ‹ä¸­é—´èŠ‚ç‚¹çš„gradã€‚ æˆ‘åé¢å°è¯•printäº†å¶å­èŠ‚ç‚¹ï¼Œå¦‚ print(CNN_model.fc.weight.grad)ï¼Œæœ€ç»ˆè·å¾—äº†æ­£ç¡®çš„gradã€‚ psï¼šæ‰€è°“ä¸­é—´èŠ‚ç‚¹ï¼Œæ˜¯ç”±å…¶ä»–èŠ‚ç‚¹è®¡ç®—æ‰€å¾—çš„tensorï¼Œè€Œå¶å­èŠ‚ç‚¹åˆ™æ˜¯è‡ªå·±å®šä¹‰å‡ºæ¥çš„ã€‚ æœ€åæˆ‘å‘ç°ï¼ŒåŸæ¥lossä¸€ç›´æ²¡é™çš„åŸå› æ˜¯å› ä¸ºæˆ‘å®šä¹‰çš„CNNè¿‡äºå¤æ‚ï¼Œå¹¶ä¸”æ•°æ®é›†åå°ï¼Œæ— æ³•å¿«é€Ÿæ”¶æ•›å¯¼è‡´çš„ã€‚]]></content>
      <tags>
        <tag>é‡åˆ°çš„é—®é¢˜</tag>
        <tag>Pytorch</tag>
        <tag>grad</tag>
        <tag>bug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linuxå¸¸ç”¨å‘½ä»¤]]></title>
    <url>%2F2018%2F08%2F03%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[å‘½ä»¤è®°å½•è‡ªå·±å¸¸ç”¨çš„å‘½ä»¤ã€‚ 1ï¸âƒ£lsï¼šæ˜¾å¼å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶å’Œç›®å½• -a åŒ…æ‹¬éšè—æ–‡ä»¶ -h å°†æ–‡ä»¶çš„å®¹é‡ä»¥æ˜“è¯»æ–¹å¼åˆ—å‡ºï¼ˆé…åˆ-sä½¿ç”¨ï¼‰ -s ä»¥å—æ•°å½¢å¼æ˜¾ç¤ºæ¯ä¸ªæ–‡ä»¶åˆ†é…çš„å°ºå¯¸ -l ä»¥è¾ƒé•¿æ ¼å¼åˆ—å‡ºä¿¡æ¯ï¼Œå¯ä»¥ç›´æ¥å†™æˆ ll 2ï¸âƒ£cd åˆ°è¾¾æŒ‡å®šåœ°å€ 3ï¸âƒ£kill æ€æ­»ç¨‹åº -l ä¿¡æ¯ç¼–å·ã€‚å½“l=9æ—¶ï¼Œæ— æ¡ä»¶ç»ˆæ­¢ï¼Œå…¶ä»–ä¿¡å·å¯èƒ½å¿½ç•¥ killall -u æ€æ­»è¯¥ç”¨æˆ·å…¨éƒ¨è¿›ç¨‹ 4ï¸âƒ£ps æŠ¥å‘Šå½“å‰ç³»ç»Ÿçš„è¿›ç¨‹çŠ¶æ€ -a æ‰€æœ‰ -p æŒ‡å®šç¨‹åº -u æŒ‡å®šç”¨æˆ· -x åˆ—å‡ºè¯¥ç”¨æˆ·çš„è¿›ç¨‹çš„è¯¦ç»†ä¿¡æ¯(æˆ‘çš„ç†è§£åº”è¯¥æ˜¯) å¦‚ï¼š 5ï¸âƒ£htop æ¯”topæ›´ä¼˜ï¼Œäº¤äº’æ›´å¥½ï¼ŒåŒæ—¶å¯ä»¥ç›´è§‚çœ‹åˆ°èµ„æºå ç”¨æƒ…å†µåŸºæœ¬å‘½ä»¤ä¸topä¸€è‡´ 6ï¸âƒ£topï¼šåŠ¨æ€æŸ¥çœ‹ç³»ç»Ÿè¿è¡ŒçŠ¶æ€ -u æŒ‡å®šç”¨æˆ·å -p æŒ‡å®šè¿›ç¨‹ 7ï¸âƒ£nvidia-smi æŸ¥çœ‹æ˜¾å¡çŠ¶æ€watch nvidia-smi å®æ—¶æŸ¥çœ‹æ˜¾å¡çŠ¶æ€ï¼Œå®šæ—¶åˆ·æ–° 8ï¸âƒ£tail æ˜¾ç¤ºæŒ‡å®šæ–‡ä»¶çš„æœ«å°¾è‹¥å¹²è¡Œ -f æ˜¾ç¤ºæ–‡ä»¶æœ€æ–°è¿½åŠ çš„å†…å®¹ -n æ˜¾ç¤ºæ–‡ä»¶å°¾éƒ¨nè¡Œå†…å®¹ -c æ˜¾ç¤ºæ–‡ä»¶å°¾éƒ¨æœ€åcä¸ªå­—ç¬¦ å¦‚ï¼š 123tail file æ˜¾ç¤ºæœ€å10è¡Œtail -n +20 file æ˜¾ç¤ºä»ç¬¬20è¡Œè‡³æœ«å°¾tail -c 10 file æ˜¾ç¤ºæ–‡ä»¶fileçš„æœ€å10ä¸ªå­—ç¬¦ ------- 9ï¸âƒ£echo ç”¨äºæ‰“å°æŒ‡å®šçš„å­—ç¬¦ä¸² ğŸ”Ÿwhich ç”¨äºæŸ¥æ‰¾å¹¶æ˜¾ç¤ºç»™å®šå‘½ä»¤çš„ç»å¯¹è·¯å¾„ï¼ŒwhichæŒ‡ä»¤ä¼šåœ¨ç¯å¢ƒå˜é‡$PATHè®¾ç½®çš„ç›®å½•é‡ŒæŸ¥æ‰¾ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶ã€‚ä½¿ç”¨whichå‘½ä»¤ï¼Œå¯ä»¥çœ‹åˆ°æŸä¸ªç³»ç»Ÿå‘½ä»¤æ˜¯å¦å­˜åœ¨ï¼Œä»¥åŠæ‰§è¡Œçš„æ˜¯å“ªä¸ªä½ç½®çš„å‘½ä»¤ã€‚å¦‚ï¼š 1ï¸âƒ£1ï¸âƒ£nohup å°†ç¨‹åºä»¥å¿½ç•¥æŒ‚èµ·ä¿¡å·çš„æ–¹å¼è¿è¡Œï¼Œç»å¸¸ç”¨äºåœ¨æœåŠ¡å™¨è·‘ä»£ç å¦‚ï¼š1nohup python xxx.py &gt;output.txt 2&gt;&amp;1 &amp; å³ï¼Œå°†è¾“å‡ºé‡å®šå‘åˆ°output.txt ï¼›æœ€åä¸€ä¸ª&amp;è¡¨ç¤ºåå°æŒ‚èµ· 1ï¸âƒ£2ï¸âƒ£cp å¤åˆ¶æ–‡ä»¶ cp [æ–‡ä»¶] [ç›®æ ‡æ–‡ä»¶å¤¹] -r é€’å½’å¤åˆ¶ï¼Œç”¨äºç›®å½•çš„å¤åˆ¶ 1ï¸âƒ£3ï¸âƒ£mv ç§»åŠ¨æ–‡ä»¶ã€ç›®å½•æˆ–æ›´å mv [æ–‡ä»¶/æ–‡ä»¶å¤¹] [æ–‡ä»¶å¤¹] -f å¼ºåˆ¶ï¼Œå½“ç›®æ ‡æ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥è¦†ç›– -i ä¼šè¯¢é—® 1ï¸âƒ£4ï¸âƒ£rm åˆ é™¤æ–‡ä»¶æˆ–ç›®å½• -f å¼ºåˆ¶åˆ é™¤ -r é€’å½’åˆ é™¤ï¼Œç”¨äºç›®å½•åˆ é™¤ 1ï¸âƒ£5ï¸âƒ£file ç”¨äºåˆ¤æ–­æ–‡ä»¶çš„åŸºæœ¬æ•°æ®å¦‚ï¼š 1ï¸âƒ£6ï¸âƒ£tar å¯¹æ–‡ä»¶æ‰“åŒ…/å‹ç¼© -t æŸ¥çœ‹æ‰“åŒ…æ–‡ä»¶çš„å†…å®¹å«æœ‰å“ªäº›æ–‡ä»¶å -x è§£å‹ç¼© -c æ–°å»ºæ‰“åŒ…æ–‡ä»¶ -C æŒ‡å®šå‹ç¼©/è§£å‹ç›®å½• -v è§£å‹/å‹ç¼©è¿‡ç¨‹ä¸­å°†å¤„ç†çš„æ–‡ä»¶åæ˜¾ç¤ºå‡ºæ¥å¸¸ç”¨çš„ï¼š 123å‹ç¼©ï¼štar -jcv -f filename.tar.bz2 è¦è¢«å¤„ç†çš„æ–‡ä»¶æˆ–ç›®å½•åç§°æŸ¥è¯¢ï¼štar -jtv -f filename.tar.bz2è§£å‹ï¼štar -jxv -f filename.tar.bz2 -C æ¬²è§£å‹ç¼©çš„ç›®å½• 1ï¸âƒ£7ï¸âƒ£wc word count ç»Ÿè®¡æ–‡ä»¶å†…å®¹ä¿¡æ¯ï¼Œå¦‚è¡Œæ•°ã€å­—ç¬¦æ•° -l æ˜¾ç¤ºæ–‡ä»¶è¡Œæ•° -c æ˜¾ç¤ºå­—èŠ‚æ•° -m æ˜¾ç¤ºå­—ç¬¦æ•° -w æ˜¾ç¤ºå­—æ•° å­—è¢«å®šä¹‰ä¸ºç”±ç©ºç™½ã€è·³æ ¼ã€æ¢è¡Œå­—ç¬¦åˆ†éš”çš„å­—ç¬¦ä¸² -L æ˜¾ç¤ºæœ€é•¿è¡Œçš„é•¿åº¦ ä¸åŠ å‚æ•°ï¼Œæ‰€æœ‰çš„éƒ½æ˜¾ç¤ºï¼Œä¾æ¬¡æ˜¯è¡Œæ•°ã€å•è¯æ•°ã€å­—èŠ‚æ•°ã€æ–‡ä»¶å 1ï¸âƒ£8ï¸âƒ£df æ˜¾ç¤ºç£ç›˜ç›¸å…³ä¿¡æ¯ -h ä»¥å¯è¯»æ€§è¾ƒé«˜çš„æ–¹å¼æ˜¾ç¤ºä¿¡æ¯ 1ï¸âƒ£9ï¸âƒ£scp æœåŠ¡å™¨ä¹‹é—´çš„æ–‡ä»¶å¤åˆ¶ å¦‚: 1scp -r /test1 zhlin@123.12.1.12:/home/zhlin âœ¨å¿«æ·é”®Ctrl+a è·³åˆ°è¡Œé¦–Ctrl+c é€€å‡ºå½“å‰è¿›ç¨‹Ctrl+e è·³åˆ°é¡µå°¾Ctrl+k åˆ é™¤å½“å‰å…‰æ ‡åé¢çš„æ–‡å­—Ctrl+l æ¸…å±ï¼Œç­‰ä»·äºclearCtrl+r æœç´¢ä¹‹å‰æ‰“è¿‡çš„å‘½ä»¤Ctrl+u åˆ é™¤å½“å‰å…‰æ ‡å‰é¢çš„æ–‡å­—âœ¨Ctrl+å·¦å³é”® å•è¯ä¹‹é—´è·³è½¬ åœ¨Macä¸Šå¯ä»¥ä½¿ç”¨option+å·¦å³é”®Ctrl+y è¿›è¡Œæ¢å¤åˆ é™¤Ctrl+z å°†å½“å‰è¿›ç¨‹è½¬åˆ°åå°ï¼Œä½¿ç”¨fgæ¢å¤ Referencehttps://blog.csdn.net/leo_618/article/details/53003111 â€”â€”â€”-æŒç»­æ›´æ–°â€”â€”â€”-]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>æŠ€å·§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä¸€ä¸ªå…³äºyieldçš„é‡æ–°è®¤è¯†]]></title>
    <url>%2F2018%2F07%2F31%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E4%B8%80%E4%B8%AA%E5%85%B3%E4%BA%8Eyield%E7%9A%84%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86%2F</url>
    <content type="text"><![CDATA[ä»Šå¤©é‡åˆ°äº†ä¸€ä¸ªç¥å¥‡çš„â€bugâ€ï¼Œè®©æˆ‘å¯¹yieldçš„ç†è§£æ›´æ·±ä¸€æ­¥ã€‚ è¿™æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œæˆ‘æœ¬æ¥æ‰“ç®—è¯•ç€printä¸€ä¸‹lineå†…éƒ¨çš„æ ¼å¼å’Œå†…å®¹ã€‚ è¿™æ˜¯è°ƒç”¨çš„ä¸»å‡½æ•°ï¼š ç»“æœè·‘å‡ºçš„ç»“æœæ˜¯ï¼š ï¼Ÿï¼Ÿï¼Ÿ æˆ‘å°è¯•åœ¨å‡½æ•°çš„å¼€å¤´æ·»åŠ printï¼š ç»“æœä»ç„¶æ²¡æœ‰ä»»ä½•çš„è¾“å‡ºã€‚ æˆ‘è¯•ç€åœ¨mainå‡½æ•°æ·»åŠ printï¼š ç»“æœï¼š ä¹Ÿå°±æ˜¯è¯´ï¼Œæ ¹æœ¬æ²¡æœ‰è¿›å…¥åˆ°get_dataset_from_txtå‡½æ•°å•Šã€‚ æˆ‘ä»¥ä¸ºæ˜¯pycharmçš„é—®é¢˜è¿˜é‡å¯äº†ä¸€éï¼Œç„¶è€Œå¹¶æ²¡æœ‰ä»»ä½•ä½œç”¨ã€‚é—®äº†å…¶ä»–äººï¼Œä»–ä»¬ä¹Ÿè§‰å¾—å¾ˆç¥å¥‡ã€‚æœ€åä¸€ä¸ªåŒå­¦çœ‹äº†ä¸€ä¸‹å‡½æ•°ï¼Œå‘ç°äº†é—®é¢˜æ‰€åœ¨ï¼šyield æˆ‘çªç„¶æƒ³èµ·æ¥ï¼Œyieldè¿”å›çš„æ˜¯ä¸€ä¸ªgeneratorï¼Œåªæœ‰åœ¨å¯¹generatorè¿›è¡Œéå†æ—¶ï¼Œæ‰ä¼šå¼€å§‹è¿è¡Œâ€¦ äºæ˜¯ï¼Œæˆ‘è¯•ç€è¿™ä¹ˆå†™ï¼Œè¯•ç€å¯¹generatoréå†ï¼š è™½ç„¶æŠ¥é”™äº†ï¼Œä½†å‡½æ•°ç»ˆäºæ˜¯è¿›å»äº†â€¦ ç»“è®ºï¼šæœ‰yieldçš„å‡½æ•°ä¼šè¿”å›ä¸€ä¸ªgeneratorï¼Œå½“å¯¹å…¶è¿›è¡Œéå†æ—¶ï¼Œå‡½æ•°æ‰ä¼šå¼€å§‹è¿è¡Œã€‚]]></content>
      <tags>
        <tag>é‡åˆ°çš„é—®é¢˜</tag>
        <tag>Python</tag>
        <tag>yield</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•2]]></title>
    <url>%2F2018%2F07%2F29%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB2%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨ä¸»è¦çœ‹äº†AllenNLP/ELMOçš„ä»£ç ï¼Œä½†å¹¶æ²¡æœ‰æ‰¾åˆ°å¾ˆå¤šå¯å¤ç”¨çš„ä»£ç ã€‚æœ¬å‘¨ä¹Ÿæ²¡æœ‰æ¯”è¾ƒæœ‰æ„ä¹‰çš„ä»£ç ã€‚ 1ï¸âƒ£get_time_diffè·å–å·²ä½¿ç”¨çš„æ—¶é—´123456789import timefrom datetime import timedeltastart_time=time.time()def get_time_dif(start_time): """è·å–å·²ä½¿ç”¨æ—¶é—´""" end_time = time.time() time_dif = end_time - start_time return timedelta(seconds=int(round(time_dif))) 2ï¸âƒ£parserä½¿ç”¨12345678parser = argparse.ArgumentParser()parser.add_argument('--save_dir', help='Location of checkpoint files')parser.add_argument('--vocab_file', help='Vocabulary file')parser.add_argument('--train_prefix', help='Prefix for train files')args = parser.parse_args() main(args) #ä½¿ç”¨]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†1]]></title>
    <url>%2F2018%2F07%2F29%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%861%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Python]assertç”¨æ³•ï¼š assert expressionç­‰ä»·äºif not expression: raise AssertionError 2ï¸âƒ£[Pytorch]Pytorch viewï¼šåˆ›å»ºä¸€ä¸ªæ–°çš„tensorï¼Œä½†ä»–ä»¬çš„dataæ˜¯å…±äº«çš„ã€‚ 3ï¸âƒ£[Pytorch]åœ¨Pytorchä¸­ï¼Œembeddingçš„indexæ˜¯ä¸èƒ½requires_grad=Trueçš„ï¼Œå¦åˆ™ä¼šå‡ºé”™ã€‚https://github.com/pytorch/pytorch/issues/7021 ä¹‹å‰çœ‹è¿‡ä¸€ä»½ä»£ç ï¼Œè®¾ç½®volatile=falseä½†æ²¡æœ‰å‡ºé”™ï¼Œæ˜¯å› ä¸ºåœ¨Pytorch0.4ä¹‹åvolatileå·²ç»è¢«å¼ƒç”¨äº†ï¼Œå› æ­¤volatile=falseä¸èµ·ä½œç”¨ï¼Œè€Œé»˜è®¤requires_grad=false 4ï¸âƒ£[Pytorch]åœ¨Pytorchä¸­ï¼Œnn.Linear(self.hidden_dim,self.vocab_size)çš„ç»´åº¦æ˜¯vocab_sizehidden_dimï¼Œä¹‹å‰å±…ç„¶æ²¡æœ‰æ³¨æ„åˆ°è¿™ä¸ªé—®é¢˜ã€‚å› ä¸ºnn.Linearçš„*ç¬¬ä¸€ä¸ªå‚æ•°è¡¨ç¤ºè¾“å…¥ç»´åº¦ï¼Œç¬¬äºŒä¸ªå‚æ•°è¡¨ç¤ºè¾“å‡ºç»´åº¦ 5ï¸âƒ£[Pytorch]Pytorchä¸­ï¼Œä½¿ç”¨viewä¸€èˆ¬æ¥è¯´å¿…é¡»è¦ç”¨ .contiguous()ã€‚ä¹Ÿå³ï¼š 1batch.view(batch_size, -1).t().contiguous() contiguous()çš„å®˜æ–¹è§£é‡Šï¼šhttps://discuss.pytorch.org/t/runtimeerror-input-is-not-contiguous/930 It means that your tensor is not a single block of memory, but a block with holes. view can be only used with contiguous tensors, so if you need to use it here, just call .contiguous() before. ä¹Ÿå°±æ˜¯è¯´ï¼Œcontiguousä¼šå°†æ•°æ®å­˜åˆ°ä¸€ä¸ªè¿ç»­çš„ç©ºé—´å†…ï¼ˆblockï¼‰ã€‚ 6ï¸âƒ£[Pytorch]è°ƒç”¨Cross_entropyæ—¶ï¼ŒPytorchä¼šå¸®åŠ©ä½ åŠ logå’Œsoftmaxã€‚ 7ï¸âƒ£[Paper]Sliced_RNN å°†RNNåˆ†å—ä»¥æé«˜å¹¶è¡Œæ€§ï¼Œç”šè‡³æ¯å±‚çš„RNNéƒ½å¯ä»¥ä¸ä¸€æ ·ï¼Œè¾¾åˆ°æŠ½å–ä¸åŒç¨‹åº¦çš„æŠ½è±¡è¯­ä¹‰ä¿¡æ¯çš„ç›®çš„ã€‚å®éªŒè¯æ˜ï¼Œåœ¨ä¸åŒä»»åŠ¡ä¸Šéƒ½æœ‰ä¸€å®šçš„æå‡ï¼Œä½†é€Ÿåº¦çš„æå‡å¾ˆå¤§ã€‚ 8ï¸âƒ£[Tf-idf]è®¡ç®—è¯è¯­å¯¹äºå¥å­çš„é‡è¦ç¨‹åº¦ https://zh.wikipedia.org/wiki/Tf-idf tfæ˜¯è¯é¢‘ï¼Œidfæ˜¯é€†å‘æ–‡ä»¶é¢‘ç‡ã€‚ä¹Ÿå³å¦‚æœè¯åœ¨è¯¥å¥å‡ºç°çš„æ¬¡æ•°è¶Šå¤šï¼Œåœ¨æ‰€æœ‰æ–‡æœ¬çš„å‡ºç°æ¬¡æ•°è¶Šå°‘ï¼Œåˆ™è¯å¯¹äºå¥å­çš„é‡è¦ç¨‹åº¦è¶Šé«˜ã€‚ 9ï¸âƒ£[Numpy]åœ¨Numpyä¸­ï¼Œä¸€ä¸ªåˆ—è¡¨è™½ç„¶æ˜¯æ¨ªç€è¡¨ç¤ºçš„ï¼Œä½†å®ƒæ˜¯åˆ—å‘é‡ã€‚æˆ‘ä¹‹å‰å±…ç„¶æ²¡æœ‰æ³¨æ„åˆ°è¿™ä¸ªé—®é¢˜ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Python</tag>
        <tag>Pytorch</tag>
        <tag>Paper</tag>
        <tag>Tf-idf</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Macé…ç½®å¤æ—¦æœ‰çº¿ç½‘]]></title>
    <url>%2F2018%2F07%2F29%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2FMac%E9%85%8D%E7%BD%AE%E5%A4%8D%E6%97%A6%E6%9C%89%E7%BA%BF%E7%BD%91%2F</url>
    <content type="text"><![CDATA[é…ç½®ipã€å­ç½‘æ©ç ã€DNSã€è·¯ç”±å™¨æœ‰çº¿ä¼¼ä¹ä¸æ”¯æŒDHCPï¼Œå› æ­¤åªå¥½è‡ªå·±è®¾ç½®ã€‚é¦–å…ˆè¿æ¥ä¸Šæœ‰çº¿ï¼Œå°†é…ç½®iPv4é€‰ä¸ºæ‰‹åŠ¨ã€‚é—®å®éªŒå®¤çš„å­¦é•¿å…·ä½“çš„ipåœ°å€ã€å­ç½‘æ©ç ã€è·¯ç”±å™¨ã€DNSæœåŠ¡å™¨ã€‚å…¶ä¸­ipåœ°å€æœ€åä¸‰ä½è¦è‡ªå·±è®¾å®šï¼Œåªè¦ä¸å’Œå…¶ä»–äººå†²çªå°±å¥½ã€‚ æ‰‹åŠ¨è®¤è¯åˆ°è®¤è¯å¹³å°ï¼Œä¸‹è½½Macå®¢æˆ·ç«¯ï¼Œå…¶å®å°±æ˜¯ä¸€ä¸ª.shæ–‡ä»¶ï¼šhttp://10.108.255.249/srun_portal_pc.php?ac_id=1&amp;&amp;phone=1 ç„¶åï¼Œæ‰“å¼€æ–‡ä»¶é…ç½®ç”¨æˆ·åå¯†ç ï¼Œæ³¨æ„åˆ°ç­‰å·åé¢è¦æœ‰åŒå¼•å·ï¼š ä¿å­˜å¹¶æ”¾å…¥ç»ˆç«¯è¿è¡Œï¼Œæ¥ä¸‹æ¥å°±å¯ä»¥ä½¿ç”¨æœ‰çº¿ç½‘äº†ã€‚ å…¶ä»–ä¼¼ä¹ï¼Œæ¯æ¬¡é‡æ–°è¿æ¥éƒ½è¦è¿™æ ·é…ç½®ï¼Œæˆ‘æ²¡æœ‰è¯•è¿‡ä¸æ¸…æ¥šï¼›æœ‰çº¿ç½‘å¥½åƒä¹Ÿæ²¡æœ‰æ¯”æ— çº¿ç½‘å¿«å¤šå°‘ï¼Œä½†åº”è¯¥ä¼šç¨³å®šä¸€äº›ã€‚]]></content>
      <tags>
        <tag>ç½‘ç»œ</tag>
        <tag>é…ç½®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sshå¿«é€Ÿç™»å½•é…ç½®]]></title>
    <url>%2F2018%2F07%2F29%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2Fssh%E5%BF%AB%E9%80%9F%E7%99%BB%E5%BD%95%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Motivationåˆ†é…äº†æœåŠ¡å™¨ä¹‹åï¼Œæ¯æ¬¡è¦sshè¿›å…¥éƒ½å¾ˆéº»çƒ¦ï¼šssh user_name@ip_address ç„¶åè¿˜è¦è¾“å…¥å¯†ç ã€‚ ç‰¹åˆ«æ˜¯å¦‚æœåˆ†é…äº†å¤šä¸ªæœåŠ¡å™¨ï¼Œé‚£æœ‰æ—¶å€™è¿˜å®¹æ˜“å¿˜è®°ipåœ°å€ã€‚å› æ­¤å¦‚æœèƒ½å¤Ÿä¸€æ¡å‘½ä»¤å°±è¿›å…¥æœåŠ¡å™¨èƒ½å¤Ÿå‡å°‘éº»çƒ¦ã€‚ä¸»è¦æœ‰ä¸‰ç‚¹ï¼š åˆ›å»ºrsa key ä¸Šä¼ public keyåˆ°æœåŠ¡å™¨ è®¾ç½®alias é…ç½®åˆ›å»ºrsa keyåœ¨ç»ˆç«¯è¾“å…¥å‘½ä»¤ï¼š 1ssh-keygen -t rsa å½“ç„¶å¦‚æœä»¥å‰æœ‰åˆ›å»ºè¿‡çš„å¯ä»¥ä¸ç”¨ã€‚ ç»“æœï¼š ä¸Šä¼ public keyåˆ°æœåŠ¡å™¨ä½¿ç”¨å‘½ä»¤ï¼š1ssh-copy-id -i ~/.ssh/id_rsa.pub user@127.0.0.1 è¾“å…¥å¯†ç å³å¯ ç»“æœï¼š è®¾ç½®aliaså®Œæˆä»¥ä¸Šæ­¥éª¤å°±å¯ä»¥ä¸è¾“å…¥å¯†ç ç™»å½•ï¼Œä½†è¿˜æ˜¯éœ€è¦è¾“å…¥ipåœ°å€å’Œç”¨æˆ·åï¼Œä¸ºäº†æ›´ç®€åŒ–æ“ä½œï¼Œç»™å‘½ä»¤èµ·ä¸ªåˆ«åã€‚éœ€è¦é…ç½® .bash_profileæ–‡ä»¶ã€‚è¾“å…¥å‘½ä»¤: 1vim ~/.bash_profile åœ¨æ–‡ä»¶åé¢æ·»åŠ ä»¥ä¸‹æ–‡å­—ï¼š 123# alias alias sshÃ—Ã—Ã—=&quot;ssh user_name@ip_address&quot;alias sshÃ—Ã—Ã—=&quot;ssh user_name@ip_address&quot; å…¶ä¸­ Ã—Ã—Ã—æ˜¯ä½ è‡ªå·±èµ·çš„åå­—ï¼Œå¯ä»¥æ˜¯æœåŠ¡å™¨çš„åå­—ï¼Œuser_nameå’Œip_addressæ˜¯è‡ªå·±æœåŠ¡å™¨çš„ç”¨æˆ·åå’Œåœ°å€ã€‚ä¿å­˜æ›´æ”¹é€€å‡ºã€‚ ç„¶åè¿˜è¦ä½¿å…¶ç”Ÿæ•ˆ: 1source ~/.bash_profile è¿™æ ·ï¼Œè¾“å…¥åˆ«åï¼Œå°±å¯ä»¥ç›´æ¥ç™»å½•äº†ï¼š å‚è€ƒhttps://www.jianshu.com/p/66d658c7cb9e]]></content>
      <tags>
        <tag>é…ç½®</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³äºå›°æƒ‘åº¦]]></title>
    <url>%2F2018%2F07%2F29%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8E%E5%9B%B0%E6%83%91%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[å‰å‡ å¤©åœ¨å†™æ–°æ‰‹ä»»åŠ¡task3çš„æ—¶å€™ï¼Œå‚è€ƒäº†Pytorchå®˜æ–¹exampleçš„word language modelï¼Œå®˜æ–¹exampleåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—å›°æƒ‘åº¦æ˜¯è¿™æ ·çš„ï¼š 1math.exp(cur_loss) å…¶ä¸­ï¼Œcur_lossè¡¨ç¤ºäº¤å‰ç†µçš„lossï¼Œå³ $-P(\hat{x})logP(x)$ï¼Œ$\hat{x}$è¡¨ç¤ºground truthã€‚ ç„¶è€Œï¼Œåœ¨æŸ¥é˜…äº†å›°æƒ‘åº¦ç›¸å…³èµ„æ–™åï¼Œæˆ‘å‘ç°ï¼Œå›°æƒ‘åº¦çš„å®šä¹‰æ˜¯è¿™æ ·çš„ï¼š \begin{aligned} PP(S)= &{P(w_{1}w_{2}...w_{N})}^{-\frac{1}{N}} \\ = &\sqrt[N]{\frac{1}{p(w_1 w_2 ... w_N)}} \\ = & \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} } \end{aligned}è¿™æ˜¯å¦ä¸€ç§å½¢å¼: \begin{aligned} Perplexity (W)=& 2^{H(W)} \\ = & {P(w_{1}w_{2}...w_{N})}^{-\frac{1}{N}} \\ = & \sqrt[N]{\frac{1}{p(w_1 w_2 ... w_N)}} \\ = & \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} } \end{aligned}å¯ä»¥çœ‹åˆ°ï¼ŒäºŒè€…æœ¬è´¨æ˜¯ä¸€æ ·çš„ã€‚ é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆåœ¨ä»£ç ä¸­ä»¥eä¸ºåº•å»è®¡ç®—å›°æƒ‘åº¦ï¼Œè€Œä¸æ˜¯2å‘¢? å®é™…ä¸Šï¼Œæ˜¯å› ä¸ºåœ¨ä¸Šè¿°å…¬å¼ä¸­ï¼Œlogæ˜¯ä»¥2ä¸ºåº•çš„ï¼Œä½†åœ¨Pytorchä¸­ï¼Œlogé»˜è®¤æ˜¯ä»¥eä¸ºåº•çš„ã€‚å› æ­¤åœ¨ä»£ç ä¸­ï¼Œéœ€è¦ç”¨eä½œä¸ºæŒ‡æ•°çš„åº•æ¥è¿˜åŸæˆå›°æƒ‘åº¦çš„åŸæœ¬å½¢å¼ï¼š \begin{aligned} \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} } \end{aligned}æœ€åè¿™æ˜¯perplexityçš„æ•°å­¦æ¨å¯¼ï¼šhttps://www.zhihu.com/question/58482430]]></content>
      <tags>
        <tag>å›°æƒ‘åº¦</tag>
        <tag>perplexity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯2]]></title>
    <url>%2F2018%2F07%2F29%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D2%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨çš„è¯—è¯æœ‰ä¸¤ç¯‡æ˜¯å·²ç»èƒŒè¿‡çš„ï¼Œæƒå½“æ˜¯å¤ä¹ äº†ä¸€éã€‚ 1ï¸âƒ£ ä¸‹ç»ˆå—å±±è¿‡æ–›æ–¯å±±äººå®¿ç½®é…’[å”] æç™½æš®ä»ç¢§å±±ä¸‹ï¼Œå±±æœˆéšäººå½’ã€‚å´é¡¾æ‰€æ¥å¾„ï¼Œè‹è‹æ¨ªç¿ å¾®ã€‚ç›¸æºåŠç”°å®¶ï¼Œç«¥ç¨šå¼€è†æ‰‰ã€‚ç»¿ç«¹å…¥å¹½å¾„ï¼Œé’èæ‹‚è¡Œè¡£ã€‚æ¬¢è¨€å¾—æ‰€æ†©ï¼Œç¾é…’èŠå…±æŒ¥ã€‚é•¿æ­ŒåŸæ¾é£ï¼Œæ›²å°½æ²³æ˜Ÿç¨€ã€‚æˆ‘é†‰å›å¤ä¹ï¼Œé™¶ç„¶å…±å¿˜æœºã€‚ http://m.xichuangzhu.com/work/57b900307db2a20054269a2a 2ï¸âƒ£ é€¢å…¥äº¬ä½¿[å”] å²‘å‚æ•…å›­ä¸œæœ›è·¯æ¼«æ¼«ï¼ŒåŒè¢–é¾™é’Ÿæ³ªä¸ä¹¾ã€‚é©¬ä¸Šç›¸é€¢æ— çº¸ç¬”ï¼Œå‡­å›ä¼ è¯­æŠ¥å¹³å®‰ã€‚ http://m.xichuangzhu.com/work/57b92218df0eea006335f923 3ï¸âƒ£ å¿µå¥´å¨‡Â·èµ¤å£æ€€å¤[å®‹] è‹è½¼å¤§æ±Ÿä¸œå»ï¼Œæµªæ·˜å°½ã€åƒå¤é£æµäººç‰©ã€‚æ•…å’è¥¿è¾¹ï¼Œäººé“æ˜¯ã€ä¸‰å›½å‘¨éƒèµ¤å£ã€‚ä¹±çŸ³ç©¿ç©ºï¼ŒæƒŠæ¶›æ‹å²¸ï¼Œå·èµ·åƒå †é›ªã€‚æ±Ÿå±±å¦‚ç”»ï¼Œä¸€æ—¶å¤šå°‘è±ªæ°ã€‚é¥æƒ³å…¬ç‘¾å½“å¹´ï¼Œå°ä¹”åˆå«äº†ï¼Œé›„å§¿è‹±å‘ã€‚ç¾½æ‰‡çº¶å·¾ï¼Œè°ˆç¬‘é—´ï¼Œæ¨¯æ©¹ç°é£çƒŸç­ã€‚æ•…å›½ç¥æ¸¸ï¼Œå¤šæƒ…åº”ç¬‘æˆ‘ï¼Œæ—©ç”Ÿåå‘ã€‚äººç”Ÿå¦‚æ¢¦ï¼Œä¸€å°Šè¿˜é…¹æ±Ÿæœˆã€‚ http://m.xichuangzhu.com/work/57b8bda2df0eea006333ecd2]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•1]]></title>
    <url>%2F2018%2F07%2F23%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB1%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£ get_batchæ³¨æ„åˆ°shuffleçš„æ ‡å‡†åšæ³• 123456789101112def get_batch(self,data,batch_size=32,is_shuffle): N=len(data) #è·å¾—æ•°æ®çš„é•¿åº¦ if is_shuffle is True: r=random.Random() r.seed() r.shuffle(data) #å¦‚æœis_shuffleä¸ºçœŸåˆ™æ‰“ä¹± #å¼€å§‹è·å¾—batchï¼Œä½¿ç”¨[ for in ] batch=[data[k:k+batch_size] for k in range(0,N,batch_size)] if N%batch_size!=0: #å¤„ç†ä¸æ•´é™¤é—®é¢˜ï¼Œå¦‚æœæœ‰æ˜¾å¼è¦æ±‚ä¸¢æ‰åˆ™ä¸éœ€è¦å¤„ç†ï¼Œè¿™é‡Œé»˜è®¤å¤„ç† remainder=N-N%batch_size #å‰©ä¸‹çš„éƒ¨åˆ† batch.append(data[temp:N]) return batch 2ï¸âƒ£ä½¿ç”¨gensimå°†GloVeè¯»å…¥å®é™…ä¸Šè¿™ä»½ä»£ç æœ‰ç‚¹é—®é¢˜ï¼Œåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œå‘ç°gloveæ–‡ä»¶éœ€è¦æ”¾åœ¨gensimçš„æ–‡ä»¶å¤¹ä¸‹æ‰èƒ½è¢«è¯»åˆ°(7.20 updated,åº”è¯¥ä½¿ç”¨ç»å¯¹åœ°å€)ï¼Œå¹¶ä¸å¥½ã€‚ æ•™ç¨‹åœ°å€ï¼šgensim: scripts.glove2word2vec â€“ Convert glove format to word2vec123456789101112131415161718#1. ä½¿ç”¨gensimè¯»å…¥word2vecmodel = gensim.models.KeyedVectors.load_word2vec_format( fname='GoogleNews-vectors-negative300-SLIM.bin', binary=True)words = model.vocab #è·å¾—è¯è¡¨vector= model[word] #wordæ˜¯wordsé‡Œé¢çš„å…ƒç´ #2. ä½¿ç”¨gensimè¯»å…¥glovefrom gensim.models import KeyedVectorsfrom gensim.test.utils import datapath, get_tmpfilefrom gensim.scripts.glove2word2vec import glove2word2vecglove_file=datapath('glove.txt') #æœ€å¥½ä½¿ç”¨ç»å¯¹åœ°å€tmp_file=get_tmpfile('word2vec.txt')glove2word2vec(glove_file,tmp_file)model=KeyedVectors.load_word2vec_format(tmp_file)#æ¥ä¸‹æ¥ä½¿ç”¨çš„æ–¹æ³•æ˜¯ä¸€æ ·çš„ 3ï¸âƒ£data_splitæ–¹æ³•1234567891011121314151617181920def data_split(seed=1, proportion=0.7): data = list(iter_corpus()) ids = list(range(len(data))) N = int(len(ids) * proportion) # number of training data rng = random.Random(seed) rng.shuffle(ids) test_ids = set(ids[N:]) train_data = [] test_data = [] for x in data: if x[1] in test_ids: # x[1]: sentence id test_data.append(x) else: train_data.append(x) return train_data, test_data 4ï¸âƒ£å¯¹stringé¢„å¤„ç†123456789101112131415161718def clean_str(string): string = re.sub(r"[^A-Za-z0-9()!?\'\`]", "", string) string = re.sub(r"\'s", " \'s", string) string = re.sub(r"\'m", " \'m", string) string = re.sub(r"\'ve", " \'ve", string) string = re.sub(r"n\'t", " n\'t", string) string = re.sub(r"\'re", " \'re", string) string = re.sub(r"\'d", " \'d", string) string = re.sub(r"\'ll", " \'ll", string) string = re.sub(r",", " , ", string) string = re.sub(r"!", " ! ", string) string = re.sub(r"\(", " \( ", string) string = re.sub(r"\)", " \) ", string) string = re.sub(r"\?", " \? ", string) string = re.sub(r"\s&#123;2,&#125;", " ", string) string = re.sub(r"\@.*?[\s\n]", "", string) string = re.sub(r"https*://.+[\s]", "", string) return string.strip().lower() 5ï¸âƒ£collate_fn(batchï¼‰é‡å†™collate_fnç»„å»ºmini-batchï¼Œåœ¨NLPä¸­å¸¸ç”¨ï¼Œå¥å­çš„ä¸ç­‰é•¿æ€§123456789101112131415161718192021222324252627def collate_fn(batch): # rewrite collate_fn to form a mini-batch lengths = np.array([len(data['sentence']) for data in batch]) sorted_index = np.argsort(-lengths) lengths = lengths[sorted_index] # descend order max_length = lengths[0] batch_size = len(batch) sentence_tensor = torch.LongTensor(batch_size, int(max_length)).zero_() for i, index in enumerate(sorted_index): sentence_tensor[i][:lengths[i]] = torch.LongTensor(batch[index]['sentence'][:max_length]) sentiments = torch.autograd.Variable(torch.LongTensor([batch[i]['sentiment'] for i in sorted_index])) if config.use_cuda: packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(Variable(sentence_tensor.t()).cuda(), lengths) #remember to transpose sentiments = sentiments.cuda() else: packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(Variable(sentence_tensor.t()),lengths) # remember to transpose return &#123;'sentence': packed_sequences, 'sentiment': sentiments&#125;## é‡å†™collate_fn(batch)ä»¥ç”¨äºdataloaderä½¿ç”¨ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼štrain_dataloader=DataLoader(train_data,batch_size=32,shuffle=True,collate_fn=collate_fn)â€‹## å…¶ä¸­ï¼Œtrain_dataloaderå¯å¾ªç¯éå†â€‹â€‹ã€‚for data in train_dataloader: ... 6ï¸âƒ£ä½¿ç”¨yieldè·å¾—æ•°æ®çš„generatoryieldçš„ç”¨æ³•123456789101112131415def get_dataset(txt_file): # return generator with open(txt_file,'r') as f: for line in f: if len(line.strip())==0: continue sentence=list(line.strip())+['&lt;eos&gt;'] yield sentence #åœ¨ä½¿ç”¨çš„æ—¶å€™ï¼šdataset=get_dataset(txt_file)for d in dataset: pass#å¦‚æœéœ€è¦è¿˜å¯ä»¥æ”¹æˆlistå½¢å¼dataset=list(get_dataset(txt_file)) 7ï¸âƒ£åŠ¨æ€åˆ›å»ºRNNå®ä¾‹æ ¹æ®rnn_typeåŠ¨æ€åˆ›å»ºå¯¹è±¡å®ä¾‹ï¼Œä½¿ç”¨äº†getattr123# rnn in ['GRU','LSTM','RNN']self.rnn = getattr(nn, self.rnn_type)(self.embedding_dim, self.hidden_dim, self.num_layers, dropout=self.dropout)]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯1]]></title>
    <url>%2F2018%2F07%2F23%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D1%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨èƒŒäº†å››ç¯‡ã€‚ 1ï¸âƒ£ ä¸´æ±Ÿä»™Â·å¤œå½’ä¸´çš‹[å®‹] è‹è½¼å¤œé¥®ä¸œå¡é†’å¤é†‰ï¼Œå½’æ¥å½·å½¿ä¸‰æ›´ã€‚å®¶ç«¥é¼»æ¯å·²é›·é¸£ï¼Œæ•²é—¨éƒ½ä¸åº”ï¼Œå€šæ–å¬æ±Ÿå£°ã€‚é•¿æ¨æ­¤èº«éæˆ‘æœ‰ï¼Œä½•æ—¶å¿˜å´è¥è¥ï¼Ÿå¤œé˜‘é£é™ç¸ çº¹å¹³ï¼Œå°èˆŸä»æ­¤é€ï¼Œæ±Ÿæµ·å¯„é¦€ç”Ÿã€‚ ç¸ ï¼ˆhÃºï¼‰çº¹çš‹ï¼ˆgaoï¼‰http://m.xichuangzhu.com/work/57ae79400a2b580063150e39 2ï¸âƒ£ è¶æ‹èŠ±Â·é˜…å°½å¤©æ¶¯ç¦»åˆ«è‹¦[æ¸…] ç‹å›½ç»´é˜…å°½å¤©æ¶¯ç¦»åˆ«è‹¦ã€‚ä¸é“å½’æ¥ï¼Œé›¶è½èŠ±å¦‚è®¸ã€‚èŠ±åº•ç›¸çœ‹æ— ä¸€è¯­ï¼Œç»¿çª—æ˜¥ä¸å¤©ä¿±è«ã€‚å¾…æŠŠç›¸æ€ç¯ä¸‹è¯‰ã€‚ä¸€ç¼•æ–°æ¬¢ï¼Œæ—§æ¨åƒåƒç¼•ã€‚æœ€æ˜¯äººé—´ç•™ä¸ä½ï¼Œæœ±é¢œè¾é•œèŠ±è¾æ ‘ã€‚ http://m.xichuangzhu.com/work/57b8ef70128fe10054c91d17 3ï¸âƒ£ é€å‹äºº[å”] æç™½é’å±±æ¨ªåŒ—éƒ­ï¼Œç™½æ°´ç»•ä¸œåŸã€‚æ­¤åœ°ä¸€ä¸ºåˆ«ï¼Œå­¤è“¬ä¸‡é‡Œå¾ã€‚æµ®äº‘æ¸¸å­æ„ï¼Œè½æ—¥æ•…äººæƒ…ã€‚æŒ¥æ‰‹è‡ªå…¹å»ï¼Œè§è§ç­é©¬é¸£ã€‚ http://m.xichuangzhu.com/work/57b8facfd342d3005ac6ffb4 4ï¸âƒ£ é»„é¹¤æ¥¼é€å­Ÿæµ©ç„¶ä¹‹å¹¿é™µ[å”] æç™½æ•…äººè¥¿è¾é»„é¹¤æ¥¼ï¼ŒçƒŸèŠ±ä¸‰æœˆä¸‹æ‰¬å·ã€‚å­¤å¸†è¿œå½±ç¢§ç©ºå°½ï¼Œå”¯è§é•¿æ±Ÿå¤©é™…æµã€‚ http://m.xichuangzhu.com/work/57b8f306128fe10054c92fb8]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å®‰è£…condaé”™è¯¯]]></title>
    <url>%2F2018%2F07%2F23%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2F%E5%AE%89%E8%A3%85conda%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…condaçš„æ—¶å€™ï¼Œä¸€å¼€å§‹ä½¿ç”¨äº†pipå®‰è£…pip install condaåœ¨å®‰è£…å¥½condaä¹‹åæƒ³è¦ä½¿ç”¨condaå‘½ä»¤ï¼Œå‡ºç°ï¼š ERROR: The install method you used for condaâ€”probably either pip install conda or easy_install condaâ€”is not compatible with using conda as an application. If your intention is to install conda as a standalone application, currently supported install methods include the Anaconda installer and the miniconda installer. You can download the miniconda installer from https://conda.io/miniconda.html. ç„¶ååˆ°å®˜ç½‘ä¸‹è½½.shæ–‡ä»¶å¹¶bashå®‰è£…ï¼Œä»ç„¶æ²¡æœ‰è§£å†³è¯¥é—®é¢˜ï¼›æ¥ç€å°è¯•pip uninstall condaï¼Œå‡ºç° æœ€ååœ¨æŸ¥é˜…äº†ç½‘ä¸Šä¹‹åï¼Œä½¿ç”¨ which condaæ‰¾åˆ°condaçš„åœ°å€ï¼Œå¹¶åˆ é™¤rm Ã—Ã—Ã— æœ€åé‡æ–°bashå®‰è£…å³å¯ã€‚]]></content>
      <tags>
        <tag>æ‚ä¸ƒæ‚å…«</tag>
        <tag>conda</tag>
        <tag>é‡åˆ°çš„é—®é¢˜</tag>
      </tags>
  </entry>
</search>
