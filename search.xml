<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>每周碎片知识28</title>
      <link href="/2019/09/15/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8628/"/>
      <url>/2019/09/15/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8628/</url>
      
        <content type="html"><![CDATA[<h3 id="Pytorch"><a href="#Pytorch" class="headerlink" title="[Pytorch]"></a>[Pytorch]</h3><p>①torch.index_select虽然相比原tensor重新分配了空间，但以图的形式保存，因此仍然可以梯度回传。</p><p>②CUDA error: device-side assert trigger<br>Assertion <code>srcIndex &lt; srcSelectDimSize</code> failed</p><p>当遇到以上报错时，可以尝试将device改成CPU以获得更具体的报错信息；如果没办法在CPU上跑的话，则可以<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_LAUNCH_BLOCKING=<span class="number">1</span> python script.py args</span><br></pre></td></tr></table></figure></p><p>来获得具体错误信息。</p><p>遇到以上问题的一种可能性就是取embedding的时候index超过最大范围了。</p><p><a href="https://discuss.pytorch.org/t/solved-assertion-srcindex-srcselectdimsize-failed-on-gpu-for-torch-cat/1804/9" target="_blank" rel="noopener">https://discuss.pytorch.org/t/solved-assertion-srcindex-srcselectdimsize-failed-on-gpu-for-torch-cat/1804/9</a><br><a href="https://discuss.pytorch.org/t/what-the-error-means-runtimeerror-device-side-assert-triggered/3249" target="_blank" rel="noopener">https://discuss.pytorch.org/t/what-the-error-means-runtimeerror-device-side-assert-triggered/3249</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文30</title>
      <link href="/2019/09/15/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8730/"/>
      <url>/2019/09/15/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8730/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>Constant-Time Machine Translation with Conditional Masked Language Models</li><li>Learning a Multitask Curriculum for Neural Machine Translation</li></ol><h2 id="Constant-Time-Machine-Translation-with-Conditional-Masked-Language-Models"><a href="#Constant-Time-Machine-Translation-with-Conditional-Masked-Language-Models" class="headerlink" title="[Constant-Time Machine Translation with Conditional Masked Language Models]"></a>[Constant-Time Machine Translation with Conditional Masked Language Models]</h2><p>提出一种新的non-autoregressive的NMT，在训练阶段，首先一次性预测所有target，然后循环mask掉低confident的词并重预测，重复该过程；在decode阶段，每个token能够看到两边的token。</p><p>结构：<br>与传统的encoder-decoder一样，只是在decode端没有mask掉未来的词；<br>在训练阶段，有两个loss，一个是预测target sequence的长度，具体是在encoder端前面插入特殊的LEHGTH符号，并用该符号预测target的长度；第二是随机mask掉一些token，预测被mask掉的词的loss。</p><h3 id="mask-predict"><a href="#mask-predict" class="headerlink" title="mask-predict"></a>mask-predict</h3><p>给定target sequence的长度$N$，定义$(y_1 , . . . , y_N )$是句子的token；而$(p_1 , . . . , p_N )$是相对应的概率。设定一个预定义的循环次数$T$，在每个训练中做mask&amp;predict的操作。具体来说：首先mask掉所有的词，利用encoder的输入预测所有的词，每个词都会有一个概率，在之后的循环中，mask掉那些概率低的词，并利用encoder的输入和已观察到的target重新预测；每个循环中被mask掉的词的个数随着循环次数的增加而减小。预测完后p会更新。</p><p>具体公式：</p><script type="math/tex; mode=display">n=N \cdot \frac{T-t}{T}</script><script type="math/tex; mode=display">Y_{\operatorname{mask}}^{(t)}=\arg \min _{i}\left(p_{i}, n\right)</script><script type="math/tex; mode=display">Y_{o b s}^{(t)}=Y \backslash Y_{m a s k}^{(t)}</script><script type="math/tex; mode=display">P\left(y | X, Y_{o b s}\right)</script><p>例子：</p><p><img src="/images/15685140449224.jpg" width="70%" height="50%"></p><p>在inference的时候，还可以同时选择多个length，一起生成句子，类似beam search，最后选择平均概率最高的句子。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>几个结论：<br>多个循环是有必要的：因为一开始会有token repetition的现象，因为生成是基于一个假设：mask的token之间是独立的，因此面对同样的observation可能生成出一样的token（multi-modality）。循环mask-predict可以缓解这个问题。</p><p>更大的iteration能够帮助长句子提升表现；</p><p>更多的length candidate能够一定程度上提升表现，类似beam search</p><h3 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h3><p>如果只有一个iteration，那么就是完全的non-autoregressive；如果不mask掉low confident的token，那么就类似Attending to Future Tokens for Bidirectional Sequence Generation。相对于Attending to Future Tokens for Bidirectional Sequence Generation，减小生成的范围/长度，预测的是几个词而不是一整个句子，且有指向性（low confident），且引入iteration的概念，就是本篇论文。</p><hr><h2 id="Learning-a-Multitask-Curriculum-for-Neural-Machine-Translation"><a href="#Learning-a-Multitask-Curriculum-for-Neural-Machine-Translation" class="headerlink" title="[Learning a Multitask Curriculum for Neural Machine Translation]"></a>[Learning a Multitask Curriculum for Neural Machine Translation]</h2><p>尝试学到一个curriculum strategy，使得学得的模型能够在<strong>多任务/domain</strong>上都表现好。</p><p>其具体做法是：每个sample都赋予多个score，其中每个score代表该sample对某个task的作用大小，通过一个可学习的weight将多个score线性加权得到，该weight通过贝叶斯优化更新。</p><p>传统CL都是focus到一个final task，而在本文希望能够在多个task都有好的表现。</p><p><img src="/images/15685163220835.jpg" width="50%" height="50%"></p><p>如图，每个sample都有三个分数，分别代表对三个任务的贡献程度。（2）（3）（4）都是focus到一个任务，从贡献低的开始训，逐渐转换成贡献高的训，最终对一个特定的任务表现好。而（5）则是希望能够对三个任务都表现好。</p><p>记sample对第n个task的贡献程度$\phi^{(n)}\left(x, y ; \theta^{(n)}, m_{f}^{(n)}\right)$，则一个sample对应一个score的vector：$\Phi(x, y)=\left[\phi_{0}(x, y), \ldots, \phi_{M-1}(x, y)\right]$。设weight $W=\left[w_{0}, \dots, w_{M-1}\right]$，则线性加权得到：</p><script type="math/tex; mode=display">\varphi(x, y)=W \cdot \Phi(x, y)</script><p>根据该score排列，利用CL来训练。<br>我们希望获得一个W，使得：</p><script type="math/tex; mode=display">W^{*}=\arg \max _{W} \mathcal{P}\left(\widehat{\mathcal{C}}(W) ; m_{f}^{(0)}, \ldots, m_{f}^{(N)}\right)</script><p>通过在不同任务上的最终表现来作为评判标准，并利用贝叶斯来做优化。</p><p><img src="/images/15685165429041.jpg" width="60%" height="50%"></p><p><img src="/images/15685167625469.jpg" width="60%" height="50%"></p><p>Curriculum Learning strategy：<br>具体的curriculum learning strategy在这里类似domain adaptation。设一个decaying function，$\lambda(t)=0.5^{t / H}$。H是超参，t是时间步，λ则代表了模型所用到的数据的比例。随着训练的进行比例越来越小，也即使用的数据与task越来越相关。</p><p>难度的定义：<br>①NLM domain relevance features</p><script type="math/tex; mode=display">d(x)=\frac{\log P_{\mathrm{domain}}(x)-\log P_{\mathrm{general}}(x)}{|x|}</script><p>②NMT quality features</p><script type="math/tex; mode=display">q(x, y)=\frac{\log P_{\text {clean }}(y | x)-\log P_{\text {noisy }}(y | x)}{|y|}</script><p>其实就是训练两个不同数据上的模型（翻译或者语言模型）然后将一个sample丢进去计算得到分数。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> Curriculum Learning </tag>
            
            <tag> NMT </tag>
            
            <tag> Machine Translation </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词36</title>
      <link href="/2019/09/15/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D36/"/>
      <url>/2019/09/15/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D36/</url>
      
        <content type="html"><![CDATA[<h3 id="辋川闲居赠裴秀才迪"><a href="#辋川闲居赠裴秀才迪" class="headerlink" title="辋川闲居赠裴秀才迪"></a>辋川闲居赠裴秀才迪</h3><p>[唐] 王维<br>寒山转苍翠，秋水日潺湲。<br>倚杖柴门外，临风听暮蝉。<br>渡头馀落日，墟里上孤烟。<br><strong>复值接舆醉，狂歌五柳前。</strong></p><hr><h3 id="终南别业"><a href="#终南别业" class="headerlink" title="终南别业"></a>终南别业</h3><p>[唐] 王维<br>中岁颇好道，晩家南山陲。<br><strong>兴来每独往，胜事空自知</strong>。<br><strong>行到水穷处，坐看云起时</strong>。<br>偶然值林叟，谈笑无还期。</p><hr><h3 id="看梅绝句五首"><a href="#看梅绝句五首" class="headerlink" title="看梅绝句五首"></a>看梅绝句五首</h3><p>[宋] 陆游<br>老子舞时不须拍，梅花乱插乌巾香。<br><strong>樽前作剧莫相笑，我死诸君思此狂</strong>。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词35</title>
      <link href="/2019/09/01/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D35/"/>
      <url>/2019/09/01/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D35/</url>
      
        <content type="html"><![CDATA[<h3 id="登金陵凤凰台"><a href="#登金陵凤凰台" class="headerlink" title="登金陵凤凰台"></a>登金陵凤凰台</h3><p>[唐] 李白<br>凤凰台上凤凰游，凤去台空江自流。<br><strong>吴宫花草埋幽径，晋代衣冠成古丘。</strong><br>三山半落青天外，二水中分白鹭洲。<br>总为浮云能蔽日，长安不见使人愁。</p><hr><h3 id="摸鱼儿"><a href="#摸鱼儿" class="headerlink" title="摸鱼儿"></a>摸鱼儿</h3><p>[宋] 辛弃疾<br>更能消、几番风雨，匆匆春又归去。惜春长怕花开早，何况落红无数。春且住，见说道、天涯芳草无归路。怨春不语。算只有殷勤、画檐蛛网，尽日惹飞絮。<br>长门事，准拟佳期又误。蛾眉曾有人妬，千金纵买相如赋，脉脉此情谁诉？君莫舞，<strong>君不见、玉环飞燕皆尘土</strong>！闲愁最苦，<strong>休去倚危栏，斜阳正在，烟柳断肠处</strong>。</p><p>妬（dù）：同“妒”</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文29</title>
      <link href="/2019/09/01/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8729/"/>
      <url>/2019/09/01/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8729/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks</li><li>A Simple Theoretical Model of Importance for Summarization</li><li>Attending to Future Tokens for Bidirectional Sequence Generation</li><li>Sequence Generation: From Both Sides to the Middle</li></ol><h2 id="Repeat-before-Forgetting-Spaced-Repetition-for-Efficient-and-Effective-Training-of-Neural-Networks"><a href="#Repeat-before-Forgetting-Spaced-Repetition-for-Efficient-and-Effective-Training-of-Neural-Networks" class="headerlink" title="[Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks]"></a>[Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks]</h2><p>提出一种基于核密度估计的训练神经网络的scheduler。</p><p>论文首先从实验上论证了神经网络与人类认知的相似性。接着基于此提出新的训练神经网络的方法。</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>人类在记忆新知识时，需要不断重复，而重复的间隔时间随着重复次数而逐渐扩大。（遗忘曲线）。同时学习过程还和该知识的难度与人类记忆的强度相关。放在神经网络中，就有：</p><script type="math/tex; mode=display">\operatorname{Pr}(r e c a l l)=\exp \left(-\frac{\text {difficulty} \times \text {delay}}{\text {strength}}\right)</script><p>其中recall就是分类任务的准确率。</p><p>因此本文基于以上三个变量提出一种基于间隔重复训练神经网络的方法。<br>实际上本质就是，在快要遗忘之前重新过一遍这个sample。通过预估的方法去获得delay的时间。</p><p>在此之前的工作：</p><p><img src="/images/15673013678116.jpg" width="50%" height="50%"></p><p>大概做法是，设定几个队列，如果当前的sample预测对了，就将该sample push到后一个队列，否则就将其push到前一个队列或第一个队列。越后面的队列被review的间隔越长。相当于认为越后面的越简单，需要被训练到的次数越少。</p><p>但这里的间隔是手动设定的（2的k次方），并不是很准。本文提出的就是动态设定这样一个间隔。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>设$x_{i}=\frac{d_{i} \times t_{i}}{s_{e}}$</p><p>d是该sample的loss，t是还有几个epoch被review到；s则是在validation data上的performance。</p><p>x是不同核函数的变量：</p><p>$f_{g a u}(x, \tau)=\exp \left(-\tau x^{2}\right)$</p><p>$f_{l a p}(x, \tau)=\exp (-\tau x)$</p><p>$f_{l i n}(x, \tau)=\left\{\begin{array}{ll}{1-\tau x} &amp; {x&lt;\frac{1}{\tau}} \\ {0} &amp; {\text { otherwise }}\end{array}\right.$</p><p>$f_{\sec }(x, \tau)=\frac{2}{\exp \left(-\tau x^{2}\right)+\exp \left(\tau x^{2}\right)}$</p><p>等等。</p><p>不同核函数的曲线：</p><p><img src="/images/15673016101025.jpg" width="40%" height="50%"></p><p>我们用核函数来预估一个样本被review的间隔。</p><p><img src="/images/15673017081202.jpg" width="50%" height="50%"></p><p>如果t小于等于1，则选做当前的训练数据。一开始大家都是1。其他未被选中的记录在delayed batch里。<br>在训练完一个epoch后，获得valid上的accuracy。通过validation来预估当前的$\tau$：</p><script type="math/tex; mode=display">\hat{\tau}=\arg \min _{\tau}\left(f\left(x_{j}, \tau\right)-a_{j}\right)^{2}, \forall h_{j} \in \mathbf{V}, a_{j} \geq \eta</script><p>其中$a$是accuracy。<br>接着利用这个$\tau$去预估当前训练数据的delay $t$。<br>最后将delay batch的t减1，因为已经训练过了一个epoch了。</p><p>实际上就相当于利用密度核函数去动态预估参数t。</p><hr><h2 id="A-Simple-Theoretical-Model-of-Importance-for-Summarization"><a href="#A-Simple-Theoretical-Model-of-Importance-for-Summarization" class="headerlink" title="[A Simple Theoretical Model of Importance for Summarization]"></a>[A Simple Theoretical Model of Importance for Summarization]</h2><p>提出summary任务的几个指标，Redundancy, Relevance, and Informativeness，以及统一这三个指标的Importance。本文贡献是将信息论引入，并证明其他过去的工作可以放在这个框架下。</p><p>记semantic unit作为基本单位，$\Omega$是所有semantic unit的集合，$X$是文本。$\mathbb{P}_{X}\left(\omega_{i}\right)$则是文本$X$出现该unit的概率。</p><p>$D$是source document；$S$是candidate summary。</p><h3 id="Redundancy"><a href="#Redundancy" class="headerlink" title="Redundancy"></a>Redundancy</h3><p>$S$的交叉熵：</p><script type="math/tex; mode=display">H(S)=-\sum_{\omega_{i}} \mathbb{P}_{S}\left(\omega_{i}\right) \cdot \log \left(\mathbb{P}_{S}\left(\omega_{i}\right)\right)</script><p>引入redundancy：</p><script type="math/tex; mode=display">\operatorname{Red}(S)=H_{\max }-H(S)</script><p>可以简写为：$\operatorname{Red}(S)=-H(S)$</p><p>当所有的semantic unit的概率相同时，redundancy最大。因为这样就没有什么有用的信息了。</p><h3 id="Relevance"><a href="#Relevance" class="headerlink" title="Relevance"></a>Relevance</h3><script type="math/tex; mode=display">\operatorname{Rel}(S, D)=\sum_{\omega_{i}} \mathbb{P}_{S}\left(\omega_{i}\right) \cdot \log \left(\mathbb{P}_{D}\left(\omega_{i}\right)\right)</script><p>衡量$S$与$D$的关联度。</p><p>A summary with a low expected surprise produces a low uncertainty about what were the original sources。summary相当于对源文档的有损压缩，因此应尽可能减小该损失。</p><p>relevance与redundancy的联系：</p><script type="math/tex; mode=display">\begin{aligned} K L(S| | D) &=C E(S, D)-H(S) \\-K L(S| | D) &=\operatorname{Rel}(S, D)-\operatorname{Red}(S) \end{aligned}</script><p>KL divergence is the information loss incurred by using D as an approximation of S (i.e., the uncertainty about D arising from observing S instead of D). <strong>A summarizer that minimizes the KL divergence minimizes Redundancy while maximizing Relevance.</strong></p><h3 id="Informativeness"><a href="#Informativeness" class="headerlink" title="Informativeness"></a>Informativeness</h3><p>relevance忽略了外部知识。将外部知识引入，a summary is informative if it induces, for a user, a great change in her knowledge about the world.</p><script type="math/tex; mode=display">\operatorname{Inf}(S, K)=-\sum_{\omega_{i}} \mathbb{P}_{S}\left(\omega_{i}\right) \cdot \log \left(\mathbb{P}_{K}\left(\omega_{i}\right)\right)</script><p>其中$K$是background knowledge。</p><p>for Informativeness, the cross-entropy between S and K should be high because we measure the amount of new information induced by the summary in our knowledge.</p><h3 id="Importance"><a href="#Importance" class="headerlink" title="Importance"></a>Importance</h3><p>其实就是将relevance与informativeness结合起来。</p><script type="math/tex; mode=display">\begin{aligned} \mathbb{P}_{\frac{D}{K}}\left(\omega_{i}\right) &=\frac{1}{C} \cdot \frac{d_{i}^{\alpha}}{k_{i}^{\beta}} \\ C &=\sum_{i} \frac{d_{i}^{\alpha}}{k_{i}^{\beta}}, \alpha, \beta \in \mathbb{R}^{+} \end{aligned}</script><hr><h2 id="Attending-to-Future-Tokens-for-Bidirectional-Sequence-Generation"><a href="#Attending-to-Future-Tokens-for-Bidirectional-Sequence-Generation" class="headerlink" title="[Attending to Future Tokens for Bidirectional Sequence Generation]"></a>[Attending to Future Tokens for Bidirectional Sequence Generation]</h2><p>基于bert的思想，引入双向的序列生成，也即生成的过程中可以看到左右两边。</p><p>基本做法是：首先将x与y拼起来，然后将y用placeholder替代，过transformer encoder去uncover这些placeholder。</p><p><img src="/images/15673034453669.jpg" width="80%" height="50%"></p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>在训练时候，引入Placeholder Replacement Strategy，采用伯努利分布或高斯分布随机将部分的词用placeholder替代，通过上下文将其recover。</p><p>在生成时，有几种方案：</p><p>One-step greedy： 直接同时将所有的placeholder一次性recover。<br>Highest probability：一步一步来，每次uncover概率最大的那个token。<br>Lowest entropy： 选择熵最小的，这代表了其不确定性最小。<br>Left-to-right：就是从左到右，但未来的placeholder也会attend到。</p><h3 id="一点思考"><a href="#一点思考" class="headerlink" title="一点思考"></a>一点思考</h3><p>这实际上就有点像任意顺序的序列生成。但似乎设计有些粗糙，且实验没有在翻译上跑，可能是在翻译上没有效果。且论文似乎没有提到如何加position encoding。<br>总体思想还是比较清晰的，如果用于翻译设计得应该更精巧一些。另外一个问题是停止条件是什么论文似乎也没提到，大概也是当生成到EOS就停止吧。placeholder的个数是怎么确定的？only placeholder tokens up to some pre-determined maximum sequence length。</p><hr><h2 id="Sequence-Generation-From-Both-Sides-to-the-Middle"><a href="#Sequence-Generation-From-Both-Sides-to-the-Middle" class="headerlink" title="[Sequence Generation: From Both Sides to the Middle]"></a>[Sequence Generation: From Both Sides to the Middle]</h2><p>提出从左到右和从右到左的同时decoding。更多的算是模型上的创新吧，但感觉没有什么insight。</p><p>为什么要从两端到中间：<br>inference的时候能加速；能够attend到未来的词，缓解under-translation；</p><h3 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h3><p><img src="/images/15673038459325.jpg" width="50%" height="50%"></p><p>在decode端，先forward与backward自行做attention（intra），然后二者再做交互（inter）。</p><p><img src="/images/15673038731825.jpg" width="60%" height="50%"></p><h3 id="一点思考-1"><a href="#一点思考-1" class="headerlink" title="一点思考"></a>一点思考</h3><p>从两端到中间的想法不错，过去应该也有论文做过这样的尝试。但本文有点像两年前的论文，从模型的结构入手，改改模型跑跑实验就发出来，有点像实验报告。并没有提供一些更有价值的insight，难以follow。同时，本文也没有提position应该如何加？这点或许很关键。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> Curriculum Learning </tag>
            
            <tag> NMT </tag>
            
            <tag> Summary </tag>
            
            <tag> Sequence Generation </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录19</title>
      <link href="/2019/08/18/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9519/"/>
      <url>/2019/08/18/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9519/</url>
      
        <content type="html"><![CDATA[<h3 id="Python获得list中topk的index"><a href="#Python获得list中topk的index" class="headerlink" title="[Python获得list中topk的index]"></a>[Python获得list中topk的index]</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.array([<span class="number">9</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([<span class="number">9</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ind = np.argpartition(a, <span class="number">-4</span>)[<span class="number">-4</span>:]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ind</span><br><span class="line">array([<span class="number">1</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[ind]</span><br><span class="line">array([<span class="number">4</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意是没有排好序的，效率更高</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面的是排好序的</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: arr = np.array([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: arr.argsort()[<span class="number">-3</span>:][::<span class="number">-1</span>]</span><br><span class="line">Out[<span class="number">3</span>]: array([<span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词34</title>
      <link href="/2019/08/18/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D34/"/>
      <url>/2019/08/18/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D34/</url>
      
        <content type="html"><![CDATA[<h3 id="走马川行奉送封大夫出师西征"><a href="#走马川行奉送封大夫出师西征" class="headerlink" title="走马川行奉送封大夫出师西征"></a>走马川行奉送封大夫出师西征</h3><p>[唐] 岑参<br>君不见走马川行雪海边，平沙莽莽黄入天。<br>轮台九月风夜吼，一川碎石大如斗，随风满地石乱走。<br>匈奴草黄马正肥，金山西见烟尘飞，汉家大将西出师。<br>将军金甲夜不脱，半夜军行戈相拨，风头如刀面如割。<br>马毛带雪汗气蒸，五花连钱旋作冰，幕中草檄砚水凝。<br>虏骑闻之应胆慑，料知短兵不敢接，车师西门伫献捷。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文28</title>
      <link href="/2019/08/17/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8728/"/>
      <url>/2019/08/17/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8728/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>Easy Questions First? A Case Study on Curriculum Learning for Question Answering</li><li>Bridging the Gap between Training and Inference for Neural Machine Translation</li><li>Improving Multi-step Prediction of Learned Time Series Models</li><li>Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks</li><li>SEQUENCE LEVEL TRAINING WITH RECURRENT NEURAL NETWORKS</li><li>Minimum Risk Training for Neural Machine Translation</li><li>Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation</li><li>Insertion Transformer: Flexible Sequence Generation via Insertion Operations</li></ol><h2 id="Easy-Questions-First-A-Case-Study-on-Curriculum-Learning-for-Question-Answering"><a href="#Easy-Questions-First-A-Case-Study-on-Curriculum-Learning-for-Question-Answering" class="headerlink" title="[Easy Questions First? A Case Study on Curriculum Learning for Question Answering]"></a>[Easy Questions First? A Case Study on Curriculum Learning for Question Answering]</h2><p>提出在QA中引入CL，但实际上其本质不是CL而更像是SPL。</p><p>QA任务我不感兴趣，只讨论CL。</p><p>其基本做法是：每次从剩下未被选择的数据中选择对于当前模型最简单的sample，并定义了一系列的指标去衡量难易程度，这相当于是<strong>动态</strong>在选择sample，根据当前的模型能力去选择最容易学的sample，从整个过程来看，确实是从简单到难，且达到了动态选择的目的，并且不像SPL那样会直接丢弃sample，效率（或许）会更高。</p><p>具体做法：<br>每次从剩余sample集合内选择最简单的sample $q_{i} \in Q \backslash Q_{0}$。</p><p>有几种指标衡量对于当前模型的难易程度：<br>①Greedy Optimal: has the minimum expected effect on the model<br>Change in Objective causes the smallest increase in the objective.<br>②Mini-max minimizes the regularized expected risk when including the question with the answer candidate a ij that yields the maximum error.<br>③Expected Change in Objective the minimum expected effect on the model<br>④Change in Objective-Expected Change in Objective the minimum value of the difference between the change in objective and the expected change in objective<br>⑤Correctly Answered is answered by the model M with the minimum cost</p><p>同时，希望每次选的batch，尽量diverse，通过feature space的夹角去衡量。</p><p>这个思想还挺有意思的，但每次选择sample都要重新计算一遍，会不会复杂度过高？</p><hr><h2 id="Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation"><a href="#Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation" class="headerlink" title="[Bridging the Gap between Training and Inference for Neural Machine Translation]"></a>[Bridging the Gap between Training and Inference for Neural Machine Translation]</h2><p>这是ACL19的best paper。</p><p>本文主要的目标是缓解exposure bias的问题，也即训练使用ground truth而inference使用predicted words所造成的不一致。主要思路是：在训练过程中随机从predicted word和ground truth 采样作为接下来的上下文。</p><p>当前这种discrepancy的问题就是<strong>过度纠正（overcorrection）</strong>，只要模型生成了一个和ground truth不一样的，就会被立即纠正，但实际上翻译可以有好几种合理的候选，并不是和ground truth不一样就错误；同时在训练时有监督，而在inference没有信号监督，predicted word在不同阶段是从不同的分布获取的，也即data distribution vs model distribution，这就是exposure bias带来的问题。</p><p>论文举了一个例子：<br><img src="/images/15660513298758.jpg" width="40%" height="50%"></p><p>①假如模型生成了第三个词为abide，为了和ground truth相一致，模型会强制让第四个词生成with，然后with作为输入去生成the rule，但实际上整句是错误的。如cand1就是过度矫正（overcorrection）<br>②假设模型生成对了by，但也可能因为输入了by而产生了错误的’the law’，假设一种情形，模型记住了with后面一定跟the rule，为了能够生成cand3的，我们应将 with作为输入而不是by，即使我们生成了by。称这种做法为 overcorrection recovery。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>因此本文的做法就是将预测的词（oracle）和golden 在训练的时候随机sample作为输入。在一开始模型还没有收敛的时候，主要是golden，而在后期则增加oracle的比例。</p><p><img src="/images/15660514154749.jpg" width="50%" height="50%"></p><p>这样模型就能够handle在inference出现的情况，也即没有golden truth的辅助，同时可以减小训练和测试之间的gap。</p><p>有两种方法获得oracle word，一种是word level的一种是sentence level的。</p><h4 id="Oracle-word-selection"><a href="#Oracle-word-selection" class="headerlink" title="Oracle word selection"></a>Oracle word selection</h4><h5 id="word-level"><a href="#word-level" class="headerlink" title="word level"></a>word level</h5><p>也即greedy的方法，每次选择概率最高的词作为oracle</p><p><img src="/images/15660518415062.jpg" width="50%" height="50%"></p><p>为了获取更为鲁棒的oracle，引入了gumbel noise，这是一种正则化方法。</p><script type="math/tex; mode=display">\begin{aligned} \eta=&-\log (-\log u) \\ \tilde{o}_{j-1} &=\left(o_{j-1}+\eta\right) / \tau \\ \tilde{P}_{j-1} &=\operatorname{softmax}\left(\tilde{o}_{j-1}\right) \end{aligned}</script><p>当$\tau$趋于0，则逼近argmax，若趋于∞则逼近均匀采样。</p><p><img src="/images/15660519181632.jpg" width="60%" height="50%"></p><p>因此最终：</p><script type="math/tex; mode=display">y_{j-1}^{\text {oracle }}=y_{j-1}^{\mathrm{WO}}=\operatorname{argmax}\left(\tilde{P}_{j-1}\right)</script><h5 id="sentence-level"><a href="#sentence-level" class="headerlink" title="sentence level"></a>sentence level</h5><p>另一种则是扩大匹配的范围，允许更灵活的选择，也即先通过beam search让模型生成一个概率最高的句子，将该句子的词与ground truth一一对应，在生成期间也可以引入gumbel noise。一一对应的词就可以随机采样了。</p><p>但该做法有一个问题，也即可能生成的句子长度可能和ground truth不对应，因此这里引入<strong>force decoding</strong>的做法强制对应。</p><p><strong>force decoding</strong></p><p>①当生成到第j个step时top first的概率是EOS，此时$j \leqslant\left|\mathbf{y}^{*}\right|$，那么避开EOS，选择top second概率的词，使其能够继续生成下去。</p><p>②当生成到$\left\{\left|\mathbf{y}^{*}\right|+1\right\}$个时还没生成到EOS，则直接选择EOS作为结尾，强制停止。</p><h4 id="Sampling-with-Decay"><a href="#Sampling-with-Decay" class="headerlink" title="Sampling with Decay"></a>Sampling with Decay</h4><p>另一个就是在模型不同阶段sample的概率应该是不同的，在这里</p><script type="math/tex; mode=display">p=\frac{\mu}{\mu+\exp (e / \mu)}</script><p>e是epoch数。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/images/15660521569189.jpg" width="80%" height="50%"></p><p>实验表明能够有更好的效果，且更快速收敛，只看training可以发现，没有baseline那么容易overfitting。</p><p><img src="/images/15660521805452.jpg" width="50%" height="50%"></p><p><img src="/images/15660521942211.jpg" width="50%" height="50%"></p><p>对于长度越长的提升越大，说明exposure bias在长句子表现更明显。</p><p><img src="/images/15660522167431.jpg" width="40%" height="50%"></p><p>在不同数据集和不同模型上都有提升：</p><p><img src="/images/15660522455047.jpg" width="40%" height="50%"></p><h3 id="一点思考"><a href="#一点思考" class="headerlink" title="一点思考"></a>一点思考</h3><p>本篇很清晰易懂，结构也很好，同时举的例子也非常容易让人理解。再者实验效果也很好，做了很多分析。这个方向可以多关注关注，因为还有挺多可做的。</p><p>我们假设让training也完全用predicted是一个极端，完全用ground truth是另一个极端，在此二者之间的就是这种sample方式。</p><p>但似乎仍然有未解决的问题，也即target word始终是ground truth，这种方法好像还是没能解决overcorrection的问题。</p><hr><h2 id="Improving-Multi-step-Prediction-of-Learned-Time-Series-Models"><a href="#Improving-Multi-step-Prediction-of-Learned-Time-Series-Models" class="headerlink" title="[Improving Multi-step Prediction of Learned Time Series Models]"></a>[Improving Multi-step Prediction of Learned Time Series Models]</h2><p>讨论如何在时序模型下解决错误累积的问题（在翻译中就是exposure bias），该问题的本质就是train-test的iid假设被打破。本文的做法就是将prediction与training data结合形成新的数据集，也即相当于用正确的数据对prediction进行修正。并且从理论上证明了该算法的高效性。</p><p>示例图很清楚：<br><img src="/images/15660523604587.jpg" width="40%" height="50%"></p><p>图a是在训练完一个模型后做的预测与真实数据的对比；图b是将预测的序列与正确的序列结合形成新的序列，重新训新模型。其实就相当于用正确数据做了修正。</p><p>因此算法就有：</p><p><img src="/images/15660523930825.jpg" width="50%" height="50%"></p><p>算法过程清晰明了。在训练多个模型的过程中，数据会越来越多，有recurrent的感觉。其实这个方法的本质上也是在ground truth和oracle上进行sampling，因为数据D里面混杂了ground truth或oracle。</p><hr><h2 id="Scheduled-Sampling-for-Sequence-Prediction-with-Recurrent-Neural-Networks"><a href="#Scheduled-Sampling-for-Sequence-Prediction-with-Recurrent-Neural-Networks" class="headerlink" title="[Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks]"></a>[Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks]</h2><p>在sequence prediction的问题上，提出schedule sampling的方式缓解exposure bias的问题。exposure bias是指训练使用teacher force，完全用监督信息，而inference期间完全用predicted word，错误可能会被放大。</p><p>其做法是：在训练期间，按概率输入golden truth或者predicted word，该概率通过某种schedule来控制。这里的做法就是在训练期间概率采样输入predicted word，让模型自己去学习如何解决inference可能出现的问题。</p><p><img src="/images/15660525698952.jpg" width="40%" height="50%"></p><p>而schedule则是：</p><p><img src="/images/15660525988009.jpg" width="40%" height="50%"></p><p>未解决的问题：target word仍然是golden truth。sampling这类思路都会有这个问题。</p><hr><h2 id="SEQUENCE-LEVEL-TRAINING-WITH-RECURRENT-NEURAL-NETWORKS"><a href="#SEQUENCE-LEVEL-TRAINING-WITH-RECURRENT-NEURAL-NETWORKS" class="headerlink" title="[SEQUENCE LEVEL TRAINING WITH RECURRENT NEURAL NETWORKS]"></a>[SEQUENCE LEVEL TRAINING WITH RECURRENT NEURAL NETWORKS]</h2><p>本文意在解决翻译中存在的exposure bias以及测试阶段的指标和训练指标不同的问题。</p><p>其做法是：提出一种基于RL的算法，在训练过程中混合了RL和普通训练方法，既能优化word level的loss又能直接优化sentence level的bleu。同时该方法还解决了RL在random init上难以拟合超大的search space的问题，因为使用了预训练的参数。</p><p>接下来介绍几个相关的模型。</p><h3 id="WORD-LEVEL-TRAINING"><a href="#WORD-LEVEL-TRAINING" class="headerlink" title="WORD-LEVEL TRAINING"></a>WORD-LEVEL TRAINING</h3><h4 id="CROSS-ENTROPY-TRAINING-XENT"><a href="#CROSS-ENTROPY-TRAINING-XENT" class="headerlink" title="CROSS ENTROPY TRAINING (XENT)"></a>CROSS ENTROPY TRAINING (XENT)</h4><p>也即普通的训练方式，每次输入golden truth，然后优化loss<br>这是该模型在训练与inference时的示意图：</p><p><img src="/images/15660527119463.jpg" width="80%" height="50%"></p><p>主要问题：①exposure bias  ②训练指标是word level的而测试指标则是sentence level的，造成训练没法直接优化测试的指标。</p><h4 id="DATA-AS-DEMONSTRATOR-DAD"><a href="#DATA-AS-DEMONSTRATOR-DAD" class="headerlink" title="DATA AS DEMONSTRATOR (DAD)"></a>DATA AS DEMONSTRATOR (DAD)</h4><p>混合oracle与ground truth：</p><p><img src="/images/15660527382516.jpg" width="90%" height="50%"></p><p>一个问题在于，在每个时间步t，target label总是ground truth而不管输入是oracle或者ground truth。这样可能就会造成不对齐的问题，比如：<br>正确的翻译是“I took a long walk”，模型目前翻译到“I took a walk”，而在接下来模型会强制再翻译一次wark<br>另一个问题在于，梯度回传没有到oracle，也即oracle虽然是由模型生成的，但gradient的待遇和ground truth是一样的，且仍然局限于word-level。</p><h4 id="END-TO-END-BACKPROP-E2E"><a href="#END-TO-END-BACKPROP-E2E" class="headerlink" title="END-TO-END BACKPROP (E2E)"></a>END-TO-END BACKPROP (E2E)</h4><p>在训练过程中也使用oracle而不是ground truth。</p><p>在每个时间步t上，我们选择top k个预测的词作为下一个输入而不是ground truth。具体的做法是将softmax过后的distribution过一个k-max layer，将其他的都设为0，然后再renormalize使其和为1，此时我们有：</p><script type="math/tex; mode=display">\left\{i_{t+1, j}, v_{t+1, j}\right\}_{j=1, \ldots, k}=\mathrm{k}-\max p_{\theta}\left(w_{t+1} | w_{t}, h_{t}\right)</script><p>其中i是index，v是对应的score，也作为其weight，用于梯度回传。</p><p><img src="/images/15660528416693.jpg" width="90%" height="50%"></p><p>在实际实现中，一开始还是用ground truth的，直到训练后期才使用top-k的策略。</p><p>该方法确实解决了training-inference的discrepancy，但仍然还是word-level的。</p><h3 id="SEQUENCE-LEVEL-TRAINING"><a href="#SEQUENCE-LEVEL-TRAINING" class="headerlink" title="SEQUENCE LEVEL TRAINING"></a>SEQUENCE LEVEL TRAINING</h3><p>使用RL直接对bleu进行优化。RNN是agent，action是预测下一个词，agent的参数就是policy，在预测完该句子后，BLEU就是reward。</p><p>由于RL难以从random init去拟合大搜索空间，因此是先通过普通的训练RNN，然后用该参数去作为init去训练接下来的RL。</p><p>第二就是，为了训练的稳定，在训练过程中逐渐引入oracle（RL部分）。在前$(T-\Delta)$个step用普通的XENT，然后接下来的$\Delta$个step使用RL去生成。逐渐增大$\Delta$直到整个训练都是由RL生成的:</p><p><img src="/images/15660530021248.jpg" width="90%" height="50%"></p><p>因此MIXER的算法如下：</p><p><img src="/images/15660530338643.jpg" width="80%" height="50%"></p><p>总结一下几个算法：</p><p><img src="/images/15660530482870.jpg" width="80%" height="50%"></p><h3 id="一点想法"><a href="#一点想法" class="headerlink" title="一点想法"></a>一点想法</h3><p>似乎要真正解决传统训练方法的两个问题，就只能通过直接优化test evaluation metric来做到，而像直接针对解决exposure bias的方法（scheduled sampling等）都没办法真正解决这两个问题。</p><hr><h2 id="Minimum-Risk-Training-for-Neural-Machine-Translation"><a href="#Minimum-Risk-Training-for-Neural-Machine-Translation" class="headerlink" title="[Minimum Risk Training for Neural Machine Translation]"></a>[Minimum Risk Training for Neural Machine Translation]</h2><p>在翻译任务中，直接通过优化evaluation metric如bleu而不是word-level的loss来提升表现。</p><p>我们普通的方法是通过优化词级别的loss，也即MLE（maximum likelihood estimation），每个step的预测和golden truth计算cross entropy然后累加。但这个方法的两个劣势：exposure bias 和 loss function只定义在词级别而不是句子级别，而在测试阶段则是用bleu这种句子级别的metric来评测。</p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>记$\left\langle\mathbf{x}^{(s)}, \mathbf{y}^{(s)}\right\rangle$是sentence-pair，同时有$\Delta\left(\mathbf{y}, \mathbf{y}^{(s)}\right)$作为prediction与golden truth之间的差距，这是不可导的。</p><p>定义expected loss：</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{R}(\boldsymbol{\theta}) &=\sum_{s=1}^{S} \mathbb{E}_{\mathbf{y} | \mathbf{x}^{(s)} ; \boldsymbol{\theta}}\left[\Delta\left(\mathbf{y}, \mathbf{y}^{(s)}\right)\right] \\ &=\sum_{s=1}^{S} \sum_{\mathbf{y} \in \mathcal{Y}\left(\mathbf{x}^{(s)}\right)} P\left(\mathbf{y} | \mathbf{x}^{(s)} ; \boldsymbol{\theta}\right) \Delta\left(\mathbf{y}, \mathbf{y}^{(s)}\right) \end{aligned}</script><p>其中$\mathcal{Y}\left(\mathbf{x}^{(s)}\right)$是所有的可能的candidate，P就是生成该candidate的概率。</p><p>因此我们的目标就是：</p><script type="math/tex; mode=display">\hat{\boldsymbol{\theta}}_{\mathrm{MRT}}=\underset{\boldsymbol{\theta}}{\operatorname{argmin}}\{\mathcal{R}(\boldsymbol{\theta})\}</script><p>也即，希望生成出来句子中的与golden truth的差距最小的概率最大。</p><p>一个很清晰的例子：</p><p><img src="/images/15660534305129.jpg" width="80%" height="50%"></p><p>右边四列是四个不同的模型，y1，y2与y3是三个不同的candidate，其中y1与y最接近，而y2最远。第四列生成y1的概率最高，y2最低，因此可以看到其风险期望最小。也即我们的目标总是希望最大概率生成与golden truth最接近的样本。</p><p>（这里就有点像RL了，生成了几个episode，希望reward最大的episode出现的可能性更大。而普通的训练方法则是在每步都优化。）</p><p>在实际中，由于不可能遍历所有的情况，因此只选择一个candidate subset：</p><script type="math/tex; mode=display">\begin{aligned} \tilde{\mathcal{R}}(\boldsymbol{\theta}) &=\sum_{s=1}^{S} \mathbb{E}_{\mathbf{y} | \mathbf{x}^{(s)} ; \boldsymbol{\theta}, \alpha}\left[\Delta\left(\mathbf{y}, \mathbf{y}^{(s)}\right)\right] \\ &=\sum_{s=1}^{S} \sum_{\mathbf{y} \in \mathcal{S}\left(\mathbf{x}^{(s)}\right)} Q\left(\mathbf{y} | \mathbf{x}^{(s)} ; \boldsymbol{\theta}, \alpha\right) \Delta\left(\mathbf{y}, \mathbf{y}^{(s)}\right) \end{aligned}</script><p>其中$\mathcal{S}\left(\mathbf{x}^{(s)}\right) \subset \mathcal{Y}\left(\mathbf{x}^{(s)}\right)$，且$Q$是subset的distribution，保证和为1。</p><script type="math/tex; mode=display">Q\left(\mathbf{y} | \mathbf{x}^{(s)} ; \boldsymbol{\theta}, \alpha\right)=\frac{P\left(\mathbf{y} | \mathbf{x}^{(s)} ; \boldsymbol{\theta}\right)^{\alpha}}{\sum_{\mathbf{y}^{\prime} \in \mathcal{S}\left(\mathbf{x}^{(s)}\right)} P\left(\mathbf{y}^{\prime} | \mathbf{x}^{(s)} ; \boldsymbol{\theta}\right)^{\alpha}}</script><p>其中$α$控制$Q$的sharp程度。</p><p>而sample subset的方式：</p><p><img src="/images/15660535677759.jpg" width="90%" height="50%"></p><p>有点像beam-search的probabilistic sampling 版。实验设k=100。</p><h3 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h3><p>本文的MRT在SMT上有使用过。思路有点像RL，论文中也有提到这点，在思路上与MIXER相近。且training与test的行为一致（没有exposure bias的问题），也直接优化了sentence-level的指标，一次解决了两个问题。</p><hr><h2 id="Greedy-Search-with-Probabilistic-N-gram-Matching-for-Neural-Machine-Translation"><a href="#Greedy-Search-with-Probabilistic-N-gram-Matching-for-Neural-Machine-Translation" class="headerlink" title="[Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation]"></a>[Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation]</h2><p>引入可导的sentence level的评价指标，同时在此基础上训练过程也使用greedy search而不是teacher forcing，能够缓解传统训练方法带来的 exposure bias和word-level loss带来的training-test之间的指标不一致的问题。</p><h3 id="传统做法"><a href="#传统做法" class="headerlink" title="传统做法"></a>传统做法</h3><script type="math/tex; mode=display">P(\hat{\boldsymbol{y}} | \boldsymbol{x})=\prod_{j=1}^{T} p\left(\hat{y}_{j} | \hat{\boldsymbol{y}}_{<j}, \boldsymbol{x}, \theta\right)</script><script type="math/tex; mode=display">\boldsymbol{\theta}=\arg \max _{\theta}\{\mathcal{L}(\theta)\}</script><script type="math/tex; mode=display">\mathcal{L}(\theta)=\sum_{m=1}^{M} \sum_{j=1}^{l^{m}} \log \left(p\left(\hat{y}_{j}^{m} | \hat{\boldsymbol{y}}_{<j}^{m}, \boldsymbol{x}^{m}, \theta\right)\right)</script><h3 id="论文做法"><a href="#论文做法" class="headerlink" title="论文做法"></a>论文做法</h3><p>句子级别的evaluation metrics，比如BLEU是基于n-gram的匹配的。记$y$和$\hat{y}$是prediction和ground truth，长度分别为$T$和$T’$。</p><p>记n-gram $\boldsymbol{g}=\left(g_{1}, \ldots, g_{n}\right)$，则在$y$中计算ngram的count为：</p><script type="math/tex; mode=display">\mathrm{C}_{\boldsymbol{y}}(\boldsymbol{g})=\sum_{t=0}^{T-n} \prod_{i=1}^{n} 1\left\{g_{i}=y_{t+i}\right\}</script><p>在$y$与$\hat{y}$之间的count则为：</p><script type="math/tex; mode=display">\mathbf{C}_{\boldsymbol{y}}^{\hat{y}}(\boldsymbol{g})=\min \left(\mathbf{C}_{\boldsymbol{y}}(\boldsymbol{g}), \mathbf{C}_{\hat{\boldsymbol{y}}}(\boldsymbol{g})\right)</script><p>precision和recall则为：</p><script type="math/tex; mode=display">\begin{aligned} p_{n} &=\frac{\sum_{\boldsymbol{g} \in \boldsymbol{y}} \mathbf{C}_{\boldsymbol{y}}^{\hat{y}}(\boldsymbol{g})}{\sum_{\boldsymbol{g} \in \boldsymbol{y}} \mathbf{C}_{\boldsymbol{y}}(\boldsymbol{g})} \\ r_{n} &=\frac{\sum_{\boldsymbol{g} \in \boldsymbol{y}} \mathbf{C}_{\boldsymbol{y}}^{\hat{y}}(\boldsymbol{g})}{\sum_{\boldsymbol{g} \in \hat{\boldsymbol{y}}} \mathbf{C}_{\hat{\boldsymbol{y}}}(\boldsymbol{g})} \end{aligned}</script><p>而BLEU的计算方法为：</p><script type="math/tex; mode=display">\mathrm{BLEU}=\mathrm{BP} \cdot \exp \left(\sum_{n=1}^{N} w_{n} \log p_{n}\right)</script><p>BP为brevity penalty，$w_n$是n-gram的weight。</p><p>上述公式，可以看到将所有的输出的词一视同仁，但实际上在输出的不同的词是有不同的概率的，因此应该对n-gram count有更精细的描述，也即将预测的概率也引入：</p><script type="math/tex; mode=display">\widetilde{\mathbf{C}}_{\boldsymbol{y}}(\boldsymbol{g})=\sum_{t=0}^{T-n} \prod_{i=1}^{n} 1\left\{g_{i}=y_{t+i}\right\} \cdot p\left(y_{t+i} | \boldsymbol{y}_{<t+i}, \boldsymbol{x}, \theta\right)</script><p>因此bleu也更新为：</p><script type="math/tex; mode=display">\widetilde{\mathbf{C}}_{\boldsymbol{y}}^{\hat{\boldsymbol{y}}}(\boldsymbol{g})=\min \left(\widetilde{\mathbf{C}}_{\boldsymbol{y}}(\boldsymbol{g}), \mathbf{C}_{\hat{\boldsymbol{y}}}(\boldsymbol{g})\right)</script><script type="math/tex; mode=display">\tilde{p}_{n}=\frac{\sum_{\boldsymbol{g} \in \boldsymbol{y}} \widetilde{\mathbf{C}}_{\boldsymbol{y}}^{\hat{y}}(\boldsymbol{g})}{\sum_{\boldsymbol{g} \in \boldsymbol{y}} \widetilde{\mathbf{C}}_{\boldsymbol{y}}(\boldsymbol{g})}</script><script type="math/tex; mode=display">\mathrm{P}-\mathrm{BLEU}=\mathrm{BP} \cdot \exp \left(\sum_{n=1}^{N} w_{n} \log \tilde{p}_{n}\right)</script><p>最终的loss function为：</p><script type="math/tex; mode=display">\mathcal{L}(\theta)=-\sum_{m=1}^{M} \mathcal{P}\left(\boldsymbol{y}^{m}, \hat{\boldsymbol{y}}^{m}\right)</script><p>示意图：</p><p><img src="/images/15660541121139.jpg" width="80%" height="50%"></p><p>在这里，训练过程中也使用了greedy search而不是teacher forcing。且本文是使用baseline在probabilistic loss上fine-tune的，因为模型一开始的能力很弱，不足以翻译有意义的句子，没法训。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>作者做了一个实验探究是exposure bias的缓解还是采用了sentence-level的loss带来的提升？</p><p><img src="/images/15660541846618.jpg" width="50%" height="50%"></p><p>作者发现二者都有带来提升，不缓解exposure bias（也即使用了teacher forcing）能提升0.5，而使用greedy search则带来BLEU值1个点的提升。</p><h3 id="想法-1"><a href="#想法-1" class="headerlink" title="想法"></a>想法</h3><p>用了整体的指标且用了greedy search，也就缓解了传统训练方法的两个弊端。但这个方法显然没法从头训，因此需要fine-tune，但fine-tune是基于原先的模型行为进行改变，是否可能出现已经卡在minimal上的情况？如果能够找到一个方法从头训，可能会更好？</p><hr><h2 id="Insertion-Transformer-Flexible-Sequence-Generation-via-Insertion-Operations"><a href="#Insertion-Transformer-Flexible-Sequence-Generation-via-Insertion-Operations" class="headerlink" title="[Insertion Transformer: Flexible Sequence Generation via Insertion Operations]"></a>[Insertion Transformer: Flexible Sequence Generation via Insertion Operations]</h2><p>提出一种新的部分自回归的文本生成方式，基于插入操作，每次生成的词插入到文本任意位置。同时，不仅能够每次生成一个词，还能够同时生成多个词插入到不同位置。</p><p>例子：</p><p><img src="/images/15660543170148.jpg" width="90%" height="50%"></p><p>在每个iteration t，模型会产生一个联合概率分布，关于预测的content和插入位置l $l \in\left[0,\left|\hat{y}_{t}\right|\right]$。</p><h3 id="Insertion-Transformer-Model"><a href="#Insertion-Transformer-Model" class="headerlink" title="Insertion Transformer Model"></a>Insertion Transformer Model</h3><p>与transformer decoder不同的地方：<br>①去掉了mask，所有位置都能够attend到对方。<br>②标准transformer生成n个向量表示，将最后一个表示用于预测下一个词，而在这里需要生成n+1个表示，也即slot representation，每两个词之间共n-1个表示，加上开头与结尾两个。通过加前后两个特殊的标记符，过transformer后获得n+2个向量，并将每两个相邻的向量拼起来获得n+1个向量。</p><h4 id="Model-Variants"><a href="#Model-Variants" class="headerlink" title="Model Variants"></a>Model Variants</h4><p>模型有两个变体，一个是直接建模联合概率分布，另一个则是条件概率分布。</p><p>记$H \in \mathbb{R}^{(T+1) \times h}$为slot representation，T是当前的生成句子的长度，$W \in \mathbb{R}^{h \times|\mathcal{C}|}$则是softmax的映射矩阵。</p><p>因此可以直接建模联合概率分布：</p><script type="math/tex; mode=display">p(c, l)=\operatorname{softmax}(\text {flatten}(H W))</script><p>先计算$HW$然后展开过softmax，最大的值对应其vocab的位置以及相应的slot的位置。</p><p>另一种方法则是通过计算条件概率：</p><script type="math/tex; mode=display">p(c, l)=p(c | l) p(l)</script><p>其中$p(c | l)=\operatorname{softmax}\left(h_{l} W\right)$，而$p(l)=\operatorname{softmax}(H q)$，$q$是可学习的参数。</p><p>另外，为了增加slot之间的信息交流，可通过max-pooling获得上下文的向量$g$，再通过可学习的$V$生成一个共享的bias用于计算概率分布，也即：</p><script type="math/tex; mode=display">\begin{aligned} g &=\operatorname{maxpool}(H) \\ b &=g V \\ B &=\operatorname{repmat}(b,[T+1,1]) \\ p(c, l) &=\operatorname{softmax}(H W+B) \end{aligned}</script><h3 id="Training-and-Loss-Functions"><a href="#Training-and-Loss-Functions" class="headerlink" title="Training and Loss Functions"></a>Training and Loss Functions</h3><p>有两种顺序，一种是从左到右，另一种则是平衡二叉树的形式。</p><h4 id="Left-to-Right"><a href="#Left-to-Right" class="headerlink" title="Left-to-Right"></a>Left-to-Right</h4><p>给定训练样本$(x, y)$，先随机sample 长度$k \sim$ Uniform $([0,|y|])$，并且假定已经生成了左边k个词，$\hat{y}=\left(y_{1}, \dots, y_{k}\right)$，目标则是maximize location l=k的概率：</p><script type="math/tex; mode=display">\operatorname{loss}(x, \hat{y})=-\log p\left(y_{k+1}, k | x, \hat{y}\right)</script><h4 id="Balanced-Binary-Tree"><a href="#Balanced-Binary-Tree" class="headerlink" title="Balanced Binary Tree"></a>Balanced Binary Tree</h4><p>我们希望最中心的词先生成，然后左右两端的中心词，然后左右两端的左右两端的中心词，像一棵树那样。<br>如：$[ ] \rightarrow[D] \rightarrow[B, D, F] \rightarrow[A, B, C, D, E, F, G]$</p><p>为了达到这个目的，我们希望能够优先强调中心附近的词。</p><p>给定$(x, y)$，首先sample $y$的子集，为了保证不同长度被sample到的概率是一致的，先sample长度$k$，然后随机从$y$中sample $k$个词，$k$个词的呈现顺序是与$y$内一致的。</p><p>对于k+1个slot而言，我们需要计算这k+1个slot的loss。记$\left(y_{i_{l}}, y_{i_{l}+1}, \dots, y_{j_{l}}\right)$为当前尚未生成的位置的target word。先定义这之间每个位置到中心的距离：</p><script type="math/tex; mode=display">d_{l}(i)=\left|\frac{i_{l}+j_{l}}{2}-i\right|</script><p>接着使用该distance作为reward function：</p><script type="math/tex; mode=display">w_{l}(i)=\frac{\exp \left(-d_{l}(i) / \tau\right)}{\sum_{i^{\prime}=i_{l}}^{j_{l}} \exp \left(-d_{l}\left(i^{\prime}\right) / \tau\right)}</script><p>其中$\tau$趋近0时表示只考虑中心词，趋近∞则是uniform。</p><p>将其插入到loss里：</p><script type="math/tex; mode=display">slot-loss(x, \hat{y}, l)=\sum_{i=i_{l}}^{j_{l}}-\log p\left(y_{i}, l | x, \hat{y}\right) \cdot w_{l}(i)</script><p>这就强调了中心词要先被预测出来。</p><p>示意图：</p><p><img src="/images/15660935753744.jpg" width="70%" height="50%"></p><p>灰色的词是还没被预测的，红色的直方图是uniform的，经过reweight的是白色的。</p><p>最后我们将所有的位置的loss加起来：</p><script type="math/tex; mode=display">\operatorname{loss}(x, \hat{y})=\frac{1}{k+1} \sum_{l=0}^{k} \operatorname{slot}-\operatorname{loss}(x, \hat{y}, l)</script><h4 id="Uniform"><a href="#Uniform" class="headerlink" title="Uniform"></a>Uniform</h4><p>论文中还尝试了uniform的方式，也即不强调中心词：</p><script type="math/tex; mode=display">\operatorname{slot}-\operatorname{loss}(x, \hat{y}, l)=\frac{1}{j_{l}-i_{l}+1} \sum_{i=i_{l}}^{j_{l}}-\log p\left(y_{i}, l | x, \hat{y}\right)</script><p>其好处（没看懂）：<br>This neutral approach is useful insofar as it forces the model to be aware of all valid actions during each step of decoding, providing a rich learning signal during training and maximizing robustness；<br>Such an approach also bears resemblance to the principle of maximum entropy, which has successfully been employed for maximum entropy modeling across a number of domains in machine learning</p><h4 id="Termination"><a href="#Termination" class="headerlink" title="Termination"></a>Termination</h4><p>在训练过程中什么时候停止？<br>有两种：slot finalization 和 sequence finalization。</p><h5 id="slot-finalization"><a href="#slot-finalization" class="headerlink" title="slot finalization"></a>slot finalization</h5><p>当模型预测到某个location，该location对应的是在真实输出中不存在的span，那就直接将target设为end-of-slot token。<br>在测试阶段，当所有的slot都预测到end-of-slot token，则停止decode。</p><h5 id="sequence-finalization"><a href="#sequence-finalization" class="headerlink" title="sequence finalization"></a>sequence finalization</h5><p>在训练阶段，当遇到空span，则将该部分的loss定义为未定义，也即不纳入loss的计算。当整个句子被生成完了，将每个位置的loss设为end-of-sequence token的loss（相当于希望模型此时全都预测该token）。<br>在测试阶段，下面详谈。</p><h5 id="Training-Differences"><a href="#Training-Differences" class="headerlink" title="Training Differences"></a>Training Differences</h5><p>与普通autoregressive的模型相比的不同：<br>①generation step之间没有state的propagation，因为每生成完都要重新计算state，不能复用。<br>②普通的autoregressive在训练时可以一次性计算完所有的loss，而这里只能一次计算一个。因此更占内存。<br>③subsampling可能带来variance估计不准的问题。</p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><h4 id="Greedy-Decoding"><a href="#Greedy-Decoding" class="headerlink" title="Greedy Decoding"></a>Greedy Decoding</h4><p>每次选择最大的即可：</p><script type="math/tex; mode=display">\left(\hat{c}_{t}, \hat{l}_{t}\right)=\underset{c, l}{\operatorname{argmax}} p\left(c, l | x, \hat{y}_{t}\right)</script><p>如果是sequence finalization，则当直到end-of-sequence token被任意位置选中则停止。<br>如果是slot finalization，那么只在还没生成end-of-slot的位置上用argmax，直到所有位置都生成了end-of-slot。</p><h4 id="Parallel-Decoding"><a href="#Parallel-Decoding" class="headerlink" title="Parallel Decoding"></a>Parallel Decoding</h4><p>先计算每个位置的l的argmax：</p><script type="math/tex; mode=display">\hat{c}_{l, t}=\underset{c}{\operatorname{argmax}} p\left(c | l, x, \hat{y}_{t}\right)</script><p>如果是前面提到的条件概率$p(c, l)=p(l) p(c | l)$，那么此时$p(c | l)$已经有了；如果是直接计算联合概率分布则可通过$p(c | l)=p(c, l) / p(l)=p(c, l) / \sum_{c^{\prime}} p\left(c^{\prime}, l\right)$ 获得。</p><p>接着将已经生成了end-of-slot的location过滤掉，最后插入对应的预测词。</p><p>这种方法理论上可以只需要$\left\lfloor\log _{2} n\right\rfloor+ 1$个steps。</p><h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><h4 id="Part1"><a href="#Part1" class="headerlink" title="Part1"></a>Part1</h4><p><img src="/images/15660960806712.jpg" width="90%" height="50%"></p><p>括号都是报告的加了EOS最好的结果。<br>几个值得注意的点：<br>①发现EOS总是过早被生成，因此引入EOS penalty，也即将EOS的概率减掉一个β，当EOS的概率与概率第二高的差距超过β才真正生成EOS，发现确实有不错的提升。</p><p>②使用knowledge distillation有显著提升，teacher model是transformer baseline。</p><h4 id="Part2"><a href="#Part2" class="headerlink" title="Part2"></a>Part2</h4><p><img src="/images/15660961714055.jpg" width="60%" height="50%"></p><p>使用parallel decoding相比greedy decoding稍好一些，证明模型是有能力同时生成文本的。</p><p>parallel decoding的几个例子：</p><p><img src="/images/15660962394243.jpg" width="90%" height="50%"></p><h4 id="Part3"><a href="#Part3" class="headerlink" title="Part3"></a>Part3</h4><p><img src="/images/15660962832031.jpg" width="50%" height="50%"></p><p>在实验过程中发现parallel decoding确实能逼近下界。</p><h4 id="Part4"><a href="#Part4" class="headerlink" title="Part4"></a>Part4</h4><p>最后一个与其他related work的对比：</p><p><img src="/images/15660963462654.jpg" width="60%" height="50%"></p><p>可以看到其复杂度非常低。</p><hr><h2 id="本周论文小结"><a href="#本周论文小结" class="headerlink" title="[本周论文小结]"></a>[本周论文小结]</h2><p>本周由于要在组会讲机器翻译的论文，因此绝大多数都是相关的论文。其实大部分都是ACL19的best paper以及相关论文。目前机器翻译有几个有意思的方向如non-autoregressive NMT、unsupervised/semi-supervised NMT都是值得深入的。而实验室接下来也会围绕机器翻译做一些工作，我个人还是倾向将Curriculum Learninig与NMT结合。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Transformer </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> Curriculum Learning </tag>
            
            <tag> NMT </tag>
            
            <tag> exposure bias </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>佳句分享4</title>
      <link href="/2019/08/16/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BD%B3%E5%8F%A5%E5%88%86%E4%BA%AB4/"/>
      <url>/2019/08/16/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BD%B3%E5%8F%A5%E5%88%86%E4%BA%AB4/</url>
      
        <content type="html"><![CDATA[<h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><p>如果尖锐的批评完全消失，温和的批评将会变得刺耳。如果温和的批评也不被允许，沉默将被认为居心叵测。如果沉默也不再允许，赞扬不够卖力将是一种罪行。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词33</title>
      <link href="/2019/08/11/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D33/"/>
      <url>/2019/08/11/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D33/</url>
      
        <content type="html"><![CDATA[<h3 id="题龙阳县青草湖"><a href="#题龙阳县青草湖" class="headerlink" title="题龙阳县青草湖"></a>题龙阳县青草湖</h3><p>[元] 唐珙<br>西风吹老洞庭波，一夜湘君白发多。<br><strong>醉后不知天在水，满船清梦压星河。</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识27</title>
      <link href="/2019/08/10/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8627/"/>
      <url>/2019/08/10/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8627/</url>
      
        <content type="html"><![CDATA[<h3 id="Pytorch加速建议"><a href="#Pytorch加速建议" class="headerlink" title="[Pytorch加速建议]"></a>[Pytorch加速建议]</h3><p><a href="https://towardsdatascience.com/9-tips-for-training-lightning-fast-neural-networks-in-pytorch-8e63a502f565" target="_blank" rel="noopener">9 Tips For Training Lightning-Fast Neural Networks In Pytorch</a></p><p>其中几条比较有意思。</p><h4 id="Retained-graphs"><a href="#Retained-graphs" class="headerlink" title="Retained graphs"></a>Retained graphs</h4><p>在保存loss的时候，要注意：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bad</span></span><br><span class="line">losses.append(loss)</span><br><span class="line"><span class="comment"># good</span></span><br><span class="line">losses.append(loss.item())</span><br></pre></td></tr></table></figure><p>因为前者会保存整张计算图，占用大量内存。</p><h4 id="Moving-to-Multiple-GPUs"><a href="#Moving-to-Multiple-GPUs" class="headerlink" title="Moving to Multiple GPUs"></a>Moving to Multiple GPUs</h4><p>一般我们都是Split-batch，也即：</p><p><img src="/images/15654488718573.png" width="60%" height="50%"></p><p>实现Split-batch只需要使用Pytorch的DataParallel。</p><p>而另一个方法是Split Model Training，也即：</p><p><img src="/images/15654489576969.png" width="50%" height="50%"></p><p>比如说翻译模型有encoder与decoder部分，那么可以将encode/decoder放在不同GPU。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># each model is sooo big we can't fit both in memory</span></span><br><span class="line">encoder_rnn.cuda(<span class="number">0</span>)</span><br><span class="line">decoder_rnn.cuda(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run input through encoder on GPU 0</span></span><br><span class="line">encoder_out = encoder_rnn(x.cuda(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># run output through decoder on the next GPU</span></span><br><span class="line">out = decoder_rnn(encoder_out.cuda(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># normally we want to bring all outputs back to GPU 0</span></span><br><span class="line">out = out.cuda(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>当然也可以将二者结合起来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># change these lines</span></span><br><span class="line">self.encoder = RNN(...)</span><br><span class="line">self.decoder = RNN(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># to these</span></span><br><span class="line"><span class="comment"># now each RNN is based on a different gpu set</span></span><br><span class="line">self.encoder = DataParallel(self.encoder, devices=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">self.decoder = DataParallel(self.encoder, devices=[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># in forward...</span></span><br><span class="line">out = self.encoder(x.cuda(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># notice inputs on first gpu in device</span></span><br><span class="line">sout = self.decoder(out.cuda(<span class="number">4</span>))  <span class="comment"># &lt;--- the 4 here</span></span><br></pre></td></tr></table></figure><h4 id="Faster-multi-GPU-training-on-a-single-node"><a href="#Faster-multi-GPU-training-on-a-single-node" class="headerlink" title="Faster multi-GPU training on a single node"></a>Faster multi-GPU training on a single node</h4><p>在单个机器也使用DistributedDataParallel，DistributedDataParallel会比DataParallel更快，DistributedDataParallel只需要gradient的同步。</p><hr><h3 id="Python-assert"><a href="#Python-assert" class="headerlink" title="[Python assert]"></a>[Python assert]</h3><p><a href="https://www.oschina.net/translate/when-to-use-assert" target="_blank" rel="noopener">Python 使用断言的最佳时机</a></p><p>对一个函数而言，一般情况下，断言用于检查函数输入的合法性，要求输入满足一定的条件才能继续执行;</p><p>assert时候的几个情况：<br>防御性的编程：为了防范由于以后的代码变更而发生错误<br>运行时对程序逻辑的检测<br>合约性检查（比如前置条件，后置条件）<br>程序中的常量<br>检查文档</p><hr><h3 id="Python-global关键字"><a href="#Python-global关键字" class="headerlink" title="[Python global关键字]"></a>[Python global关键字]</h3><p>对于什么时候必须要使用global关键字。如果函数内部只使用变量而不修改/赋值，则不需要显式在函数内部global；否则就必须要global。因为内部做了改变（赋值），编译器会认为该变量是函数内部的local变量而不是外部的全局变量。</p><p>如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Classifier(args)</span><br><span class="line">optimizer=torch.optim.SGD(model.parameters())</span><br><span class="line">...</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> model  <span class="comment"># 必须显式加global，因为在main内部做了改变</span></span><br><span class="line">    model.train()</span><br><span class="line">    train(model,optimizer)  <span class="comment"># optimizer由于没有在main函数内部改变，因此不需要global</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
            <tag> assert </tag>
            
            <tag> global </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文27</title>
      <link href="/2019/08/10/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8727/"/>
      <url>/2019/08/10/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8727/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>Curriculum Learning for Domain Adaptation in Neural Machine Translation</li><li>Dynamically Composing Domain-Data Selection with Clean-Data Selection by “Co-Curricular Learning” for Neural Machine Translation</li><li>Pseudo-Labeling Curriculum for Unsupervised Domain Adaptation</li><li>Curriculum Learning for Natural Answer Generation</li><li>MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels</li></ol><h2 id="Curriculum-Learning-for-Domain-Adaptation-in-Neural-Machine-Translation"><a href="#Curriculum-Learning-for-Domain-Adaptation-in-Neural-Machine-Translation" class="headerlink" title="[Curriculum Learning for Domain Adaptation in Neural Machine Translation]"></a>[Curriculum Learning for Domain Adaptation in Neural Machine Translation]</h2><p>提出将CL用于翻译任务上的domain adaptation。</p><p>任务：有些领域相关的翻译没有那么多平行语料，一般有的都是general domain的，还有的就是不知道是什么领域的，比如从网络上爬下来的数据paracrawl，本文就是希望能够更好地利用这些未知领域的数据以训练更好的特定领域模型。</p><p>其基本做法是：先训一个generic的模型，然后作为initialization。接着从这些unlabeled domain的数据，根据与特定领域数据集的相似程度排序，然后利用CL从相似性最高的到相似性低的训练。本文是第一个将CL用于domain adaptation。</p><p>Pipeline：</p><p><img src="/images/15654423291460.jpg" width="60%" height="50%"></p><h3 id="排序指标-Domain-Similarity-Scoring"><a href="#排序指标-Domain-Similarity-Scoring" class="headerlink" title="排序指标(Domain Similarity Scoring)"></a>排序指标(Domain Similarity Scoring)</h3><p>第一个问题即，如何判断哪些句子是domain相关的？</p><p>记$I$是in-domain的数据；$N$是unlabeled domain的数据，排序所要做的就是根据$I$的数据按照相似度来排序，并取前n个用于训练。</p><p>本文使用了两种指标。</p><h4 id="Moore-Lewis-Method"><a href="#Moore-Lewis-Method" class="headerlink" title="Moore-Lewis Method"></a>Moore-Lewis Method</h4><p>对于每个在$N$里面的句子$s$都会分配一个分数：</p><script type="math/tex; mode=display">H_{I}(s)-H_{N}(s)</script><p>其中$H_{I}$是在$I$上训练的语言模型的cross entropy；$H_{N}$是在从$N$上随机采样的大小与$I$相近的数据所训练得到的语言模型的cross entropy。</p><p>该指标越小，代表句子$s$与in-domain数据集$I$更相近。</p><h4 id="Cynical-Data-Selection"><a href="#Cynical-Data-Selection" class="headerlink" title="Cynical Data Selection"></a>Cynical Data Selection</h4><p>循环选择句子构建training corpus来大致对$I$建模。每个iteration，每个sentence都会计算将其加入到已经构建好的training corpus所带来的cross-entropy decrease，而带来最大decrease的句子则被选中，这里的cross-entropy是previously selected n-sentence corpus与I之间的。</p><h3 id="Curriculum-Learning-Training-Strategy"><a href="#Curriculum-Learning-Training-Strategy" class="headerlink" title="Curriculum Learning Training Strategy"></a>Curriculum Learning Training Strategy</h3><p>首先根据上述指标排序，然后切分成多个shards；训练过程是切分成若干个phase的，每个phase只能用部分的数据；在第一phase，只能用最简单的shard训练，然后随着phase的增加，增加剩余最简单的shard；注意到数据呈现的顺序不是固定的，并不是严格简单到难，而是随机从当前phase能看到的shard中采样。这样能够带来一定的随机性，对模型训练有帮助。</p><p>在实验中是将数据分成了40个shards，每个phase随机采样1000个batch。</p><hr><h2 id="Dynamically-Composing-Domain-Data-Selection-with-Clean-Data-Selection-by-“Co-Curricular-Learning”-for-Neural-Machine-Translation"><a href="#Dynamically-Composing-Domain-Data-Selection-with-Clean-Data-Selection-by-“Co-Curricular-Learning”-for-Neural-Machine-Translation" class="headerlink" title="[Dynamically Composing Domain-Data Selection with Clean-Data Selection by “Co-Curricular Learning” for Neural Machine Translation]"></a>[Dynamically Composing Domain-Data Selection with Clean-Data Selection by “Co-Curricular Learning” for Neural Machine Translation]</h2><p>提出将domain-data selection与clean-data selection结合，并提出联合课程学习，用以翻译任务上的domain adaptation。</p><p>任务背景描述：domain-data selection是从大数据集上根据特定领域的小数据集选择与该特定领域最相关的数据用以扩充数据量来训练；同理clean-data selection则是选择干净/噪声较少的数据来训练。</p><p>本文将这二者的数据选择结合起来并用于CL的训练。</p><h3 id="domain-noise的测量"><a href="#domain-noise的测量" class="headerlink" title="domain/noise的测量"></a>domain/noise的测量</h3><p>一般是使用language model来衡量。</p><p>假设我们有一个general-domain的语言模型$\widetilde{\vartheta}$和一个in-domain的语言模型$\widehat{\vartheta}$，对于一个句子而言其domain相关性就有：</p><script type="math/tex; mode=display">\varphi(x ; \widetilde{\vartheta}, \widehat{\vartheta})=\frac{\log P(x ; \widehat{\vartheta})-\log P(x ; \widetilde{\vartheta})}{|x|}</script><p>同样，假设我们有$\widetilde{\theta}$的NMT baseline模型，训练于有噪声的数据；$\widehat{\theta}$则是在干净的数据上基于$\widetilde{\theta}$ fine-tune的NMT模型，则衡量noise level有：</p><script type="math/tex; mode=display">\phi(x, y ; \widetilde{\theta}, \widehat{\theta})=\frac{\log P(y | x ; \widehat{\theta})-\log P(y | x ; \widetilde{\theta})}{|y|}</script><h3 id="问题设置"><a href="#问题设置" class="headerlink" title="问题设置"></a>问题设置</h3><p><br>记$\widetilde{D_{X Y}}$是背景双语数据集，可以是从网上爬下来的，有不同domain和noise；$D_{X}^{\mathrm{ID}}$是in-domain的单语语料，可以较小，我们可以用这个来对domain relevance来排序；$\widehat{D_{X Y}^{\mathrm{OD}}}$是小的，可信任（干净）的out-of-domain的平行语料，我们可以用这份数据来语料的噪声程度做排序；$\widehat{D_{X Y}^{\mathrm{ID}}}$则是双语的in-domain语料，我们在这里假设in-domain的双语预料不存在，希望借助其他三个来学会domain adaptation。</p><h3 id="Co-Curricular-Learning"><a href="#Co-Curricular-Learning" class="headerlink" title="Co-Curricular Learning"></a>Co-Curricular Learning</h3><p>$\mathcal{D}_{\lambda}^{\phi}(t, D)$ 是一个selection function，在时间t上返回top λ(t)的经过排序的数据作为训练。</p><p>这里使用pace function：</p><script type="math/tex; mode=display">\lambda(t)=0.5^{t / H}</script><p>注意这是随着时间递减的。也即我们希望遇到后面越domain相关/数据越干净。一开始是训练一个general的表示，可以理解成是一个initialization，然后后面再用好的数据去fine-tune。</p><p>由于既要考虑clean-data又要考虑in-domain data，所以就引出两个策略。</p><h4 id="Mixed-Co-Curriculum"><a href="#Mixed-Co-Curriculum" class="headerlink" title="Mixed Co-Curriculum"></a>Mixed Co-Curriculum</h4><p>我们可以直接将clean-data selection与in-domain selection的分数直接加起来作为总的排名分数：</p><script type="math/tex; mode=display">\psi(x, y)=\phi(x, y)+\varphi(x)</script><p>然后再用同一个pace-function选择在时间t下的topk个数据用以训练。</p><p>但这个方法的问题在于这两个分数并不一定是同一个量级的，不一定是一个分布族，不应该直接加起来。</p><h4 id="Cascaded-Co-Curriculum"><a href="#Cascaded-Co-Curriculum" class="headerlink" title="Cascaded Co-Curriculum"></a>Cascaded Co-Curriculum</h4><p>另一种则是分别定义二者的pace function，然后结合起来。</p><script type="math/tex; mode=display">\beta(t)=0.5^{t / F},\gamma(t)=0.5^{t / G}</script><p>分别是二者的pace function。</p><p>因此结合起来，做两个的交集：</p><script type="math/tex; mode=display">\left(\mathcal{D}_{\gamma}^{\varphi} \circ \mathcal{D}_{\beta}^{\phi}\right)\left(t, \widetilde{D_{X Y}}\right)=\mathcal{D}_{\gamma}^{\varphi}\left(t, \mathcal{D}_{\beta}^{\phi}\left(t, \widetilde{D_{X Y}}\right)\right)</script><p>所以每个sample的weight为：</p><script type="math/tex; mode=display">W_{t}(x, y)=\left\{\begin{array}{ll}{\frac{1}{\left|\mathcal{D}_{\gamma}^{\varphi} \circ \mathcal{D}_{\beta}^{\phi}\right|}} & {\text { if }(x, y) \in \mathcal{D}_{\gamma}^{\varphi} \circ \mathcal{D}_{\beta}^{\phi}} \\ {0} & {\text { otherwise }}\end{array}\right.</script><p>以下是一个toy example：</p><p><img src="/images/15654446572190.jpg" width="50%" height="50%"></p><p>也即只保留两个指标都选择保留的数据。</p><h3 id="CL-optimization"><a href="#CL-optimization" class="headerlink" title="CL optimization"></a>CL optimization</h3><p>该算法如何优化？本文采用EM-style的算法，这是一个迭代的过程。</p><p><img src="/images/15654447383111.jpg" width="50%" height="50%"></p><p>模型在更新过程中还会反过来重新更新scoring function。</p><h3 id="一些思考"><a href="#一些思考" class="headerlink" title="一些思考"></a>一些思考</h3><p>原先的CL是从简单到难。这里的是从不相关/不干净 到 相关/干净， 其motivation是希望一开始训练一个general的表示，然后再不断地通过越来越相关/干净的数据去fine-tune。而传统CL则是希望先通过简单的数据使模型更平稳地发展。二者似乎出发点不同，如果严格地说，本文不应该算是一种传统CL（从简单到难）？</p><p>实际上论文里面也提到了，这更像是一种multi-task transfer learning，假设每个时间t都是一个task，本文是定义了一系列的task $\mathcal{M}=\left\langle m_{1}, \ldots, m_{t} \ldots, m_{f}\right\rangle$，其中最后的task才是我们感兴趣的（也即in-domain），而中间的task都是一系列的垫脚石，希望能够将知识一步一步地transfer到最终的task上，这也是为什么随着训练过程的进行可用数据越来越少的原因。</p><p>那么传统CL与这种multi-task transfer learning之间是否矛盾的地方？二者的motivation听起来都很合理，他们之间是否本质上有一个联结点能够使二者不矛盾？</p><p>其实想一想似乎也不矛盾，主要看最终的目的是啥，以及标准是啥。CL是先从简单到难，最终是希望在同一个数据集上表现更好；而本文的transfer leanring则不同，是在general数据集上逐渐引导至特定domain，希望在特定domain的数据集上表现好，而不是希望在训练的general domain上表现好。二者的评测标准是不同的，出发点是不同的，因此应该是不矛盾的。</p><hr><h2 id="Pseudo-Labeling-Curriculum-for-Unsupervised-Domain-Adaptation"><a href="#Pseudo-Labeling-Curriculum-for-Unsupervised-Domain-Adaptation" class="headerlink" title="[Pseudo-Labeling Curriculum for Unsupervised Domain Adaptation]"></a>[Pseudo-Labeling Curriculum for Unsupervised Domain Adaptation]</h2><p>本文提出一种利用伪标签加上CL训练达到图像分类domain adaptation（领域自适应）的目的。</p><p>任务介绍：给定有标签的source，以及无标签的target，其中source与target的领域是不同的，任务的目的是能够利用有标签的source训练得到能够有迁移到target领域能力的模型。</p><p>做法：本文利用之前文章CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images提到的density-based clustering，先将图像根据density分类，然后首先对density较大的图像打标签，并利用这些图像训练分类器，然后再对density小一些的打标签，不断更新与打标签训练，最终获得由迁移能力的模型。</p><p><img src="/images/15654455360307.jpg" width="80%" height="50%"></p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="/images/15654455687375.jpg" width="70%" height="50%"></p><p>记source samples $D_{\mathcal{S}}=\left\{\left(x_{i}^{S}, y_{i}^{S}\right)\right\}_{i=1}^{N_{S}}$ 是有标签的；target samples $D_{\mathcal{T}}=\left\{x_{i}^{t}\right\}_{i=1}^{N_{t}}$ 是无标签的$\hat{\boldsymbol{y}}_{\boldsymbol{i}}^{\boldsymbol{t}}$ 则是伪标签，目的是训练一个$C_t$能够将target samples分类。</p><p>$G_f$是共享的特征提取器，$C_t$与$C_s$分别是source/target的分类器，$G_d$是二分类器，输入source或者target，能够判断输入是source还是target samples。</p><p>一开始，我们训练$G_f$，$C_s$，$G_d$，因为一开始模型能力弱，打标签的可靠性低：</p><script type="math/tex; mode=display">J_{1}\left(\theta_{f}, \theta_{d}, \theta_{s}\right)=\frac{1}{N_{s}} \sum_{i=1}^{N_{s}} L_{y}\left(C_{s}\left(f_{i}^{s}\right), y_{i}^{s}\right)-\frac{\lambda}{N_{s}} \sum_{i=1}^{N_{s}} L_{d}\left(G_{d}\left(f_{i}^{s}\right), d_{i}\right)-\frac{\lambda}{N_{t}} \sum_{i=1}^{N_{t}} L_{d}\left(G_{d}\left(f_{i}^{t}\right), d_{i}\right)</script><p>其中$f$就是$G_f$的输出。</p><p>接着，通过之前提到的聚类方法，将target聚为3类，$D_{e}, D_{m},D_{h}$。</p><p>给local density高的打标签，假设local density的更为可靠，我们打完这些标签后就开始利用这些来训练$C_s$，然后更新参数，注意到这里更新的是所有与之相关的参数，也包括$G_f$等。</p><p>更新完了模型重新对数据打标签，此时就可以将$D_m$的数据用以训练了，然后训练完再更新完标签，就可以将$D_h$用以训练了。</p><p>这里的思想有点借鉴了GAN了。</p><hr><h2 id="Curriculum-Learning-for-Natural-Answer-Generation"><a href="#Curriculum-Learning-for-Natural-Answer-Generation" class="headerlink" title="[Curriculum Learning for Natural Answer Generation]"></a>[Curriculum Learning for Natural Answer Generation]</h2><p>将CL应用于NAG的训练上。</p><p>NAG这个任务我不熟，只讨论CL，仍然是从指标定义与训练策略两步讨论。</p><h3 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h3><p>定义复杂度的指标：<br>通过两个指标，将sample分为两类：target instance与common instance。</p><h4 id="Term-Frequency-Selector"><a href="#Term-Frequency-Selector" class="headerlink" title="Term Frequency Selector"></a>Term Frequency Selector</h4><p>通过answer的term frequency，如果frequency太大，说明这个词太过于普通，小的TF说明这个词有价值，但太小可能说明是噪声，所以也要设个下限。</p><h4 id="Grammar-Selector"><a href="#Grammar-Selector" class="headerlink" title="Grammar Selector"></a>Grammar Selector</h4><p>利用Stanford parser score作为grammar的metric，拥有好的grammar的句子通常有更高的分数，同时由于短句容易获得更高的分数，因此按比例让short与long answer的比例各占0.5。</p><p>被选中的就是target instance，也就是比较难的，否则就是common instance，也即比较简单的。</p><h3 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h3><p>希望模型一开始学习简单结构的，短的答案，使得模型学会基本的QA model，然后学会更复杂的内容，能够有能力生成自然的答案，所以先用common instance，然后再用target instance。</p><p>采用的是概率采样的schedul。记$Q_{c},Q_{t}$ 分别是common和target instance。一开始希望概率$w_{Q_{c}} \gg w_{Q_{t}}$，然后逐渐$w_{Q_{c}}$减小而$w_{Q_{t}}$增加，最后$w_{Q_{c}} \ll w_{Q_{t}}$。</p><p>因此我们有：</p><script type="math/tex; mode=display">w_{Q_{t}}=\left(\frac{\text {epoch}_{t}}{ | \text {epoch} |}\right)^{2},w_{Q_{c}}=1-w_{Q_{t}}</script><hr><h2 id="MentorNet-Learning-Data-Driven-Curriculum-for-Very-Deep-Neural-Networks-on-Corrupted-Labels"><a href="#MentorNet-Learning-Data-Driven-Curriculum-for-Very-Deep-Neural-Networks-on-Corrupted-Labels" class="headerlink" title="[MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels]"></a>[MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels]</h2><p>提出通过训练另一个MentorNet网络来辅助训练网络，用以解决在受污染label的数据上训练的问题。<br>论文提出的CL是能够数据驱动的，但本质上并不是CL而是SPL。<br>其实这篇后面的理论没怎么看懂。</p><p>基本做法是有一个辅助网络MentorNet，能够给予当前sample到的数据以一定权重，该权重是动态的，而不像传统SPL那样设定一个固定的schedule λ，小于λ的就用于训练，这是由人来确定schedule的，忽略了被训练的网络的反馈，而MentorNet则是数据驱动的，同时MentorNet与StudentNet是可以同时训练更新的。</p><p>因此，MentorNet的输出就是每个sample的weight。他主要有两个可学习的地方，一个是学习预定义好的curriculum；另一个就是从数据中学。</p><h3 id="可学习的Curriculum"><a href="#可学习的Curriculum" class="headerlink" title="可学习的Curriculum"></a>可学习的Curriculum</h3><h4 id="学习预定义的curriculum"><a href="#学习预定义的curriculum" class="headerlink" title="学习预定义的curriculum"></a>学习预定义的curriculum</h4><p>实际上就是在拟合预定义的curriculum的值。</p><script type="math/tex; mode=display">g_{m}\left(\mathbf{z}_{i} ; \Theta^{*}\right)=\left\{\begin{array}{ll}{\mathbb{1}\left(\ell_{i} \leq \lambda_{1}\right)} & {\lambda_{2}=0} \\ {\min \left(\max \left(0,1-\frac{\ell_{i}-\lambda_{1}}{\lambda_{2}}\right), 1\right)} & {\lambda_{2} \neq 0}\end{array}\right.</script><p>$g_{m}$即MentorNet的输出。我们在给定$\mathbf{z}_{i}=\phi\left(\mathbf{x}_{i}, y_{i}, \mathbf{w}\right)$，也即每个sample的一些feature，模型能够输出和预定义的curriculum一样的值。</p><h4 id="从数据中学习Curriculum"><a href="#从数据中学习Curriculum" class="headerlink" title="从数据中学习Curriculum"></a>从数据中学习Curriculum</h4><p>我们希望从数据集中$\left\{\left(\phi\left(\mathbf{x}_{i}, y_{i}, \mathbf{w}\right), v_{i}^{*}\right)\right\}$学习如何打分，其中v=1表示该label是正确的的。但显然我们不能直接从训练数据中获得该数据集，因为可能存在被污染label的数据。因此论文的做法则是从另一个可信任的小数据集上学习，相当于知识迁移。比如我们可以在CIFAR-10上训练获得这样一个MentorNet然后在CIFAR-100上用。</p><p>那么要如何学？</p><p><img src="/images/15654473154044.jpg" width="60%" height="50%"></p><p>MentorNet在输入epoch的百分比以及label后，过一个embedding；然后另一边将过去的loss以及loss的变化多少过一个双向的LSTM，论文中取LSTM的step size=1，也即只考虑一次过去的变化。然后二者cat起来进入fc层，最终获得了一个概率。其中最后一层是dropout，是论文提出的burn-in的过程。burn-in指的是一开始在训练的前20% epoch让MentorNet固定输出$g_{m}\left(\mathbf{z}_{i} ; \Theta^{*}\right)=r_{i}$，其中$r_{i}$是伯努利分布的采样结果，相当于让training sample随机被dropout p%，这样能让StudentNet更稳定预测，且专注于学习简单的pattern （why？）。</p><h3 id="一点思考"><a href="#一点思考" class="headerlink" title="一点思考"></a>一点思考</h3><p>从data中学习一个curriculum，也即去尝试给定一定的feature去拟合其label的正确概率，这有点像是meta-leanring在做的事情；同时在cifar10拟合在cifar100使用，也有点迁移学习的感觉。但这篇的本质并不是真正的CL，而是SPL，因为没有预先按照difficulty排序。同时，这让我想起CurriculumNet，CurriculumNet做的也是从不靠谱的数据中学到一个好的模型，他的做法则是利用聚类将数据分为三类然后按顺序训练；而这里则是通过知识迁移，在一个靠谱的数据集上训feature，然后在另一个上面时候用，这也不失为另一种思路，但如果不是同一类型的数据集如cifar10与cifar100，是否能够迁移也不好说。最后，这篇论文可能写得有些绕了，看起来有点累。</p><hr><h2 id="本周论文小结"><a href="#本周论文小结" class="headerlink" title="[本周论文小结]"></a>[本周论文小结]</h2><p>本周继续专注于Curriculum Learning的论文。但比较巧的是本周的论文都是将CL应用于比较实际的问题，如corrupted label或者domain adaptation。对于这样的实际问题，更多需要的恰恰是训练方法上的创新，而CL正好是属于这方面的方法，对于解决这样实际问题，如何引入CL是值得关注的。corrupted label切中了当前网络数据很多但噪声很大这一痛点，如何解决这一问题，利用好大数据，或许比设计精巧的模型所能带来的提升更大；而domain adaptation同样也是一大痛点，虽然数据很多，但部分领域相关的数据稀缺，比如翻译领域能收集到的都是新闻的平行语料，或者小语种的数据集很难收集，特定专业领域如医疗的数据也难以获取。因此可以多多关注此类论文，思考如何解决这些实际问题。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> Curriculum Learning </tag>
            
            <tag> Domain Adaptation </tag>
            
            <tag> Pseudo Labeling </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Curriculum Learning schedule 总结</title>
      <link href="/2019/08/09/%E8%AE%BA%E6%96%87/Curriculum%20Learning%20schedule%E6%80%BB%E7%BB%93/"/>
      <url>/2019/08/09/%E8%AE%BA%E6%96%87/Curriculum%20Learning%20schedule%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>Curriculum Learning主要有两块：①sample的难度定义；②设定特定的schedule</p><p>本文总结一下不同论文中出现的Curriculum Learning schedule，并将其分为几类。</p><p>不定期更新…</p><h2 id="论文总结"><a href="#论文总结" class="headerlink" title="论文总结"></a>论文总结</h2><h3 id="Early"><a href="#Early" class="headerlink" title="Early"></a>Early</h3><h4 id="Curriculum-Learning"><a href="#Curriculum-Learning" class="headerlink" title="Curriculum Learning"></a>Curriculum Learning</h4><p>在shape recognition任务上是达到switch epoch或者直到触发early stopping换难的数据</p><h4 id="Learning-to-execute"><a href="#Learning-to-execute" class="headerlink" title="Learning to execute"></a>Learning to execute</h4><p>naive strategy: 当valid没有提升时增加难度；mix: 选择一个难度范围，随机sample；前二者的结合：部分是naive部分是mix</p><h3 id="2015"><a href="#2015" class="headerlink" title="2015"></a>2015</h3><h4 id="Self-Paced-Curriculum-Learning"><a href="#Self-Paced-Curriculum-Learning" class="headerlink" title="Self-Paced Curriculum Learning"></a>Self-Paced Curriculum Learning</h4><p>本质是self-paced learning</p><h4 id="Curriculum-learning-of-multiple-tasks"><a href="#Curriculum-learning-of-multiple-tasks" class="headerlink" title="Curriculum learning of multiple tasks"></a>Curriculum learning of multiple tasks</h4><p>SVM模型直到收敛再换难的数据</p><h3 id="2016"><a href="#2016" class="headerlink" title="2016"></a>2016</h3><h4 id="Visualizing-and-Understanding-Curriculum-Learning-for-Long-Short-Term-Memory-Networks"><a href="#Visualizing-and-Understanding-Curriculum-Learning-for-Long-Short-Term-Memory-Networks" class="headerlink" title="Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks"></a>Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks</h4><p>直到p个epoch或收敛再增加较难的数据。</p><p><img src="/images/15653615039236.jpg" width="36%" height="50%"></p><h3 id="2107"><a href="#2107" class="headerlink" title="2107"></a>2107</h3><h4 id="Automated-Curriculum-Learning-for-Neural-Networks"><a href="#Automated-Curriculum-Learning-for-Neural-Networks" class="headerlink" title="Automated Curriculum Learning for Neural Networks"></a>Automated Curriculum Learning for Neural Networks</h4><p> RL自动选择task(multi-task)</p><h4 id="Curriculum-Learning-for-Multi-Task-Classification-of-Visual-Attributes"><a href="#Curriculum-Learning-for-Multi-Task-Classification-of-Visual-Attributes" class="headerlink" title="Curriculum Learning for Multi-Task Classification of Visual Attributes"></a>Curriculum Learning for Multi-Task Classification of Visual Attributes</h4><p>直到loss最小再切换至另一数据</p><h4 id="Curriculum-Learning-and-Minibatch-Bucketing-in-Neural-Machine-Translation"><a href="#Curriculum-Learning-and-Minibatch-Bucketing-in-Neural-Machine-Translation" class="headerlink" title="Curriculum Learning and Minibatch Bucketing in Neural Machine Translation"></a>Curriculum Learning and Minibatch Bucketing in Neural Machine Translation</h4><p>分为多个bin，当第一个bin的数据被取得剩下第二个bin的一半，添加第二个bin的数据，从两个bin剩下的样例中取，直到剩下的和第三个bin样例数一样。以此类推</p><h4 id="Teacher-Student-Curriculum-Learning"><a href="#Teacher-Student-Curriculum-Learning" class="headerlink" title="Teacher-Student Curriculum Learning"></a>Teacher-Student Curriculum Learning</h4><p>按概率，概率与速率相关，RL动态选择能让模型提升速率最大的task，当速率下降，概率也下降，当某个task的score下降了，说明他忘了这部分的知识，又提升该sample的概率</p><p><img src="/images/15653616659179.jpg" width="70%" height="50%"></p><h3 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h3><h4 id="An-Empirical-Exploration-of-Curriculum-Learning-for-Neural-Machine-Translation"><a href="#An-Empirical-Exploration-of-Curriculum-Learning-for-Neural-Machine-Translation" class="headerlink" title="An Empirical Exploration of Curriculum Learning for Neural Machine Translation"></a>An Empirical Exploration of Curriculum Learning for Neural Machine Translation</h4><p>将数据分为多个shard，训练的每个phase增加一个shard的数据。</p><p><img src="/images/15653627775691.jpg" width="50%" height="50%"></p><h4 id="LEARNING-TO-TEACH"><a href="#LEARNING-TO-TEACH" class="headerlink" title="LEARNING TO TEACH"></a>LEARNING TO TEACH</h4><p>利用RL帮助模型选择数据，也即有一个teacher模型给数据打标签，1代表给学生model训练，0则被抛弃掉</p><h4 id="Curriculum-Learning-for-Natural-Answer-Generation"><a href="#Curriculum-Learning-for-Natural-Answer-Generation" class="headerlink" title="Curriculum Learning for Natural Answer Generation"></a>Curriculum Learning for Natural Answer Generation</h4><p>根据概率从两个shard采样，采样概率会随着epoch而改变。</p><script type="math/tex; mode=display">\begin{aligned} w_{Q_{t}} &=\left(\frac{\text { epoch}_{t}}{ | \text {epoch}_{t} |}\right)^{2} \\ w_{Q_{c}} &=1-w_{Q_{t}} \end{aligned}</script><h3 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h3><h4 id="Curriculumnet-Weakly-supervised-learning-from-large-scale-web-images"><a href="#Curriculumnet-Weakly-supervised-learning-from-large-scale-web-images" class="headerlink" title="Curriculumnet: Weakly supervised learning from large-scale web images"></a>Curriculumnet: Weakly supervised learning from large-scale web images</h4><p>分成3个shard，到收敛后添加新的shard</p><h4 id="On-The-Power-of-Curriculum-Learning-in-Training-Deep-Network"><a href="#On-The-Power-of-Curriculum-Learning-in-Training-Deep-Network" class="headerlink" title="On The Power of Curriculum Learning in Training Deep Network"></a>On The Power of Curriculum Learning in Training Deep Network</h4><p>设定固定iteration数后增加数据</p><p><img src="/images/15654067211386.jpg" width="60%" height="50%"></p><h4 id="Reinforcement-Learning-based-Curriculum-Optimization-for-Neural-Machine-Translation"><a href="#Reinforcement-Learning-based-Curriculum-Optimization-for-Neural-Machine-Translation" class="headerlink" title="Reinforcement Learning based Curriculum Optimization for Neural Machine Translation"></a>Reinforcement Learning based Curriculum Optimization for Neural Machine Translation</h4><p>将数据切分为多个bin，利用RL动态选择训练哪个bin</p><h4 id="Dynamic-Curriculum-Learning-for-Imbalanced-Data-Classification"><a href="#Dynamic-Curriculum-Learning-for-Imbalanced-Data-Classification" class="headerlink" title="Dynamic Curriculum Learning for Imbalanced Data Classification"></a>Dynamic Curriculum Learning for Imbalanced Data Classification</h4><p>将数据分为几个shard，按概率采样，有schedule调整概率</p><p><img src="/images/15654068084345.jpg" width="40%" height="50%"></p><h4 id="Simple-and-Effective-Curriculum-Pointer-Generator-Networks-for-Reading-Comprehension-over-Long-Narratives"><a href="#Simple-and-Effective-Curriculum-Pointer-Generator-Networks-for-Reading-Comprehension-over-Long-Narratives" class="headerlink" title="Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives"></a>Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives</h4><p>当没有提升时，换部分数据到简单的数据set内，直到所有数据都被换，再将整个数据集换成另一个。</p><p><img src="/images/15654069250310.jpg" width="34%" height="50%"></p><h4 id="Dynamically-Composing-Domain-Data-Selection-with-Clean-Data-Selection-by-“Co-Curricular-Learning”-for-Neural-Machine-Translation"><a href="#Dynamically-Composing-Domain-Data-Selection-with-Clean-Data-Selection-by-“Co-Curricular-Learning”-for-Neural-Machine-Translation" class="headerlink" title="Dynamically Composing Domain-Data Selection with Clean-Data Selection by “Co-Curricular Learning” for Neural Machine Translation"></a>Dynamically Composing Domain-Data Selection with Clean-Data Selection by “Co-Curricular Learning” for Neural Machine Translation</h4><p>单调递减的光滑曲线控制当前可用于训练数据的大小。</p><h4 id="Curriculum-Learning-for-Domain-Adaptation-in-Neural-Machine-Translation"><a href="#Curriculum-Learning-for-Domain-Adaptation-in-Neural-Machine-Translation" class="headerlink" title="Curriculum Learning for Domain Adaptation in Neural Machine Translation"></a>Curriculum Learning for Domain Adaptation in Neural Machine Translation</h4><p>分成shard，每个训练的phase增加一个shard。</p><h4 id="Pseudo-Labeling-Curriculum-for-Unsupervised-Domain-Adaptation"><a href="#Pseudo-Labeling-Curriculum-for-Unsupervised-Domain-Adaptation" class="headerlink" title="Pseudo-Labeling Curriculum for Unsupervised Domain Adaptation"></a>Pseudo-Labeling Curriculum for Unsupervised Domain Adaptation</h4><p>分多个shard，收敛后加shard</p><h4 id="Competence-based-Curriculum-Learning-for-Neural-Machine-Translation"><a href="#Competence-based-Curriculum-Learning-for-Neural-Machine-Translation" class="headerlink" title="Competence-based Curriculum Learning for Neural Machine Translation"></a>Competence-based Curriculum Learning for Neural Machine Translation</h4><p>给定一个单调递增的曲线控制当前可用于训练的数据的比例。</p><p><img src="/images/15654072010953.jpg" width="50%" height="50%"></p><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>按大类有三种：一种控制训练可见的数据；一种控制当前训练数据被sample的概率；另一种RL直接替模型选择。</p><p>对于第一类而言，可分成：</p><ol><li>给定schedule<ol><li>每k个step就增加p%可见的数据</li><li>将数据分为m个shard，每k个step/phase则增加一个shard</li><li>其他更精细的schedule。如sample到剩多少时添加新数据（Curriculum Learning and Minibatch Bucketing in Neural Machine Translation），或swap新数据到旧数据里（Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives）</li></ol></li><li>达到某个条件：到epoch数或收敛就增加一个shard或换数据（Curriculum learning of multiple tasks）</li></ol><p>对于第二类而言，可分为：</p><ol><li>定义一个平滑schedule函数，与epoch或step挂钩。比如，Curriculum Learning for Natural Answer Generation或Dynamic Curriculum Learning for Imbalanced Data Classification。一般此类都是将数据分为几个块，概率是定义在块上而不是sample上</li><li>RL控制，与当前模型对该数据的表现相关（Teacher-Student Curriculum Learning）</li></ol><p>对于第三类RL而言，较为丰富：<br>有确定multi-task的task顺序的；<br>有从多个预定义的bin中动态选择哪个bin的数据的；<br>从被sample到的batch中选择可让模型训练的；<br>还有动态控制所有数据被sample的概率。</p><p>未来可以做一个表格，将属于某类的都归在一起。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Curriculum Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识26</title>
      <link href="/2019/08/04/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8626/"/>
      <url>/2019/08/04/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8626/</url>
      
        <content type="html"><![CDATA[<h3 id="Python"><a href="#Python" class="headerlink" title="[Python]"></a>[Python]</h3><p>Python工程的文件夹的init的作用是设置对外开放的接口（之前一直不知道😅）。</p><hr><h3 id="Mac"><a href="#Mac" class="headerlink" title="[Mac]"></a>[Mac]</h3><p>解决Mac Mojave在浏览网页时出现的”您的浏览器限制了第三方Cookie“：</p><p><a href="https://chuill.com/post/107.html" target="_blank" rel="noopener">https://chuill.com/post/107.html</a></p><hr><h3 id="iTerm"><a href="#iTerm" class="headerlink" title="[iTerm]"></a>[iTerm]</h3><p>使用iTerm在词与词之间跳转：</p><p><a href="https://coderwall.com/p/h6yfda/use-and-to-jump-forwards-backwards-words-in-iterm-2-on-os-x" target="_blank" rel="noopener">https://coderwall.com/p/h6yfda/use-and-to-jump-forwards-backwards-words-in-iterm-2-on-os-x</a></p><hr><h3 id="Linux"><a href="#Linux" class="headerlink" title="[Linux]"></a>[Linux]</h3><p>如何cd进入开头为‘-’的文件夹：在文件夹前输入‘- -’</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> -- -1</span><br></pre></td></tr></table></figure><p><a href="https://segmentfault.com/q/1010000005684176" target="_blank" rel="noopener">https://segmentfault.com/q/1010000005684176</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> Mac </tag>
            
            <tag> iTerm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词32</title>
      <link href="/2019/08/04/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D32/"/>
      <url>/2019/08/04/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D32/</url>
      
        <content type="html"><![CDATA[<h3 id="鹊踏枝-·-谁道闲情抛掷久"><a href="#鹊踏枝-·-谁道闲情抛掷久" class="headerlink" title="鹊踏枝 · 谁道闲情抛掷久"></a>鹊踏枝 · 谁道闲情抛掷久</h3><p>[五代十国] 冯延巳<br>谁道闲情抛掷久？每到春来，惆怅还依旧。日日花前常病酒，不辞镜里朱颜瘦。<br>河畔青芜堤上柳，为问新愁，何事年年有？<strong>独立小楼风满袖，平林新月人归后</strong>。</p><hr><h3 id="怨情"><a href="#怨情" class="headerlink" title="怨情"></a>怨情</h3><p>[唐] 李白<br>美人卷珠帘，深坐颦蛾眉。<br>但见泪痕湿，不知心恨谁。</p><hr><h3 id="登乐游原"><a href="#登乐游原" class="headerlink" title="登乐游原"></a>登乐游原</h3><p>[唐] 李商隐<br>向晚意不适，驱车登古原。<br><strong>夕阳无限好，只是近黄昏</strong>。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>记又一次debug的血泪教训</title>
      <link href="/2019/08/04/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E8%AE%B0%E5%8F%88%E4%B8%80%E6%AC%A1debug%E7%9A%84%E8%A1%80%E6%B3%AA%E6%95%99%E8%AE%AD/"/>
      <url>/2019/08/04/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E8%AE%B0%E5%8F%88%E4%B8%80%E6%AC%A1debug%E7%9A%84%E8%A1%80%E6%B3%AA%E6%95%99%E8%AE%AD/</url>
      
        <content type="html"><![CDATA[<p>这次的血泪教训就是，shuffle真的很重要！</p><p>训练模型发现一个问题：train集和dev集的performance差别很大。</p><p>我的心路历程：</p><ol><li>怀疑自己的iterator写错了，将其换成dataloader之后，仍然有此问题。</li><li>有没有可能是模型的问题？ 尝试换成以前写过的模型，但仍然有这个现象</li><li>难道是数据集的问题？我尝试print了几条数据，一切正常，也没有出现大量UNK。</li><li>是我训练方法出现问题？<ol><li>尝试调整学习率。当调大学习率时，训练集的performance非常差，只有10%，而valid集有35%；当调小学习率时，训练集的performance有70%，而valid集仍然只有30%左右。</li><li>尝试不clip。训练集的performance变高了，但valid仍然很差。训练多几个epoch也是一样。</li><li>我把训练集的数据换成验证集的数据，用验证集来训练。神奇的是，居然不会过拟合。训练的performance甚至比验证的performance更差，但明明是同一个数据集训练的。</li></ol></li><li>难道是我统计的问题？<ol><li>会不会是我在统计loss和accuracy出现问题？ 但肉眼debug了半天，完全没找到问题。</li><li>打印一下predict的值，发现一个batch内部预测的全部都是一样的label。是模型分类的问题？但不应该啊，明明check过模型了，应该不是模型的锅</li><li>嗯？难道是label的问题？统计一下label的分布，确实label类别不平衡，但也在合理范围内。</li><li>试试打印一下batch的label吧。 …这下发现，为什么batch内部的label全部都是同一类的？难怪predict时也是同一类。是不是数据本来就是按label顺序排的？</li></ol></li></ol><p>oh，不会是没有shuffle吧。check了一下代码，发现我的iterator是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, self.data_len, self.batch_size):</span><br><span class="line">        start = i</span><br><span class="line">        end = min(self.data_len, i + self.batch_size)</span><br><span class="line">        sents = self.data[<span class="string">'sents'</span>][start:end]</span><br><span class="line">        labels = self.data[<span class="string">'labels'</span>][start:end]</span><br><span class="line">        batch = &#123;<span class="string">'sents'</span>: sents, <span class="string">'labels'</span>: labels&#125;</span><br><span class="line">        batch = convert2tensor(batch)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> batch</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> self.shuffle:</span><br><span class="line">        start_state = random.getstate()</span><br><span class="line">        random.shuffle(self.data[<span class="string">'sents'</span>])</span><br><span class="line">        random.setstate(start_state)</span><br><span class="line">        random.shuffle(self.data[<span class="string">'labels'</span>])</span><br></pre></td></tr></table></figure><p>有shuffle啊。等等，我好像是在iter完后再shuffle的。但这真的有问题吗？可能第一次没shuffle，但后面都shuffle了为啥performance还是没有上去？</p><p>反正找不到bug，那就试一下吧，我把shuffle代码放在前面。结果是，训练过程的结果变正常了…</p><p>想了一下原因。<br>第一，数据真的非常需要shuffle，一些数据集是按照label来排的，假如不shuffle直接按顺序喂数据会出现什么情况呢？模型一开始接触到的都是同一类的数据，那么就会疯狂overfit到该类的pattern，等到他有机会接触下一类后，可能已经陷入local minimum了，再怎么训都雷打不动了，这也就是为什么我在训练过程中当调高学习率时出现training set在10%而valid set在30%，这是因为模型已经无法拟合training set的其他类了；或者如果学习率小，那么还没overfit得很严重，在接触到下一类时，仍然会很快overfit到该类，在一个epoch后，模型会overfit到最后一个接触的类，此时做evaluation，当然结果好不到哪里去，但在training set上却还可以。</p><p>第二，为什么在第一个epoch后做shuffle，在后续的epoch模型仍然不正常呢？和上述的原因可能相同，因为overfit到一个特定类的pattern了，再也无法从local minimum出来了。假如学习率调的足够小，或许还有救，但一般我们用的都是Adam之类的，在训练过程中学习率可能越变越大，到最后可能还是没救。</p><p>所以啊，一定不要忘记shuffle！</p><p>最后贴上我在debug过程中的记录，来纪念这次的血泪教训。</p><p><img src="/images/debug%20record.png" width="80%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 遇到的问题 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Transformer中position在代码实现的一点思考</title>
      <link href="/2019/08/04/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8ETransformer%E4%B8%ADposition%E5%9C%A8%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%B8%80%E7%82%B9%E6%80%9D%E8%80%83/"/>
      <url>/2019/08/04/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8ETransformer%E4%B8%ADposition%E5%9C%A8%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%B8%80%E7%82%B9%E6%80%9D%E8%80%83/</url>
      
        <content type="html"><![CDATA[<p>Transformer需要显式加position，不同版本的代码有不同创建position的方式。前几天在实现中遇到了一些小问题，因此记录在此。</p><p>过去在获取position的时候，一般都是在外部创建，也即在做batch的时候创建。具体而言：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(batch)</span>:</span></span><br><span class="line">    sentences, labels = [], []</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> batch:</span><br><span class="line">        sentences.append(b[<span class="number">0</span>])</span><br><span class="line">        labels.append(b[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    max_len = max(len(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> sentences)</span><br><span class="line">    batch_seq = np.array([</span><br><span class="line">        sent + [<span class="number">0</span>] * (max_len - len(sent))</span><br><span class="line">        <span class="keyword">for</span> sent <span class="keyword">in</span> sentences</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    batch_pos = np.array([</span><br><span class="line">        [pos_i + <span class="number">1</span> <span class="keyword">if</span> word_i != <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">         <span class="keyword">for</span> pos_i, word_i <span class="keyword">in</span> enumerate(sent)] <span class="keyword">for</span> sent <span class="keyword">in</span> batch_seq</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    batch_label = np.array(labels)</span><br><span class="line"></span><br><span class="line">    batch_seq = torch.LongTensor(batch_seq)</span><br><span class="line">    batch_pos = torch.LongTensor(batch_pos)</span><br><span class="line">    batch_label = torch.LongTensor(batch_label)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> batch_seq, batch_pos, batch_label</span><br></pre></td></tr></table></figure><p><code>collate_fn</code>是Dataloader的参数，具体的作用是在获取了sample和label后显式padding以及转换为tensor等操作。</p><p>而在模型训练的时候，则将position作为参数显式传入forward里：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model(batch_seq, batch_pos)</span><br></pre></td></tr></table></figure><p>这是一种方法，但我认为看起来不大优雅。假设以下情形：我希望将Transformer作为encoder来分类，同时希望与LSTM作为encoder来对比。但LSTM不需要position，且position作为参数已经写死在函数里了，这样一来，就不能很方便地换模块了。</p><p>因此，我希望能够在模型内部创建position，使得传入的参数只有<code>batch_seq</code>，这样更加优雅。</p><p>我一开始的办法是封装一个<code>EmbeddingLayer</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbeddingLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args, pretrained_matrix=None)</span>:</span></span><br><span class="line">        super(EmbeddingLayer, self).__init__()</span><br><span class="line">        self.word_embedding = nn.Embedding(args.vocab_size, args.embed_dim, padding_idx=<span class="number">0</span>)</span><br><span class="line">        self.embed_factor = args.embed_dim ** (<span class="number">1</span> / <span class="number">2</span>)</span><br><span class="line">        self.dropout = nn.Dropout(args.dropout)</span><br><span class="line">        <span class="keyword">if</span> pretrained_matrix <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            pretrained_matrix = torch.from_numpy(pretrained_matrix).type(torch.FloatTensor)</span><br><span class="line">            self.word_embedding.weight = nn.Parameter(pretrained_matrix, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        n_position = args.max_sent_len + <span class="number">1</span></span><br><span class="line">        self.position_embedding = nn.Embedding.from_pretrained(</span><br><span class="line">            get_sinusoid_encoding_table(n_position, args.embed_dim, padding_idx=<span class="number">0</span>),</span><br><span class="line">            freeze=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, seq)</span>:</span></span><br><span class="line">        x = self.word_embedding(seq) * self.embed_factor</span><br><span class="line"></span><br><span class="line">        pos = self._create_pos(seq)</span><br><span class="line">        x = x + self.position_embedding(pos)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_pos</span><span class="params">(self, seq)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param seq: batch,seq</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        seq_pos = np.array([</span><br><span class="line">             [pos_i + <span class="number">1</span> <span class="keyword">if</span> word_i != <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">              <span class="keyword">for</span> pos_i, word_i <span class="keyword">in</span> enumerate(sent)] <span class="keyword">for</span> sent <span class="keyword">in</span> seq</span><br><span class="line">         ])</span><br><span class="line">        <span class="keyword">return</span> seq_pos.to(seq.device)</span><br></pre></td></tr></table></figure><p>也即在EmbeddingLayer内部根据seq来生成position。</p><p>虽然逻辑上是正确的，但看GPU的利用率总是很低。排查了很久之后，才发现上述<code>_create_pos(self, seq)</code>的效率太低了。</p><p>看起来似乎没什么问题，这和<code>collate_fn</code>的做法是一样的。但为什么就是效率那么低？</p><p>思考了一下，我推测是因为，model内部一般都是tensor操作，一般都是在GPU上的操作；而<code>_create_pos</code>作为model内部的函数，却是做了在CPU上的操作。这样一来，本来整个GPU流水线的运算，却要等这个CPU的操作完成了才能往下做。这就导致了利用率低下的问题。</p><p>为了验证我的想法，我尝试将代码改成tensor的操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_create_pos</span><span class="params">(self, seq)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param seq: batch,seq</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    batch_size, max_seq_len = seq.size()</span><br><span class="line">    pos_tensor = torch.arange(<span class="number">1</span>, max_seq_len + <span class="number">1</span>)[<span class="keyword">None</span>, :].expand(batch_size, <span class="number">-1</span>).long()  <span class="comment"># batch,max_seq_len we don't do padding</span></span><br></pre></td></tr></table></figure><p>首先上述代码是完全的tensor操作，是在GPU上完成的；第二，我意识到其实不需要显式将sequence中padding的位置对应的position也置为0。因为只要padding的位置不参与计算loss就没有关系；或者将padding的操作交给后续的mask来做即可。</p><p>这样改一下，发现GPU利用率终于大幅提升，基本上95%以上。</p><p>那么另一问题又来了，为啥之前<code>collate_fn</code>的做法，也即在外部创建pos没问题，同样的做法移到model内部来就有问题？</p><p>我推测，第一，在CPU上的操作是可以多线程的，这边CPU边创建新的batch，另一边model在GPU边训练，这两个（CPU和GPU）实质上是并行的而不是串行的；第二，如果将其移到内部操作，就变成了外部CPU创建好batch，传入model后，model内部还要经历 GPU —&gt; CPU —&gt; GPU的操作，显然CPU会拖累整个进度，且导致利用率低下的问题。</p><hr><p>就这么一个小小的问题，我就debug了一天，一开始怀疑是我自己的iterator写的效率低，然后换成了官方的Dataloader且开了10个进程，但仍然有问题；后来怀疑是模型写错了，check了半天都没有发现哪里有问题；然后是开Pycharm的profile查看是哪些函数占用太多时间，但因为<code>_create_pos</code>本身是没有运行很长时间的，只是阻碍了GPU的运行效率，因此也没有发现这个问题；也是到最后，尝试不用position后，才发现效率有大幅提升。</p><p>从这次的debug经历，有两个深刻教训：①model内部尽可能利用GPU而不要做太多的CPU操作，否则就可能出现上述问题；②在debug要大胆尝试控制变量，而不要仅仅是肉眼去看代码找bug。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录18</title>
      <link href="/2019/08/04/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9518/"/>
      <url>/2019/08/04/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9518/</url>
      
        <content type="html"><![CDATA[<h3 id="不定长iterator的写法"><a href="#不定长iterator的写法" class="headerlink" title="[不定长iterator的写法]"></a>[不定长iterator的写法]</h3><p>不确定iterator的长度的写法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Iter</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span><span class="params">(self)</span>:</span></span><br><span class="line">        span_length = self.c * self.data_len</span><br><span class="line">        rand_index = random.sample(list(range(math.ceil(span_length))), self.batch_size)</span><br><span class="line">        sampled_sents = [self.data[<span class="string">'sents'</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> rand_index]</span><br><span class="line">        sampled_labels = [self.data[<span class="string">'labels'</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> rand_index]</span><br><span class="line">        batch = &#123;<span class="string">'sents'</span>: sampled_sents, <span class="string">'labels'</span>: sampled_labels&#125;</span><br><span class="line">        batch = convert2tensor(batch)</span><br><span class="line">        <span class="keyword">return</span> batch</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文26</title>
      <link href="/2019/08/04/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8726/"/>
      <url>/2019/08/04/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8726/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>Dynamic Curriculum Learning for Imbalanced Data Classification</li><li>Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives</li><li>CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images</li></ol><h2 id="Dynamic-Curriculum-Learning-for-Imbalanced-Data-Classification"><a href="#Dynamic-Curriculum-Learning-for-Imbalanced-Data-Classification" class="headerlink" title="[Dynamic Curriculum Learning for Imbalanced Data Classification]"></a>[Dynamic Curriculum Learning for Imbalanced Data Classification]</h2><p>提出CL专注于分类中的不平衡问题。但实际上应该不算是CL。</p><p>本质是sampling+loss调整。核心思想是：先让模型学到一个正确的general representation，然后再让模型学会分类。</p><h4 id="四种schedule的变体"><a href="#四种schedule的变体" class="headerlink" title="四种schedule的变体"></a>四种schedule的变体</h4><p>先是提出四种schedule的变体，在下文中会用到。</p><p><img src="/images/15648828137885.jpg" width="50%" height="50%"></p><h4 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h4><script type="math/tex; mode=display">\begin{array}{c}{\mathcal{L}_{\mathrm{DSL}}=-\frac{1}{N} \sum_{j=1}^{M} \sum_{i=1}^{N_{j}} w_{j} * \log \left(p\left(y_{i, j}=\overline{y}_{i, j} | \mathbf{x}_{i, j}\right)\right)} \\ {w_{j}=\left\{\begin{array}{ll}{\frac{D_{j}(l)}{B_{j}}} & {\text { if } \frac{D_{j}(l)}{B_{j}} \geq 1} \\ {0 / 1} & {\text { if } \frac{D_{j}(l)}{B_{j}}<1}\end{array}\right.}\end{array}</script><p>其中：$D(l)=D_{t r a i n} g(l)$，$g(l)$是递减函数（上述四种schedule function）。</p><p>上述公式的意思即对sample得到的batch内类的分布的重调整。假如当前的batch中第j个类期望的分布比整个训练分布${B_{j}}$大，则直接加强其weight；否则就做sample，被sample到的设为1，没被sample到的设为0。保证batch内的分布和期望的分布一致。一开始时，期望分布和总体分布一致，随着训练过程的进行，$g(l)$下降，到最后则是所有的类的期望分布是一致的。</p><p>这种imbalance到balance的motivation是，如果一开始就balance，会丢弃掉大量数据的有用信息，过于强调minority的特性，容易overfitting。<br>论文提到这里有CL的特性（也即从简单到难），我猜测作者的意思是数量多的class相对于数量少的class平均更为简单，因为更容易学。否则我没看出来有什么CL的思想。</p><h4 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h4><p>对loss加了一项正则化metric learning loss，该方法是其他论文的方法（但我没看懂这个公式的具体作用），大概意思就是能够’avoid the dominant effect of majority classes‘，即减小数量大的类的主导影响。并且论文对该loss做了一定改进。</p><p>从：</p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{crl}}=\frac{\sum_{T} \max \left(0, m_{j}+d\left(\mathbf{x}_{a l l, j}, \mathbf{x}_{+, j}\right)-d\left(\mathbf{x}_{a l l, j}, \mathbf{x}_{-, j}\right)\right)}{|T|}</script><p>改为：</p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{TEA}}=\frac{\sum_{T} \max \left(0, m_{j}+d\left(\mathbf{x}_{e a s y, j}, \mathbf{x}_{+, j}\right)-d\left(\mathbf{x}_{e s y, j}, \mathbf{x}_{-, j}\right)\right)}{|T|}</script><p>该公式的作用：targets at learning a soft feature embedding to separate different samples in feature space without assigning labels。</p><p>因此总的loss由cross entropy与triplet loss组成，而组成的比例由$f$控制。</p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{DCL}}=\mathcal{L}_{\mathrm{DSL}}+f(l) * \mathcal{L}_{\mathrm{TEA}}</script><script type="math/tex; mode=display">f(l)=\left\{\begin{array}{ll}{\frac{1}{2} \cos \left(\frac{l}{L} \pi\right)+\frac{1}{2}+\epsilon} & {\text { if } l<p L} \\ {\epsilon} & {\text { if } l \geq p L}\end{array}\right.</script><p>最后，论文还将DCL作为一种框架做泛化，将几个算法都容纳进来：几种算法都是他的一种特殊情况。</p><p><img src="/images/15648834974285.jpg" width="60%" height="50%"></p><h4 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h4><p>本文的出发点是不错的，思想很简单有效，sampling部分的schedule我觉得不错，也可能是以前有前人的基础了。但如果说是CL，我觉得关系不是很大，虽然有输入数据的schedule，但没有事先的按难度排，感觉更像是是self-paced learning。</p><hr><h2 id="Simple-and-Effective-Curriculum-Pointer-Generator-Networks-for-Reading-Comprehension-over-Long-Narratives"><a href="#Simple-and-Effective-Curriculum-Pointer-Generator-Networks-for-Reading-Comprehension-over-Long-Narratives" class="headerlink" title="[Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives]"></a>[Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives]</h2><p>提出一种新的框架用以长文本的QA。具体模型我并不关心，这里讨论的主要是CL的策略。</p><p>在CL方面，提出Diverse curriculum learning scheme，通过两个指标来定义difficulty。answerability与understandability)</p><h4 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h4><h5 id="Answerability"><a href="#Answerability" class="headerlink" title="Answerability"></a>Answerability</h5><p>对于一个sample来说，基于question抽取的就是hard setting；基于answer的则是easy setting。<br>“we consider the set of documents retrieved based on questions as the hard setting, H. Conversely, the set of retrieved documents using answers is regarded as the easy setting, E.”</p><h5 id="Understandability"><a href="#Understandability" class="headerlink" title="Understandability"></a>Understandability</h5><p>依据document的长度来划分。预定义好的几个size，$\{50,100,200,500\}$。</p><p>所以按照size以及answerability将sample划分几个chunk。</p><script type="math/tex; mode=display">\begin{aligned} k & \in\{50,100,200,500\} \\ E_{n} & \leftarrow F(\text { corpus, answer, } n) \\ H_{n} & \leftarrow F(\text { corpus, question }, n) \end{aligned}</script><p>也即小于50的有easy和hard两个chunk，同理其他size的一样。</p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>原则：从小的size到大的size，从easy setting到hard setting。论文认为Answerability应该优先于Understandability考虑。</p><p>具体而言：从$E_{50}$这个chunk开始，当valid没有提升时，将小百分比δ（比如5%)的数据从难的swap到简单的chunk里（$H_k$到$E_k$）以提升answerability，当这么做1/δ次之后，chunk的所有数据就都是hard的了，然后此时将整个chunk swap，换成下一个size的$E_k$。重复步骤。</p><p><img src="/images/15648849009112.jpg" width="50%" height="50%"></p><h4 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h4><p>通过定义难度的<strong>多维度</strong>将其分为多个chunk，并且提出了比较精巧的scheme（swap的思路与之前的Curriculum Learning and Minibatch Bucketing in Neural Machine Translation有些类似）。<br><strong>diverse+chunk的思路</strong></p><hr><h2 id="CurriculumNet-Weakly-Supervised-Learning-from-Large-Scale-Web-Images"><a href="#CurriculumNet-Weakly-Supervised-Learning-from-Large-Scale-Web-Images" class="headerlink" title="[CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images]"></a>[CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images]</h2><p>提出了一种高效的方法能够在大量弱监督的网络图片（数据来源：输入query所获得的网络图片而不是人工标注的）上训练的方法，引入了CL。并且发现noisy label的数据也能够提升表现，并且有regularization的作用。</p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p><img src="/images/15648851046110.jpg" width="60%" height="50%"></p><p>三部曲：首先在整个数据上训练一个模型，能够大致学得图片的representation；接着通过该初始模型，将每张图片映射到一个feature space，其潜在的结构和之间的关系能够被挖掘，也即能够将图片按照复杂度排序且分成subset；第三，从简单到难训练，也即先训clean label的数据，然后不断添加noisy的数据。</p><h3 id="initial-feature-generation"><a href="#initial-feature-generation" class="headerlink" title="initial feature generation"></a>initial feature generation</h3><p>在整个数据集上训练一个模型。</p><h3 id="curriculum-design"><a href="#curriculum-design" class="headerlink" title="curriculum design"></a>curriculum design</h3><p>假设：在每个category下，clean label的数据将会有较为接近的表示，而noisy label的数据则variance/diversity很大。</p><p>则我们可以通过聚类的方法将同一个category下的数据分为三个subset，分别对应clean; noisy 与highly noisy。</p><p>具体如何做/聚类？</p><p>首先CNN获得的feature进入fc层获得一个深层的feature space的表示</p><script type="math/tex; mode=display">P_{i} \rightarrow f\left(P_{i}\right)</script><p>接着计算欧氏距离，获得一个任意i到j之间的距离矩阵</p><script type="math/tex; mode=display">D_{i j}=\left\|f\left(P_{i}\right)-f\left(P_{j}\right)\right\|^{2}</script><p>此时，计算每张image的local density：</p><script type="math/tex; mode=display">\begin{array}{c}{\rho_{i}=\sum_{j} X\left(D_{i j}-d_{c}\right)}  {\qquad X(d)=\left\{\begin{array}{ll}{1} & {d<0} \\ {0} & {\text { other }}\end{array}\right.}\end{array}</script><p>也即，距离小于$d_c$的累计1。$d_c$是通过排序后取第$k$%得来的。则$\rho$越大代表其与其他image更接近。</p><p>接着计算：</p><script type="math/tex; mode=display">\delta_{i}=\left\{\begin{array}{ll}{\min _{j : \rho_{j}>\rho_{i}}\left(D_{i j}\right)} & {i f \exists j \text { s.t. } \rho_{j}>\rho_{i}} \\ {\max \left(D_{i j}\right)} & {\text { otherwise }}\end{array}\right.</script><p>也即，若对于图片i，若存在比他local density更大的，将与该最大local density的图片的距离设为图片i的距离；否则，就说明该图片就是cluster center。</p><p>那么有了中心之后，就可以聚类了，$δ$大的表示其就是中心。可以根据图像到中心的距离大小分为三类（这里不清楚是否理解得对）。对于一个类而言，大density意味着分布更密集；而小density则更加分散。</p><p>可以看到，clean data的分布会更加密集而noisy data更加分散：</p><p><img src="/images/15648859217378.jpg" width="40%" height="50%"></p><h3 id="Curriculum-Learning"><a href="#Curriculum-Learning" class="headerlink" title="Curriculum Learning"></a>Curriculum Learning</h3><p>当分好三类后，训练就是不断将noisy数据放入到clean数据训练的过程。</p><p><img src="/images/15648859490417.jpg" width="50%" height="50%"></p><p>注意subset是将不同category的同一类（clean/noisy/highly noisy)放在一起。</p><p>实验证明，noisy data能够增强模型的泛化性，使得模型不会overfitting到clean data；同时由于模型已经先在clean data上训练得到一个较好的模型了，因此也不会收到noisy data的不好的影响。</p><h3 id="思考-1"><a href="#思考-1" class="headerlink" title="思考"></a>思考</h3><p>我觉得这个思路不错，在category内部划分subset，能够更好保证clean data聚在一起。聚类倒不是很重要，因为可以有不同的聚类方法。同时，本篇论文证明了先训练一个能够获得representation表示且足够好到可以聚类是可行的。而CL训练方法上则没有什么新的东西，本质上就是分为几个shard/bucket，然后随着训练过程mix data，这在之前的论文An Empirical Exploration of Curriculum Learning for Neural Machine Translation还有其他的CL的论文里面都有。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Classification </tag>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> Curriculum Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识25</title>
      <link href="/2019/07/28/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8625/"/>
      <url>/2019/07/28/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8625/</url>
      
        <content type="html"><![CDATA[<h3 id="fairseq-CUDA"><a href="#fairseq-CUDA" class="headerlink" title="[fairseq CUDA]"></a>[fairseq CUDA]</h3><p>常常遇到fairseq跑翻译，跑着跑着就print很多tensor，显示CUDA问题。</p><p>尝试添加<code>--ddp-backend=no_c10d</code>之后，似乎就不会出现该问题了。也即：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node <span class="number">4</span> \</span><br><span class="line">train.py --ddp-backend=no_c10d ...</span><br></pre></td></tr></table></figure><hr><h3 id="Overleaf"><a href="#Overleaf" class="headerlink" title="[Overleaf]"></a>[Overleaf]</h3><p>一般overleaf并不会产生中间文件，假如确实需要bbl而不是bib文件（比如提交到arxiv时就需要bbl），可以点击compile旁边的log and other files，翻到最下面就可以选择下载bbl。</p><p><a href="https://tex.stackexchange.com/questions/462314/overleaf-v2-how-to-get-bbl-file" target="_blank" rel="noopener">https://tex.stackexchange.com/questions/462314/overleaf-v2-how-to-get-bbl-file</a></p><hr><h3 id="Arxiv"><a href="#Arxiv" class="headerlink" title="[Arxiv]"></a>[Arxiv]</h3><p>arxiv上传时出现warning的问题：‘This version (v8.31) of natbib is stricter in its formatting requirements for bibitem entries than the previous version used at arXiv (v7.1)’。</p><p>检查一下，可能是bbl与主文件的名字不同，改成相同的可能就可以了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> fairseq </tag>
            
            <tag> CUDA </tag>
            
            <tag> Arxiv </tag>
            
            <tag> LaTeX </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文25</title>
      <link href="/2019/07/26/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8725/"/>
      <url>/2019/07/26/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8725/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>Easy Samples First: Self-paced Reranking for Zero-Example Multimedia Search</li><li>Self-Paced Learning with Diversity</li><li>Self-paced dictionary learning for image classification</li><li>Self-Paced Learning for Matrix Factorization</li><li>Self-Paced Boost Learning for Classification</li></ol><h2 id="Easy-Samples-First-Self-paced-Reranking-for-Zero-Example-Multimedia-Search"><a href="#Easy-Samples-First-Self-paced-Reranking-for-Zero-Example-Multimedia-Search" class="headerlink" title="[Easy Samples First: Self-paced Reranking for Zero-Example Multimedia Search]"></a>[Easy Samples First: Self-paced Reranking for Zero-Example Multimedia Search]</h2><p>将self-paced learning用于reranking任务中。并且将self-paced learning更一般化，也即将原先的{0,1} weight扩展到实数域。</p><p>具体而言，原先的self-paced learning：</p><script type="math/tex; mode=display">\left(\mathbf{w}_{t+1}, \mathbf{v}_{t+1}\right)=\underset{\mathbf{w} \in \mathbb{R}^{d}, \mathbf{v} \in\{0,1\}^{n}}{\operatorname{argmin}}\left(r(\mathbf{w})+\sum_{i=1}^{n} v_{i} f\left(\mathbf{x}_{i}, \mathbf{y}_{i} ; \mathbf{w}\right)-\frac{1}{K} \sum_{i=1}^{n} v_{i}\right)</script><p>其中 $v\in\{0,1 \}$。后项的正则项为 $f(\mathbf{v} ; k)=-\frac{1}{k}|\mathbf{v}|_{1}=-\frac{1}{k} \sum_{i=1}^{n} v_{i}$</p><p>得出的解为：</p><script type="math/tex; mode=display">v_{i}^{*}=\left\{\begin{array}{ll}{1} & {\frac{1}{m} \sum_{j=1}^{m} C \ell_{i j}<\frac{1}{k}} \\ {0} & {\frac{1}{m} \sum_{j=1}^{m} C \ell_{i j} \geq \frac{1}{k}}\end{array}\right.</script><p>而现在：<br>①线性，也即与loss是线性相关的：</p><script type="math/tex; mode=display">f(\mathbf{v} ; k)=\frac{1}{k}\left(\frac{1}{2}\|\mathbf{v}\|_{2}^{2}-\sum_{i=1}^{n} v_{i}\right)</script><p>得出的解为：</p><script type="math/tex; mode=display">v_{i}^{*}=\left\{\begin{array}{ll}{-k\left(\frac{1}{m} \sum_{j=1}^{m} C \ell_{i j}\right)+1} & {\frac{1}{m} \sum_{i=1}^{m} C \ell_{i j}<\frac{1}{k}} \\ {0} & {\frac{1}{m} \sum_{i=1}^{m} C \ell_{i j} \geq \frac{1}{k}}\end{array}\right.</script><p>②log关系</p><script type="math/tex; mode=display">f(\mathbf{v} ; k)=\sum_{i=1}^{n}\left(\zeta v_{i}-\frac{\zeta^{v_{i}}}{\log \zeta}\right)</script><p>解为：</p><script type="math/tex; mode=display">v_{i}^{*}=\left\{\begin{array}{ll}{\frac{1}{\log \zeta} \log \left(\frac{1}{m} \sum_{j=1}^{m} C \ell_{i j}+\zeta\right)} & {\frac{1}{m} \sum_{i=1}^{m} C \ell_{i j}<\frac{1}{k}} \\ {0} & {\frac{1}{m} \sum_{i=1}^{m} C \ell_{i j} \geq \frac{1}{k}}\end{array}\right.</script><p>③soft与hard的混合</p><script type="math/tex; mode=display">f\left(\mathbf{v} ; k, k^{\prime}\right)=-\zeta \sum_{i=1}^{n} \log \left(v_{i}+\zeta k\right)</script><p>解为：</p><script type="math/tex; mode=display">v_{i}^{*}=\left\{\begin{array}{ll}{1} & {\frac{1}{m} \sum_{i=1}^{m} C \ell_{i j} \leq \frac{1}{k^{\prime}}} \\ {0} & {\frac{1}{m} \sum_{i=1}^{m} C \ell_{i j} \geq \frac{1}{k}} \\ {\frac{m \zeta}{\sum_{i=1}^{m} C \ell_{i j}}-k \zeta} & {\text { otherwise }}\end{array}\right.</script><p>也即，loss太小置为1，太大置为0.中间则是实数域。</p><p><img src="/images/15642823398246.jpg" width="60%" height="50%"></p><hr><h2 id="Self-Paced-Learning-with-Diversity"><a href="#Self-Paced-Learning-with-Diversity" class="headerlink" title="[Self-Paced Learning with Diversity]"></a>[Self-Paced Learning with Diversity]</h2><p>提出在self-paced learning中，不仅仅要选择简单的，还要增加diversity这一指标，使得模型不会overfitting到某个pattern上，同时diverse sample能够帮助模型学到更为comprehensive的知识。</p><p><img src="/images/15642828676000.jpg" width="80%" height="50%"></p><p>由于self-paced learning没有先验知识，一开始接收到的某领域的sample，之后会倾向于一直选择该领域的sample，因为之前获取到该领域的sample，使得该领域的sample的loss相较其他领域的loss更低。这样就容易overfit到某个特定的pattern。为了解决这一问题，需要显式将diversity作为objective。</p><p>基于这一思想，有：<br>假设训练数据$\mathbf{X}=\left(\mathbf{x}_{1}, \cdots, \mathbf{x}_{n}\right) \in \mathbb{R}^{m \times n}$，分成b个group $\mathbf{X}^{(1)}, \cdots, \mathbf{X}^{(b)}$。weight vector为$\mathbf{v}=\left[\mathbf{v}^{(1)}, \cdots, \mathbf{v}^{(b)}\right]$，其中$\mathbf{v}^{(j)}=\left(v_{1}^{(j)}, \cdots, v_{n_{j}}^{(j)}\right)^{T} \in[0,1]^{n_{j}}$</p><p>因此objective function为：</p><script type="math/tex; mode=display">\min _{\mathbf{w}, \mathbf{v}} \mathbb{E}(\mathbf{w}, \mathbf{v} ; \lambda, \gamma)=\sum_{i=1}^{n} v_{i} L\left(y_{i}, f\left(\mathbf{x}_{i}, \mathbf{w}\right)\right)-\lambda \sum_{i=1}^{n} v_{i}-\gamma\|\mathbf{v}\|_{2,1}, \text { s.t. } \mathbf{v} \in[0,1]^{n}</script><p>其中新的regularization为$-|\mathbf{v}|_{2,1}=-\sum_{j=1}^{b}\left|\mathbf{v}^{(j)}\right|_{2}$，该regularization能够使模型尽量去选择不同group的sample。</p><p>当每个group只有一个sample时，公式退化为：</p><script type="math/tex; mode=display">\min _{\mathbf{w}, \mathbf{v}} \mathbb{E}(\mathbf{w}, \mathbf{v} ; \lambda)=\sum_{i=1}^{n} v_{i} L\left(y_{i}, f\left(\mathbf{x}_{i}, \mathbf{w}\right)\right)-\lambda \sum_{i=1}^{n} v_{i}, \text { s.t. } \mathbf{v} \in[0,1]^{n}</script><p>因此最终的算法：</p><p><img src="/images/15642831390970.jpg" width="90%" height="50%"></p><p><img src="/images/15642831523214.jpg" width="90%" height="50%"></p><p>思考：从diversity的角度确实很有道理，如果不是这篇文章我想不到diversity。作者对问题的认识很到位。</p><hr><h2 id="Self-paced-dictionary-learning-for-image-classification"><a href="#Self-paced-dictionary-learning-for-image-classification" class="headerlink" title="[Self-paced dictionary learning for image classification]"></a>[Self-paced dictionary learning for image classification]</h2><p>将self-paced learning用于dictionary learning（对该领域不了解）。</p><p><img src="/images/15642834501198.jpg" width="60%" height="50%"></p><p>另一贡献是提出新的阈值更新公式：</p><script type="math/tex; mode=display">\sigma=f(\pi, t)=\pi+\log \left(\pi^{2}+c\right) t \quad(c \geq 1)</script><p>保证第一轮有超过一半的example认定为easy。</p><hr><h2 id="Self-Paced-Learning-for-Matrix-Factorization"><a href="#Self-Paced-Learning-for-Matrix-Factorization" class="headerlink" title="[Self-Paced Learning for Matrix Factorization]"></a>[Self-Paced Learning for Matrix Factorization]</h2><p>将self-paced learning用于矩阵分解，同时将hard weight改成soft weight。</p><p><img src="/images/15642835464066.jpg" width="60%" height="50%"></p><p><img src="/images/15642836554962.jpg" width="65%" height="50%"></p><hr><h2 id="Self-Paced-Boost-Learning-for-Classification"><a href="#Self-Paced-Boost-Learning-for-Classification" class="headerlink" title="[Self-Paced Boost Learning for Classification]"></a>[Self-Paced Boost Learning for Classification]</h2><p>提出将boost和self-paced的思想结合在一起的算法。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>在监督学习中，有两个重要的原则：effectiveness和robustness。effectiveness专注于让模型分对，提高accuracy；而robustness则是减少noise或者confusing example的影响。<br>boosting算法则专注于effectiveness，因为渐进地专注于分错的example。但这样就容易使得模型缺少鲁棒性/泛化性。<br>self-paced learning则是从简单到难，这样可以通过稳步提高模型能力之后减少confusing/noise example对模型的影响，也即一开始就能够学习简单的pattern，而不会受到confusing/noise example所有的复杂pattern的影响。</p><p>boosting与self-paced有共通之处以及互补之处：<br>二者都是一个渐进的过程，从weak state到strong state。<br>boosting更考虑effectiveness，而SPL则更考虑robustness；boosting对不充分学习的样本施加负面抑制（不大懂意思，应该是说算法强调不充分学习的样本），而SPL正面鼓励简单易学的样本。boosting更专注于class之间的margin，尝试去拟合each sample，通过动态选择具有不同模式的简单样本；SPL更关注类内变化。<br>因此，boosting倾向于反映local的pattern，更加对noise敏感，而SPL则更加平滑，具有更强的鲁棒性。</p><p>将二者结合起来：positive encouragement (on reliable samples) and negative suppression (on misclassified samples)</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>问题定义：训练数据$\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{n}$，其中$x_{i} \in \mathbb{R}^{d}$，$y_{i} \in\{1,2, \ldots, C\}$。</p><p>要学习一个映射$\tilde{y}(x)=\underset{r \in\{1, \ldots, C\}}{\operatorname{argmax}} F_{r}(x ; \Theta)$。而多分类的目标/loss function为：</p><script type="math/tex; mode=display">\begin{array}{c}{\min _{\Theta} \sum_{i=1}^{n} \sum_{r=1}^{C} L\left(\rho_{i r}\right)+\nu R(\Theta)} \\ {\text {s.t.} \forall i, r, \rho_{i r}=F_{y_{i}}(x ; \Theta)-F_{r}(x ; \Theta)}\end{array}</script><p><strong>$F$决定effectiveness；而$L$决定robustness</strong>。</p><p>在boosting中：</p><script type="math/tex; mode=display">F_{r}(x ; W)=\sum_{j=1}^{k} w_{r j} h_{j}(x), r=1, \ldots, C</script><p>其中$\left\{h_{j}(\cdot) \in \mathcal{H}\right\}_{j=1}^{k}$，$h_{j}(\cdot) : \mathbb{R}^{d} \rightarrow\{0,1\}$是weak classifier。$W$是分配给每个classifier的权重。</p><p>在boosting中加入SPL的思想，也即引导模型先学简单的再学难的：</p><script type="math/tex; mode=display">\begin{array}{c}{\min _{W, v} \sum_{i=1}^{n} v_{i} \sum_{r=1}^{C} L\left(\rho_{i r}\right)+\sum_{i=1}^{n} g\left(v_{i} ; \lambda\right)+\nu R(W)} \\ {\text {s.t.} \forall i, r, \rho_{i r}=H_{i :} w_{y_{i}}-H_{i :} w_{r} ; W \geqslant 0 ; v \in[0,1]^{n}}\end{array}</script><p>其中$\left[H_{i j}\right]=\left[h_{j}\left(x_{i}\right)\right]$，$W=\left[w_{1}, \cdots, w_{C}\right] \in \mathbb{R}^{k \times C}$</p><p>将$L$具体化为logistics函数，$R(W)$是$l_{2,1} \text{-norm}$。则：</p><script type="math/tex; mode=display">\begin{array}{l}{\min _{W, v} \sum_{i, r} v_{i} \ln \left(1+e^{-\rho_{i r}}\right)+\sum_{i} g\left(v_{i} ; \lambda\right)+\nu\|W\|_{2,1}} \\ {\text {s.t.} \forall i, r, \rho_{i r}=H_{i :} w_{y_{i}}-H_{i :} w_{r} ; W \geqslant 0 ; v \in[0,1]^{n}}\end{array}</script><p>采用column generation method来解决上述公式（这段看不懂咋算出来的）：</p><p><img src="/images/15642848899908.jpg" width="60%" height="50%"></p><p><img src="/images/15642849290180.jpg" width="57%" height="50%"></p><blockquote><p>the future weak learners will put emphasis on samples that are both insufﬁciently learned currently and easily learned previously</p></blockquote><p>也即是boosting与SPL的结合。</p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>boosting由于只能通过分对分错来给权重而不是根据loss，是粗粒度的，强调要解决难的；而SPL则是强调先学简单的，再解决难的不迟。二者巧妙互补。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> SPL </tag>
            
            <tag> Self-paced Learning </tag>
            
            <tag> Boosting </tag>
            
            <tag> Boost </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词31</title>
      <link href="/2019/07/26/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D31/"/>
      <url>/2019/07/26/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D31/</url>
      
        <content type="html"><![CDATA[<h3 id="滁州西涧"><a href="#滁州西涧" class="headerlink" title="滁州西涧"></a>滁州西涧</h3><p>[唐] 韦应物<br>独怜幽草涧边生，上有黄鹂深树鸣。<br>春潮带雨晚来急，野渡无人舟自横。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>佳句分享3</title>
      <link href="/2019/07/26/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BD%B3%E5%8F%A5%E5%88%86%E4%BA%AB3/"/>
      <url>/2019/07/26/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BD%B3%E5%8F%A5%E5%88%86%E4%BA%AB3/</url>
      
        <content type="html"><![CDATA[<h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><p>世界上只有一种真正的英雄主义，就是认清了生活的真相后还依然热爱它。</p><h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><p>宠辱不惊，闲看庭前花开花落；去留无意，漫随天外云卷云舒。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>佳句分享2</title>
      <link href="/2019/07/07/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BD%B3%E5%8F%A5%E5%88%86%E4%BA%AB2/"/>
      <url>/2019/07/07/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BD%B3%E5%8F%A5%E5%88%86%E4%BA%AB2/</url>
      
        <content type="html"><![CDATA[<h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><p>Learn to be ordinary before you wish to be extraordinary.</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文24</title>
      <link href="/2019/07/07/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8724/"/>
      <url>/2019/07/07/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8724/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>Curriculum Learning for Multi-Task Classification of Visual Attributes</li><li>Highway Networks</li><li>Deep Residual Learning for Image Recognition</li><li>Gated Feedback Recurrent Neural Networks</li><li>Densely Connected Convolutional Networks</li></ol><h2 id="Curriculum-Learning-for-Multi-Task-Classiﬁcation-of-Visual-Attributes"><a href="#Curriculum-Learning-for-Multi-Task-Classiﬁcation-of-Visual-Attributes" class="headerlink" title="[Curriculum Learning for Multi-Task Classiﬁcation of Visual Attributes]"></a>[Curriculum Learning for Multi-Task Classiﬁcation of Visual Attributes]</h2><p>大致思路是：将数据分为两个task，先训练强相关的task，然后再训练弱相关的task。具体来说，对于visual attribute，先训练强相关label，然后再训练弱相关的label。<br><img src="/images/15624682482609.jpg" width="60%" height="50%"></p><p>具体形式：</p><p><img src="/images/15624684432867.jpg" width="80%" height="50%"></p><p>如何计算相关性：</p><script type="math/tex; mode=display">p_{i}=\sum_{j=1, j \neq i}^{T} \frac{\operatorname{cov}\left(y_{t_{i}}, y_{t_{j}}\right)}{\sigma\left(y_{t_{i}}\right) \sigma\left(y_{t_{j}}\right)}, i=1, \ldots, T</script><p>top 50%是强相关的，剩下都是弱相关的。</p><p>我的理解是，似乎是将label给切分了：</p><p><img src="/images/15624684979775.jpg" width="60%" height="50%"></p><hr><h2 id="Highway-Networks"><a href="#Highway-Networks" class="headerlink" title="[Highway Networks]"></a>[Highway Networks]</h2><p>原先的普通layer：</p><script type="math/tex; mode=display">\mathbf{y}=H\left(\mathbf{x}, \mathbf{W}_{\mathbf{H}}\right)</script><p>为了能够训练很深的网络，改成：</p><script type="math/tex; mode=display">\mathbf{y}=H\left(\mathbf{x}, \mathbf{W}_{\mathbf{H}}\right) \cdot T\left(\mathbf{x}, \mathbf{W}_{\mathbf{T}}\right)+\mathbf{x} \cdot C\left(\mathbf{x}, \mathbf{W}_{\mathbf{C}}\right)</script><p>也即增加两个仿射变换。其中T是transformer gate，而C是carry gate。这样能够有助于模型的优化。</p><p>简单起见：</p><script type="math/tex; mode=display">\mathbf{y}=H\left(\mathbf{x}, \mathbf{W}_{\mathbf{H}}\right) \cdot T\left(\mathbf{x}, \mathbf{W}_{\mathbf{T}}\right)+\mathbf{x} \cdot\left(1-T\left(\mathbf{x}, \mathbf{W}_{\mathbf{T}}\right)\right)</script><p>在T中，可以设bias为负，也即一开始偏向carry的行为。</p><hr><h2 id="Deep-Residual-Learning-for-Image-Recognition"><a href="#Deep-Residual-Learning-for-Image-Recognition" class="headerlink" title="[Deep Residual Learning for Image Recognition]"></a>[Deep Residual Learning for Image Recognition]</h2><p><img src="/images/15624688998473.jpg" width="50%" height="50%"></p><p>本质是残差比直接学习x的变换更容易。对模型的优化更容易。</p><blockquote><p>if an identity mapping were optimal, it would be easier to push the residual to zero than to ﬁt an identity mapping by a stack of nonlinear layers</p></blockquote><p>可否理解highway network是ResNet的进阶版？</p><hr><h2 id="Gated-Feedback-Recurrent-Neural-Networks"><a href="#Gated-Feedback-Recurrent-Neural-Networks" class="headerlink" title="[Gated Feedback Recurrent Neural Networks]"></a>[Gated Feedback Recurrent Neural Networks]</h2><p><img src="/images/15624692408778.jpg" width="80%" height="50%"></p><p>对于多层的RNN，第l层的time step为t的h，可以接收上一个time step的大于l层的也接收小于l层的hidden state。</p><hr><h2 id="Densely-Connected-Convolutional-Networks"><a href="#Densely-Connected-Convolutional-Networks" class="headerlink" title="[Densely Connected Convolutional Networks]"></a>[Densely Connected Convolutional Networks]</h2><p>在一个block内，每层都连接到后面的层。</p><p><img src="/images/15624693085589.jpg" width="70%" height="50%"></p><p>注意到，在连接形式上，resnet是相加：</p><script type="math/tex; mode=display">\mathbf{x}_{\ell}=H_{\ell}\left(\mathbf{x}_{\ell-1}\right)+\mathbf{x}_{\ell-1}</script><p>而DenseNet则是concat：</p><script type="math/tex; mode=display">\mathbf{x}_{\ell}=H_{\ell}\left(\left[\mathbf{x}_{0}, \mathbf{x}_{1}, \dots, \mathbf{x}_{\ell-1}\right]\right)</script><p>这里的H是Bn-Relu-Conv。</p>]]></content>
      
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> RNN </tag>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> Curriculum Learning </tag>
            
            <tag> ResNet </tag>
            
            <tag> Highway </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Pytorch中inf导数的nan问题</title>
      <link href="/2019/07/03/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPyTorch%E4%B8%ADinf%E5%AF%BC%E6%95%B0%E7%9A%84nan%E9%97%AE%E9%A2%98/"/>
      <url>/2019/07/03/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPyTorch%E4%B8%ADinf%E5%AF%BC%E6%95%B0%E7%9A%84nan%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>想要实现一个功能。将Transformer中多层的attention矩阵加权平均。也即：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pre_attn shape: batch_size*n_head,6,target_len,source_len</span></span><br><span class="line">self.attn_alpha = Parameter(torch.zeros(<span class="number">1</span>, <span class="number">6</span>))</span><br><span class="line">normalized_alpha = F.softmax(self.attn_alpha, dim=<span class="number">-1</span>)  <span class="comment"># 1,6</span></span><br><span class="line">normalized_alpha = normalized_alpha.reshape(<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 1,6,1,1</span></span><br><span class="line"></span><br><span class="line">weighted_attn = prev_attn * normalized_alpha</span><br><span class="line">weighted_attn = weighted_attn.sum(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>获得的<code>weighted_attn</code>就是所有层的attention矩阵的加权平均。再将<code>weighted_attn</code>用于后面的计算。其中<code>attn_alpha</code>是可学习的参数。</p><p>功能很简单，但在backward完后，<code>attn_alpha</code>就会一下子跳到nan。按理说，虽然attn矩阵里面存在-inf，但只要是同一个batch，inf存在的索引位置应该都是一样的，即使加权求和也不会导致某一行全为-inf，使得在softmax后存在nan的情况。</p><p>在航总排查了一晚上后，也尝试了各种假设，最终发现，是因为梯度回传时的问题。也即当<code>weighted_attn = prev_attn * normalized_alpha</code>这句代码梯度回传的时候，由于存在’-inf’的值，alpha的梯度就会有nan（因为上句代码中alpha的导数是<code>prev_attn</code>，当<code>prev_attn</code>存在inf时，则grad则为nan。</p><p>解决方案是，首先获得attention矩阵的mask，接着使用masked_fill将inf的部分置为0，再和alpha相乘，此时就不会有nan的情况出现了。也即：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pre_attn shape: batch_size*n_head,6,target_len,source_len</span></span><br><span class="line">self.attn_alpha = Parameter(torch.zeros(<span class="number">1</span>, <span class="number">6</span>))</span><br><span class="line">normalized_alpha = F.softmax(self.attn_alpha, dim=<span class="number">-1</span>)  <span class="comment"># 1,6</span></span><br><span class="line">normalized_alpha = normalized_alpha.reshape(<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 1,6,1,1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ----多加这几行 --- #</span></span><br><span class="line">_attn_mask = prev_attn == float(<span class="string">'-inf'</span>)  <span class="comment"># -inf的位置为1</span></span><br><span class="line">new_pre_attn = pre_attn.data.masked_fill(_attn_mask,<span class="number">0</span>) <span class="comment"># 将-inf填充为0</span></span><br><span class="line"><span class="comment"># --------------- # </span></span><br><span class="line"></span><br><span class="line">weighted_attn = new_pre_attn * normalized_alpha</span><br><span class="line">weighted_attn = weighted_attn.sum(dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>总结起来，则是，当tensor存在inf时，与它相乘的tensor如果是可更新的，则该tensor的grad为nan。所以在处理有inf的tensor要特别注意，可能出现相乘后梯度回传grad为nan的情况，还有一种情况则是若某一行全为inf，过softmax后则也会出现nan。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> nan </tag>
            
            <tag> inf </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识24</title>
      <link href="/2019/06/30/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8624/"/>
      <url>/2019/06/30/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8624/</url>
      
        <content type="html"><![CDATA[<h3 id="Homebrew"><a href="#Homebrew" class="headerlink" title="[Homebrew]"></a>[Homebrew]</h3><p>Homebrew安装遇到 Permission denied @ dir_s_mkdir</p><p>No need to chown the whole /usr/local if brew only fails to create a single directory.<br>For example, I fixed this error:<br>Permission denied @ dir_s_mkdir - /usr/local/Frameworks<br>With this command:<br>sudo install -d -o $(whoami) -g admin /usr/local/Frameworks</p><p><a href="https://gist.github.com/irazasyed/7732946" target="_blank" rel="noopener">https://gist.github.com/irazasyed/7732946</a></p><hr><h3 id="Alfred"><a href="#Alfred" class="headerlink" title="[Alfred]"></a>[Alfred]</h3><p>mac 上 QQ 会阻止 Alfred 锁屏功能，这是因为快捷键冲突，取消查看联系人的快捷键即可。</p><p><img src="/images/15618638268867.jpg" width="60%" height="50%"></p><p><a href="https://www.v2ex.com/t/477934" target="_blank" rel="noopener">https://www.v2ex.com/t/477934</a></p><hr><h3 id="Autodiff"><a href="#Autodiff" class="headerlink" title="[Autodiff]"></a>[Autodiff]</h3><p>Autodiff有两种模式，forward和reverse。当前的深度学习框架都用的reverse。</p><p><a href="https://blog.csdn.net/aws3217150/article/details/70214422" target="_blank" rel="noopener">https://blog.csdn.net/aws3217150/article/details/70214422</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Homebrew </tag>
            
            <tag> Alfred </tag>
            
            <tag> Mac </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DRL Lecture 8:Imitation Learning</title>
      <link href="/2019/06/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%208:%20Imitation%20Learning/"/>
      <url>/2019/06/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%208:%20Imitation%20Learning/</url>
      
        <content type="html"><![CDATA[<p>讨论了在没有reward的情况下，如何利用expert来进行RL。</p><p>问题定义：在一些问题上是没有reward的，给定一些expert的demonstration example，如何利用这些example使得机器能够学习？</p><h3 id="Behavior-Cloning"><a href="#Behavior-Cloning" class="headerlink" title="Behavior Cloning"></a>Behavior Cloning</h3><p>本质就是监督学习，给定训练数据，要模型输入s能够获得尽量和expert相似的action。</p><p><img src="/images/15618627790626.jpg" width="60%" height="50%"></p><p>由于expert example是较少的，机器可能遇到没遇到的情况。<br>同时由于机器的capacity是有限的，可能选择无关的行为去学习。<br>还有可能带来由于训练数据和测试数据的分布不同导致的问题。因为RL有序列性，如果使用Behavior Cloning，在某个state下采用了不同的action，则之后的state都会完全不同（失之毫厘谬以千里）</p><h3 id="Inverse-Reinforcement-Learning-IRL"><a href="#Inverse-Reinforcement-Learning-IRL" class="headerlink" title="Inverse Reinforcement Learning (IRL)"></a>Inverse Reinforcement Learning (IRL)</h3><p>通过expert example来学习reward function，在学习完reward function后让agent与环境交互获得agent example。接着调整reward function使得expert example一定大于agent的example。不断循环。这和GAN的思想有点像：</p><p><img src="/images/15618628831028.jpg" width="60%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Reinforcement Learning </tag>
            
            <tag> 李宏毅强化学习课程 </tag>
            
            <tag> Imitation Learning </tag>
            
            <tag> IRL </tag>
            
            <tag> Inverse Reinforcement Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词30</title>
      <link href="/2019/06/30/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D30/"/>
      <url>/2019/06/30/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D30/</url>
      
        <content type="html"><![CDATA[<h3 id="水龙吟-·-登建康赏心亭"><a href="#水龙吟-·-登建康赏心亭" class="headerlink" title="水龙吟 · 登建康赏心亭"></a>水龙吟 · 登建康赏心亭</h3><p>[宋] 辛弃疾<br>楚天千里清秋，水随天去秋无际。遥岑远目，献愁供恨，玉簪螺髻。落日楼头，断鸿声里，江南游子。把吴钩看了，栏干拍遍，无人会、登临意。<br>休说鲈鱼堪脍，尽西风、季鹰归未？求田问舍，怕应羞见，刘郎才气。可惜流年，忧愁风雨，树犹如此。<strong>倩何人唤取，红巾翠袖，揾英雄泪</strong>。</p><p>倩（qìng）：请托。<br>揾（wèn）：擦拭。</p><hr><h3 id="满江红"><a href="#满江红" class="headerlink" title="满江红"></a>满江红</h3><p>[宋] 岳飞<br>怒发冲冠，凭栏处、潇潇雨歇。抬望眼，仰天长啸，壮怀激烈。<strong>三十功名尘与土，八千里路云和月</strong>。<strong>莫等闲，白了少年头，空悲切</strong>！<br>靖康耻，犹未雪。臣子恨，何时灭？驾长车、踏破贺兰山缺！壮志饥餐胡虏肉，笑谈渴饮匈奴血。<strong>待从头、收拾旧山河，朝天阙！</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文23</title>
      <link href="/2019/06/29/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8723/"/>
      <url>/2019/06/29/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8723/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>Reinforcement Learning based Curriculum Optimization for Neural Machine Translation</li><li>Curriculum Dropout</li><li>Self-Paced Curriculum Learning</li><li>Learning the Easy Things First: Self-Paced Visual Category Discovery</li><li>Curriculum Learning and Minibatch Bucketing in Neural Machine Translation</li><li>Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks</li><li>LEARNING TO TEACH</li><li>Learning to learn by gradient descent by gradient descent</li><li>Curriculum Learning of Multiple Tasks</li></ol><h2 id="Reinforcement-Learning-based-Curriculum-Optimization-for-Neural-Machine-Translation"><a href="#Reinforcement-Learning-based-Curriculum-Optimization-for-Neural-Machine-Translation" class="headerlink" title="[Reinforcement Learning based Curriculum Optimization for Neural Machine Translation]"></a>[Reinforcement Learning based Curriculum Optimization for Neural Machine Translation]</h2><p>使用RL来进行学习curriculum learning，使得模型能够在各式各样的数据集上表现良好（也即高效利用noise很大的数据集，如Paracrawl)。</p><p><img src="/images/15618207015732.jpg" width="60%" height="50%"></p><p>使用一个指标CDS来将数据切分：</p><script type="math/tex; mode=display">s(e, f)=\log p_{\theta_{c}}(f | e)-\log p_{\theta_{n}}(f | e)</script><p>$e$和$f$是翻译对。其中$\theta_c$是在可信任的数据集上训练的模型；$\theta_n$在noisy corpus上训练的（比如Paracrawl)。</p><p>RL的几个基本因素：</p><p>Observation Engineering：the observation is the vector containing sentence-level log-likelihoods produced by the NMT system for this prototype batch  参考（Reinforced co-training.）</p><p>Reward Engineering：The reward is a function of the log-likelihood of the development set of interest.</p><p>Action：将数据集分为多个bin，action就是选择在哪个bin里面选择数据集。</p><hr><h2 id="Curriculum-Dropout"><a href="#Curriculum-Dropout" class="headerlink" title="[Curriculum Dropout]"></a>[Curriculum Dropout]</h2><p>顾名思义，就是逐渐增加dropout rate。</p><p><img src="/images/15618211418694.jpg" width="40%" height="50%"></p><hr><h2 id="Self-Paced-Curriculum-Learning"><a href="#Self-Paced-Curriculum-Learning" class="headerlink" title="[Self-Paced Curriculum Learning]"></a>[Self-Paced Curriculum Learning]</h2><p>将self-paced learning与curriculum learning结合。</p><h3 id="Curriculum-Learning"><a href="#Curriculum-Learning" class="headerlink" title="Curriculum Learning"></a>Curriculum Learning</h3><p>通过先验知识对example的难度进行预定义，然后按照先后顺序训练模型；</p><p>这是一种Instructor-driven的训练方法；并不考虑learner的feedback</p><h3 id="Self-paced-Learning"><a href="#Self-paced-Learning" class="headerlink" title="Self-paced Learning"></a>Self-paced Learning</h3><p>是直接将目标函数和curriculum一起结合起来，也即在训练的过程中根据模型的学习情况（loss）调整curriculum。灵活，但不考虑先验知识。</p><script type="math/tex; mode=display">\min _{\mathbf{w}, \mathbf{v} \in[0,1]^{n}} \mathbb{E}(\mathbf{w}, \mathbf{v} ; \lambda)=\sum_{i=1}^{n} v_{i} L\left(y_{i}, f\left(\mathbf{x}_{i}, \mathbf{w}\right)\right)-\lambda \sum_{i=1}^{n} v_{i}</script><p>当loss小于$\lambda$时，就被认为是easy sample，$v=1$；若大于$\lambda$则$v=0$。</p><p>λ controls the pace at which the model learns new samples, and physically λ corresponds to the “age” of the model</p><p>由于学习过程完全被loss主导，因此可能会overfitting。</p><h3 id="Self-paced-Curriculum-Learning"><a href="#Self-paced-Curriculum-Learning" class="headerlink" title="Self-paced Curriculum Learning"></a>Self-paced Curriculum Learning</h3><p>在二者基础上结合，在SPL的框架下引入CL的先验知识。也即既考虑了先验知识，又考虑了模型学习的反馈（loss）。</p><script type="math/tex; mode=display">\min _{\mathbf{w}, \mathbf{v} \in[0,1]^{n}} \mathbb{E}(\mathbf{w}, \mathbf{v} ; \lambda, \Psi)=\sum_{i=1}^{n} v_{i} L\left(y_{i}, g\left(\mathbf{x}_{i}, \mathbf{w}\right)\right)+f(\mathbf{v} ; \lambda),\text { s.t. } \mathbf{v} \in \Psi</script><p>其中$\Psi$代表了预定义的curriculum的集合。</p><p>相当于CL提供了一个弱sample的顺序，建议模型要先学哪些，但他有自由去调整学习目标。<br>我的理解是，$\Psi$是可以动态增大的。但是模型是否要将集合里的example用于训练还是要看上述的目标函数的，loss小的时候代表的easy example，会用于学习。</p><p><img src="/images/15618570682762.jpg" width="60%" height="50%"></p><p>这是CL/SPL/SPCL的区别：</p><p><img src="/images/15618570907601.jpg" width="100%" height="50%"></p><p>思考：</p><p>$\lambda$代表了模型的成熟度，控制的是模型自身的competence；而$\Psi$集合大小代表了instructor认为模型的成熟度。这二者的结合能够让模型更灵活。但是competence还是需要一个手动的schedule。</p><hr><h2 id="Learning-the-Easy-Things-First-Self-Paced-Visual-Category-Discovery"><a href="#Learning-the-Easy-Things-First-Self-Paced-Visual-Category-Discovery" class="headerlink" title="[Learning the Easy Things First: Self-Paced Visual Category Discovery]"></a>[Learning the Easy Things First: Self-Paced Visual Category Discovery]</h2><p>将self-paced的思路应用于visual category discovery。与传统self-paced learning不同的是，并没有将self-paced绑定在loss function上。</p><p>每一次计算两个指标 objectness和context-awareness。将最简单的选出来训练，然后再计算指标，再选出简单的训练。不断循环。</p><p>self-paced与curriculum learning不同，没有一个固定的teacher来判断难易程度，根据每次自己学习的进程来判断难度。本文相较传统self-paced不同，因为将loss和对难易程度的判断两个步骤分离开来。</p><hr><h2 id="Curriculum-Learning-and-Minibatch-Bucketing-in-Neural-Machine-Translation"><a href="#Curriculum-Learning-and-Minibatch-Bucketing-in-Neural-Machine-Translation" class="headerlink" title="[Curriculum Learning and Minibatch Bucketing in Neural Machine Translation]"></a>[Curriculum Learning and Minibatch Bucketing in Neural Machine Translation]</h2><p>在翻译任务上做了一些训练方法上的组合尝试，给出了一些结论。</p><h3 id="Minibatch-Bucketing"><a href="#Minibatch-Bucketing" class="headerlink" title="Minibatch Bucketing"></a>Minibatch Bucketing</h3><p>首先是尝试了在同一个batch里不仅句子长度相同（加速训练），还希望同一个batch内部有某种linguistic的信息(sentence length, number of coordinating conjunctions, number of nouns, number of proper nouns and the number of verbs in the training data pairs)。可能由于他选的这些linguistic并不好，最终并没有发现结果的提升。</p><p><img src="/images/15618573588977.jpg" width="55%" height="50%"></p><h3 id="Curriculum-Learning-1"><a href="#Curriculum-Learning-1" class="headerlink" title="Curriculum Learning"></a>Curriculum Learning</h3><p>对curriculum learning进行了改进，实际上普通的CL间接地强调了easier example，因为他们被sample了多次，所以这里采用了一种新的方法能够让每个example在一个epoch都只被sample一次。</p><p>按照难度将样例分为几个bin，首先从最简单的bin开始取样例，直到该bin的剩余样例个数和第二个bin的样例一样，然后从这两个bin剩下的样例中取样例，直到剩下和第三个bin样例个数一样。这样能保证在一个epoch内每个example的概率是一致的。</p><p>如何判断难易程度？长度，词的频率等</p><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p>①<br>首先，在训练完一个epoch后，使用CL相比没使用CL有提升：</p><p><img src="/images/15618574547524.jpg" width="55%" height="50%"></p><p>②<br>在一个epoch内的训练曲线：</p><p><img src="/images/15618574884386.jpg" width="70%" height="50%"></p><p>可以看到用CL的在每次新加example后都会有一个陡峭的跳跃。特别要注意按照source长度和按照target长度的CL有很大的不同，可能是按照target的CL给了训练进程一个较大的penalization。</p><p>③<br>模型很容易过拟合recent example，因此如果只提供难一点的example，那么在easy的example就容易下降。所以需要mixing strategy。</p><p><img src="/images/15618575566938.jpg" width="70%" height="50%"></p><p>从CL by target length可以看出，陡峭的跳跃说明模型在新加入数据之前很快地adapt在短的句子，而长句一进来又很快adapt到长句。这种快速转换似乎说明了模型的快速适应性。<br>如果不回顾简单的句子，见sorted by length的曲线，可以看到performance很差。<br>同时如果reverse CL，也即一开始evenly cover所有句子，然后只使用短的句子，那么可以看到一开始效果不错，到后面就降低了，这是因为模型快速适应了生成短的句子，就没法生成test集的正常长度的句子。</p><p>④<br>注意到以上都是在一个epoch内（也即过完了一遍训练数据）的结论。在这个epoch后继续几种训练方式（基于‘CL by target length’）。</p><p><img src="/images/15618576497211.jpg" width="70%" height="50%"></p><p>重新从最简单的开始（second epoch of CL by target length)，会伤害performance，但到后面还是有提升的。如果在第二个epoch用shuffle的数据训练，那么可以看到是几乎没有提升的，可能是因为模型已经陷入了当前的optimum了。</p><h3 id="思考与结论"><a href="#思考与结论" class="headerlink" title="思考与结论"></a>思考与结论</h3><p>这里的回顾式的CL，无法减少训练时间，因为要到最后才能获得超越baseline的performance。<br>同时实验证明了，模型的快速适应性，很容易overfitting到最近的训练样例上，因此要设计mixing strategy。</p><hr><h2 id="Visualizing-and-Understanding-Curriculum-Learning-for-Long-Short-Term-Memory-Networks"><a href="#Visualizing-and-Understanding-Curriculum-Learning-for-Long-Short-Term-Memory-Networks" class="headerlink" title="[Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks]"></a>[Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks]</h2><p>对CL在LSTM的训练的影响进行分析。得到一些结论。</p><p>快速回顾了两种CL：One-Pass Curriculum和Baby Steps Curriculum。</p><p>One-Pass Curriculum：将训练数据分为几个bucket，然后从简单的bucket开始，训练完简单的bucket之后跳到难的bucket。</p><p><img src="/images/15618578493722.jpg" width="60%" height="50%"></p><p>Baby Steps Curriculum：从简单的开始，但在增加难的数据后，不会discard简单的数据。</p><p><img src="/images/15618578792843.jpg" width="60%" height="50%"></p><h3 id="实验-amp-结论"><a href="#实验-amp-结论" class="headerlink" title="实验&amp;结论"></a>实验&amp;结论</h3><p>①<br>Baby Step在多个任务上都明显更好，如，digit sum</p><p><img src="/images/15618579415036.jpg" width="90%" height="50%"></p><p>不仅仅是结果好，同时其variance也更小：</p><p><img src="/images/15618579749575.jpg" width="70%" height="50%"></p><p>②<br>在不同复杂度的模型上，CL效果都好，当模型越大，效果的差距会越小。注意到，CL在参数数量更少的情况下效果更好。</p><p><img src="/images/15618584599852.jpg" width="60%" height="50%"></p><p>③<br>在情感分析任务上的结果</p><p><img src="/images/15618585223767.jpg" width="60%" height="50%"></p><p>首先是使用CL效果好，其次是使用conjunction（是指连接两个情感极性相反句子的词（but等）（conjunctions where a span of text contradicts or supports overall sentiment polarity））效果差距更大。说明使用CL使得模型的鲁棒性更强。</p><p>同时CL在使用不同词来预测，其表现较为一致：</p><p><img src="/images/15618586132751.jpg" width="70%" height="50%"></p><p>同时，在数据量更少的情况下，CL的效果越明显：</p><p><img src="/images/15618586384812.jpg" width="60%" height="50%"></p><p>结论：<br>CL在数据少，模型小的情况下很重要。</p><hr><h2 id="LEARNING-TO-TEACH"><a href="#LEARNING-TO-TEACH" class="headerlink" title="[LEARNING TO TEACH]"></a>[LEARNING TO TEACH]</h2><p>采用RL的方法来进行schedule learning。 主要结构是，一个teacher决定给学生的数据，一个学生通过给定的数据训练，并获得reward和state作为feedback返回给teacher。 </p><p>teacher的目标是提供数据，loss function和hypothesis space。实际上论文只讨论了数据的提供。目标：</p><script type="math/tex; mode=display">\min _{D, L, \Omega} \mathcal{M}\left(\mu(D, L, \Omega), D_{t e s t}\right)</script><p><img src="/images/15618587842064.jpg" width="55%" height="50%"></p><p>RL的几个要素：</p><p>action：随机sample数据，然后从这些sample的数据里再筛选出数据。也即对所有sample的数据打标签，1代表给学生model训练，0则被抛弃掉。</p><p>state：实际上就是一些人工定好的feature。数据的feature，比如label category，句子长度，linguistic feature等；student model的feature，也即代表了当前NN被训练得多好的feature，历史training loss和历史的validation accuracy等；还有就是二者的结合，比如predicted probability ；data的loss等</p><p>reward：和student model收敛速度相关，也即记录第一个在测试集上准确率超过某个阈值的mini-batch的索引，然后计算：</p><script type="math/tex; mode=display">r_{T}=-\log \left(i_{\tau} / T^{\prime}\right)</script><p>这是为了鼓励早点收敛。</p><p>本篇文章框架设定得很好，但并没有讨论另外两个。</p><hr><h2 id="Learning-to-learn-by-gradient-descent-by-gradient-descent"><a href="#Learning-to-learn-by-gradient-descent-by-gradient-descent" class="headerlink" title="[Learning to learn by gradient descent by gradient descent]"></a>[Learning to learn by gradient descent by gradient descent]</h2><p>meta-learning的一种方法。被题目吸引，大概看了看。</p><p>利用LSTM来学习optimizer的梯度，以帮助模型更好的训练。</p><p><img src="/images/15618590469237.jpg" width="50%" height="50%"></p><p><img src="/images/15618590754597.jpg" width="60%" height="50%"></p><p>其中虚线不回传，实线回传。</p><p>同时，注意到为了让参数的顺序对输出没有影响，因为假设每个参数坐标都是独立的，因此使用separate hidden state，但LSTM的参数是共享的，也即每个输入梯度都单独处理。</p><p><img src="/images/15618591345801.jpg" width="45%" height="50%"></p><hr><h2 id="Teacher-Student-Curriculum-Learning"><a href="#Teacher-Student-Curriculum-Learning" class="headerlink" title="[Teacher-Student Curriculum Learning]"></a>[Teacher-Student Curriculum Learning]</h2><p>没仔细看，大概思想是，一个teacher帮忙选择sub-task让student学。</p><p><img src="/images/15618591847957.jpg" width="50%" height="50%"></p><p>state代表的是student的整个状态，neural network parameters and optimizer state) and is not observable to the Teacher.</p><p>action是teacher所采取的动作，也即选择某个task；</p><p>observation 是在选择了task后所获得的score；</p><p>reward也即在该timestep的score的改变 $r_{t}=x_{t}^{(i)}-x_{t_{i}^{\prime}}^{(i)}$</p><p>CL的地方在于从简单的学起（也即带来的改变最大的task），然后当其提升的速率降低了，则降低其sample的概率。</p><p>总结起来，CL的几个原则：</p><p><img src="/images/15618592716048.jpg" width="80%" height="50%"></p><p>理想化的CL：</p><p><img src="/images/15618593002197.jpg" width="76%" height="50%"></p><p>当某个task的score下降了，说明他忘了这部分的知识，又要提升该sample的概率。</p><hr><h2 id="Curriculum-Learning-of-Multiple-Tasks"><a href="#Curriculum-Learning-of-Multiple-Tasks" class="headerlink" title="[Curriculum Learning of Multiple Tasks]"></a>[Curriculum Learning of Multiple Tasks]</h2><p>学习多个task，按照先后顺序来，而不是联合训练。上一个task学到的weight用于下一个task的初始化。</p><p><img src="/images/15618593717627.jpg" width="55%" height="50%"></p><p>自动选择task顺序。我的理解是，每当训练完一个subtask，测试所有其他subtask，选择表现最好的那个（某个指标，平均期望误差），然后选择该subtask继续训练。</p>]]></content>
      
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> Curriculum Learning </tag>
            
            <tag> NMT </tag>
            
            <tag> LSTM </tag>
            
            <tag> Dropout </tag>
            
            <tag> meta-learning </tag>
            
            <tag> multi-task </tag>
            
            <tag> SPL </tag>
            
            <tag> Self-paced Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词29</title>
      <link href="/2019/06/23/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D29/"/>
      <url>/2019/06/23/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D29/</url>
      
        <content type="html"><![CDATA[<h3 id="永遇乐-·-京口北固亭怀古"><a href="#永遇乐-·-京口北固亭怀古" class="headerlink" title="永遇乐 · 京口北固亭怀古"></a>永遇乐 · 京口北固亭怀古</h3><p>[宋] 辛弃疾<br>千古江山，英雄无觅，孙仲谋处。舞榭歌台，风流总被，雨打风吹去。斜阳草树，寻常巷陌，人道寄奴曾住。想当年、金戈铁马，气吞万里如虎。<br>元嘉草草，封狼居胥，赢得仓皇北顾。四十三年，望中犹记，烽火扬州路。可堪回首，佛貍祠下，一片神鸦社鼓。凭谁问，<strong>廉颇老矣，尚能饭否</strong>？</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>佳句分享1</title>
      <link href="/2019/06/23/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BD%B3%E5%8F%A5%E5%88%86%E4%BA%AB1/"/>
      <url>/2019/06/23/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BD%B3%E5%8F%A5%E5%88%86%E4%BA%AB1/</url>
      
        <content type="html"><![CDATA[<h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><p>仓促本身就是最要不得的态度。当你做某件事的时候，一旦想要求快 ，就表示你再也不关心它，而想去做别的事 —《禅与摩托车维修艺术》</p><h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><p>自律使我们与众不同，自律令我们活得更高级。<br>也正是自律，使我们获得更自由的人生。<br>假如我们像动物一样，听从欲望，逃避痛苦，我们并不是真的自由行动。我们只是成了欲望和冲动的奴隶。我们不是在选择，而是在服从。但人之所以为人， 就在于，人不是被欲望主宰，而是自我主宰。 —康德</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识23</title>
      <link href="/2019/06/23/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8623/"/>
      <url>/2019/06/23/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8623/</url>
      
        <content type="html"><![CDATA[<h3 id="NAS"><a href="#NAS" class="headerlink" title="[NAS]"></a>[NAS]</h3><p>关于NAS(Neural Architecture Search)的科普文。<br><a href="https://medium.com/@ashiqbuet14/neural-architecture-search-nas-the-future-of-deep-learning-c99356351136" target="_blank" rel="noopener">https://medium.com/@ashiqbuet14/neural-architecture-search-nas-the-future-of-deep-learning-c99356351136</a></p><p><img src="/images/15612604991653.jpg" width="60%" height="50%"></p><p><strong>Search space</strong>：就是一个接一个的layer，也可以包括skip connection；</p><p><img src="/images/15612605302179.jpg" width="60%" height="50%"></p><p>如果希望大的框架定好，只是里面的layer搜索，这称为micro-search。</p><p><strong>RL</strong>：可以使用强化学习来做NAS，以RNN为基本模型。</p><p><img src="/images/15612606028985.jpg" width="77%" height="50%"></p><p>其实就是每个输出指示一个layer选项，然后通过RL获得的reward来更新。</p><p><img src="/images/15612606279730.jpg" width="65%" height="50%"></p><p><strong>Progressive Neural Architecture Search(PNAS)</strong>：这就是前面提到的固定整个大的框架（block），然后搜索里面的layer。</p><p><img src="/images/15612606813899.jpg" width="70%" height="50%"></p><p><img src="/images/15612606939857.jpg" width="40%" height="50%"></p><p><img src="/images/15612607076908.jpg" width="68%" height="50%"></p><p>可以通过每层选完去掉一些选项来减少排列组合巨大的总数。</p><p><strong>Differentiable Architecture Search(DARTS)</strong>：将选择layer的discrete的动作变成连续的，使得能够通过求导的方式更新。</p><p>其本质就是两个node之间连多个operation，然后训练获得每个operation的比例，只保留最大的。</p><p><img src="/images/15612607664055.jpg" width="40%" height="50%"></p><p><img src="/images/15612607797249.jpg" width="80%" height="50%"></p><p>这个用连续来达到离散的做法还挺有创新的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> NAS </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DRL Lecture 7:Sparse Reward</title>
      <link href="/2019/06/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%207:%20Sparse%20Reward/"/>
      <url>/2019/06/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%207:%20Sparse%20Reward/</url>
      
        <content type="html"><![CDATA[<p>讨论了当RL遇到sparse reward时的几个解决方案。</p><h2 id="Reward-Shaping"><a href="#Reward-Shaping" class="headerlink" title="Reward Shaping"></a>Reward Shaping</h2><h3 id="hand-crafted"><a href="#hand-crafted" class="headerlink" title="hand-crafted"></a>hand-crafted</h3><p>也即虚构出reward引导agent走向自己期望的结果。</p><p><img src="/images/15612581947292.jpg" width="80%" height="50%"></p><p>如上图，仔细定义了游戏中每个操作的reward。</p><h3 id="Curiosity"><a href="#Curiosity" class="headerlink" title="Curiosity"></a>Curiosity</h3><p>往agent里添加好奇心。</p><p><img src="/images/15612582253155.jpg" width="60%" height="50%"></p><p>输入是$a_t$和$s_t$尝试预测出$s_{t+1}$，如果预测的和真实的差距较大时，则该action的reward大，这样能够鼓励agent探索更多的操作。</p><p><img src="/images/15612583393730.jpg" width="60%" height="50%"></p><p>但有时候难以预测的state并不代表其重要。应当过滤掉这样的state，比如游戏中树叶飘动，但这个state完全不重要。因此对上述模型进行改进：</p><p><img src="/images/15612583648401.jpg" width="60%" height="50%"></p><p>添加feature extractor，同时添加另一个网络，来通过$s_t$和$s_{t+1}$预测action，这样就能够过滤掉state中没意义的部分。</p><h2 id="Curriculum-Learning"><a href="#Curriculum-Learning" class="headerlink" title="Curriculum Learning"></a>Curriculum Learning</h2><p>从简单的开始学起，比如玩游戏的例子：</p><p><img src="/images/15612584604093.jpg" width="74%" height="50%"></p><p>这个需要人工较为精细的调整。</p><h3 id="Reverse-Curriculum-Generation"><a href="#Reverse-Curriculum-Generation" class="headerlink" title="Reverse Curriculum Generation"></a>Reverse Curriculum Generation</h3><p>首先给定一个gold state，也即目标，然后寻找与gold state最接近的state获得相应的reward。</p><p><img src="/images/15612585028635.jpg" width="40%" height="50%"></p><p>然后去掉reward太大或太小的。在留下来的state中再获取与他们接近的state，继续以上流程。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Reinforcement Learning </tag>
            
            <tag> 李宏毅强化学习课程 </tag>
            
            <tag> Sparse Reward </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DRL Lecture 6:Actor-Critic</title>
      <link href="/2019/06/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%206:%20Actor-Critic/"/>
      <url>/2019/06/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%206:%20Actor-Critic/</url>
      
        <content type="html"><![CDATA[<p>介绍了actor-critic的算法，结合了policy gradient和Q-learning。</p><h2 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h2><p>原先policy gradient的算法是直接学习一个policy：</p><script type="math/tex; mode=display">\nabla \overline{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(\sum_{t^{\prime}=t}^{T_{n}} \gamma^{t^{\prime}-t} r_{t^{\prime}}^{n}-b\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)</script><p>但显然reward是不稳定的，它代表了采取action之后的reward的期望值，当sample次数不够多，其预估的也不准。</p><p>因此在这里将Q-learning引入到预估reward中，也即policy gradient和q-learning的结合。</p><p>也即我们将reward替换成$E\left[G_{t}^{n}\right]=Q^{\pi_{\theta}}\left(s_{t}^{n}, a_{t}^{n}\right)$。同时根据baseline的定义，我们将其替换成$V^{\pi_{\theta}}\left(s_{t}^{n}\right)$。</p><p>所以括号内的$\sum_{t^{\prime}=t}^{T_{n}} \gamma^{t^{\prime}-t} r_{t^{\prime}}^{n}-b$就变成$Q^{\pi \theta}\left(s_{t}^{n}, a_{t}^{n}\right)-V^{\pi_{\theta}}\left(s_{t}^{n}\right)$。</p><p>实际上我们不需要分别训练两个网络，直接整合成一个网络即可。也即将$Q^{\pi \theta}\left(s_{t}^{n}, a_{t}^{n}\right)-V^{\pi_{\theta}}\left(s_{t}^{n}\right)$改成$r_{t}^{n}+V^{\pi}\left(s_{t+1}^{n}\right)-V^{\pi}\left(s_{t}^{n}\right)$。</p><p>因此整个流程：</p><p><img src="/images/15612575693660.jpg" width="40%" height="50%"></p><p>形式化也即：</p><script type="math/tex; mode=display">\nabla \overline{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(r_{t}^{n}+V^{\pi}\left(s_{t+1}^{n}\right)-V^{\pi}\left(s_{t}^{n}\right)\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)</script><p>由于$\pi$和$V$的输入都是$s$，在实际操作中可以将这两个网络的前几层参数共享：</p><p><img src="/images/15612576221669.jpg" width="50%" height="50%"></p><p>同时对$\pi$的输出加以限制，希望有更大的entropy，这样能够探索更多情况。</p><h2 id="Pathwise-Derivative-Policy-Gradient"><a href="#Pathwise-Derivative-Policy-Gradient" class="headerlink" title="Pathwise Derivative Policy Gradient"></a>Pathwise Derivative Policy Gradient</h2><p>接下来介绍了一种新的方法，直接学习一个$\pi$，输入$s$可以获得能够最大化Q的action。这和GAN的思想很相似。</p><p><img src="/images/15612577121214.jpg" width="60%" height="50%"></p><p>这样$\pi$天然地能够处理continuous的情况。</p><p>所以整个流程：</p><p><img src="/images/15612577508356.jpg" width="55%" height="50%"></p><p>先交互，学习一个好的$Q$，然后将这个$Q$作为标准，学习$\pi$使得输出的$Q$最大。和GAN很像。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Reinforcement Learning </tag>
            
            <tag> 李宏毅强化学习课程 </tag>
            
            <tag> Actor-Critic </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DRL Lecture 5:Q-learning (Continuous Action)</title>
      <link href="/2019/06/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%205:%20Q-learning%20(Continuous%20Action)/"/>
      <url>/2019/06/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%205:%20Q-learning%20(Continuous%20Action)/</url>
      
        <content type="html"><![CDATA[<p>讨论了如何将Q-learning用于连续的action中。</p><p>前面提到 Q-learning就是：</p><script type="math/tex; mode=display">a=\arg \max _{a} Q(s, a)</script><p>若a是连续的，几种解决方案：</p><p>①sample一堆action$\left\{a_{1}, a_{2}, \cdots, a_{N}\right\}$，然后按照discrete的情况来处理。但精度不高，因为没法sample太多情况。</p><p>②使用gradient ascent来计算处理上式。该方法显然太耗时，因为每个sample都等于要训练一遍模型。</p><p>③设计专门的网络使得该优化可行。<br>首先输入state：</p><p><img src="/images/15612569949700.jpg" width="60%" height="50%"></p><p>获得一个$\mu$，$\Sigma$和$V$。接着和action交互：</p><script type="math/tex; mode=display">Q(s, a)=-(a-\mu(s))^{T} \Sigma(s)(a-\mu(s))+V(s)</script><p>显然,第一项若$\Sigma$半正定，必定小于等于0，所以当$a=\mu(s)$时$Q$最大。实际上$\Sigma$是通过先获得一个矩阵$A$，然后$A\times A^{T}$保证其正定性。</p><p>因此：</p><script type="math/tex; mode=display">\mu(s)=\arg \max _{a} Q(s, a)</script><p>④别用Q-learning处理连续的情况，因为处理还是比较麻烦的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Reinforcement Learning </tag>
            
            <tag> 李宏毅强化学习课程 </tag>
            
            <tag> Q-learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DRL Lecture 4:Q-learning (Advanced Tips)</title>
      <link href="/2019/06/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%204:%20Q-learning%20(Advanced%20Tips)/"/>
      <url>/2019/06/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%204:%20Q-learning%20(Advanced%20Tips)/</url>
      
        <content type="html"><![CDATA[<p>介绍一些进阶的Q-learning tips，能够帮助Q-learning提升表现。</p><h3 id="Double-DQN"><a href="#Double-DQN" class="headerlink" title="Double DQN"></a>Double DQN</h3><p>发现Q-value总是容易被高估，原因是算法中有$Q\left(s_{t}, a_{t}\right)=r_{t}+\max _{a} Q\left(s_{t+1}, a\right)$。该公式的max使得$Q$总是选择最大的action，使得$Q$的拟合总是偏大。</p><p><img src="/images/15612561913726.jpg" width="40%" height="50%"></p><p>那么在这里多加一个$Q^{\prime}$以规避上述情况，也即：</p><script type="math/tex; mode=display">Q\left(s_{t}, a_{t}\right)=r_{t}+Q^{\prime}\left(s_{t+1}, \arg \max _{a} Q\left(s_{t+1}, a\right)\right)</script><p>若$Q$高估了a，$Q^{\prime}$不高估那么$Q^{\prime}$的值也不会那么大则左式的值就不会被高估；若$Q^{\prime}$对某个action高估了，只要$Q$不高估该action，那么也不会选择该action。</p><h3 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h3><p>将模型结构做了改变：</p><p><img src="/images/15612563207080.jpg" width="60%" height="50%"></p><p>也即将$Q$分离开来。一种解释是这样的分离可以使得数据的使用更有效率，使得模型更为灵活。课件上还举了一个例子。 同时还可以对$A$加一些限制，比如向量和为0。</p><h3 id="Prioritized-Reply"><a href="#Prioritized-Reply" class="headerlink" title="Prioritized Reply"></a>Prioritized Reply</h3><p>对replay buffer进行改进。对TD error较大的优先sample，也即对那些学得不好的example优先学习。</p><h3 id="Multi-step"><a href="#Multi-step" class="headerlink" title="Multi-step"></a>Multi-step</h3><p>将MC和TD综合起来。给定$\left(s_{t}, a_{t}, r_{t}, \cdots, s_{t+N}, a_{t+N}, r_{t+N}, s_{t+N+1}\right)$，有：</p><p><img src="/images/15612565388930.jpg" width="65%" height="50%"></p><p>也即介于MC的整个episode完成后再计算和TD的每个step都计算一次。</p><h3 id="Noisy-Net"><a href="#Noisy-Net" class="headerlink" title="Noisy Net"></a>Noisy Net</h3><p>epsilon greedy也可以看做是加噪声，但是是加在action上：</p><script type="math/tex; mode=display">a=\left\{\begin{aligned} \arg \max _{a} Q(s, a), & \text {with probability } 1-\varepsilon \\ \text {random}, & \text { otherwise } \end{aligned}\right.</script><p>这样使得模型的行为不一致，可能不大好。</p><p>而Noisy Net是在parameter上加噪声。也即在episode开始之前对$Q$加噪声，变成$\tilde{Q}$：</p><script type="math/tex; mode=display">a=\arg \max _{a} \tilde{Q}(s, a)</script><p>而在episode期间不会改变noise。这样更有系统性的探索可能会更好，因为模型行为一致。</p><h3 id="Distributional-Q-function"><a href="#Distributional-Q-function" class="headerlink" title="Distributional Q-function"></a>Distributional Q-function</h3><p>基本思想是令$Q$预测每个行为的reward的分布而不仅仅是一个期望值。因为期望值损失了太多信息了，不同的distribution可能有同样大小的期望。</p><p><img src="/images/15612567428631.jpg" width="50%" height="50%"></p><p>实际上操作也即：</p><p><img src="/images/15612567546608.jpg" width="60%" height="50%"></p><p>Distributional Q-function往往不会高估expectation而是低估。因为在预测distribution时已经限定了最高和最低的范围了，对于那些大于或小于的值都忽略掉。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Reinforcement Learning </tag>
            
            <tag> 李宏毅强化学习课程 </tag>
            
            <tag> Q-learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DRL Lecture 3:Q-learning (Basic Idea)</title>
      <link href="/2019/06/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%203:%20Q-learning%20(Basic%20Idea)/"/>
      <url>/2019/06/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%203:%20Q-learning%20(Basic%20Idea)/</url>
      
        <content type="html"><![CDATA[<p>简单介绍Q-learning的思想以及相关的训练tips。</p><h3 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h3><p>与Policy Gradient不同的是，Q-Learning是属于value based，也即学习一个critic去估计特定actor π在某个state s下的累积reward。</p><p>注意到Q-learning虽然只学习了critic，但仍然可以用于做决策。</p><p>首先是如何预估critic？<br>critic的本质就是函数映射$V^{\pi}(s)$，输入$s$，输出一个scalar作为使用了actor $\pi$的累积reward。有两种方法：Monte-Carlo (MC) based approach和Temporal-difference (TD) approach。</p><h4 id="Monte-Carlo-MC-based-approach"><a href="#Monte-Carlo-MC-based-approach" class="headerlink" title="Monte-Carlo (MC) based approach"></a>Monte-Carlo (MC) based approach</h4><p>critic看完整个episode，然后对$s$做出预估：</p><p><img src="/images/15612136873633.jpg" width="35%" height="50%"></p><p>其实质上就是在做regression。</p><h4 id="Temporal-difference-TD-approach"><a href="#Temporal-difference-TD-approach" class="headerlink" title="Temporal-difference (TD) approach"></a>Temporal-difference (TD) approach</h4><p>由于有些episode非常长，等跑完再预估效率太低，因此直接对每个step进行预估。<br>对于一个time step $\cdots s_{t}, a_{t}, r_{t}, s_{t+1} \cdots$，直接预估：</p><p><img src="/images/15612137939102.jpg" width="70%" height="50%"></p><p>介绍完$V^{\pi}(s)$，还有一种critic，输入$s$和$a$以获得一个累积reward。这里和$V^{\pi}(s)$不同的是，对于同一个π，能够评估强制采用$a$所获得的reward。</p><p><img src="/images/15612139717700.jpg" width="70%" height="50%"></p><h3 id="如何使用critic决策"><a href="#如何使用critic决策" class="headerlink" title="如何使用critic决策"></a>如何使用critic决策</h3><p>在训练完了一个critic，如何用于进行决策？</p><p>其基本思想是：给定$Q$，总能找到一个$\pi^{\prime}$ 比$\pi$好，也即$V^{\pi^{\prime}}(s) \geq V^{\pi}(s)$。形式化：</p><script type="math/tex; mode=display">\pi^{\prime}(s)=\arg \max _{a} Q^{\pi}(s, a)</script><p>也就是说，在学习完一个特定$\pi$对应的$Q$后，只需要遵照每遇到一个state，选择能使$Q$最大化的action，该新的$\pi$就会比原来的$\pi$更优。</p><p>为什么一定会更优。提供证明：</p><p><img src="/images/15612142985976.jpg" width="60%" height="50%"></p><p>每一步选择最优都会比原来好一些。</p><h3 id="如何训练Q-function"><a href="#如何训练Q-function" class="headerlink" title="如何训练Q function"></a>如何训练Q function</h3><p>我们通过类似TD的方法来训练。给定$\cdots s_{t}, a_{t}, r_{t}, s_{t+1} \cdots$,我们有：</p><script type="math/tex; mode=display">\mathrm{Q}^{\pi}\left(s_{t}, a_{t}\right)=r_{t}+\mathrm{Q}^{\pi}\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)</script><p>其中由于公式有两个Q，若让两个Q同时变，不好做回归。因此我们让右边的Q固定住，训练左边的Q，在更新完几次后，直接将左边的Q覆盖右边。重复多次：</p><p><img src="/images/15612553664628.jpg" width="65%" height="50%"></p><p>另一个问题是如何收集数据？</p><p>由于action是基于Q函数的，也即每次都采用贪心的策略，会使得在初始情况下固定的action会一直出现，无法探索到其他情况。因此采用两种策略：</p><p>①epsilon greedy：</p><p><script type="math/tex">a=\left\{\begin{aligned} \arg \max _{a} Q(s, a), & \text { with probability } 1-\varepsilon \\ \text {random}, & \text { otherwise } \end{aligned}\right.</script></p><p>其中$\varepsilon$随着时间而逐渐变小。</p><p>②boltzmann exploration：<br>按概率采样</p><script type="math/tex; mode=display">P(a | s)=\frac{\exp (Q(s, a))}{\sum_{a} \exp (Q(s, a))}</script><p>还有一个小技巧用于更好利用sample，也即replay buffer：将每次π与环境交互的episode都放在一个buffer里面，可以都用来学习Q函数，即使数据来自不同的policy也没关系。这和off-policy有点像。为什么可以这么用一种解释是这样可以使得数据更diverse，同时减少sample次数加快训练。</p><p><img src="/images/15612559617307.jpg" width="40%" height="50%"></p><p>因此一个典型的q-learning则是：</p><p><img src="/images/15612560018865.jpg" width="60%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Reinforcement Learning </tag>
            
            <tag> 李宏毅强化学习课程 </tag>
            
            <tag> Q-learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文22</title>
      <link href="/2019/06/22/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8722/"/>
      <url>/2019/06/22/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8722/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>Boosting Neural Machine Translation</li><li>Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks</li><li>On The Power of Curriculum Learning in Training Deep Networks</li><li>XLNet: Generalized Autoregressive Pretraining for Language Understanding</li><li>An Empirical Exploration of Curriculum Learning for Neural Machine Translation</li></ol><h2 id="Boosting-Neural-Machine-Translation"><a href="#Boosting-Neural-Machine-Translation" class="headerlink" title="[Boosting Neural Machine Translation]"></a>[Boosting Neural Machine Translation]</h2><p>通过将机器学习中的boosting引入NMT，对翻译效果有一定提升。同时还提出了另外几种方法，对输入数据pipeline进行了修改，发现都有一定的提升。本文的中心思想就是focus on difficult examples，作者认为更多关注于difficult example，能够对模型有提升的作用。</p><h3 id="几种policy"><a href="#几种policy" class="headerlink" title="几种policy"></a>几种policy</h3><p><img src="/images/15611921654433.jpg" width="40%" height="50%"></p><p>original：就是将所有的数据都过一遍<br>boost：将最难的10%重复一遍<br>reduce：将最简单的20%去掉。具体操作是，每个epoch重新衡量一次，每三个epoch作为一个训练，也即三个epoch内部分别使用100% 80% 64%的数据<br>bootstrap：每个epoch都re-sample一遍，也即允许重复以及部分句子消失。</p><p>难度是通过perplexity来衡量的，因为每个epoch在训练时就已经计算过perplexity了，因此没有引入额外的计算复杂度。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>论文的实验采用的是双向LSTM作为翻译的模块。</p><p><img src="/images/15611922669857.jpg" width="40%" height="50%"></p><p><img src="/images/15611922830047.jpg" width="40%" height="50%"></p><p>几个实验结论：<br>boosting能够加速拟合，并且结果更好；<br>reduce用更少的数据，达到最好的效果。这点令人印象深刻<br>boostrap，稍微好一些，且训练更稳定。</p><p>为什么要focus on difficult example?</p><blockquote><p>We emulate a human spending additional energy on learning complex concepts<br>To force the system to pay much attention on them can adjust it towards “mastering” more information for these sentences.</p></blockquote><h3 id="几个想法"><a href="#几个想法" class="headerlink" title="几个想法"></a>几个想法</h3><p>这篇文章是一篇短文，很明显很多实验没做，估计是到deadline了就提交了，比如分析不同比例的结果，以及一些消融实验也没做。</p><p>为什么boostrap能够有提升并且有更稳定的训练？这种resample的方式能够带来一定的uncertainty，可能会有一定的帮助，虽然帮助不大，论文里面也仅仅提到了uncertainty，显然应该做进一步的分析。</p><p>这篇论文提供的insight我认为还是有一定启发的，首先这并不是curriculum-learning，也即没有从简单到难，而是正常的训练，只不过通过增加更多的difficult example，同时去掉了部分太简单的sample，说明仅仅是修改数据分布而不是修改数据的输入顺序（本质上也是修改数据分布），也能够带来提升效果；第二，通过减少简单数据，是否意味着，每个模型都有一个下限（特别是对于神经网络这种能力很强的模型），低过这个下限的数据对模型的训练是没有帮助的，反而可能会使模型overfit到某个简单的pattern（这和learning to execute的结论似乎有些类似）；同时，增加更多的difficult sample，使得模型的上限被提高了；以及，是否可以将curriculum learning与该思路结合起来，达到更好的结果，一方面由易到难，另一方面修改数据分布，使得模型更多关注难的数据。</p><hr><h2 id="Curriculum-Learning-by-Transfer-Learning-Theory-and-Experiments-with-Deep-Networks"><a href="#Curriculum-Learning-by-Transfer-Learning-Theory-and-Experiments-with-Deep-Networks" class="headerlink" title="[Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks]"></a>[Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks]</h2><p>ICML18的文章，极其硬核，根本看不懂理论的部分。</p><p>先贴出ICML oral的三张PPT：</p><p><img src="/images/15611926258594.jpg" width="60%" height="50%"></p><p><img src="/images/15611926643253.jpg" width="60%" height="50%"></p><p><img src="/images/15611926800604.jpg" width="60%" height="50%"></p><p>从理论上证明了：</p><p>①the rate of convergence of an ideal curriculum learning method is monotonically increasing with the difﬁculty of the examples</p><p>②convergence is faster when using points which incur higher loss with respect to the current hypothesis.<br>当difficulty score是固定的，对于current hypothesis而言，高的loss能够比低的loss拟合速度更快。该结论非常直观。也就是说，难的example更有益是针对当前而言（current hypothesis）而简单的example更有益是针对final hypothesis而言的。</p><p>③使用pretrain好的模型来作为difficulty的估计，a signiﬁcant boost in convergence speed at the beginning of training</p><p>④使用curriculum learning能够显著加速拟合；当任务难度（这和模型本身的容量也有关，模型越弱相对的任务难度也就越大，同时也和regularization相关，越强代表模型的自由度越弱）越大时，使用CL的效果就越明显。</p><p>几个结果：</p><p><img src="/images/15611928935044.jpg" width="90%" height="50%"></p><p><img src="/images/15611929075096.jpg" width="45%" height="50%"></p><p>思考：<br>文中的结论还是可以参考参考的，但theoretical的结论毕竟是在凸函数上得到的，似乎说服力不大。文中的思路是从理论上证明凸函数的结论；然后通过实验在非凸函数上从实践证明相似结论。其主要的贡献在于一些CL相关的结论和引入transfer learning作为difficulty score。</p><hr><h2 id="On-The-Power-of-Curriculum-Learning-in-Training-Deep-Networks"><a href="#On-The-Power-of-Curriculum-Learning-in-Training-Deep-Networks" class="headerlink" title="[On The Power of Curriculum Learning in Training Deep Networks]"></a>[On The Power of Curriculum Learning in Training Deep Networks]</h2><p>ICML19一篇很硬核的文章，说实话里面的证明以及部分实验设计我还是没怎么搞懂。但是一些结论值得注意。这属于有启发的一类论文。</p><p>通过transfer-learning和self-taught的方法获得新的curriculum-learning算法，同时通过理论证明获得了一些有启发性的结论。</p><p>curriculum learning有两个挑战： 如何定义数据的难度；以及数据喂给模型的速度，太快会让模型更confused，太慢导致学习太慢</p><p>本文对这两个挑战都有一定的解决方案：分别定义了scoring function 和 pacing function</p><p>scoring function有两种：transfer learning和self-tutoring，一个就是pretrained model，另一个是使用训练好的未采用curriculum learning的模型。</p><p>pacing function：①Fixed exponential pacing没固定次数的step就提升一下 ②Varied exponential pacing 提升的step可以是变化的 ③Single-step pacing 简化版的①</p><p><img src="/images/15611932107816.jpg" width="50%" height="50%"></p><p>关于current hypothesis与target hypothesis：</p><p>有些方法中（self-paced learning hard example mining 或 active learning）更倾向于hard example。实际上和CL不同，是因为focus on hard example是基于当前模型的状态去定义难度的（current hypothesis），CL则是基于最终的状态（target hypothesis）。实际上这两种并不矛盾，有研究表明模型可以同时受益于这两种。这篇文章也从理论角度去证明了这一结论。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>关于curriculum by transfer的结论：<br>Curriculum learning is clearly and signiﬁcantly beneﬁcial - learning starts faster, and converges to a better solution.<br>the observed advantage of CL is more signiﬁcant when the task is more difﬁcult（很直观，因为越难的任务越需要CL）</p><p>其中anti-curriculum指的是按照难度从高到低排；random则是对难度随机排：</p><p><img src="/images/15611933223227.jpg" width="70%" height="50%"></p><p>其他结论：</p><p>使用不同的transfer function都指向了相似的gradient方向；与使用所有数据相比，transfer function则指向了不同的方向；同时使用所有数据的gradient和使用random scoring function的gradient相似，说明random能够较为合理的去estimate真正的empirical gradient。如图：</p><p><img src="/images/15611934853212.jpg" width="65%" height="50%"></p><h3 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h3><p>略过大量公式。直接谈结论：</p><p>①通过CL修改后的optimization landscape拥有和原来一样的optimization function；并且修改后的global maximum 比原先的更明显（pronounced）</p><p>②如果数据分布p和最优的utility $U_{\tilde{\vartheta}}(X)$ 是正相关的，且比其他的$U_{\vartheta}(X)$更正相关，那么往optimal parameter $\tilde{\vartheta}$ 总体上会更加steeper（陡峭）。</p><p>③the optimization landscape is modiﬁed to amplify the difference between the optimal parameters vector and all other parameter values whose covariance with the optimal solution (the covariance is measured between the induced prior vectors) is small, and speciﬁcally smaller than the variance of the optimum. </p><h3 id="结论与思考"><a href="#结论与思考" class="headerlink" title="结论与思考"></a>结论与思考</h3><p>就我的理解而言，本文的最大贡献就是：统一了原先的从简单到难（curriculum learning或self-paced learning）和focus on difficulty examples（boosting或hard data mining），只要修改后的数据分布与optimal utility是正相关的，那么就可以提升表现，因此两种strategy都是有效的。 It may even be possible to find a curriculum which is directly correlated with the optimal utility, and that outperforms both methods</p><p>不过这篇文章有些奇怪，empirical和theoretical的部分完全割裂的感觉。</p><hr><h2 id="XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding"><a href="#XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding" class="headerlink" title="[XLNet: Generalized Autoregressive Pretraining for Language Understanding]"></a>[XLNet: Generalized Autoregressive Pretraining for Language Understanding]</h2><p>最近比较火的文章，对Bert的全面超越。将bert的双向和context以及language model的long range dependency巧妙结合，获得新的pretrain model。</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>预训练语言模型可以分为两种 autoregressive(AR)语言模型和 autoencoding（AE）。AR就是传统的语言模型，从前到后或从后到前预测，典型的就是GPT；AE则是通过受破坏的数据还原出原始数据，Bert就是其中一员。</p><p>但这两种方法各有缺点：</p><p>bert的AE方法假设了所有被预测的token是独立的（也即mask掉的词相互之间是独立的，也即上一个mask的词并不能对预测下一个mask的词有帮助），但自然语言中这种依赖关系应该是存在的；同时[Mask]在真实数据中并不存在，也即存在input noise，导致pretrain-ﬁnetune discrepancy。</p><p>而AR的问题主要在没有充分利用前后的上下文，只使用了部分。</p><p>本文通过permutation来达到规避这两个缺点的目的，也即 既利用了前后上下文，又没有input noise，同时还没有independence assumption。</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>定义一个factorization orders（注意是虚拟的顺序，原始顺序还是会保留的），将原始的顺序打乱。使得一个词的预测可以由两侧的词来帮助。</p><p>如图，输入的原始顺序还是不变，但通过 mask attention来达到不同的factorization order的目的，在不同order下预测同一个$x_3$，由于factorization order的顺序不同，在3之前的词发挥了作用，而在他之后的词就没有参与预测。</p><p><img src="/images/15611948252404.jpg" width="70%" height="50%"></p><p>形式化则有：</p><script type="math/tex; mode=display">\max _{\theta} \mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_{T}}\left[\sum_{t=1}^{T} \log p_{\theta}\left(x_{z_{t}} | \mathbf{x}_{\mathbf{z}_{<t}}\right)\right]</script><p>其中$z$是permutation/factorization order。</p><p>显然这个模型如果只使用原来的transformer结构是有问题的，也即target position unaware。假设今天有两种factorization order，在t之前的内容都是一致的，在t上则有不同的词，那么他们预测的分布都会是一样的，但按理说不应该一样，因为target不一样。所以需要让target发挥作用。</p><p>因此在这里修改了一下transformer架构，引入Two-Stream Self-Attention for Target-Aware Representations。</p><p><img src="/images/15611949233220.jpg" width="90%" height="50%"></p><p>可以看到，现在兵分两路，每一层都得到两个表示。其中$h$和原来transformer一样，而$g$则是新引进的，也即在Q中只使用<strong>position</strong>而没有content。</p><p>形式化有：</p><script type="math/tex; mode=display">\begin{array}{l}{g_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathbf{Q}=g_{z_{t}}^{(m-1)}, \mathbf{K V}=\mathbf{h}_{\mathbf{z}<t}^{(m-1)} ; \theta\right)} \text{  (query stream: use } z_t \text{ but cannot see }  x_{z_t}) \\ {h_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathbf{Q}=h_{z_{t}}^{(m-1)}, \mathbf{K V}=\mathbf{h}_{\mathbf{z} \leq t}^{(m-1)} ; \theta\right)} \text{  (content stream: use both } z_t \text{ and }  x_{z_t})\end{array}</script><p>所以，进行预测词操作的时候只使用$g$去预测相应位置上的内容即可。</p><p>一些细节：<br>① 为了让训练更容易，只预测最后的几个tokens（factorization order的最后几个）；<br>②将transformer-xl引入，也即相对位置和历史信息的idea<br>③引入相对位置的segment encoding，使得更灵活，因为这样就可以encode超过两个输入的segment了，使用absolute segment则不行.</p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>相对bert而言，有更多的上下文信号，因为bert使用了mask，使得mask之间不能相互帮助。同时也有原先language model的特点，也即顺序预测的特点，这样就可以直接用于一些有该特点的下游任务。作者整个思路行云流水，并且对模型的本质看得很透。</p><p>但这种方法因为要保存两份hidden state，会需要更多的内存和计算资源。</p><p>这篇文章的结果很强，且模型也有说服力。但训练使用了512张TPU，以及用了超过Bert的数据量（数据量是否对超过Bert有很大的帮助？）。不得不说这类文章普通人只能看看，NLP已经进入了军备竞赛了。</p><hr><h2 id="An-Empirical-Exploration-of-Curriculum-Learning-for-Neural-Machine-Translation"><a href="#An-Empirical-Exploration-of-Curriculum-Learning-for-Neural-Machine-Translation" class="headerlink" title="[An Empirical Exploration of Curriculum Learning for Neural Machine Translation]"></a>[An Empirical Exploration of Curriculum Learning for Neural Machine Translation]</h2><p>讨论了一些关于在NMT上使用CL的策略（相比原本的CL更加灵活），并做了一系列实验得到一些结论。</p><h3 id="策略创新"><a href="#策略创新" class="headerlink" title="策略创新"></a>策略创新</h3><p>①<br>首先是将sample distribution的概念扩展到shard的层面而不是单一的example。也即：</p><p><img src="/images/15612123996612.jpg" width="50%" height="50%"></p><p>将example结成group。将相似difficulty的放在同一个shard里面。</p><p>具体应如何做？有三种方案：<br>①设定一个难度score的阈值，显然这个方法不好弄，因为不好手动设阈值。<br>②直接等大小分，每个shard的个数一样，但这样可能会带来shard内部的难度score的波动。  ③本文采用的是<strong>Jenks Natural Breaks classiﬁcation algorithm</strong>，也即shard内部的variance尽量小，shard之间的variance尽量大</p><p>②<br>第二是sample difficulty criteria：<br>采用了两种方法：一个是训练辅助（小的）模型来做判断；另一个是采用linguistic的feature</p><p>第一种也即Model-based Difﬁculty Criteria，给定source sentence，获得target的概率。<br>第二种是Linguistic Difﬁculty Criteria，也即word frequency，然后将句子按照least frequent word来排序（这实际上和逐步添加词表大小，然后训练所有词都在该词表的句子是等价的）</p><p>③<br>第三是schedule：</p><p><img src="/images/15612125916364.jpg" width="60%" height="50%"></p><p>每一行代表一个阶段（phase）。</p><p>注意到default是有shuffle的（shard之间的shuffle）；而noshuffle是shard内部有shuffle，但之间没有shuffle。</p><h3 id="实验结论"><a href="#实验结论" class="headerlink" title="实验结论"></a>实验结论</h3><p>似乎严格按照顺序来做（noshuffle)并没有帮助（相对default而言）；<br>对learning rate敏感；<br>CL确实有帮助，difficulty criteria是关键，词表频率和使用小模型来做标准都有用，但在这里句子长度没用。</p><h3 id="几个思考"><a href="#几个思考" class="headerlink" title="几个思考"></a>几个思考</h3><p>实验做得有点奇怪；论文中的phase似乎是每次数据过一遍就到下一个phase了，然而shard分得也太少了，那其实前几个epoch就把CL的phase全部走完了；<br>为什么noshuffle没有帮助，是否意味着在一个phase内部严格按照从易到难是没有帮助的，而模型更需要的是那些对它当前最难的那一批，而在这个phase内是先出现还是后出现都没有关系？这样是否可以在phase内部使用boosting？或者干脆删掉不重要的例子。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> Curriculum Learning </tag>
            
            <tag> NMT </tag>
            
            <tag> Language Modeling </tag>
            
            <tag> boosting </tag>
            
            <tag> XLNet </tag>
            
            <tag> pretrain </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DRL Lecture 2:Proximal Policy Optimization (PPO)</title>
      <link href="/2019/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%202:%20Proximal%20Policy%20Optimization%20(PPO)/"/>
      <url>/2019/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%202:%20Proximal%20Policy%20Optimization%20(PPO)/</url>
      
        <content type="html"><![CDATA[<p>Policy Gradient的问题：每次sample的data只能使用一次，较为耗费时间。因此引入off-policy，每次sample的数据可以多次使用。</p><p>特点：on-policy中学习的agent与和环境交互的agent是一致的；而off-policy则是有两个agent，其中一个agent负责与环境交互来获得episode，另一个agent则通过这些episode更新参数。</p><p>在这里利用importance-sampling来达到这一目的。</p><h3 id="importance-sampling"><a href="#importance-sampling" class="headerlink" title="importance-sampling"></a>importance-sampling</h3><p>给定概率$p$要计算$f(x)$的期望：可以采用sample的方式来达到该目的。也即：</p><script type="math/tex; mode=display">E_{x \sim p}[f(x)] \approx \frac{1}{N} \sum_{i=1}^{N} f\left(x^{i}\right)</script><p>而若$p$分布无法获得，可以利用另一个可获得的分布$q$来近似。也即：</p><script type="math/tex; mode=display">E_{x \sim p}[f(x)]=\int f(x) p(x) d x=\int f(x) \frac{p(x)}{q(x)} q(x) d x=E_{x \sim q}[f(x) \frac{p(x)}{q(x)}]</script><p>然后再由$q$来做sampling。</p><p>注意这里$p$与$q$的分布不应该差距太大，否则其variance则会相差很大，造成近似结果差距较大。</p><h3 id="off-policy"><a href="#off-policy" class="headerlink" title="off-policy"></a>off-policy</h3><p>将importance sampling用于off-policy。</p><p>原来：</p><script type="math/tex; mode=display">\nabla \overline{R}_{\theta}=E_{\tau \sim p_{\theta}(\tau)}\left[R(\tau) \nabla \log p_{\theta}(\tau)\right]</script><p>变成：</p><script type="math/tex; mode=display">\nabla \overline{R}_{\theta}=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\frac{p_{\theta}(\tau)}{p_{\theta^{\prime}}(\tau)} R(\tau) \log p_{\theta}(\tau)\right]</script><p>其中$p_{\theta^{\prime}}$是另一个agent的分布，也即data是从该agent的分布sample得到的，因此可以使用该data来多次训练θ。</p><p>因此梯度公式则为：</p><script type="math/tex; mode=display">\begin{array}{l}{=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta}}\left[A^{\theta}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right]} \\ {=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{P_{\theta}\left(s_{t}, a_{t}\right)}{P_{\theta^{\prime}}\left(s_{t}, a_{t}\right)} A^{\theta}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right]} \\ {=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} \frac{p_{\theta}\left(s_{t}\right)}{p_{\theta^{\prime}}\left(s_{t}\right)} A^{\theta}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right]}\end{array}</script><p>其中，我们将$A^{\theta}\left(s_{t}, a_{t}\right)$改成$A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)$，因为数据是从$\theta^{\prime}$ 来的；同时我们假设$\frac{p_{\theta}\left(s_{t}\right)}{p_{\theta^{\prime}}\left(s_{t}\right)}=1$，一方面是不方便计算，作此假设可以方便计算；另一方面该假设也有一定合理性，因为某个state出现的几率应该和agent的关系较小。</p><p>通过梯度公式我们可以还原出原式： </p><script type="math/tex; mode=display">J^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]</script><p>另一个问题是，在什么时候停止使用同一组data更新参数？也即同一组data能够更新参数几次？</p><p>一个原则是：我们希望两个agent的分布差异小一些，因为分布差异一旦大了，variance则会变大，因此这里添加一个constraint。</p><script type="math/tex; mode=display">J_{P P O}^{\theta^{\prime}}(\theta)=J^{\theta^{\prime}}(\theta)-\beta K L\left(\theta, \theta^{\prime}\right)</script><p>注意这里的KL散度是对action分布的KL散度而不是参数的。</p><p>所以最终的算法为：</p><p><img src="/images/15606497157322.jpg" width="60%" height="50%"></p><p>其中$\theta_{k}$是上次更新完的agent的参数。也即与环境交互的agent的参数总是上个iteration 另一个agent更新后的参数。</p><p>还有几个细节，$\beta$可以是adaptive的；以及PPO2对PPO中constraint的改进，不过都是细枝末节。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Reinforcement Learning </tag>
            
            <tag> 李宏毅强化学习课程 </tag>
            
            <tag> PPO </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DRL Lecture 1:Policy Gradient (Review)</title>
      <link href="/2019/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%201:%20Policy%20Gradient%20(Review)/"/>
      <url>/2019/06/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/DRL%20Lecture%201:%20Policy%20Gradient%20(Review)/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.youtube.com/playlist?list=PLJV_el3uVTsODxQFgzMzPLa16h6B8kWM_" target="_blank" rel="noopener">李宏毅深度强化学习课程</a>笔记。</p><hr><p>本lecture主要是复习强化学习的policy gradient。基本的介绍都在之前的<a href="http://www.linzehui.me/2018/10/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2018:%20Deep%20Reinforcement%20Learning/">强化学习笔记</a>里面。</p><p>基本大致框架为：</p><p><img src="/images/15606479563499.jpg" width="50%" height="50%"></p><p>每次sample几个episode，然后更新模型：</p><p><img src="/images/15606479845586.jpg" width="50%" height="50%"></p><p>值得提的几个新的点：<br>①我们要对reward加baseline，因为防止reward一直为正，鼓励其他出现几率小但reward高的episode被sample到。因此加一个baseline：</p><script type="math/tex; mode=display">\nabla \overline{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(R\left(\tau^{n}\right) -b \right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)</script><p>其中$b$可以设为$b \approx E[R(\tau)]$</p><p>②对每个action应该分配不同的weight，鼓励reward高的action出现：<br>一种方法，是将全局的R换成累积的R：</p><script type="math/tex; mode=display">\nabla \overline{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(\sum_{t^{\prime}=t}^{T_{n}} r_{t^{\prime}}^{n} -b \right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)</script><p>也即当前action到episode结束累加的reward。</p><p>更进一步，添加衰减因子，其中$\gamma&lt;1$：</p><script type="math/tex; mode=display">\nabla \overline{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(\sum_{t^{\prime}=t}^{T_{n}} \gamma^{t^{\prime}-t} r_{t^{\prime}}^{n} -b \right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)</script><p>当然，为了简便，将reward $R(\tau^{n})$ 统一写成 $A^θ (s_t,a_t )$，代表的是在当前state下采用action的reward。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Reinforcement Learning </tag>
            
            <tag> 李宏毅强化学习课程 </tag>
            
            <tag> Policy Gradient </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文21</title>
      <link href="/2019/06/16/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8721/"/>
      <url>/2019/06/16/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8721/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>Named Entity Recognition with Bidirectional LSTM-CNNs</li></ol><h2 id="Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs"><a href="#Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs" class="headerlink" title="[Named Entity Recognition with Bidirectional LSTM-CNNs]"></a>[Named Entity Recognition with Bidirectional LSTM-CNNs]</h2><p>关于NER的经典论文。</p><p>总的结构很简单，其实就是利用word embedding + CNN-char features + additional word features作为总的features，过一个双向LSTM获得上下文相关的向量表示，最终获得分类结果。</p><p><img src="/images/15606462750491.jpg" width="55%" height="50%"></p><p>其中CNN的feature是character级别的：</p><p><img src="/images/15606463343904.jpg" width="55%" height="50%"></p><p>同时还加了一些人工的feature：比如大写的feature；比如外部词典等。</p><p>最终</p><p><img src="/images/15606463626767.jpg" width="50%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> NER </tag>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词28</title>
      <link href="/2019/06/16/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D28/"/>
      <url>/2019/06/16/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D28/</url>
      
        <content type="html"><![CDATA[<h3 id="蝶恋花"><a href="#蝶恋花" class="headerlink" title="蝶恋花"></a>蝶恋花</h3><p>[宋] 欧阳修<br>庭院深深深几许？杨柳堆烟，帘幕无重数。玉勒雕鞍游冶处，楼高不见章台路。<br>雨横风狂三月暮，门掩黄昏，无计留春住。<strong>泪眼问花花不语，乱红飞过秋千去</strong>。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识22</title>
      <link href="/2019/06/10/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8622/"/>
      <url>/2019/06/10/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8622/</url>
      
        <content type="html"><![CDATA[<h3 id="Sequence-Labeling"><a href="#Sequence-Labeling" class="headerlink" title="[Sequence Labeling]"></a>[Sequence Labeling]</h3><p>介绍sequence labeling博客：<br><a href="https://www.cnblogs.com/jiangxinyang/p/9368482.html" target="_blank" rel="noopener">https://www.cnblogs.com/jiangxinyang/p/9368482.html</a></p><p>关于几个任务的定义：</p><p>命名实体识别（Named Entity Recognition, NER)：从文本中识别出命名实体，实体一般包括人名(PER)、地名(LOC)、机构名(ORG)、时间、日期、货币、百分比等。<br>组块分析 (Chunking)：标出句子中的短语块，例如名词短语（NP），动词短语（VP）等<br>词性标注 (Part-of-speech Tagging, POS)：确定文本中每个词的词性。词性包括动词（Verb）、名词（Noun）、代词（pronoun）等。</p><h3 id="Active-learning"><a href="#Active-learning" class="headerlink" title="[Active learning]"></a>[Active learning]</h3><p>定义：</p><p>样本信息就是说在训练数据集当中每个样本带给模型训练的信息是不同的，即每个样本为模型训练的贡献有大有小，它们之间是有差异的。<br>因此，为了尽可能地减小训练集及标注成本，在机器学习领域中，提出主动学习（active learning）方法，优化分类模型。<br>主动学习(active learning)，指的是这样一种学习方法：<br>有的时候，有类标的数据比较稀少而没有类标的数据是相当丰富的，但是对数据进行人工标注又非常昂贵，这时候，学习算法可以主动地提出一些标注请求，将一些经过筛选的数据提交给专家进行标注。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Sequence Labeling </tag>
            
            <tag> Active learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文20</title>
      <link href="/2019/06/09/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8720/"/>
      <url>/2019/06/09/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8720/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>Neural Machine Translation in Linear Time</li><li>Curriculum Learning</li><li>Bilingual Word Embeddings for Phrase-Based Machine Translation</li><li>LEARNING TO EXECUTE</li><li>Self-Paced Learning for Latent Variable Models</li></ol><h2 id="Neural-Machine-Translation-in-Linear-Time"><a href="#Neural-Machine-Translation-in-Linear-Time" class="headerlink" title="[Neural Machine Translation in Linear Time]"></a>[Neural Machine Translation in Linear Time]</h2><p>使用新的结构将翻译控制在线性复杂度。感觉思路还蛮清晰，挺新颖的。</p><p><img src="/images/15600510169611.jpg" width="55%" height="50%"></p><p>主要有三个点：</p><p>第一，结构上采用bytenet，也即采用了空洞卷积的网络，使得计算代价减小。</p><p>第二，直接将decoder堆在encoder上，每次decoder只取对应对应位置上的encoder，这和一般的基于encoder-decoder的方法不同。</p><p>第三，采用dynamic unfolding以解决encoder与decoder长度不同的问题。人工预先定义好decoder的上界：</p><script type="math/tex; mode=display">|\hat{\mathbf{t}}|=a|\mathbf{s}|+b</script><p>也即encoder将会生成这么多的representation。</p><p>而实际上target是可以超出这个上界的，直到生成EOS为止，如果超出上界则在该步不利用encoder的representation，上界只是用以指导encoder应该生成多少representation。</p><p><img src="/images/15600511595880.jpg" width="80%" height="50%"></p><p>该模型非常轻量级；但我还是没有怎么搞懂encoder如何生成出比source长度还长的信息的。</p><hr><h2 id="Curriculum-Learning"><a href="#Curriculum-Learning" class="headerlink" title="[Curriculum Learning]"></a>[Curriculum Learning]</h2><p>课程学习的开山之作。对curriculum learning进行了形式化的总结，并且通过几个实验对curriculum learning的性质进行了探索。该文章属于对其他人有启发的性质。</p><p>curriculum learning的思想：在训练过程中，先使用简单的样例进行训练，然后逐渐将难的样例加入到训练样例中。本质就是从易到难的过程，符合人的学习过程，并且能够获得更好的效果。</p><p>形式化：<br>感觉不重要，其实就是对data distribution进行了重排。</p><p>性质：</p><p>①获得更好的结果 </p><p>②能够加速拟合，‘Cleaner Examples May Yield Better Generalization Faster ’，因为简单的example能够避免confuse the learner </p><p>③curriculum learning展现出regularizer的性质，也即在同样training error的情况下能够获得更低的generalization error。</p><p>对于learner来说，在训练的每个阶段都有对他来说太简单和太难的知识，太简单的学不到什么，太难的学不会。因此在每个阶段选择对learner而言interesting的例子，不太难也不太简单的，是最好的。</p><p>同时curriculum learning与boosting algorithm不同，因为boosting algorithm 是强调难的example而curriculum learning只是每个阶段按照难度对example赋予不同的权值。</p><p>curriculum learning可以看做是transfer learning的一种特殊形式。transfer learning是利用原始任务来引导learner在最终任务获得更好的结果；而curriculum learning就是利用训练上的技巧来引导learner获得更好的最终结果。</p><p>其实curriculum learning的难点就在于如何定义easy example，因为不同任务可能有不同的定义。</p><hr><h2 id="Bilingual-Word-Embeddings-for-Phrase-Based-Machine-Translation"><a href="#Bilingual-Word-Embeddings-for-Phrase-Based-Machine-Translation" class="headerlink" title="[Bilingual Word Embeddings for Phrase-Based Machine Translation]"></a>[Bilingual Word Embeddings for Phrase-Based Machine Translation]</h2><p>讲关于训练双语word embedding的。我只关注curriculum learning部分，但其实讲的不多。</p><hr><h2 id="LEARNING-TO-EXECUTE"><a href="#LEARNING-TO-EXECUTE" class="headerlink" title="[LEARNING TO EXECUTE]"></a>[LEARNING TO EXECUTE]</h2><p>这篇论文是使用LSTM来评估短的计算机程序。我比较关注curriculum learning的部分，其中的一些解释或许能够有一些启发。</p><p>本文在使用传统curriculum learning训练的过程中，发现效果并不好，因此转而使用一种混合训练方法。也即部分随机采样，部分采用传统curriculum learning的逐渐提升难度。</p><p>为什么传统的不够好？<br>作者的解释是，如果从简单的开始学，LSTM会将所有的memory用以记忆简单的样本，在训练过程中就构建好了这套pattern，而在逐渐增加难度时，要求对memory的pattern重组，而这点不容易做到。而如果采样部分随机样本，则能够同时学习到部分难的example，从而<strong>防止overfit到某个简单的pattern上</strong>。</p><hr><h2 id="Self-Paced-Learning-for-Latent-Variable-Models"><a href="#Self-Paced-Learning-for-Latent-Variable-Models" class="headerlink" title="[Self-Paced Learning for Latent Variable Models]"></a>[Self-Paced Learning for Latent Variable Models]</h2><p>传统机器学习的优化算法。<del>证明了curriculum learning也能在传统机器学习算法中取得效果。</del> 7.2更新：之前理解错了，self-paced learning与Curriculum Learning虽有共通之处但并不是一个东西。CL的difficulty score是在训练前固定的，而SPL是在训练过程中根据example的loss来动态决定的。</p><script type="math/tex; mode=display">\left(\mathbf{w}_{t+1}, \mathbf{v}_{t+1}\right)=\underset{\mathbf{w} \in \mathbb{R}^{d}, \mathbf{v} \in\{0,1\}^{n}}{\operatorname{argmin}}\left(r(\mathbf{w})+\sum_{i=1}^{n} v_{i} f\left(\mathbf{x}_{i}, \mathbf{y}_{i} ; \mathbf{w}\right)-\frac{1}{K} \sum_{i=1}^{n} v_{i}\right)</script><p>只有那些能够很好fit的sample作数，也即使用二元变量v来控制。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> Curriculum Learning </tag>
            
            <tag> NMT </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词27</title>
      <link href="/2019/06/09/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D27/"/>
      <url>/2019/06/09/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D27/</url>
      
        <content type="html"><![CDATA[<h3 id="夜上受降城闻笛"><a href="#夜上受降城闻笛" class="headerlink" title="夜上受降城闻笛"></a>夜上受降城闻笛</h3><p>[唐] 李益<br>回乐峰前沙似雪，受降城外月如霜。<br><strong>不知何处吹芦管，一夜征人尽望乡</strong>。</p><p><a href="http://lib.xcz.im/work/57b91effa633bd00665e4f15" target="_blank" rel="noopener">http://lib.xcz.im/work/57b91effa633bd00665e4f15</a></p><hr><h3 id="饮中八仙歌"><a href="#饮中八仙歌" class="headerlink" title="饮中八仙歌"></a>饮中八仙歌</h3><p>[唐] 杜甫<br>…<br>李白斗酒诗百篇，长安市上酒家眠，<br>天子呼来不上船，自称臣是酒中仙。<br>…</p><p><a href="http://lib.xcz.im/work/57b8e3a7c4c97100558e7dc6" target="_blank" rel="noopener">http://lib.xcz.im/work/57b8e3a7c4c97100558e7dc6</a></p><p>新唐书·李白传》载：李白应诏至长安，唐玄宗在金銮殿召见他，并赐食，亲为调羹，诏为供奉翰林。有一次，玄宗在沉香亭召他写配乐的诗，而他却在长安酒肆喝得大醉。范传正《李白新墓碑》载：玄宗泛舟白莲地，召李白来写文章，而这时李白已在翰林院喝醉了，玄宗就命高力士扶他上船来见。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识21</title>
      <link href="/2019/06/02/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8621/"/>
      <url>/2019/06/02/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8621/</url>
      
        <content type="html"><![CDATA[<h3 id="Dying-Relu"><a href="#Dying-Relu" class="headerlink" title="[Dying Relu]"></a>[Dying Relu]</h3><p>Dying Relu现象指的是，在使用Relu作为激活函数时，因为学习率较大或某些原因，导致某一层的bias学到较大的负值，使得该层在过完Relu激活函数后的输出始终是0。</p><p><img src="/images/15594400435056.jpg" width="30%" height="50%"></p><p>当进入到这一状态时，基本上没办法再回到正常状态。因为在回传时，值为0导致梯度也为0。</p><p><a href="https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks" target="_blank" rel="noopener">https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Dying Relu </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词26</title>
      <link href="/2019/05/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D26/"/>
      <url>/2019/05/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D26/</url>
      
        <content type="html"><![CDATA[<h3 id="假使我们不去打仗"><a href="#假使我们不去打仗" class="headerlink" title="假使我们不去打仗"></a>假使我们不去打仗</h3><p>假使我们不去打仗，<br>敌人用刺刀<br>杀死了我们，<br>还要用手指着我们骨头说：<br>“看，<br>这是奴隶！”</p><hr><h3 id="蜀先主庙"><a href="#蜀先主庙" class="headerlink" title="蜀先主庙"></a>蜀先主庙</h3><p>[唐] 刘禹锡<br><strong>天地英雄气，千秋尚凛然。</strong><br>势分三足鼎，业复五铢钱。<br>得相能开国，生儿不象贤。<br>凄凉蜀故妓，来舞魏宫前。</p><p><a href="http://lib.xcz.im/work/57b90d29d342d3005ac78cb5" target="_blank" rel="noopener">http://lib.xcz.im/work/57b90d29d342d3005ac78cb5</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>AIS2019论文报告会之杭州行流水账</title>
      <link href="/2019/05/28/%E8%A7%81%E9%97%BB&amp;%E6%83%B3%E6%B3%95/AIS2019%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E6%9D%AD%E5%B7%9E%E8%A1%8C/"/>
      <url>/2019/05/28/%E8%A7%81%E9%97%BB&amp;%E6%83%B3%E6%B3%95/AIS2019%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E6%9D%AD%E5%B7%9E%E8%A1%8C/</url>
      
        <content type="html"><![CDATA[<p>趁着AIS2019论文报告会，终于有机会公费旅游啦😄</p><p>杭州郊区还是蛮繁华的哇。妹想到。</p><p><img src="/images/15590541433996.jpg" width="70%" height="50%"></p><p>为了等lzy下车，到十点钟才吃晚饭，难顶🙄，吃到12点半。</p><p><img src="/images/lzy.jpg" width="70%" height="50%"></p><p>找了半小时找不到酒店，还是求助了同行的其他小伙伴才找到🤦‍♂️</p><p><img src="/images/room.jpg" width="70%" height="50%"></p><p><strong>Day1</strong>：</p><p>睡到11点才匆匆赶到会场</p><p><img src="/images/15590935394726.jpg" width="70%" height="50%"></p><p>听了几场就昏昏欲睡了，而且会议室内网络也太差了8️⃣</p><p><img src="/images/15590935878376.jpg" width="70%" height="50%"></p><p>决定下午偷跑</p><p>下午鸽了他们，回到宿舍一觉又睡到5点。百无聊赖，还是到西湖走走8️⃣😆</p><p><img src="/images/15590936999475.jpg" width="70%" height="50%"></p><p><img src="/images/15590937248588.jpg" width="70%" height="50%"></p><p><img src="/images/15590938011843.jpg" width="70%" height="50%"></p><p>天下着雨，又是大晚上，其实啥也看不到。走了两个小时，从断桥走到雷峰塔🤦‍♂️。</p><p>回到宿舍，本来要睡了，但又颇有些饿。把同行的小伙伴都拉上，开始找宵夜😋</p><p><img src="/images/751559093999_.pic_hd.jpg" width="70%" height="50%"></p><p>等ljz下来等到三个人都开始打游戏了🙄</p><p><img src="/images/771559094127_.pic_hd.jpg" width="40%" height="50%"></p><p>开冲！<br><img src="/images/781559094310_.pic_hd.jpg" width="70%" height="50%"></p><p>六个人点了4瓶酒最后退了两瓶。大家可真是太菜了8️⃣🤦‍♂️</p><hr><p>Day2:</p><p>又是睡到11点，吃了点东西休息一下就去吃午饭了。</p><p>肯德基的饭真是难吃到爆，差点吐了🤢。</p><p><img src="/images/15590944817604.jpg" width="40%" height="50%"></p><p>在座位上百无聊赖看了两个小时视频，出发到会场，等其他人一起到陆总🏠吃饭😬（此行最大的motivation</p><p>陆总🏠的饭菜也太丰盛了8️⃣，吃到撑。还认识了几个新的小伙伴(ws与xlw)。了解到超多以前软件学院的八卦。总的来说非常开心</p><p><img src="/images/791559094738_.pic_hd.jpg" width="60%" height="50%"></p><p><img src="/images/801559094815_.pic_hd.jpg" width="60%" height="50%"></p><p>溜了溜了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> AIS </tag>
            
            <tag> 活动 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文19</title>
      <link href="/2019/05/28/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8719/"/>
      <url>/2019/05/28/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8719/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>Generating Long Sequences with Sparse Transformers</li><li>CBAM: Convolutional Block Attention Module</li><li>Pyramid Scene Parsing Network</li></ol><h2 id="Generating-Long-Sequences-with-Sparse-Transformers"><a href="#Generating-Long-Sequences-with-Sparse-Transformers" class="headerlink" title="[Generating Long Sequences with Sparse Transformers]"></a>[Generating Long Sequences with Sparse Transformers]</h2><p>Motivation：transformer对长序列不友好，复杂度较高；本文提出sparse transformer对<strong>生成</strong>长序列的计算进行优化，复杂度能够降到$O(n \sqrt{n})$。</p><p>实际上就是将全连接的attention分化成几个sparse的attention。</p><p><img src="/images/15590983353143.jpg" width="60%" height="50%"></p><p>如上图，实际上就是让部分head attend前面几个local；部分head attend到更早的信息，并且是跳着看的（两种pattern）。或者还可以让所有head都attend到这些sparse的点。</p><p>当然也可以扩展到多维空间。将head分为n组，每个组跳着看的不一样。</p><p>其他似乎都是细节，没啥帮助。</p><p>并且还改了一下transformer的计算过程，以及做了GPU kernel底层的加速。其他似乎没有什么insight的地方。</p><p>思考：<br>这实际上就是假设了一个较强的先验了，并且在生成的时候确实可能存在这种pattern，因为生成要达到好的效果，每个点都必须总结前面所有点的信息。但如果不是生成，是否也有这种pattern？<br>论文强行将文本也切成二维，而且用了30层；感觉不是那么有道理。因为这种sparse是从图像中观察得到的，是否也能应用于文本？会不会有可能是30层才达到这么好的效果？</p><hr><h2 id="CBAM-Convolutional-Block-Attention-Module"><a href="#CBAM-Convolutional-Block-Attention-Module" class="headerlink" title="[CBAM: Convolutional Block Attention Module]"></a>[CBAM: Convolutional Block Attention Module]</h2><p>提出在channel维度与空间维度的双重attention。和SE-Net相比加了一层空间维度上的交互，做法几乎都差不多。</p><p><img src="/images/15590990182833.jpg" width="60%" height="50%"></p><p>具体做法：<br><img src="/images/15590991719663.jpg" width="60%" height="50%"></p><p>①在channel维度上做attention</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{M}_{\mathbf{c}}(\mathbf{F}) &=\sigma(M L P(A v g P o o l(\mathbf{F}))+M L P(M a x P o o l(\mathbf{F}))) \\ &=\sigma\left(\mathbf{W}_{\mathbf{1}}\left(\mathbf{W}_{\mathbf{0}}\left(\mathbf{F}_{\mathbf{a v g}}^{\mathbf{c}}\right)\right)+\mathbf{W}_{\mathbf{1}}\left(\mathbf{W}_{\mathbf{0}}\left(\mathbf{F}_{\mathbf{m a x}}^{\mathbf{c}}\right)\right)\right) \end{aligned}</script><p>对输入按空间维度拍扁获得C维的pooling。实际上就是用average pooling和max pooling 过线性层+sigmoid。和SE-Net相比只是多利用了max-pooling。</p><p>②在空间维度上做attention</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{M}_{\mathbf{s}}(\mathbf{F}) &=\sigma\left(f^{7 \times 7}([A v g P o o l(\mathbf{F}) ; M a x P \operatorname{ool}(\mathbf{F})])\right) \\ &=\sigma\left(f^{7 \times 7}\left(\left[\mathbf{F}_{\mathbf{a v g}}^{\mathbf{s}} ; \mathbf{F}_{\mathbf{m a x}}^{\mathbf{s}}\right]\right)\right) \end{aligned}</script><p>同样是按channel维度拍扁做max/mean pooling。获得的是H<em>W</em>1的维度，然后拼起来过CNN+sigmoid。</p><p>思考：似乎创新性不足，并没有提供一些有用的insight。</p><hr><h2 id="Pyramid-Scene-Parsing-Network"><a href="#Pyramid-Scene-Parsing-Network" class="headerlink" title="[Pyramid Scene Parsing Network]"></a>[Pyramid Scene Parsing Network]</h2><p><img src="/images/15590993469941.jpg" width="70%" height="50%"></p><p>利用pyramid pooling（也即不同kernel size的pooling的结合）做切割的任务。</p><p>这是pyramid pooling，也即分层次的pooling：</p><p><img src="/images/15590994146803.jpg" width="60%" height="50%"></p><p>其他没感觉。</p>]]></content>
      
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> Paper </tag>
            
            <tag> Transformer </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> CBAM </tag>
            
            <tag> pyramid pooling </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>只有在你工作堆积如山时，你才可能享受闲暇</title>
      <link href="/2019/05/27/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E5%8F%AA%E6%9C%89%E5%9C%A8%E4%BD%A0%E5%B7%A5%E4%BD%9C%E5%A0%86%E7%A7%AF%E5%A6%82%E5%B1%B1%E6%97%B6%EF%BC%8C%E4%BD%A0%E6%89%8D%E5%8F%AF%E8%83%BD%E4%BA%AB%E5%8F%97%E9%97%B2%E6%9A%87/"/>
      <url>/2019/05/27/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E5%8F%AA%E6%9C%89%E5%9C%A8%E4%BD%A0%E5%B7%A5%E4%BD%9C%E5%A0%86%E7%A7%AF%E5%A6%82%E5%B1%B1%E6%97%B6%EF%BC%8C%E4%BD%A0%E6%89%8D%E5%8F%AF%E8%83%BD%E4%BA%AB%E5%8F%97%E9%97%B2%E6%9A%87/</url>
      
        <content type="html"><![CDATA[<p>“只有在你工作堆积如山时，你才可能享受闲暇。当你无事可做时，空闲就变得一点也不有趣，因为空闲就是你的工作，而且是最耗人的工作。闲懒和吻一样，当它被盗走了之后，它的味道才是甜的。”——— 杰罗姆·K·杰罗姆</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>无名英雄纪念碑铭</title>
      <link href="/2019/05/17/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%97%A0%E5%90%8D%E8%8B%B1%E9%9B%84%E7%BA%AA%E5%BF%B5%E7%A2%91%E9%93%AD/"/>
      <url>/2019/05/17/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%97%A0%E5%90%8D%E8%8B%B1%E9%9B%84%E7%BA%AA%E5%BF%B5%E7%A2%91%E9%93%AD/</url>
      
        <content type="html"><![CDATA[<h3 id="无名英雄纪念碑铭"><a href="#无名英雄纪念碑铭" class="headerlink" title="无名英雄纪念碑铭"></a>无名英雄纪念碑铭</h3><p>夫天下有大勇者，智不能测，刚不能制，猝然临之而不惊，无故加之而不怒，此其智甚远，所怀甚大也。所怀者何？天下有饥者，如己之饥，天下有溺者，如己之溺耳。民族危急，别亲离子而赴水火，易面事敌而求大同。风萧水寒，旌霜履血，或成或败，或囚或殁(mò)，人不知之，乃至殒后无名。  铭曰：呜呼！大音希声，大象无形。来兮精魄，安兮英灵。长河为咽，青山为证；岂曰无声？河山即名！  人有所忘，史有所轻。一统可期，民族将兴，肃之嘉石，沐手勒铭。噫我子孙，代代永旌。 公元二零一三年十月立。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>无题</title>
      <link href="/2019/05/14/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%F0%9F%87%A8%F0%9F%87%B3/"/>
      <url>/2019/05/14/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%F0%9F%87%A8%F0%9F%87%B3/</url>
      
        <content type="html"><![CDATA[<p>五千年前，<br>我们和古埃及人一样面对洪水；<br>四千年前，<br>我们和巴比伦人一样玩青铜器；<br>三千年前，<br>我们和古希腊人一样思考哲学；<br>两千年前，<br>我们和罗马人一样四处征伐；<br>一千年前，<br>我们和阿拉伯人一样无比富足；<br>五千年了，<br>我们一直在世界的牌桌上打着麻将，<br>而另外几家已经换过好多轮。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录17</title>
      <link href="/2019/05/12/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9517/"/>
      <url>/2019/05/12/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9517/</url>
      
        <content type="html"><![CDATA[<h3 id="Pytorch-restart写法"><a href="#Pytorch-restart写法" class="headerlink" title="[Pytorch restart写法]"></a>[Pytorch restart写法]</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.restart:</span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(args.restart_dir, <span class="string">'model.pt'</span>), <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        model = torch.load(f)</span><br></pre></td></tr></table></figure><hr><h3 id="Pytorch获得模型参数量"><a href="#Pytorch获得模型参数量" class="headerlink" title="[Pytorch获得模型参数量]"></a>[Pytorch获得模型参数量]</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args.n_all_param = sum([p.nelement() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()])</span><br></pre></td></tr></table></figure><hr><h3 id="Pytorch将数据保存为二进制方便快速读入"><a href="#Pytorch将数据保存为二进制方便快速读入" class="headerlink" title="[Pytorch将数据保存为二进制方便快速读入]"></a>[Pytorch将数据保存为二进制方便快速读入]</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># transformer-xl样例</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_lm_corpus</span><span class="params">(datadir, dataset)</span>:</span></span><br><span class="line">    fn = os.path.join(datadir, <span class="string">'cache.pt'</span>)</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(fn):</span><br><span class="line">        print(<span class="string">'Loading cached dataset...'</span>)</span><br><span class="line">        corpus = torch.load(fn)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'Producing dataset &#123;&#125;...'</span>.format(dataset))</span><br><span class="line">        kwargs = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> dataset <span class="keyword">in</span> [<span class="string">'wt103'</span>, <span class="string">'wt2'</span>]:</span><br><span class="line">            kwargs[<span class="string">'special'</span>] = [<span class="string">'&lt;eos&gt;'</span>]</span><br><span class="line">            kwargs[<span class="string">'lower_case'</span>] = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">elif</span> dataset == <span class="string">'ptb'</span>:</span><br><span class="line">            kwargs[<span class="string">'special'</span>] = [<span class="string">'&lt;eos&gt;'</span>]</span><br><span class="line">            kwargs[<span class="string">'lower_case'</span>] = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">elif</span> dataset == <span class="string">'lm1b'</span>:</span><br><span class="line">            kwargs[<span class="string">'special'</span>] = []</span><br><span class="line">            kwargs[<span class="string">'lower_case'</span>] = <span class="keyword">False</span></span><br><span class="line">            kwargs[<span class="string">'vocab_file'</span>] = os.path.join(datadir, <span class="string">'1b_word_vocab.txt'</span>)</span><br><span class="line">        <span class="keyword">elif</span> dataset <span class="keyword">in</span> [<span class="string">'enwik8'</span>, <span class="string">'text8'</span>]:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">        corpus = Corpus(datadir, dataset, **kwargs)</span><br><span class="line">        torch.save(corpus, fn)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> corpus</span><br></pre></td></tr></table></figure><hr><h3 id="Pytorch自带API实现inverse-sqrt的lr-schedule"><a href="#Pytorch自带API实现inverse-sqrt的lr-schedule" class="headerlink" title="[Pytorch自带API实现inverse sqrt的lr schedule]"></a>[Pytorch自带API实现inverse sqrt的lr schedule]</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from transformer-xl</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># originally used for Transformer (in Attention is all you need)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lr_lambda</span><span class="params">(step)</span>:</span></span><br><span class="line">    <span class="comment"># return a multiplier instead of a learning rate</span></span><br><span class="line">    <span class="keyword">if</span> step == <span class="number">0</span> <span class="keyword">and</span> args.warmup_step == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.</span> / (step ** <span class="number">0.5</span>) <span class="keyword">if</span> step &gt; args.warmup_step \</span><br><span class="line">            <span class="keyword">else</span> step / (args.warmup_step ** <span class="number">1.5</span>)</span><br><span class="line">            </span><br><span class="line">scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词25</title>
      <link href="/2019/05/12/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D25/"/>
      <url>/2019/05/12/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D25/</url>
      
        <content type="html"><![CDATA[<h3 id="西江月"><a href="#西江月" class="headerlink" title="西江月"></a>西江月</h3><p><strong>世事一场大梦，人生几度新凉</strong>？夜来风叶已鸣廊，看取眉头鬓上。<br>酒贱常愁客少，月明多被云妨。中秋谁与共孤光，把盏凄然北望。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文18</title>
      <link href="/2019/05/12/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8718/"/>
      <url>/2019/05/12/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8718/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</li><li>Ordered Neurons- Integrating Tree Structures into Recurrent Neural Networks</li><li>Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification</li><li>Unified Language Model Pre-training for Natural Language Understanding and Generation</li><li>Language Models are Unsupervised Multitask Learners</li><li>MASS: Masked Sequence to Sequence Pre-training for Language Generation</li><li>Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks</li><li>PSANet: Point-wise Spatial Attention Network for Scene Parsing</li><li>CCNet: Criss-Cross Attention for Semantic Segmentation</li></ol><h2 id="GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond"><a href="#GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond" class="headerlink" title="[GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond]"></a>[GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond]</h2><p>提出一种新的对长距离依赖建模的方法，并结合了之前其他研究者的工作，抽象得到全局上下文建模（global context modeling）的框架。</p><p>目前长距离依赖建模有两种：一种是引入self-attention机制，获得query-dependent的全局上下文，如NL-Net；另一种则是query-independent的全局上下文，如SE-Net。</p><h3 id="Simplified-NL-Net"><a href="#Simplified-NL-Net" class="headerlink" title="Simplified NL-Net"></a>Simplified NL-Net</h3><p>Motivation：通过对Non-local network的分析，发现网络实际上学到的是query无关的上下文，因此可以直接对NL-Net进行简化。</p><p>首先是对NL-Net的观察，通过可视化，以及统计得到的数据，可以发现，NL-Net对于每个query来说，其学到的全局信息差异很小。</p><p><img src="/images/15576253999087.jpg" width="80%" height="50%"></p><p><img src="/images/15576254102911.jpg" width="60%" height="50%"></p><p>同时，NL-Net由于这种query-dependent的长距离依赖建模，拥有较高的复杂度（平方级别）。因此首先我们可以对NL-Net进行简化。</p><p><img src="/images/15576254433886.jpg" width="60%" height="50%"></p><p>原来的NL-Net是：</p><script type="math/tex; mode=display">\mathbf{z}_{i}=\mathbf{x}_{i}+W_{z} \sum_{j=1}^{N_{p}} \frac{f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)}{\mathcal{C}(\mathbf{x})}\left(W_{v} \cdot \mathbf{x}_{j}\right)</script><p>每两个feature之间计算一个attention分数。</p><p>将query-dependent去掉后，则有：</p><script type="math/tex; mode=display">\mathbf{z}_{i}=\mathbf{x}_{i}+\sum_{j=1}^{N_{p}} \frac{\exp \left(W_{k} \mathbf{x}_{j}\right)}{\sum_{m=1}^{N_{p}} \exp \left(W_{k} \mathbf{x}_{m}\right)}\left(W_{v} \cdot \mathbf{x}_{j}\right)</script><p>还可以将$W_{v}$移到外面，更进一步地简化：</p><script type="math/tex; mode=display">\mathbf{z}_{i}=\mathbf{x}_{i}+W_{v} \sum_{j=1}^{N_{p}} \frac{\exp \left(W_{k} \mathbf{x}_{j}\right)}{\sum_{m=1}^{N_{p}} \exp \left(W_{k} \mathbf{x}_{m}\right)} \mathbf{x}_{j}</script><h3 id="Global-Context-Modeling-Framework"><a href="#Global-Context-Modeling-Framework" class="headerlink" title="Global Context Modeling Framework"></a>Global Context Modeling Framework</h3><p>通过对比近期相关的工作，作者将全局上下文建模的方法抽象出来，分为三个步骤：<br>a)global attention pooling 将全局的信息收集起来，伴随着一个全连接和softmax<br>b)feature transform via a 1x1 convolution Wv  将所获得的feature进行线性转换。<br>c)feature aggregation 将global feature与每个position融合。</p><p>形式化则有：</p><script type="math/tex; mode=display">\mathbf{z}_{i}=F\left(\mathbf{x}_{i}, \delta\left(\sum_{j=1}^{N_{p}} \alpha_{j} \mathbf{x}_{j}\right)\right)</script><p>如果从这个角度去理解，那么SE-Net也是属于该框架的一种实例。</p><p><img src="/images/15576257652930.jpg" width="87%" height="50%"></p><p>作者在该框架的基础上提出了新的实例，也即GC-Net，同时有NL-Net的高效建模的优点和SE-Net的计算效率高的优点。</p><script type="math/tex; mode=display">\mathbf{z}_{i}=\mathbf{x}_{i}+W_{v 2} \operatorname{ReLU}\left(\operatorname{LN}\left(W_{v 1} \sum_{j=1}^{N_{p}} \frac{e^{W_{k} \mathbf{x}_{j}}}{\sum_{m=1}^{N_{p}} e^{W_{k} \mathbf{x}_{m}}} \mathbf{x}_{j}\right)\right)</script><p>第一，基本采用NL-Net的形式，简化成query-independent的形式，并且使用的是加的形式而不是SE-Net的rescale的形式，将global feature与每个位置融合。<br>第二，采用SE-Net的bottleneck的形式去减少参数和计算量，并且在此基础上多了一步layer norm使得模型更易训练，实践证明，layer norm能够提升表现。</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>对比NL-Net：不同之处在于global attention pooling，用的是query-independent<br>对比SE-Net：不同之处在于fusion module（框架的第三步），使用的是addition而不是rescale；以及在bottleneck上做了一点改进，加了layer norm。</p><p>思考：<br>文章有意思的点是立足于实验观察，这点值得学习，从实践中发现问题。同时，应学会用抽象的思想去总结前人的工作（比如NL-Net实际上也是对前面的工作的抽象总结，实际上个人认为并没有什么大的创新），从一个更高的角度去看问题能将问题看得更清晰。</p><p>抽象出框架是有必要的吗？我看在这篇论文里面有些勉强，因为完全可以说inspired by SE-Net for the computation efficiency… 但非要抽象成框架，会不会只是要表现出对模型的理解够深？以及增加点内容？</p><p>以及query-dependent是否真的没必要？似乎在其他论文中的结论反而是相反的。</p><hr><h2 id="Ordered-Neurons-Integrating-Tree-Structures-into-Recurrent-Neural-Networks"><a href="#Ordered-Neurons-Integrating-Tree-Structures-into-Recurrent-Neural-Networks" class="headerlink" title="[Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks]"></a>[Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks]</h2><p>ICLR19 best paper。</p><p>在LSTM中引入了新的inductive bias，隐式对句子进行树的建模。</p><p>Motivation：<br>语言都有一定的树的结构而不是序列结构。前人研究表明，在NLP中引入树的结构有助于增强泛化，帮助减少长程依赖问题，能够获得更好的抽象表示。<br>一些方法包括增加监督信号，也即语法树等，但该方法有限制，如标注数据不足，在一些领域语法规则容易被打破（如推特）；同时随着时间的推移，语法也在变化。<br>同时，一些研究也表明，有足够容量的LSTM在能够隐式地对句子进行树的建模。</p><p>因此在本文引入新的inductive bias，<strong>完全数据驱动</strong>（相对于显式构建树），隐式地对句子进行树的建模。这种归纳偏置促进了每个神经元内存储的信息的生命周期的分化。high-ranking的神经元保存长程的信息，在一个较长时间步内保存，而low-ranking则保存短程信息，能够快速被遗忘。引入cumulative softmax，一种新的激活函数来生成master input gate和forget gate 保证当一个神经元被更新/遗忘，其后的神经元都会被更新/遗忘。</p><h3 id="ORDERED-NEURONS"><a href="#ORDERED-NEURONS" class="headerlink" title="ORDERED NEURONS"></a>ORDERED NEURONS</h3><p>给定语言序列$S=\left(x_{1}, \dots, x_{T}\right)$以及对应的constituency tree，我们希望在计算时间步t的时候，隐状态$h_t$能包含该节点到根节点路径上所有节点的信息。直观上，我们希望该路径上的节点都能够被$h_t$的一部分神经元表示。由于$h_t$的维度是固定的，而路径上的节点则是动态的，因此一种最好的情况就是能够动态分配每个节点在hidden state的维度。</p><p>在上述思路的基础上，作者引入了ordered neurons，强制让神经元表示不同时间尺度上的信息。正如前面提到的，high-ranking的神经元保存的时间长，代表的就是接近树根的节点，而low-ranking的神经元保存时间短，代表的就是小的成分（如phrase）。原则是：要更新/遗忘high-ranking的神经元，应该先把low-ranking的神经元先更新/遗忘掉。</p><p>如图：</p><p><img src="/images/15576268851485.jpg" width="80%" height="50%"></p><h3 id="ON-LSTM-“Ordered-Neurons-LSTM”"><a href="#ON-LSTM-“Ordered-Neurons-LSTM”" class="headerlink" title="ON-LSTM (“Ordered Neurons LSTM”)"></a>ON-LSTM (“Ordered Neurons LSTM”)</h3><p>LSTM结构：</p><p>$\begin{array}{ll}{f_{t}=\sigma\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\right)} \\ {i_{t}=\sigma\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\right)} \\ {o_{t}=\sigma\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\right)} \\ {\hat{c}_{t}=\tanh \left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\right)} \\ {h_{t}=o_{t} \circ \tanh \left(c_{t}\right)}\end{array}$</p><p>OH-LSTM与LSTM的不同在于修改了cell state的更新方式。</p><p>首先定义新的激活函数：</p><script type="math/tex; mode=display">\hat{g}=\operatorname{cumax}(\ldots)=\operatorname{cumsum}(\operatorname{softmax}(\ldots))</script><p>$\hat{g}$可以看做是二元gate的期望，而g将cell state分成两个segment $g=(0, \dots, 0,1, \dots, 1)$。</p><p>因为离散的方法不好优化，以及太过严格。具体的解释和证明在论文里。</p><p>引入两个新的gate，master forget gate和master input gate。forget gate的值单调递增；而input gate的值单调递减。</p><p>$\begin{aligned} \tilde{f}_{t} &amp;=\operatorname{cumax}\left(W_{\tilde{f}} x_{t}+U_{\tilde{f}} h_{t-1}+b_{\tilde{f}}\right) \\ \tilde{i}_{t} &amp;=1-\operatorname{cumax}\left(W_{\tilde{i}} x_{t}+U_{i} h_{t-1}+b_{\tilde{i}}\right) \end{aligned}$</p><p>新的更新规则：<br>$\begin{aligned} \omega_{t} &amp;=\tilde{f}_{t} \circ \tilde{i}_{t} \\ \hat{f}_{t} &amp;=f_{t} \circ \omega_{t}+\left(\tilde{f}_{t}-\omega_{t}\right)=\tilde{f}_{t} \circ\left(f_{t} \circ \tilde{i}_{t}+1-\tilde{i}_{t}\right) \\ \hat{i}_{t} &amp;=i_{t} \circ \omega_{t}+\left(\tilde{i}_{t}-\omega_{t}\right)=\tilde{i}_{t} \circ\left(i_{t} \circ \tilde{f}_{t}+1-\tilde{f}_{t}\right) \\ c_{t} &amp;=\hat{f}_{t} \circ c_{t-1}+\hat{i}_{t} \circ \hat{c}_{t} \end{aligned}$</p><p>master forget gate 控制的是erasing行为；master input gate则是控制writing行为。具体的例子见论文。$\omega_{t}$则是$\tilde{f}_{t}$与$\tilde{i}_{t}$的重叠部分，表示的是 相应的神经元段编码包含一些先前单词和当前输入单词$x_t$的不完整成分。</p><p>同时，master gates专注一些粗粒度的控制，因此没必要和hidden state一样的维度，分配小一些的维度$D_{m}=\frac{D}{C}$即可。因此每C-sized个chunk有同样的master gates。</p><p>思考：<br>设计很精巧，通过将神经元分化，隐式建模。更新规则部分我其实还没仔细去看，但该思想确实很有意思。</p><hr><h2 id="Cached-Long-Short-Term-Memory-Neural-Networks-for-Document-Level-Sentiment-Classification"><a href="#Cached-Long-Short-Term-Memory-Neural-Networks-for-Document-Level-Sentiment-Classification" class="headerlink" title="[Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification]"></a>[Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification]</h2><p>配合上一篇ON-LSTM看，发现他们之间具有相似之处。</p><p>本文在LSTM上引入cache机制，将memory切分为多个group并赋予不同的forget rate，使模型更好地保留全局的信息，对document级别的情感分类能够有更好的结果。</p><p>LSTM：</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{i}^{(t)} &=\sigma\left(\mathbf{W}_{i} \mathbf{x}^{(t)}+\mathbf{U}_{i} \mathbf{h}^{(t-1)}\right) \\ \mathbf{f}^{(t)} &=\sigma\left(\mathbf{W}_{f} \mathbf{x}^{(t)}+\mathbf{U}_{f} \mathbf{h}^{(t-1)}\right) \\ \mathbf{o}^{(t)} &=\sigma\left(\mathbf{W}_{o} \mathbf{x}^{(t)}+\mathbf{U}_{o} \mathbf{h}^{(t-1)}\right) \\ \tilde{\mathbf{c}}^{(t)} &=\tanh \left(\mathbf{W}_{c} \mathbf{x}^{(t)}+\mathbf{U}_{c} \mathbf{h}^{(t-1)}\right) \\ \mathbf{c}^{(t)} &=\mathbf{f}^{(t)} \odot \mathbf{c}^{(t-1)}+\mathbf{i}^{(t)} \odot \tilde{\mathbf{c}}^{(t)} ) \\ \mathbf{h}^{(t)} &=\mathbf{o}^{(t)} \odot \tanh \left(\mathbf{c}^{(t)}\right) \end{aligned}</script><p>在本文中，实际上是使用了LSTM的变体，将input gate与forget gate合并为一个，也即：</p><script type="math/tex; mode=display">\mathbf{c}^{(t)}=\mathbf{f}^{(t)} \odot \mathbf{c}^{(t-1)}+\left(\mathbf{1}-\mathbf{f}^{(t)}\right) \odot \tilde{\mathbf{c}}^{(t)}</script><h3 id="Cached-LSTM"><a href="#Cached-LSTM" class="headerlink" title="Cached LSTM"></a>Cached LSTM</h3><p>改进LSTM，将memory分为多个group，每个group代表不同的长程依赖，分配不同的forget rate。直观上，高的rate代表了短程依赖，低的rate代表长程依赖。</p><p>将memory cell切成K块$\left\{G_{1}, \cdots, G_{K}\right\}$。因此有如下公式：</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{r}_{k}^{(t)} &=\psi_{k}\left(\sigma\left(\mathbf{W}_{r}^{k} \mathbf{x}^{(t)}+\sum_{j=1}^{K} \mathbf{U}_{f}^{j \rightarrow k} \mathbf{h}_{j}^{(t-1)}\right)\right) \\ \mathbf{o}_{k}^{(t)} &=\sigma\left(\mathbf{W}_{o}^{k} \mathbf{x}^{(t)}+\sum_{j=1}^{K} \mathbf{U}_{o}^{j \rightarrow k} \mathbf{h}_{j}^{(t-1)}\right) \\ \tilde{\mathbf{c}}_{k}^{(t)} &=\tanh \left(\mathbf{W}_{c}^{(t)} \mathbf{x}^{(t-1)}+\left(\mathbf{r}_{k}^{(t)}\right) \odot \tilde{\mathbf{c}}_{k}^{(t)}\right) \\ \mathbf{h}_{k}^{(t)} &=\mathbf{o}_{k}^{(t)} \odot \tanh \left(\mathbf{c}_{k}^{(t)}\right) \end{aligned}</script><p>$\psi_{k}(\mathbf{z})$是压缩函数：</p><script type="math/tex; mode=display">\mathbf{r}_{k}=\psi_{k}(\mathbf{z})=\frac{1}{K} \cdot \mathbf{z}+\frac{k-1}{K}</script><p>将forget rate压缩在一定范围$\left(\frac{k-1}{K}, \frac{k}{K}\right)$。</p><p>类似bi-LSTM，同样CLSTM可以有双向。在做分类时，只将代表长程依赖的那组取出来过softmax。</p><p><img src="/images/15576279206849.jpg" width="55%" height="50%"></p><p>思考：<br>与Ordered Neuron相比，这里显式地将固定维度切分成多个组，相比而言Ordered Neuron更加灵活，但二者还是有相当的相似程度的，虽然任务和motivation不同。</p><hr><h2 id="Unified-Language-Model-Pre-training-for-Natural-Language-Understanding-and-Generation"><a href="#Unified-Language-Model-Pre-training-for-Natural-Language-Understanding-and-Generation" class="headerlink" title="[Unified Language Model Pre-training for Natural Language Understanding and Generation]"></a>[Unified Language Model Pre-training for Natural Language Understanding and Generation]</h2><p>将pretrain扩展到生成领域，使用生成任务来对语言模型进行pretrain。</p><p><img src="/images/15576282560620.jpg" width="80%" height="50%"></p><p>同时使用三种语言模型来进行pretrain。一种是bidirectional的，和bert一样；一种是从左到右/从右到左单向的，和GPT一样；另一种是做生成的，也即encoder端相互都attend到，而decoder端只能看到encoder部分和decoder的左边。</p><p>统一使用[MASK]的方法（bert）同时训练这三种语言模型，这样可以使用同一套训练流程同时训练三种模型。</p><p>本文的主要贡献就是将生成加进来了吧，并且效果还可以。其他并没有很大的创新点。</p><hr><h2 id="Language-Models-are-Unsupervised-Multitask-Learners"><a href="#Language-Models-are-Unsupervised-Multitask-Learners" class="headerlink" title="[Language Models are Unsupervised Multitask Learners]"></a>[Language Models are Unsupervised Multitask Learners]</h2><p>大名鼎鼎的GPT2.0 通过增加更多层，增加更多数据，训练一个更好的语言模型，并且尝试在不fine-tune的情况下完成下游任务，并取得不错的效果。</p><p>全文都在讨论怎么获得数据以及怎么训练。实际上帮助并不大，但个人认为本文最大的贡献就是尝试去做生成，并且在zero-shot的情景下去探索language model的上限。</p><p>其他我没仔细看。</p><hr><h2 id="MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation"><a href="#MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation" class="headerlink" title="[MASS: Masked Sequence to Sequence Pre-training for Language Generation]"></a>[MASS: Masked Sequence to Sequence Pre-training for Language Generation]</h2><p>引入encoder-decoder结构来做pretrain，可以同时训练encoder和decoder，可以用于生成任务。 idea还是挺有意思的。</p><p>Motivation:<br>采用encoder-decoder框架能进一步更好地用于生成任务上，而不像bert和GPT那样只有一个encoder或decoder，没法对attention预训练，对生成任务不友好。</p><p>做法：在encoder端mask连续的词，然后使用transformer对其进行encode；然后再decoder端输入同样的句子，但是masked掉的正好和encoder相反，和翻译一样，使用attention机制去训练，但只预测encoder端被mask掉的词。</p><p><img src="/images/15576285238806.jpg" width="80%" height="50%"></p><p>作者认为这样做的好处：<br>对encoder端的mask能够强制让encoder端更好地学习未被mask掉的词的意义，这样才能预测mask掉的词；对decoder端的input进行mask能够强制模型更多依赖于source端，而不是前面的input。</p><p>作者还将MASS与Bert/GPT做了对比，发现Bert/GPT是MASS的一个特例。MASS有一个超参k，控制mask掉的segment长度。当k=1时，则是BERT；当k=m，也即整个句子长度时则是GPT。</p><p><img src="/images/15576285809670.jpg" width="50%" height="50%"></p><p><img src="/images/15576285948426.jpg" width="100%" height="50%"></p><p>当k=1时，我们可以理解为decoder端没有input信息，全部信息来自encoder，和Bert对比一下，虽然在结构上不一样，但做的事情是一样的，此时decoder的角色就是bert里面的分类器。<br>当k=m时，实际上就是将encoder端的所有信息都mask掉了，此时decoder要预测只能靠decoder端的input，这实际上就是一个标准的语言模型。</p><p>在几个生成任务上的结果相当不错，我没仔细看。</p><p>思考：将bert和GPT抽象出来，作为其框架的一种特殊形式，这又和NL-Net有一些相似。</p><hr><h2 id="Gather-Excite-Exploiting-Feature-Context-in-Convolutional-Neural-Networks"><a href="#Gather-Excite-Exploiting-Feature-Context-in-Convolutional-Neural-Networks" class="headerlink" title="[Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks]"></a>[Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks]</h2><p>另一种Gather-Distribute思想的实例。该模型同样是为了捕获长距离上下文，以提升表现。</p><p>分为两步：gather，将较大空间内的信息聚集起来，excite，将信息重新分发给local features。</p><p><img src="/images/15576287381755.jpg" width="80%" height="50%"></p><p>Motivation：CNN的信息流动方式。每次抽取周围的信息聚合在一起，随着层数的增多逐渐抽象，其感受野也逐渐增大。本文提出的模型实际上就是在同一层内让每个点都感受到其周围更大空间的信息。</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>模型定义：<br>输入：$x=\left\{x^{c} : c \in\{1, \ldots, C\}\right\}$<br>C代表C个feature maps，也即channel维。</p><p>定义selection operator：<br>$\iota(u, e)=\left\{e u+\delta : \delta \in[-\lfloor(2 e-1) / 2\rfloor,\lfloor(2 e-1) / 2\rfloor]^{2}\right\}$</p><p>u是输出的元素，e是extent ratio，代表的实际上就是一个窗口大小。</p><p>因此gather operator可以定义为映射函数：<br>$\xi_{G} : \mathbb{R}^{H \times W \times C} \rightarrow \mathbb{R}^{H^{\prime} \times W^{\prime} \times C}\left(H^{\prime}=\left\lceil\frac{H}{e}\right\rceil, W^{\prime}=\left\lceil\frac{W}{e}\right\rceil\right)$<br>$\xi_{G}(x)_{u}^{c}=\xi_{G}\left(x \odot \mathbf{1}_{\iota_{(u, e)}}^{c}\right)$</p><p>其实就是对该窗口内的元素进行了映射（如mean-pooling）。其中$u \in\left\{1, \ldots, H^{\prime}\right\} \times\left\{1, \ldots, W^{\prime}\right\}, c \in\{1, \ldots, C\}$</p><p>从上式可以看出，gather操作实际上就是对于每个输出u，其感受野为单个channel的一个窗口。如果该窗口恰好覆盖了整个空间，则称该gather操作有global extent。</p><p>而excite操作则是利用gather获得的上下文信息，更新每个feature。也即：</p><script type="math/tex; mode=display">\begin{array}{l}{\xi_{E}(x, \hat{x})=x \odot f(\hat{x})} \\ {f : \mathbb{R}^{H^{\prime} \times W^{\prime} \times C} \rightarrow[0,1]^{\overline{H} \times W \times C}}\end{array}</script><p>那么G是如何获得的？可以有两种，一种是无参数，另一种是有参数。</p><h4 id="无参数GE"><a href="#无参数GE" class="headerlink" title="无参数GE"></a>无参数GE</h4><p>实际上就是mean-pooling。<br>则整个GE-Net为：</p><script type="math/tex; mode=display">y^{c}=x \odot \sigma\left(\text {interp}\left(\xi_{G}(x)^{c}\right)\right)</script><p>其中interp(·)代表了最邻近插值。实际上可以理解成，将一个较大窗口的信息都mean-pooling一下，然后该窗口的feature都用mean-pooling的值乘一下（因为最邻近的特点，该窗口的插值都是自身）。</p><p>当设计不同的e时，也即窗口大小，可以看到窗口越大，其表现越好。</p><p><img src="/images/15576291070929.jpg" width="40%" height="50%"></p><h4 id="有参数GE"><a href="#有参数GE" class="headerlink" title="有参数GE"></a>有参数GE</h4><p>采用strided depth-wise convolution。</p><p>同样越大的e越好：</p><p><img src="/images/15576291449392.jpg" width="40%" height="50%"></p><p>并且表现会比无参数的更好。</p><p>实验表明，在整个模型的中间层或者后面层（有更多的channel）加GE会更好。</p><h3 id="与SE-Net的关系"><a href="#与SE-Net的关系" class="headerlink" title="与SE-Net的关系"></a>与SE-Net的关系</h3><p>SE-Net可以看做是特殊的GE-Net。SE-Net的gather操作就是全局的mean-pooling；而在excite时多了一层全连接的网络（？论文说就是一层全连接，但似乎不是这样的）。</p><p><img src="/images/15576291939204.jpg" width="70%" height="50%"></p><p>论文中还将SE-Net和GE-Net结合起来，发现有更大的提升，证明二者不是排斥的。</p><p>应用GE-Net的几个例子：</p><p><img src="/images/15576292257307.jpg" width="80%" height="50%"></p><p>我的思考：<br>与SE-Net的关系密切（实际上就是同一拨人做的）。但这里强调的是channel之间没有联系，仅仅是通过扩大感受野，增强global的信息；而SE-Net则是强调的channel之间的联系，并没有考虑channel内部的关系，相当于GE-Net具有全局感受野。如果GE-Net有全局感受野，那么他比SE-Net就差在channel之间的联系了。</p><hr><h2 id="PSANet-Point-wise-Spatial-Attention-Network-for-Scene-Parsing"><a href="#PSANet-Point-wise-Spatial-Attention-Network-for-Scene-Parsing" class="headerlink" title="[PSANet: Point-wise Spatial Attention Network for Scene Parsing]"></a>[PSANet: Point-wise Spatial Attention Network for Scene Parsing]</h2><p>提出另一种解决local constraint的方案，也即使得feature之间能够建立长距离依赖。feature map上的每个位置通过attention map与其他点进行连接，同时信息流动是双向的，每个点同时进行收集与分发的操作。</p><p>通常而言，信息的aggregation可以形式化成：</p><script type="math/tex; mode=display">\mathbf{z}_{i}=\frac{1}{N} \sum_{\forall j \in \Omega(i)} F\left(\mathbf{x}_{i}, \mathbf{x}_{j}, \Delta_{i j}\right) \mathbf{x}_{j}</script><p>$\mathbf{z}_{i}$是第i个位置的输出；$\mathbf{x}_{j}$是输入的feature map $X$。$\forall j \in \Omega(i)$ 是与i相关的所有位置的feature集合。$ F\left(\mathbf{x}_{i}, \mathbf{x}_{j}, \Delta_{i j}\right)$ 代表的是j到i的信息流动。$\Delta$ 代表的是相对位置信息。</p><p>可以将上述式子简化为：</p><script type="math/tex; mode=display">\mathbf{z}_{i}=\frac{1}{N} \sum_{\forall j \in \Omega(i)} F_{\Delta_{i j}}\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \mathbf{x}_{j}</script><p>其中$\left\{F_{\Delta_{i j}}\right\}$是位置相关的函数映射。</p><p>当一个feature map的位置很多时，$x_i$与$x_j$的pair将会很大。</p><p>因此将上式函数映射简化为：</p><script type="math/tex; mode=display">F_{\Delta_{i j}}\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \approx F_{\Delta_{i j}}\left(\mathbf{x}_{i}\right)</script><p>也即$j$到$i$的信息流动只与$i$位置的feature以及$i$与$j$之间的相对位置有关。</p><p>同理，还可以将函数映射简化成：</p><script type="math/tex; mode=display">F_{\Delta_{i j}}\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \approx F_{\Delta_{i j}}\left(\mathbf{x}_{j}\right)</script><p>也即信息流动只与$i$与$j$的相对位置以及$j$位置上的feature有关。</p><p>将上述两个简化函数结合起来，可以获得双向信息传播路径。也即：</p><script type="math/tex; mode=display">F_{\Delta_{i j}}\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \approx F_{\Delta_{i j}}\left(\mathbf{x}_{i}\right)+F_{\Delta_{i j}}\left(\mathbf{x}_{j}\right)</script><p>此时我们可以得到：</p><script type="math/tex; mode=display">\mathbf{z}_{i}=\frac{1}{N} \sum_{\forall j \in \Omega(i)} F_{\Delta_{i j}}\left(\mathbf{x}_{i}\right) \mathbf{x}_{j}+\frac{1}{N} \sum_{\forall j \in \Omega(i)} F_{\Delta_{i j}}\left(\mathbf{x}_{j}\right) \mathbf{x}_{j}</script><p>第一项$F_{\Delta_{i j}}\left(\mathbf{x}_{i}\right)$encode了在其他位置上的信息在多大程度上能够帮助位置i（通过位置i的feature以及相对位置）。</p><p>第二项$F_{\Delta_{i j}}\left(\mathbf{x}_{j}\right)$所做的也即预测其他位置上的feature的重要性（通过相对位置，以及位置j的feature）。</p><p>如下图：</p><p><img src="/images/15576301327447.jpg" width="70%" height="50%"></p><p>上述两个F实际上可以看做是在预测一个attention的值，去做aggregation。也即：</p><script type="math/tex; mode=display">\mathbf{z}_{i}=\frac{1}{N} \sum_{\forall j} \mathbf{a}_{i, j}^{c} \mathbf{x}_{j}+\frac{1}{N} \sum_{\forall j} \mathbf{a}_{i, j}^{d} \mathbf{x}_{j}</script><p>问题在于如何获得a？<br>下图是较为清晰的一个框架图：</p><p><img src="/images/15576302667770.jpg" width="70%" height="50%"></p><p>可以看出是通过多个CNN来获得attention矩阵的。</p><p>上下两条线很类似。第一步是先压缩channel以减少计算量（C2&lt;C1)。第二步扩展channel为$(2H-1)\times(2W-1)$，下面解释为什么。接下来在重新获得$H\times W$的channel维，该channel维的每一维所代表的就是一个feature（共有$H\times W$个feature）的attention值。最后乘起来再concat一下，获得最后的output。</p><p>为什么是$(2H-1)\times(2W-1)$的channel维，因为希望将该feature剪裁一下变成$H\times W$，正好可以表示相对位置。</p><p><img src="/images/15576304353141.jpg" width="80%" height="50%"></p><p>对于一个$(2H-1)\times(2W-1)$的feature可以展开成二维的，其中位置i为中心，仅有$H\times W$个有用。具体而言,在第k行第l列的位置i，则有用的矩阵是从$H-k$行和$W-l$列开始的。这个做法倒挺有意思的。</p><p>与NL-Net的关系：NL-Net没有考虑相对位置信息。</p><p>思考：<br>该方法似乎确实相比NL-Net的计算量小，虽然看起来也很大。NL-Net的计算量是(HW)^2。而这里的数量级是HW。究其原因，是因为attention是预测出来的。</p><p>从获得attention矩阵的方式可以看出，channel与channel之间有交互。</p><p>attention矩阵是预测出来的（$1\times 1$的convolution），而不是一对pair计算出来的。似乎就没那么有道理。</p><p><del>并且，上下两条支线的操作都是一样的，只是将其解释为双向信息流动；那还可以解释成像Transformer那样，多个head，将同一个表示映射到多个隐空间中增强表示。</del>之前理解错了、</p><hr><h2 id="CCNet-Criss-Cross-Attention-for-Semantic-Segmentation"><a href="#CCNet-Criss-Cross-Attention-for-Semantic-Segmentation" class="headerlink" title="[CCNet: Criss-Cross Attention for Semantic Segmentation]"></a>[CCNet: Criss-Cross Attention for Semantic Segmentation]</h2><p>对NL-Net的改进，通过引入十字交叉的attention和recurrent结构，减少了计算量，同时使得模型能够捕获长距离依赖，以及提升了模型表现。</p><p>Motivation:<br>NL-Net会生成一个很大的attention map，其复杂度为${\mathcal{O}}((H \times W)\times(H \times W))$。</p><p>主要做法，将该position-wise的attention分解成两步：第一步是每个点只和其同一行和同一列的进行attention，将attention的操作循环多次，达到每个点间接和其他点都做了attention。其复杂度则为 $\mathcal{O}((H \times W) \times(H+W-1))$</p><p>两种方法的对比：</p><p><img src="/images/15576307417016.jpg" width="50%" height="50%"></p><p>注意到recurrent的结构中参数是共享的。</p><p>具体的框架：</p><p><img src="/images/15576307988403.jpg" width="80%" height="50%"></p><p>先进行降维，做完criss-cross attention后的output与原先的x拼起来，再过CNN等进行融合。</p><h3 id="Criss-Cross-Attention"><a href="#Criss-Cross-Attention" class="headerlink" title="Criss-Cross Attention"></a>Criss-Cross Attention</h3><p><img src="/images/15576308514373.jpg" width="50%" height="50%"></p><p>过三个线性层得到QKV（和transformer类似）；接着Q与K做纵横交叉的attention，获得softmax，接着再和V对应的位置相乘。</p><p>具体而言：<br>输入：$\mathbf{H} \in \mathbb{R}^{C \times W \times H}$<br>过线性层：$\{\mathbf{Q}, \mathbf{K}\} \in \mathbb{R}^{C^{\prime} \times W \times H}$<br>attention分数：$\mathbf{A} \in \mathbb{R}^{(H+W-1) \times W \times H}$<br>与u元素做attention的feature集合，也即同一行或同一列的feature：$\boldsymbol{\Omega}_{\mathbf{u}} \in \mathbb{R}^{(H+W-1) \times C^{\prime}} \cdot \mathbf{\Omega}_{\mathbf{i}, \mathbf{u}} \in \mathbb{R}^{C^{\prime}}$<br>做attention：$d_{i, u}=\mathbf{Q}_{\mathbf{u}} \mathbf{\Omega}_{\mathbf{i}, \mathbf{u}^{\top}}$<br>再做softmax，最终获得output：$\mathbf{H}_{\mathbf{u}}^{\prime}=\sum_{i \in\left|\mathbf{\Phi}_{\mathbf{u}}\right|} \mathbf{A}_{\mathbf{i}, \mathbf{u}} \mathbf{\Phi}_{\mathbf{i}, \mathbf{u}}+\mathbf{H}_{\mathbf{u}}$<br>$\boldsymbol{\Phi}_{\mathbf{i}, \mathbf{u}}$与$\boldsymbol{\Omega}_{\mathbf{u}}$是同一集合。</p><h3 id="Recurrent-Criss-Cross-Attention"><a href="#Recurrent-Criss-Cross-Attention" class="headerlink" title="Recurrent Criss-Cross Attention"></a>Recurrent Criss-Cross Attention</h3><p>多做几次，每次都共享，就是recurrent了。<br>当循环次数是2时，每个点都能够attend到其他任何点了。</p><p><img src="/images/15576311079687.jpg" width="66%" height="50%"></p><p>思考：<br>通过纵横来间接attend到所有点，这个想法还蛮有趣的。并且减少了计算量。就是这种纵横的方法代码要怎么实现？有些好奇。<br>同时本文的图也很漂亮，每个图都恰到好处，可以通过图就大致理解本文在讲什么。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Classification </tag>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> LSTM </tag>
            
            <tag> Language Modeling </tag>
            
            <tag> pretrain </tag>
            
            <tag> long-term dependency </tag>
            
            <tag> GC-Net </tag>
            
            <tag> Ordered Neuron </tag>
            
            <tag> GPT </tag>
            
            <tag> GE-Net </tag>
            
            <tag> PSANet </tag>
            
            <tag> CCNet </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Mac上很好用的软件推荐</title>
      <link href="/2019/05/07/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Mac%E4%B8%8A%E5%BE%88%E5%A5%BD%E7%94%A8%E7%9A%84%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/"/>
      <url>/2019/05/07/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Mac%E4%B8%8A%E5%BE%88%E5%A5%BD%E7%94%A8%E7%9A%84%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/</url>
      
        <content type="html"><![CDATA[<p>记录个人觉得很好用的Mac软件，让Mac作为（程序员👨‍💻‍/科研人员👨‍🔬）生产力工具更顺手，提升生产效率。</p><h3 id="Alfred3"><a href="#Alfred3" class="headerlink" title="[Alfred3]"></a>[Alfred3]</h3><p>网上有太多推荐Alfred的了，我主要是用Alfred做一些常规操作，如打开/搜索文件，文本扩展等，以及一些workflow，如有道翻译。</p><p><img src="/images/15572414068036.jpg" width="50%" height="50%"></p><h3 id="SwitchResX"><a href="#SwitchResX" class="headerlink" title="[SwitchResX]"></a>[SwitchResX]</h3><p>用于显示器的调整，对于我而言主要用于外接屏幕开启HiDPI。非常好用！</p><p><img src="/images/15572767190389.jpg" width="100%" height="50%"></p><p>如何开启HiDPI：<br><a href="https://www.zhihu.com/question/35300978/answer/126332986" target="_blank" rel="noopener">https://www.zhihu.com/question/35300978/answer/126332986</a></p><h3 id="gfxCardStatus"><a href="#gfxCardStatus" class="headerlink" title="[gfxCardStatus]"></a>[gfxCardStatus]</h3><p>用于切换外接显卡和独立显卡。能够在menu bar上调整，比每次进入system preference设置方便一些。</p><p><img src="/images/15572214150588.jpg" width="25%" height="50%"></p><h3 id="Mendeley"><a href="#Mendeley" class="headerlink" title="[Mendeley]"></a>[Mendeley]</h3><p>文献管理工具，有丰富的功能和同步功能。多平台且免费，很省心。</p><p><img src="/images/15572214826817.jpg" width="100%" height="50%"></p><h3 id="Bartender3"><a href="#Bartender3" class="headerlink" title="[Bartender3]"></a>[Bartender3]</h3><p>如果太多图标都显示在menu bar上，会影响观感，不能一下子找到自己想要的东西。使用Bartender3能够隐藏部分图标，并且很优雅。</p><p>隐藏状态：<br><img src="/images/15572217046758.jpg" width="60%" height="50%"></p><p>展开状态：<br><img src="/images/15572217330397.jpg" width="25%" height="50%"></p><h3 id="Todoist"><a href="#Todoist" class="headerlink" title="[Todoist]"></a>[Todoist]</h3><p>存放要完成的事务，还可以存放一些其他的东西。界面非常好看，功能丰富，并且也是全平台的。付费，但完全值得。对我而言Todoist帮助我将工作整理得井井有条。除了工作，我还会保存一些其他list（如菜单/愿望清单😄）。</p><p><img src="/images/15572221262487.jpg" width="90%" height="50%"></p><h3 id="MWeb"><a href="#MWeb" class="headerlink" title="[MWeb]"></a>[MWeb]</h3><p>Markdown写作工具。我用这个软件写所有的博客，颜值很高，并且功能也很强大，能够实时预览效果。插入图片和公式非常非常方便。</p><p><img src="/images/15572255396438.jpg" width="110%" height="60%"></p><h3 id="IINA"><a href="#IINA" class="headerlink" title="[IINA]"></a>[IINA]</h3><p>Mac上最好用的播放器。</p><p><img src="/images/15572256826384.jpg" width="80%" height="50%"></p><h3 id="VMware-Fusion"><a href="#VMware-Fusion" class="headerlink" title="[VMware Fusion]"></a>[VMware Fusion]</h3><p>偶尔需要用到虚拟机，可以无缝切换多系统。</p><p><img src="/images/15572258471958.jpg" width="100%" height="50%"></p><h3 id="Dash"><a href="#Dash" class="headerlink" title="[Dash]"></a>[Dash]</h3><p>API浏览器，一个窗口查所有语言/包的API。</p><p><img src="/images/15572259911688.jpg" width="80%" height="50%"></p><h3 id="Cinch"><a href="#Cinch" class="headerlink" title="[Cinch]"></a>[Cinch]</h3><p>[Deprecated]</p><p><del>Mac上的分屏确实不如Windows上的好用，Cinch致力于在Mac上也有Windows的分屏体验。将窗口向上拖或向两边拖，能够自动全屏或者分屏。</del> 对多屏幕不是很友好，如果是向右拖分屏可能会拖到外接屏幕上。</p><!--<img src="/images/15572261520468.jpg" width="100%" height="50%">--><h3 id="Moom"><a href="#Moom" class="headerlink" title="[Moom]"></a>[Moom]</h3><p>功能极其强大的分屏应用。既适合普通鼠标党使用，也支持高度定制化适合键盘党的使用。</p><p><img src="/images/15613895824781.jpg" width="55%" height="50%"></p><p><a href="https://zhuanlan.zhihu.com/p/20258341" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20258341</a></p><h3 id="Mathpix-Snipping-Tool"><a href="#Mathpix-Snipping-Tool" class="headerlink" title="[Mathpix Snipping Tool]"></a>[Mathpix Snipping Tool]</h3><p>强烈推荐这款软件！写博客时经常要插入论文里面的公式，如果自己打不仅麻烦，有些还不知道怎么打。Mathpix能够将截图自动转换成公式，并且识别率挺高，省了很多力气。</p><p><img src="/images/15572263265931.jpg" width="70%" height="50%"></p><h3 id="Xnip"><a href="#Xnip" class="headerlink" title="[Xnip]"></a>[Xnip]</h3><p>一款很方便的截图软件。提供了许多如马赛克画图的功能，还支持截长图。</p><h3 id="Paste"><a href="#Paste" class="headerlink" title="[Paste]"></a>[Paste]</h3><p>保存所有之前复制内容的历史，包括文本，图片，文件等，还可以保存一些常用的内容。并且其最大的亮点在于能够与iOS同步，也即在一个平台复制的内容可以直接在另一个平台用到。</p><p><img src="/images/15572265842706.jpg" width="100%" height="50%"></p><p><img src="/images/15572267155023.png" width="35%" height="50%"></p><h3 id="Transmit"><a href="#Transmit" class="headerlink" title="[Transmit]"></a>[Transmit]</h3><p>Mac上FTP做得很好的一个软件，颜值也挺高。</p><p><img src="/images/15572269805499.jpg" width="100%" height="50%"></p><h3 id="iStat-Menus"><a href="#iStat-Menus" class="headerlink" title="[iStat Menus]"></a>[iStat Menus]</h3><p>监控系统状态（CPU/GPU/内存/网络）的软件。稳定且美观。挂在menu bar上可以很方便查看。</p><p><img src="/images/15572272550552.jpg" width="20%" height="50%"></p><p><img src="/images/istat.png" width="100%" height="50%"></p><h3 id="Downie3"><a href="#Downie3" class="headerlink" title="[Downie3]"></a>[Downie3]</h3><p>轻量级的一个下载视频的工具，能够自动/手动提取浏览器的视频链接。</p><p><img src="/images/15572277461936.jpg" width="90%" height="50%"></p><h3 id="The-Unarchiver"><a href="#The-Unarchiver" class="headerlink" title="[The Unarchiver]"></a>[The Unarchiver]</h3><p>轻量级的解压软件，轻到甚至感觉不到他的存在。</p><h3 id="OneNote"><a href="#OneNote" class="headerlink" title="[OneNote]"></a>[OneNote]</h3><p>记笔记的利器，office套件中我用得最频繁的软件。记录论文阅读笔记/想法/实验。独一无二的灵活性，可以在页面的任何地方创建笔记（我试了市面上所有流行的笔记软件，都没有这样的灵活性）。同时还是全平台，还支持笔，在iPad上的笔迹可以很快速同步到Mac上。当然目前而言功能还没有Windows上的OneNote那么强大。</p><p><img src="/images/15572281358175.jpg" width="90%" height="50%"></p><h3 id="Maipo"><a href="#Maipo" class="headerlink" title="[Maipo]"></a>[Maipo]</h3><p>Mac端很不错的微博客户端。可以很方便地在电脑端刷微博学习（大雾 。</p><p><img src="/images/15572363838509.jpg" width="70%" height="50%"></p><h3 id="linux-command"><a href="#linux-command" class="headerlink" title="[linux-command]"></a>[linux-command]</h3><p>方便搜索Linux命令。</p><p><img src="/images/15572363564485.jpg" width="90%" height="50%"></p><h3 id="iTerm"><a href="#iTerm" class="headerlink" title="[iTerm]"></a>[iTerm]</h3><p>命令行是👨‍💻‍必备。而iTerm相对原生terminal有更丰富的设置以及更强大的功能。</p><p><img src="/images/15572775111514.jpg" width="90%" height="50%"></p><h3 id="AppCleaner"><a href="#AppCleaner" class="headerlink" title="[AppCleaner]"></a>[AppCleaner]</h3><p>轻松地删除Mac软件，可以检测该软件所带的其他文件，一并删除。配合Alfred的workflow很方便。</p><p><img src="/images/15572414475923.jpg" width="70%" height="50%"></p><h3 id="Mos"><a href="#Mos" class="headerlink" title="[Mos]"></a>[Mos]</h3><p>Mac的触控板和鼠标的逻辑是反的。如果希望触控板和鼠标一起用，并且逻辑各不相同，则可以使用Mos，Mos能够将鼠标翻转。</p><p><img src="/images/15572415681851.jpg" width="70%" height="50%"></p><h3 id="PDF-Expert"><a href="#PDF-Expert" class="headerlink" title="[PDF Expert]"></a>[PDF Expert]</h3><p>强烈推荐的PDF阅读编辑软件。简单易用，并且功能也足够强大。很重要的一点是可以多平台同步（Mac/iOS），还可以使用”接力“功能，减少切换设备的麻烦。</p><p><img src="/images/15572417246059.jpg" width="80%" height="50%"></p><h3 id="Contexts"><a href="#Contexts" class="headerlink" title="[Contexts]"></a>[Contexts]</h3><p>快速切换窗口的效率工具。特别是在多显示器的情况下，往往会找不到想要的窗口。对我而言，Contexts极大增加了效率。调出窗口列表，接着搜索即可，还可以通过鼠标点击屏幕边上的浮动窗口切换。</p><p><img src="/images/context.png" width="90%" height="50%"></p><h3 id="SnippetsLab"><a href="#SnippetsLab" class="headerlink" title="[SnippetsLab]"></a>[SnippetsLab]</h3><p>收藏一些有用的代码片段。保持记录的习惯能够提高代码效率。</p><p><img src="/images/15572422193062.jpg" width="100%" height="50%"></p><h3 id="Sublime-Text"><a href="#Sublime-Text" class="headerlink" title="[Sublime Text]"></a>[Sublime Text]</h3><p>不需要推荐，大名鼎鼎的文本编辑软件。👨‍💻‍必备。</p><h3 id="AdGuard-for-Safari"><a href="#AdGuard-for-Safari" class="headerlink" title="[AdGuard for Safari]"></a>[AdGuard for Safari]</h3><p>因为Safari的轻量以及颜值放弃了使用很久的Chrome，但也意味着放弃了丰富的浏览器插件。在Chrome可以使用AdBlock，在Safari则可以使用AdGuard，非常轻量，甚至感觉不到它的存在。</p><h3 id="CatchMouse"><a href="#CatchMouse" class="headerlink" title="[CatchMouse]"></a>[CatchMouse]</h3><p>针对多屏幕而设计。有时候会找不到鼠标在哪个屏幕。通过设置快捷键，能够快速将鼠标移动到指定屏幕，在切换的时候还会缩放鼠标的图标作为提醒。（找了好久才找到这个符合我需求的软件）</p><p><img src="/images/15572424938492.jpg" width="60%" height="50%"></p><p><img src="/images/15572425907175.jpg" width="60%" height="50%"></p><h3 id="Zoom"><a href="#Zoom" class="headerlink" title="[Zoom]"></a>[Zoom]</h3><p>会议电话的软件。开远程PaperReading可以用🌚。</p><h3 id="Mate-Translate"><a href="#Mate-Translate" class="headerlink" title="[Mate Translate]"></a>[Mate Translate]</h3><p>集成在右键的翻译工具。在看论文或者浏览网页时可以随时翻译句子。</p><p><img src="/images/15592926038793.jpg" width="40%" height="50%"></p><p><img src="/images/15592926623566.jpg" width="55%" height="50%"></p><h3 id="ToothFairy"><a href="#ToothFairy" class="headerlink" title="[ToothFairy]"></a>[ToothFairy]</h3><p>一键连接蓝牙设备的应用。驻扎在MenuBar上可以一键连接自己的蓝牙设备。我一般用于连接无线蓝牙耳机（XM3）和Airpods。</p><p><img src="/images/15613850296455.jpg" width="10%" height="50%"></p><h3 id="Quicklook插件"><a href="#Quicklook插件" class="headerlink" title="[Quicklook插件]"></a>[Quicklook插件]</h3><p>Mac上一个很人性化的操作就是可以空格预览。但对于一些格式支持还不够好，比如无扩展名的无法支持预览，Markdown只能预览源代码，预览源代码只支持预览纯文本，并没有高亮显示。因此Quicklook插件可以解决这些细节问题。</p><p><a href="https://github.com/sindresorhus/quick-look-plugins" target="_blank" rel="noopener">https://github.com/sindresorhus/quick-look-plugins</a></p><p>预览Markdown：</p><p><img src="/images/15613853773728.jpg" width="50%" height="50%"></p><p>预览无扩展名的纯文本：</p><p><img src="/images/15613855291056.jpg" width="60%" height="50%"></p><h3 id="UltraCompare"><a href="#UltraCompare" class="headerlink" title="[UltraCompare]"></a>[UltraCompare]</h3><p>文件对比工具。</p><p><img src="/images/15648879772101.jpg" width="70%" height="50%"></p><h3 id="Texpad"><a href="#Texpad" class="headerlink" title="[Texpad]"></a>[Texpad]</h3><p>Mac上进行LaTex编辑的不二选择。界面优雅，功能强大，编译速度飞快。重要的是各种table和label都在左边显示出来，可以方便快速跳转。在用overleaf的时候就经常要到处找table。</p><p><img src="/images/15648883057108.jpg" width="90%" height="50%"></p><h3 id="Alternote"><a href="#Alternote" class="headerlink" title="[Alternote]"></a>[Alternote]</h3><p>Mac上note的一定程度上的替代品。Mac自带的备忘录在键入超过1000字之后就会变得卡顿，因此需要一款替代品。在对比了几款之后选择Alternote。Alternote是印象笔记的第三方app，只保留最基本的功能，且界面更加优雅，也符合我轻量级的需求。</p><p>❌Agenda 太过复杂了，没必要<br>❌simplenote 有些过于简单，只有一级菜单<br>❌Quip 是办公套件，与我的需求不符<br>❌zoho   不好用，我不喜欢这种风格<br>❌Pendo<br>✅alternote  界面不错且简洁<br>❌notability 太过复杂<br>❌paper 不是我的风格</p><p><img src="/images/15648886006772.jpg" width="40%" height="50%"></p><h3 id="Scapple"><a href="#Scapple" class="headerlink" title="[Scapple]"></a>[Scapple]</h3><p>思维导图工具。轻量，易用，功能强大。</p><p><img src="/images/15648890419056.jpg" width="70%" height="50%"></p><h3 id="MonitorControl"><a href="#MonitorControl" class="headerlink" title="[MonitorControl]"></a>[MonitorControl]</h3><p>能够控制外接屏的亮度对比度等，不再需要按物理键了。</p><p><img src="/images/15664846144052.jpg" width="30%" height="50%"></p><h3 id="Noizio"><a href="#Noizio" class="headerlink" title="[Noizio]"></a>[Noizio]</h3><p>白噪声应用。戴上降噪耳机再开启白噪声，可以完全屏蔽周围的声音包括人声。并且有身临其境的感觉，能够更加专注于工作。</p><p><img src="/images/15664847839748.jpg" width="40%" height="50%"></p><h3 id="pap-er"><a href="#pap-er" class="headerlink" title="[pap.er]"></a>[pap.er]</h3><p>一个轻量级的壁纸应用。拥有超高清的海量的壁纸。</p><p><img src="/images/15664848978708.jpg" width="30%" height="50%"></p><h3 id="CheatSheet"><a href="#CheatSheet" class="headerlink" title="[CheatSheet]"></a>[CheatSheet]</h3><p>查看各种应用的快捷键。打开它，长按command就会跳出当前活跃的应用的所有快捷键。</p><p><img src="/images/15664850960793.jpg" width="30%" height="50%"></p><h3 id="TimeMachineEditor"><a href="#TimeMachineEditor" class="headerlink" title="[TimeMachineEditor]"></a>[TimeMachineEditor]</h3><p>能够修改TimeMachine的备份计划。如果是像我这样长期插着TimeMachine备份盘的，经常会在使用的时候因为备份而导致电脑卡顿。一般而言TM是每小时备份一次，无论电脑是否在工作，而我们并不需要如此高频率的备份，因此可以通过TimeMachineEditor来设定不同的备份计划，如设定当电脑不活跃的时候开始备份。</p><p><img src="/images/15674259207857.jpg" width="50%" height="50%"></p><hr><p>最后晒晒自己的工作台☺️（本科一直心心念念的工作台，似乎终于达成了，虽然桌面大小还是小了一些）。有好的生产工具真是可以让工作变成一种享受😄，开心。</p><p><img src="/images/15572429074807.jpg" width="90%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 杂七杂八 </tag>
            
            <tag> Mac </tag>
            
            <tag> app </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于transformer-xl中rel-shift实现的解读</title>
      <link href="/2019/05/07/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E5%85%B3%E4%BA%8Etransformer-xl%E4%B8%ADrel-shift%E5%AE%9E%E7%8E%B0%E7%9A%84%E8%A7%A3%E8%AF%BB/"/>
      <url>/2019/05/07/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E5%85%B3%E4%BA%8Etransformer-xl%E4%B8%ADrel-shift%E5%AE%9E%E7%8E%B0%E7%9A%84%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>transformer-xl中有一步使用相对位置计算attention weight：</p><p>$\mathbf{A}_{i, j}^{\mathrm{rel}}=\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}}_{(b)}+\underbrace{u^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{v^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}}_{(d)}$</p><p>由于相对位置要计算所有的query与key对，因此是平方的复杂度。而在论文的附录中提到可以通过简单的推导将复杂度降为线性。<br>简单地说，我们希望获得：<br>$\mathbf{B} = \left[ \begin{array}{cccccc}{q_{0}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{M}} &amp; {\cdots} &amp; {q_{0}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{0}} &amp; {0} &amp; {\cdots} &amp; {0} \\ {q_{1}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{M+1}} &amp; {\cdots} &amp; {q_{1}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{1}} &amp; {q_{1}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{0}} &amp; {\cdots} &amp; {0} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {q_{L-1}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{M+L-1}} &amp; {\cdots} &amp; {q_{L-1}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{M+L-1}} &amp; {q_{L-1}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{L-1}} &amp; {\cdots} &amp; {q_{L-1}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{0}}\end{array}\right] \\  = \left[ \begin{array}{cccccc}{q_{0}^{\top} \mathbf{Q}_{L-1}} &amp; {\cdots} &amp; {q_{0}^{\top} \mathbf{Q}_{M+L-1}} &amp; {0} &amp; {\cdots} &amp; {0} \\ {q_{1}^{\top} \mathbf{Q}_{L-2}} &amp; {\cdots} &amp; {q_{1}^{\top} \mathbf{Q}_{M+L-2}} &amp; {q_{1}^{\top} \mathbf{Q}_{M+L-1}} &amp; {\cdots} &amp; {0} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {q_{L-1}^{\top} \mathbf{Q}_{0}} &amp; {\cdots} &amp; {q_{L-1}^{\top} \mathbf{Q}_{M}} &amp; {q_{L-1}^{\top} \mathbf{Q}_{M+1}} &amp; {\cdots} &amp; {q_{L-1}^{\top} \mathbf{Q}_{M+L-1}}\end{array}\right]$</p><p>其中：<br>$\mathbf{Q} :=\left[ \begin{array}{c}{\mathbf{R}_{M+L-1}^{\top}} \\ {\mathbf{R}_{M+L-2}^{\top}} \\ {\vdots} \\ {\mathbf{R}_{1}^{\top}} \\ {\mathbf{R}_{0}^{\top}}\end{array}\right] \mathbf{W}_{k, R}^{\top}=\left[ \begin{array}{c}{\left[\mathbf{W}_{k, R} \mathbf{R}_{M+L-1}\right]^{\top}} \\ {\vdots} \\ {\vdots} \\ {\left[\mathbf{W}_{k, R} \mathbf{R}_{1}\right]^{\top}} \\ {\left[\mathbf{W}_{k, R} \mathbf{R}_{0}\right]^{\top}}\end{array}\right] \in \mathbb{R}^{(M+L) \times d}$</p><p>而我们可以直接获得的是：<br>$\tilde{\mathbf{B}}=\mathbf{q} \mathbf{Q}^{\top}=\left[ \begin{array}{cccccc}{q_{0}^{\top} \mathbf{Q}_{0}} &amp; {\cdots} &amp; {q_{0}^{\top} \mathbf{Q}_{M}} &amp; {q_{0}^{\top} \mathbf{Q}_{M+1}} &amp; {\cdots} &amp; {q_{0}^{\top} \mathbf{Q}_{M+L-1}} \\ {q_{1}^{\top} \mathbf{Q}_{0}} &amp; {\cdots} &amp; {q_{1}^{\top} \mathbf{Q}_{M}} &amp; {q_{1}^{\top} \mathbf{Q}_{M+1}} &amp; {\cdots} &amp; {q_{1}^{\top} \mathbf{Q}_{M+L-1}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {q_{L-1}^{\top} \mathbf{Q}_{0}} &amp; {\cdots} &amp; {q_{L-1}^{\top} \mathbf{Q}_{M}} &amp; {q_{L-1}^{\top} \mathbf{Q}_{M+1}} &amp; {\cdots} &amp; {q_{L-1}^{\top} \mathbf{Q}_{M+L-1}}\end{array}\right]<br>$</p><p>$\tilde{\mathbf{B}}$与$\mathbf{B}$的区别在于$\mathbf{B}$是$\tilde{\mathbf{B}}$的left-shifted版本，其中第一行左移了L-1，后面每行依次递减左移个数，最后一行则不左移。</p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>抽象地看，我们要做的事情就是，给定一个矩阵，每行都进行左移，而移动的个数随行数递增而递减。</p><p>我目前想到的一种方法是使用gather，将想要的index提前定好，然后使用Pytorch的gather就能够实现。</p><p>而transformer-xl实现了另一种更好的方法：<code>_rel_shift</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_rel_shift</span><span class="params">(self, x, zero_triu=False)</span>:</span></span><br><span class="line">    <span class="comment"># x: q,k,bs,n_head</span></span><br><span class="line">    zero_pad = torch.zeros((x.size(<span class="number">0</span>), <span class="number">1</span>, *x.size()[<span class="number">2</span>:]),</span><br><span class="line">                           device=x.device, dtype=x.dtype)</span><br><span class="line">    x_padded = torch.cat([zero_pad, x], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    x_padded = x_padded.view(x.size(<span class="number">1</span>) + <span class="number">1</span>, x.size(<span class="number">0</span>), *x.size()[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">    x = x_padded[<span class="number">1</span>:].view_as(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>第一步是，将x的第一列填上padding，此时<code>x.size()=q,k+1,bs,n_head</code>，接下来将其重新reshape，则变成了<code>x.size()=k+1,q,bs,n_head</code>，最后将第一行去掉，变成<code>x.size()=k,q,bs,n_head</code>，再将其reshape回x原来的样子。</p><p>为什么这么做实现了我们想要的左移的功能？我们应该从一维的角度去理解。因为实际上在内存中所有元素都是按照一维去排列的。</p><p>原来的矩阵：<br><img src="/images/15572009149790.jpg" width="60%" height="50%"></p><p>实际上就是有q个key按照一行去排列。</p><p>在做完padding之后，则：<br><img src="/images/15572009689231.jpg" width="60%" height="50%"></p><p>实际上就是在每个key前面插入了0。</p><p>接下来view，实际上数据的先后顺序还是没有变（因为不是transpose）：<br><img src="/images/15572010355613.jpg" width="60%" height="50%"></p><p>实际上只是强行将该行切成一个一个q而已。</p><p>那么最后一个操作，将第一行丢掉，实际上就是要把原来的x的第一行强行左移q-1个（因为有padding）。那么为什么后面的行能够左移的个数依次减少？别忘了padding，第一行左移了q-1个，但第二个key前面也有一个padding，所以相当于将其向右推了一格；第三个又有一个padding，就在原来的基础上又推了一格，也即推了两格。因此最后达到了我们想要的目的。</p><p>实际上要理解该方法，需要牢牢把握数据存储的本质是一整行。</p><p>该方法没有数据的拷贝，全部都是view操作，因此更高效。</p><p>不得不佩服想到该方法的人的工程能力，同时也感谢戴宁带我理解该方法的本质，一开始我是死活不理解的。以后或许可以将该思想灵活应用到其他方面。</p>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
            <tag> transformer-xl </tag>
            
            <tag> rel-shift </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pytorch中遇到的问题（合集）</title>
      <link href="/2019/05/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/PyTorch%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%88%E5%90%88%E9%9B%86%EF%BC%89/"/>
      <url>/2019/05/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/PyTorch%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%88%E5%90%88%E9%9B%86%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>断断续续记录一下在写代码过程中遇到的问题以及解决方案。</p><h3 id="Parameter-nan"><a href="#Parameter-nan" class="headerlink" title="[Parameter nan]"></a>[Parameter nan]</h3><p><a href="http://www.linzehui.me/2019/05/07/碎片知识/关于Pytorch中Parameter的nan/">http://www.linzehui.me/2019/05/07/碎片知识/关于Pytorch中Parameter的nan/</a></p><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>在定义Parameter时遇到其元素存在nan的问题。</p><p><img src="/images/15571956719188.jpg" width="80%" height="50%"></p><h4 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h4><p><code>torch.Tensor(m,n)</code>只分配空间而没有将其中的值更新。</p><h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p><code>torch.Tensor(m,n)</code>是不建议使用的初始化方法。改成<code>torch.nn.Parameter(torch.rand(10,10))</code>或<code>torch.nn.Parameter(torch.zeros(10,10))</code>。</p><hr><h3 id="inplace-operation"><a href="#inplace-operation" class="headerlink" title="[+= inplace operation]"></a>[+= inplace operation]</h3><p><a href="http://www.linzehui.me/2018/12/09/碎片知识/Python中的+=操作/">http://www.linzehui.me/2018/12/09/碎片知识/Python中的+=操作/</a></p><h4 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output+=pos  <span class="comment"># pos是不可更新的tensor，output是可更新的tensor</span></span><br></pre></td></tr></table></figure><p>程序报错：“one of the variables needed for gradient computation has been modified by an inplace operation”。</p><h4 id="原因-1"><a href="#原因-1" class="headerlink" title="原因"></a>原因</h4><p>在Python中，<code>i=i+1</code>和<code>i+=1</code>是不同的，如果被操作数没有部署 ’<strong>iadd</strong>‘方法，则<code>i=i+1</code>和<code>i+=1</code>是等价的，’+=‘并不会产生in-place操作；当被操作数有部署该方法且正确部署，则是会产生in-place操作的。当没有in-place操作时，<code>i=i+1</code>表示对i重分配，也即i指向了另一个空间而不是原来的空间。<br>在Pytorch中，也有部署’<strong>iadd</strong>()‘操作，所以对于<code>output+=pos</code>，output内部的值被改变了，也即在计算图中引入了环，在反向求导时则会出错。</p><h4 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h4><p>尽量不使用inplace操作，即使是官方的API，如<code>unsqueeze_()</code>。已经好几次被inplace操作坑了。</p><hr><h3 id="squeeze-dim"><a href="#squeeze-dim" class="headerlink" title="[squeeze dim]"></a>[squeeze dim]</h3><p><a href="http://www.linzehui.me/2019/05/06/碎片知识/每周碎片知识20/">http://www.linzehui.me/2019/05/06/碎片知识/每周碎片知识20/</a></p><h4 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h4><p>当需要squeeze时，未指定squeeze的维度，导致后面的维数不一致，报错。</p><h4 id="原因-2"><a href="#原因-2" class="headerlink" title="原因"></a>原因</h4><p>由于在某些极端情况下，可能会出现batch size为1的情况，当遇到这个情况时，squeeze会将其一并压缩掉，使得后面会出错。</p><h4 id="解决方案-2"><a href="#解决方案-2" class="headerlink" title="解决方案"></a>解决方案</h4><p>尽可能显式指定要压缩的维度，除非很明确就要将所有的压缩掉。</p><hr><h3 id="inf—-gt-nan"><a href="#inf—-gt-nan" class="headerlink" title="[inf—&gt;nan]"></a>[inf—&gt;nan]</h3><p><a href="http://www.linzehui.me/2019/07/03/碎片知识/关于PyTorch中inf导数的nan问题/">http://www.linzehui.me/2019/07/03/碎片知识/关于PyTorch中inf导数的nan问题/</a></p><h4 id="问题描述-3"><a href="#问题描述-3" class="headerlink" title="问题描述"></a>问题描述</h4><p>两个tensor相乘，若其中一个tensor带有inf，则另一个tensor（该tensor可更新）的grad则为nan。</p><h4 id="原因-3"><a href="#原因-3" class="headerlink" title="原因"></a>原因</h4><p>乘法的导数的定义。两个tensor相乘，导数为对方。在PyTorch中，inf的grad则为nan。</p><h4 id="解决方案-3"><a href="#解决方案-3" class="headerlink" title="解决方案"></a>解决方案</h4><p>在相乘前，将带有inf的tensor做masked_fill，inf被置为0后再相乘。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> nan </tag>
            
            <tag> inf </tag>
            
            <tag> Parameter </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Pytorch中Parameter的nan</title>
      <link href="/2019/05/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADParameter%E7%9A%84nan/"/>
      <url>/2019/05/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADParameter%E7%9A%84nan/</url>
      
        <content type="html"><![CDATA[<p>前几天遇到一个很神奇的bug，在Model里面定义一个Parameter，Parameter出现了nan。<br>如：<br><img src="/images/15571956719188.jpg" width="80%" height="50%"></p><p><del>找了一圈网上没有找到其原因，已经在论坛提问了。</del><br>我的解决方案是显式对其进行初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.init == <span class="string">'uniform'</span>:</span><br><span class="line">    nn.init.uniform_(self.u, -args.init_range, args.init_range)</span><br><span class="line">    nn.init.uniform_(self.v, -args.init_range, args.init_range)</span><br><span class="line"></span><br><span class="line"><span class="keyword">elif</span> args.init == <span class="string">'normal'</span>:</span><br><span class="line">    nn.init.normal_(self.u, <span class="number">0.0</span>, args.init_std)</span><br><span class="line">    nn.init.normal_(self.v, <span class="number">0.0</span>, args.init_std)</span><br></pre></td></tr></table></figure><hr><p>原来是torch.Tensor的锅，torch.Tensor会分配内存空间，但不会清空该空间的值，因此里面可能会有奇怪的值。正确的做法应该是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Parameter(torch.rand(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">torch.nn.Parameter(torch.zeros(<span class="number">10</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>参考资料：<br><a href="https://discuss.pytorch.org/t/nan-in-torch-tensor/8987" target="_blank" rel="noopener">https://discuss.pytorch.org/t/nan-in-torch-tensor/8987</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> nan </tag>
            
            <tag> Parameter </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录16</title>
      <link href="/2019/05/06/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9516/"/>
      <url>/2019/05/06/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9516/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch检查tensor-nan"><a href="#1️⃣-Pytorch检查tensor-nan" class="headerlink" title="1️⃣[Pytorch检查tensor nan]"></a>1️⃣[Pytorch检查tensor nan]</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 法一，基于nan!=nan </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, np.nan])</span><br><span class="line">tensor([  <span class="number">1.</span>,   <span class="number">2.</span>, nan.])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x != x</span><br><span class="line">tensor([ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>], dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 法二，torch.isnan(x)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.isnan(x)</span><br><span class="line">tensor([ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>], dtype=torch.uint8)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识20</title>
      <link href="/2019/05/06/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8620/"/>
      <url>/2019/05/06/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8620/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>①<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.gather(input, dim, index, out=<span class="keyword">None</span>) → Tensor</span><br></pre></td></tr></table></figure></p><p>能够根据index的值在指定维度收集数值。可以用于切slice。</p><p>②<br>expand和repeat不同，不会分配新的内存。如果一个tensor使用expand再cat到其他tensor上，这个expand还会省内存吗？<br>不会。<br>在cat的时候会重新分配整个tensor的内存，并且将元素一个一个copy过去。</p><p><a href="https://discuss.pytorch.org/t/efficiency-of-torch-cat/8830" target="_blank" rel="noopener">https://discuss.pytorch.org/t/efficiency-of-torch-cat/8830</a></p><blockquote><p>it pre-allocates the full tensor and then copy into it each element</p></blockquote><p>③</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.einsum(equation, *operands) → Tensor</span><br></pre></td></tr></table></figure><blockquote><p>This function provides a way of computing multilinear expressions (i.e. sums of products) using the Einstein summation convention.</p></blockquote><p>发现一个神奇的api，Pytorch支持爱因斯坦求和约定(Einstein summation convention)。也即在给定两个tensor时，可以指定维度进行求和，相当灵活，可以理解成bmm或者mm的扩展版，这样在做一些tensor之间的操作就不需要view/permute调整成bmm支持的格式了。</p><p>官方例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.randn(<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.einsum(<span class="string">'i,j-&gt;ij'</span>, x, y)  <span class="comment"># outer product</span></span><br><span class="line">tensor([[<span class="number">-0.0570</span>, <span class="number">-0.0286</span>, <span class="number">-0.0231</span>,  <span class="number">0.0197</span>],</span><br><span class="line">        [ <span class="number">1.2616</span>,  <span class="number">0.6335</span>,  <span class="number">0.5113</span>, <span class="number">-0.4351</span>],</span><br><span class="line">        [ <span class="number">1.4452</span>,  <span class="number">0.7257</span>,  <span class="number">0.5857</span>, <span class="number">-0.4984</span>],</span><br><span class="line">        [<span class="number">-0.4647</span>, <span class="number">-0.2333</span>, <span class="number">-0.1883</span>,  <span class="number">0.1603</span>],</span><br><span class="line">        [<span class="number">-1.1130</span>, <span class="number">-0.5588</span>, <span class="number">-0.4510</span>,  <span class="number">0.3838</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = torch.randn(<span class="number">3</span>,<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>l = torch.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = torch.randn(<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.einsum(<span class="string">'bn,anm,bm-&gt;ba'</span>, l, A, r) <span class="comment"># compare torch.nn.functional.bilinear</span></span><br><span class="line">tensor([[<span class="number">-0.3430</span>, <span class="number">-5.2405</span>,  <span class="number">0.4494</span>],</span><br><span class="line">        [ <span class="number">0.3311</span>,  <span class="number">5.5201</span>, <span class="number">-3.0356</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>As = torch.randn(<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Bs = torch.randn(<span class="number">3</span>,<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.einsum(<span class="string">'bij,bjk-&gt;bik'</span>, As, Bs) <span class="comment"># batch matrix multiplication</span></span><br><span class="line">tensor([[[<span class="number">-1.0564</span>, <span class="number">-1.5904</span>,  <span class="number">3.2023</span>,  <span class="number">3.1271</span>],</span><br><span class="line">         [<span class="number">-1.6706</span>, <span class="number">-0.8097</span>, <span class="number">-0.8025</span>, <span class="number">-2.1183</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">4.2239</span>,  <span class="number">0.3107</span>, <span class="number">-0.5756</span>, <span class="number">-0.2354</span>],</span><br><span class="line">         [<span class="number">-1.4558</span>, <span class="number">-0.3460</span>,  <span class="number">1.5087</span>, <span class="number">-0.8530</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">2.8153</span>,  <span class="number">1.8787</span>, <span class="number">-4.3839</span>, <span class="number">-1.2112</span>],</span><br><span class="line">         [ <span class="number">0.3728</span>, <span class="number">-2.1131</span>,  <span class="number">0.0921</span>,  <span class="number">0.8305</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = torch.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.einsum(<span class="string">'ii-&gt;i'</span>, A) <span class="comment"># diagonal</span></span><br><span class="line">tensor([<span class="number">-0.7825</span>,  <span class="number">0.8291</span>, <span class="number">-0.1936</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = torch.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.einsum(<span class="string">'...ii-&gt;...i'</span>, A) <span class="comment"># batch diagonal</span></span><br><span class="line">tensor([[<span class="number">-1.0864</span>,  <span class="number">0.7292</span>,  <span class="number">0.0569</span>],</span><br><span class="line">        [<span class="number">-0.9725</span>, <span class="number">-1.0270</span>,  <span class="number">0.6493</span>],</span><br><span class="line">        [ <span class="number">0.5832</span>, <span class="number">-1.1716</span>, <span class="number">-1.5084</span>],</span><br><span class="line">        [ <span class="number">0.4041</span>, <span class="number">-1.1690</span>,  <span class="number">0.8570</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.einsum(<span class="string">'...ij-&gt;...ji'</span>, A).shape <span class="comment"># batch permute</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><p>我对这个api一般的用法就是，将两个tensor的每一维用不同的记号标号，然后想一下我想要的tensor的格式，按照记号写下就可以直接得到了。</p><p>④<br>squeeze在使用的时候尽量指定维度，否则可能会出现在训练最后一个batch时，batch_size正好是1，就把batch_size给squeeze掉了。（已经两次遇到这样的bug了）</p><p>⑤</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apply(fn)</span><br></pre></td></tr></table></figure><blockquote><p>Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch-nn-init).</p></blockquote><p>可以用于所有的子模块的初始化，好像很方便的样子。但我突然想到这种方法可能会不小心把embedding初始化给覆盖了，如果embedding有用pretrain初始化的话。</p><p>官方例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">        print(m)</span><br><span class="line">        <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">            m.weight.data.fill_(<span class="number">1.0</span>)</span><br><span class="line">            print(m.weight)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">2</span>), nn.Linear(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.apply(init_weights)</span><br><span class="line">Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line">Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">  (<span class="number">1</span>): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">)</span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">  (<span class="number">1</span>): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><h3 id="2️⃣-bpc"><a href="#2️⃣-bpc" class="headerlink" title="2️⃣[bpc]"></a>2️⃣[bpc]</h3><p>bits per character(bpc)是language model一个评价指标，另一个常用指标是ppl（困惑度）。实际上bpc和ppl都是和交叉熵挂钩的，其计算公式为：</p><script type="math/tex; mode=display">\begin{aligned} b p c(s t r i n g)=\frac{1}{T} \sum_{t=1}^{T} H\left(P_{t}, \hat{P}_{t}\right) &=-\frac{1}{T} \sum_{t=1}^{T} \sum_{c=1}^{n} P_{t}(c) \log _{2} \hat{P}_{t}(c) \\ &=-\frac{1}{T} \sum_{t=1}^{T} \log _{2} \hat{P}_{t}\left(x_{t}\right) \end{aligned}</script><p>在代码中计算交叉熵的loss是以e为底的，因此需要将loss除以$\log _{e}2$即可（log的换底公式）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cur_loss / math.log(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p><a href="https://stats.stackexchange.com/questions/211858/how-to-compute-bits-per-character-bpc" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/211858/how-to-compute-bits-per-character-bpc</a><br><a href="https://arxiv.org/pdf/1308.0850.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1308.0850.pdf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> bpc </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词24</title>
      <link href="/2019/05/05/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D24/"/>
      <url>/2019/05/05/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D24/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-观书"><a href="#1️⃣-观书" class="headerlink" title="1️⃣ 观书"></a>1️⃣ 观书</h3><p>[明] 于谦<br>书卷多情似故人，晨昏忧乐每相亲。<br>眼前直下三千字，胸次全无一点尘。<br>活水源流随处满，东风花柳逐时新。<br><strong>金鞍玉勒寻芳客，未信吾庐别有春。</strong></p><p><a href="http://lib.xcz.im/work/582ee1a2da2f600063ec45ea" target="_blank" rel="noopener">http://lib.xcz.im/work/582ee1a2da2f600063ec45ea</a></p><hr>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文17</title>
      <link href="/2019/04/28/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8717/"/>
      <url>/2019/04/28/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8717/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>PHRASE-BASED ATTENTIONS</li><li>Regularizing and Optimizing LSTM Language Models</li></ol><h2 id="1️⃣-PHRASE-BASED-ATTENTIONS"><a href="#1️⃣-PHRASE-BASED-ATTENTIONS" class="headerlink" title="1️⃣[PHRASE-BASED ATTENTIONS]"></a>1️⃣[PHRASE-BASED ATTENTIONS]</h2><p>这篇投了ICLR但没中。<br>提出对Transformer的attention机制进行改进，以词组为单位进行attention，引入词组的对齐来提升翻译表现。提出的想法也是比较简单直观的。</p><p>回顾：<br>transformer的做法：<br>$\begin{aligned} \text { Attention }\left(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}, \boldsymbol{W}_{q}, \boldsymbol{W}_{k}, \boldsymbol{W}_{v}\right) &amp;=\mathcal{S}\left(\frac{\left(\boldsymbol{Q} \boldsymbol{W}_{q}\right)\left(\boldsymbol{K} \boldsymbol{W}_{k}\right)^{T}}{\sqrt{d_{k}}}\right)\left(\boldsymbol{V} \boldsymbol{W}_{v}\right) \\ \text { Head }^{i} &amp;=\text { Attention }\left(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}, \boldsymbol{W}_{q}^{i}, \boldsymbol{W}_{k}^{i}, \boldsymbol{W}_{v}^{i}\right) \text { for } i=1 \ldots h \\ \text { AttentionOutput }(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}, \boldsymbol{W}) &amp;=\text { concat (Head}^{1}, \text { Head}^{2}, \ldots, \text { Head}^{h} ) \boldsymbol{W} \end{aligned}<br>$</p><h3 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h3><h4 id="PHRASE-BASED-ATTENTION-METHODS"><a href="#PHRASE-BASED-ATTENTION-METHODS" class="headerlink" title="PHRASE-BASED ATTENTION METHODS"></a>PHRASE-BASED ATTENTION METHODS</h4><p>其本质是使用CNN操作使得词有phrase的信息。也即：</p><script type="math/tex; mode=display">O_{t}=\mathbf{w} \oplus_{k=0}^{n} \mathbf{x}_{t \pm k}</script><p>下面使用$\operatorname{Conv}_{n}(\boldsymbol{X}, \boldsymbol{W})$代表$\boldsymbol{W}$对$\boldsymbol{X}$进行卷积操作。其中$\boldsymbol{W} \in \mathbb{R}^{n \times d_{1} \times d_{2}}$</p><p>接下来提出两种方法。</p><h5 id="KEY-VALUE-CONVOLUTION"><a href="#KEY-VALUE-CONVOLUTION" class="headerlink" title="KEY-VALUE CONVOLUTION"></a>KEY-VALUE CONVOLUTION</h5><script type="math/tex; mode=display">\operatorname{Conv} \mathrm{KV}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V})=\mathcal{S}\left(\frac{\left(\boldsymbol{Q} \boldsymbol{W}_{q}\right) \operatorname{Conv}_{n}\left(\boldsymbol{K}, \boldsymbol{W}_{k}\right)^{T}}{\sqrt{d_{k}}}\right) \operatorname{Conv}_{n}\left(\boldsymbol{V}, \boldsymbol{W}_{v}\right)</script><p>Q不变，只对K和V进行卷积。</p><h5 id="QUERY-AS-KERNEL-CONVOLUTION"><a href="#QUERY-AS-KERNEL-CONVOLUTION" class="headerlink" title="QUERY AS-KERNEL CONVOLUTION"></a>QUERY AS-KERNEL CONVOLUTION</h5><script type="math/tex; mode=display">\operatorname{QUERYK}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V})=\mathcal{S}\left(\frac{\operatorname{Conv}_{n}\left(\boldsymbol{K} \boldsymbol{W}_{k}, \boldsymbol{Q} \boldsymbol{W}_{q}\right)}{\sqrt{d_{k} * n}}\right) \operatorname{Conv}_{n}\left(\boldsymbol{V}, \boldsymbol{W}_{v}\right)</script><p>将Q作为convolution的kernel参数进行卷积。$\boldsymbol{W}_{q} \in \mathbb{R}^{n \times d_{q} \times d_{k}}, \boldsymbol{W}_{k} \in \mathbb{R}^{d_{k} \times d_{k}}, \boldsymbol{W}_{v} \in \mathbb{R}^{n \times d_{v} \times d_{v}}$</p><p>以上是基本形式，扩展到多个head可以有多种方法。</p><h4 id="MULTI-HEADED-PHRASAL-ATTENTION"><a href="#MULTI-HEADED-PHRASAL-ATTENTION" class="headerlink" title="MULTI-HEADED PHRASAL ATTENTION"></a>MULTI-HEADED PHRASAL ATTENTION</h4><h5 id="HOMOGENEOUS-N-GRAM-ATTENTION"><a href="#HOMOGENEOUS-N-GRAM-ATTENTION" class="headerlink" title="HOMOGENEOUS N-GRAM ATTENTION"></a>HOMOGENEOUS N-GRAM ATTENTION</h5><p><img src="/images/15564587340044.jpg" width="90%" height="50%"></p><p>每个head专注某种gram。但这样似乎不是很好，因为强行对某些head引入这种特性，有时候词与词之间没有这种关系，这样会带来噪声。</p><h5 id="HETEROGENEOUS-N-GRAM-ATTENTION"><a href="#HETEROGENEOUS-N-GRAM-ATTENTION" class="headerlink" title="HETEROGENEOUS N-GRAM ATTENTION"></a>HETEROGENEOUS N-GRAM ATTENTION</h5><p><img src="/images/15564587795887.jpg" width="90%" height="50%"></p><p>将所有的gram都同时attend。</p><p>也即：</p><script type="math/tex; mode=display">\mathcal{S}\left(\frac{\left(\boldsymbol{Q} \boldsymbol{W}_{q}\right)\left[\left(\boldsymbol{K} \boldsymbol{W}_{k, 1}\right)^{T} ; \operatorname{Conv}_{2}\left(\boldsymbol{K}, \boldsymbol{W}_{k, 2}\right)^{T} ; \ldots\right]}{\sqrt{d_{k}}}\right)\left[\left(\boldsymbol{V} \boldsymbol{W}_{v, 1}\right) ; \operatorname{Conv}_{2}\left(\boldsymbol{V}, \boldsymbol{W}_{v, 2}\right) ; \ldots\right]</script><p>或：</p><script type="math/tex; mode=display">\mathcal{S}\left(\left[\frac{\left(\boldsymbol{Q} \boldsymbol{W}_{q, 1}\right)\left(\boldsymbol{K} \boldsymbol{W}_{k, 1}\right)^{T}}{\sqrt{d}} ; \frac{\operatorname{Conv}_{2}\left(\boldsymbol{K} \boldsymbol{W}_{k, 2}, \boldsymbol{Q} \boldsymbol{W}_{q, 2}\right)}{\sqrt{d * n_{2}}} ; \ldots\right]\right)\left[\left(\boldsymbol{V} \boldsymbol{W}_{v, 1}\right) ; \operatorname{Conv}_{2}\left(\boldsymbol{V}, \boldsymbol{W}_{v, 2}\right) ; \ldots\right]</script><h4 id="INTERLEAVED-PHRASES-TO-PHRASE-HETEROGENEOUS-ATTENTION"><a href="#INTERLEAVED-PHRASES-TO-PHRASE-HETEROGENEOUS-ATTENTION" class="headerlink" title="INTERLEAVED PHRASES TO PHRASE HETEROGENEOUS ATTENTION"></a>INTERLEAVED PHRASES TO PHRASE HETEROGENEOUS ATTENTION</h4><p>上面介绍的都是source端的phrase到target的token，有时候需要反过来，因此可以交叉地交互。<br><img src="/images/15564588967513.jpg" width="90%" height="50%"></p><p>我们先对Q进行两种卷积，获得unigram和bigram。然后与KV的unigram与比bigram进行交叉。<br>$\boldsymbol{A}_{1, \mathrm{ConvKV}}=\mathcal{S}\left(\frac{\left(\boldsymbol{Q} \boldsymbol{W}_{q_{1}}\right)\left[\left(\boldsymbol{K} \boldsymbol{W}_{\boldsymbol{k}, 1}\right)^{T} ; \operatorname{Conv}_{2}\left(\boldsymbol{K}, \boldsymbol{W}_{k, 2}\right)^{T}\right]}{\sqrt{d_{k}}}\right)\left[\left(\boldsymbol{V} \boldsymbol{W}_{v, 1}\right) ; \operatorname{Conv}_{2}\left(\boldsymbol{V}, \boldsymbol{W}_{v, 2}\right)\right]$</p><p>$\boldsymbol{A}_{2, \text {ConvKV }}=\mathcal{S}\left(\frac{\operatorname{Conv}_{2}\left(\boldsymbol{Q}, \boldsymbol{W}_{q_{2}}\right)\left[\left(\boldsymbol{K} \boldsymbol{W}_{\boldsymbol{k}, 1}\right)^{T} ; \operatorname{Conv}_{2}\left(\boldsymbol{K}, \boldsymbol{W}_{\boldsymbol{k}, 2}\right)^{T}\right]}{\sqrt{d_{k}}}\right)\left[\left(\boldsymbol{V} \boldsymbol{W}_{v, 1}\right) ; \operatorname{Conv}_{2}\left(\boldsymbol{V}, \boldsymbol{W}_{v, 2}\right)\right]$</p><p>$\boldsymbol{A}_{1, \text {QueryK }}=\mathcal{S}\left(\left[\frac{\left(\boldsymbol{Q} \boldsymbol{W}_{q_{1}, 1}\right)\left(\boldsymbol{K} \boldsymbol{W}_{\boldsymbol{k}, 1}\right)^{T}}{\sqrt{d}} ; \frac{\operatorname{Conv}_{2}\left(\boldsymbol{K} \boldsymbol{W}_{\boldsymbol{k}, 2}, \boldsymbol{Q} \boldsymbol{W}_{q_{1}, 2}\right)}{\sqrt{d * n_{2}}}\right]\right)\left[\left(\boldsymbol{V} \boldsymbol{W}_{v, 1}\right) ; \operatorname{Conv}_{2}\left(\boldsymbol{V}, \boldsymbol{W}_{v, 2}\right)\right]$</p><p>$\boldsymbol{A}_{2, \text { QueryK }}=\mathcal{S}\left(\left[\frac{\operatorname{Conv}_{2}\left(\boldsymbol{Q}, \boldsymbol{W}_{q_{2}, 1}\right)\left(\boldsymbol{K} \boldsymbol{W}_{\boldsymbol{k}, 1}\right)^{T}}{\sqrt{d}} ; \frac{\operatorname{Conv}_{2}\left(\boldsymbol{K} \boldsymbol{W}_{\boldsymbol{k}, 2}, \operatorname{Conv}_{2}\left(\boldsymbol{Q}, \boldsymbol{W}_{q_{2}, 2}\right)\right)}{\sqrt{d * n_{2}}}\right]\right)\left[\left(\boldsymbol{V} \boldsymbol{W}_{v, 1}\right) ; \operatorname{Conv}_{2}\left(\boldsymbol{V}, \boldsymbol{W}_{v, 2}\right)\right]$</p><p>思考：<br>这样似乎参数量会暴增，其实应该对比的就不是transformer base了，应该是参数量大致相等的transformer，这也在review里面提到过。同时我觉得这个方法是否有些太复杂，不够简单明了。以及结果似乎不大令人信服，因为他的baseline没有复现出transformer base的结果（due to the limited GPU)。</p><hr><h2 id="2️⃣-Regularizing-and-Optimizing-LSTM-Language-Models"><a href="#2️⃣-Regularizing-and-Optimizing-LSTM-Language-Models" class="headerlink" title="2️⃣[Regularizing and Optimizing LSTM Language Models]"></a>2️⃣[Regularizing and Optimizing LSTM Language Models]</h2><p>提出一些优化提升LSTM-based语言模型的方法。此即大名鼎鼎的AWD-LSTM。</p><h3 id="Weight-dropped-LSTM"><a href="#Weight-dropped-LSTM" class="headerlink" title="Weight-dropped LSTM"></a>Weight-dropped LSTM</h3><p>LSTM公式回顾：</p><script type="math/tex; mode=display">\begin{aligned} i_{t} &=\sigma\left(W^{i} x_{t}+U^{i} h_{t-1}\right) \\ f_{t} &=\sigma\left(W^{f} x_{t}+U^{f} h_{t-1}\right) \\ o_{t} &=\sigma\left(W^{o} x_{t}+U^{o} h_{t-1}\right) \\ \tilde{c}_{t} &=\tanh \left(W^{c} x_{t}+U^{c} h_{t-1}\right) \\ c_{t} &=i_{t} \odot \tilde{c}_{t}+f_{t} \odot+\tilde{c}_{t-1} \\ h_{t} &=o_{t} \odot \tanh \left(c_{t}\right) \end{aligned}</script><p>对hidden-to-hidden的weight应用DropConnect。也即对其中的$\left[U^{i}, U^{f}, U^{o}, U^{c}\right]$进行dropconnect。注意到mask矩阵在同一个batch的每个时间步t都是一样的。</p><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>之前的工作表明，在语言模型中，使用普通的SGD，不带momentum，能超过其他的优化方法。普通SGD：<br>$w_{k+1}=w_{k}-\gamma_{k} \hat{\nabla} f\left(w_{k}\right)$</p><p>本文提出在averaged SGD(ASGD）的基础上进行改进。</p><p>ASGD和上式一致，只不过最后更新完是将最后几次更新的weight做了平均并返回。也即：</p><script type="math/tex; mode=display">\frac{1}{(K-T+1)} \sum_{i=T}^{K} w_{i}</script><p>其中K是total的循环次数；T是人工定义的阈值。但T的阈值需要人工调，因此该方法不是很好。最理想的就是在SGD拟合到一个稳定状态时再平均。</p><p>因此提出一种新的方法以解决上述问题，通过validation loss触发机制。</p><p><img src="/images/15564599688438.jpg" width="50%" height="50%"></p><h3 id="Extended-regularization-techniques"><a href="#Extended-regularization-techniques" class="headerlink" title="Extended regularization techniques"></a>Extended regularization techniques</h3><h4 id="Variable-length-backpropagation-sequences"><a href="#Variable-length-backpropagation-sequences" class="headerlink" title="Variable length backpropagation sequences"></a>Variable length backpropagation sequences</h4><p>若每次都固定窗口切分句子，则总会有一些词没法更新自己，如最后一个词，同时除了第一个词，其他的词都只能接收到部分bp。这实际上是一种data inefficient。</p><p>可以从切分句子的方法上进行改进。使用随机采样句子长度的方式去缓解这一问题。以较高的p选择seq长度，1-p选择seq/2。接着以此为高斯均值，以正态分布$\mathcal{N}(\operatorname{seq}, s)$采样句子长度。</p><h4 id="Variational-dropout"><a href="#Variational-dropout" class="headerlink" title="Variational dropout"></a>Variational dropout</h4><p>在LSTM中，除了hidden-to-hidden的，其他地方都采用variational dropout。</p><h4 id="Embedding-dropout"><a href="#Embedding-dropout" class="headerlink" title="Embedding dropout"></a>Embedding dropout</h4><p>字级别，也即将整个字的embedding去掉。同时由于是在embedding matrix上做的，在一个完整的forward pass与backward pass都用了，因此就相当于使用variational dropout用在one-hot embedding与embedding lookup之间。</p><h4 id="Weight-tying"><a href="#Weight-tying" class="headerlink" title="Weight tying"></a>Weight tying</h4><p> embedding与softmax的权重绑定。</p><h4 id="Independent-embedding-size-and-hidden-size"><a href="#Independent-embedding-size-and-hidden-size" class="headerlink" title="Independent embedding size and hidden size"></a>Independent embedding size and hidden size</h4><p>LSTM的第一层与最后一层与embedding size一致，其它层的就有自己的hidden size。</p><h4 id="Activation-Regularization-AR-and-Temporal-Activation-Regularization-TAR"><a href="#Activation-Regularization-AR-and-Temporal-Activation-Regularization-TAR" class="headerlink" title="Activation Regularization (AR) and Temporal Activation Regularization (TAR)"></a>Activation Regularization (AR) and Temporal Activation Regularization (TAR)</h4><p>AR：<br>L2正则化：$\alpha L_{2}\left(m \odot h_{t}\right)$<br>其中m是mask，h是hidden state</p><p>TAR：<br>$\beta L_{2}\left(h_{t}-h_{t+1}\right)$<br>减少两个h之间的差距。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> attention </tag>
            
            <tag> 每周论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词23</title>
      <link href="/2019/04/28/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D23/"/>
      <url>/2019/04/28/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D23/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣行香子-·-述怀"><a href="#1️⃣行香子-·-述怀" class="headerlink" title="1️⃣行香子 · 述怀"></a>1️⃣行香子 · 述怀</h3><p>[宋] 苏轼<br><strong>清夜无尘，月色如银</strong>。酒斟时、须满十分。浮名浮利，虚苦劳神。<strong>叹隙中驹，石中火，梦中身</strong>。<br>虽抱文章，开口谁亲。且陶陶、乐尽天真。几时归去，作个闲人。对一张琴，一壶酒，一溪云。</p><p>隙中驹：语出《庄子·知北游》：“人生天地之间，若白驹之过隙，忽然而已。“<br>石中火，梦中身：比喻生命短促，像击石迸出一闪即灭的火花，像在梦境中短暂的经历。石中火，语出北齐刘昼《新论·惜时》：“人之短生，犹如石火，炯然而过。”梦中身，语出《关尹子·四符》：“知此身如梦中身。”</p><p><a href="http://lib.xcz.im/work/57b2c8fa7db2a20054377ecd" target="_blank" rel="noopener">http://lib.xcz.im/work/57b2c8fa7db2a20054377ecd</a></p><hr><h3 id="2️⃣旷怡亭口占"><a href="#2️⃣旷怡亭口占" class="headerlink" title="2️⃣旷怡亭口占"></a>2️⃣旷怡亭口占</h3><p>[现代] 马一浮<br>流转知何世，江山尚此亭。<br>登临皆旷士，丧乱有遗经。<br><strong>已识乾坤大，犹怜草木青</strong>。<br>长空送鸟印，留幻与人灵。</p><p><a href="http://lib.xcz.im/work/5992e274570c35006b8394b3" target="_blank" rel="noopener">http://lib.xcz.im/work/5992e274570c35006b8394b3</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识19</title>
      <link href="/2019/04/22/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8619/"/>
      <url>/2019/04/22/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8619/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>expand&amp;repeat</p><p>expand只能对维数为1的维度进行扩展，且扩展过程中不分配新内存；repeat能对任意维度进行扩展，但需要分配新内存。</p><p>如果满足expand的需要，应尽量使用expand。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词22</title>
      <link href="/2019/04/21/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D22/"/>
      <url>/2019/04/21/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D22/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣九日齐安登高"><a href="#1️⃣九日齐安登高" class="headerlink" title="1️⃣九日齐安登高"></a>1️⃣九日齐安登高</h3><p>[唐] 杜牧<br>江涵秋影雁初飞，与客携壶上翠微。<br>尘世难逢开口笑，菊花须插满头归。<br>但将酩酊酬佳节，不用登临恨落晖。<br><strong>古往今来只如此，牛山何必独沾衣？</strong></p><p><a href="http://lib.xcz.im/work/57ba4972efa631005a799815" target="_blank" rel="noopener">http://lib.xcz.im/work/57ba4972efa631005a799815</a></p><hr>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文16</title>
      <link href="/2019/04/21/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8716/"/>
      <url>/2019/04/21/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8716/</url>
      
        <content type="html"><![CDATA[<p>本周论文：</p><ol><li>MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES</li><li>Fine-Grained Attention Mechanism for Neural Machine Translation</li><li>Competence-based Curriculum Learning for Neural Machine Translation</li></ol><h2 id="1️⃣-MEASURING-THE-INTRINSIC-DIMENSION-OF-OBJECTIVE-LANDSCAPES"><a href="#1️⃣-MEASURING-THE-INTRINSIC-DIMENSION-OF-OBJECTIVE-LANDSCAPES" class="headerlink" title="1️⃣[MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES]"></a>1️⃣[MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES]</h2><p>本文探究深度学习中的过量参数问题，通过定义intrinsic dimension，去衡量特定模型在特定数据集上所需维度。</p><p>在给定模型结构和loss function时，整个优化空间也随之确定，训练过程类似于在一个空间内移动使得loss尽量小。</p><p>给定一个有D个参数的模型，通过限制训练随机slice的参数，也即选取一个随机有d个参数的子空间训练，不断增加d，使得预定义的solution第一次出现，则称d为intrinsic dimension，可以理解为该d是解决某特定问题所需的参数量。</p><p>如何做？<br>$\theta^{(D)}=\theta_{0}^{(D)}+P \theta^{(d)}$<br>其中P是随机生成的$D\times d$的投影矩阵，而$\theta (d)$ 是子空间的参数；$P$是固定的而不是可训练的，且$P$可以是归一化为单位长度且正交的。</p><p><img src="/images/15559193313833.jpg" width="40%" height="50%"></p><p>（这里的投影现在还是不能理解？等之后看这方面的论文再说吧）</p><p>因为一些随机性以及实际效果问题，比如正则化效果在子空间无法达到在全空间的效果，因此在这里定义$d_{\mathrm{int} 90}$，也即达到baseline的90%所需要的参数量。</p><p>一些结果：<br><img src="/images/15559195161338.jpg" width="70%" height="50%"></p><p>MNIST的模型可以看到所需参数非常少；横向对比，CNN会比全连接所需的少多了，这也符合我们的直觉，也即CNN比全连接更高效。</p><p><img src="/images/15559195378407.jpg" width="70%" height="50%"></p><p>在全连接中，对于模型不同的宽度以及layer数，发现他们的dint90相差不大，说明对于特定任务，同一个模型家族所需要的参数量是类似的。</p><hr><h2 id="2️⃣-Fine-Grained-Attention-Mechanism-for-Neural-Machine-Translation"><a href="#2️⃣-Fine-Grained-Attention-Mechanism-for-Neural-Machine-Translation" class="headerlink" title="2️⃣[Fine-Grained Attention Mechanism for Neural Machine Translation]"></a>2️⃣[Fine-Grained Attention Mechanism for Neural Machine Translation]</h2><p>本文提出对attention进行细化，将原来的每个词分配一个score扩展为每个词分配d维个score，并在机器翻译上有一定提升。</p><p><img src="/images/15559197269175.jpg" width="80%" height="50%"></p><p>简单地说，原来的attention机制是：<br>$e_{t^{\prime}, t}=f_{\mathrm{Att}}\left(\boldsymbol{z}_{t^{\prime}-1}, \boldsymbol{h}_{t}\right)$</p><p>其中$t^{\prime}$是decoder端的时间步，$t$则是encoder端的第$t$个词。</p><p>而本文的细粒度attention机制：<br>$e_{t^{\prime}, t}^{d}=f_{\mathrm{Att} \mathrm{Y} 2 \mathrm{D}}^{d}\left(\boldsymbol{z}_{t^{\prime}-1}, \boldsymbol{h}_{t}, \boldsymbol{y}_{t^{\prime}-1}\right)$</p><p>也即在原来的基础上做了d次操作，也即实际上在获得每一维的分数时，是能看到其他维的信息的。（如果是我自己做，我可能会将他们隔绝开来。）</p><p>思考：<br>将RNN作为baseline，为什么不使用transformer？当时transformer已经出了，可能是transformer上没效果？因为transformer自带多head，可能表示能力就已经足够了。</p><hr><h2 id="3️⃣-Competence-based-Curriculum-Learning-for-Neural-Machine-Translation"><a href="#3️⃣-Competence-based-Curriculum-Learning-for-Neural-Machine-Translation" class="headerlink" title="3️⃣[Competence-based Curriculum Learning for Neural Machine Translation]"></a>3️⃣[Competence-based Curriculum Learning for Neural Machine Translation]</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>提出一种新的<strong>训练</strong>翻译模型的算法，基本思想是让模型从简单的样例开始学起，随着训练过程的进行逐渐增加难度较大的样例，该方法能够增强模型训练的稳定性，且在效果上也有提升，同时还能减少收敛所需的训练时间。</p><p><img src="/images/15559206435828.jpg" width="50%" height="50%"></p><p>论文的Motivation：如果训练数据以特定的顺序输入，也即从简单的数据开始学，等到模型有一定的能力后再去学难的数据，这样也更符合人类的直觉；同时，从机器学习的角度去看，这种方法可以避免过早陷入不好的局部最优解。</p><p>论文还提到了对于翻译而言，模型很难训练，需要复杂的调参，费时费力。特别是对于Transformer而言，需要精细的learning rate schedule。</p><p>本文提出的方法，只有一个参数，因此不需要精细的调参，同时因为只改变输入的pipeline，因此很方便地使用到已有的模型。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>引入两个概念：<br><strong>Difficulty</strong>：代表一个训练样例的难度，可能和模型当前的状态相关。比如句子长度就是衡量样例难度的一个指标。</p><p><strong>Competence</strong>：范围0-1的数值，代表模型训练的进度，定义为模型状态的一个函数。更进一步，定义$c(t)$为模型在时间步t所允许使用的训练样例的比例。也即训练样例根据difficulty排列，在时间步$t$只允许top $c(t)$的数据使用。</p><p>根据上述两个定义，引入算法：<br><img src="/images/15559212081520.jpg" width="50%" height="50%"></p><p><img src="/images/15559212315953.jpg" width="100%" height="50%"></p><p><img src="/images/15559212598339.jpg" width="50%" height="50%"></p><p>那么有两个问题，如何衡量difficulty以及competence？</p><h4 id="Difficulty-Metrics"><a href="#Difficulty-Metrics" class="headerlink" title="Difficulty Metrics"></a>Difficulty Metrics</h4><p>①句子长度<br>长句子更难翻译，因为长句子往往包含了短句子，同时在生成目标语言时，容易出现错误传播。</p><script type="math/tex; mode=display">d_{\text { length }}\left(s_{i}\right) \triangleq N_{i}</script><p>②Word Rarity<br>若一个句子存在罕见词，更难翻译该句子，因为模型需要多次看见该词才能学到鲁棒的表示；同时罕见词的梯度容易有较大的方差。</p><p>因此我们定义相对词频：</p><script type="math/tex; mode=display">\hat{p}\left(w_{j}\right) \triangleq \frac{1}{N_{\text {total }}} \sum_{i=1}^{M} \sum_{k=1}^{N_{i}} \mathbb{1}_{w_{k}^{i}=w_{j}}</script><p>其中，$j=1, \ldots, \{\text {unique words in corpus}\}$，$\mathbb{1}$ 为指示函数。</p><p>因此最终度量方法为：<br>$d_{\text {rarity}}\left(s_{i}\right) \triangleq-\sum_{k=1}^{N_{i}} \log \hat{p}\left(w_{k}^{i}\right)$</p><p>这样即考虑到了长度也考虑到了词频，同时该方法有点类似language model，可以理解为language model的近似。</p><h4 id="Competence-Functions"><a href="#Competence-Functions" class="headerlink" title="Competence Functions"></a>Competence Functions</h4><p>我们定义competence function只与时间步$t$有关，因此只需要考虑具体的形式。<br>①linear：</p><script type="math/tex; mode=display">c(t) \triangleq \min \left(1, t r+c_{0}\right)</script><p>$c_{0}$是初始值。我们也可以定义T为时间步阈值，当超过该阈值，我们认为模型已经完全有能力了，则上式还可以写成：</p><script type="math/tex; mode=display">c_{\text { linear }}(t) \triangleq \min \left(1, t \frac{1-c_{0}}{T}+c_{0}\right)</script><p>②Root：<br>线性的一个不好的地方，当样例增加时，每个样例被sample的几率减小，因此新加进去的样例被sample到的几率也减小，因此应每次减少新加入的样例，使得模型有足够的时间去学习知识。<br>也即：</p><script type="math/tex; mode=display">\frac{d c(t)}{d t}=\frac{P}{c(t)}</script><p>积分后可得：</p><script type="math/tex; mode=display">c_{\mathrm{sqrt}}(t) \triangleq \min \left(1, \sqrt{t \frac{1-c_{0}^{2}}{T}+c_{0}^{2}}\right)</script><p>当然还可以将开n次方根</p><script type="math/tex; mode=display">c_{\mathrm{root}-p}(t) \triangleq \min \left(1, \sqrt[p]{t \frac{1-c_{0}^{p}}{T}+c_{0}^{p}}\right)</script><p>使得曲线更为陡峭，也即给每个样例的时间更多。</p><p>曲线对比：<br><img src="/images/15559218944874.jpg" width="50%" height="50%"></p><p>实验证明是p=2时最好。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/images/15559219317633.jpg" width="70%" height="50%"></p><p>实验有相当不错的结果，在RNN以及在Transformer上都有提升，并且是在不用learning rate schedule的情况下，并且时间更短。</p><p>几个实验现象：<br>①RNN的提升较少，而Transformer很多，说明RNN比Transformer更鲁棒。RNN比Transformer训练更为稳定。<br>②对于Transformer而言，若同样使用learning rate schedule，仍然有帮助，说明该方法是较为通用的。<br>③不使用lr schedule而只使用本文方法，也能达到不使用本文方法而使用lr schedule的结果，但需要更多的step。</p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>为什么该方法能work？<br>符合直觉，模型从简单到难，更好训。同时从机器学习角度，如果完全正常的sample，则容易陷入局部最小或者saddle point，因此需要更长时间或者不好的泛化性能。</p><p>同时论文还提到了，为什么Transformer在增加batch能够有更好的收敛，这是因为一开始训练的noisy gradient太大，若增加batch能够信噪比，而本文方法在某种程度上也解决了该问题。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> Curriculum Learning </tag>
            
            <tag> NMT </tag>
            
            <tag> intrinsic dimension </tag>
            
            <tag> attention mechanism </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Transformer相关近期盘点</title>
      <link href="/2019/04/12/%E8%AE%BA%E6%96%87/Transformer%E7%9B%B8%E5%85%B3%E8%BF%91%E6%9C%9F%E7%9B%98%E7%82%B9/"/>
      <url>/2019/04/12/%E8%AE%BA%E6%96%87/Transformer%E7%9B%B8%E5%85%B3%E8%BF%91%E6%9C%9F%E7%9B%98%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p>近年来自然语言处理有相当大的进展，囿于个人浅薄的能力，因此仅谈谈自己较为了解的与Transformer相关一路下来的一些工作。这篇的主要目的是完成邱博给我的任务，也顺便梳理一下思绪。</p><hr><p>自2017年的问世，Transformer就吸引了大批学者的注意，2018年Bert的出现，更是将Transformer推上了NLP舞台的中央。Transformer以其高效率（高并行性）以及极强的模型能力，俨然有替代传统RNN/CNN的态势。因此本次就讨论讨论Transformer及其系列，同时最后加上我个人关于RNN/CNN/Transformer的一点思考。</p><p>要点：</p><ol><li>Transformer及其变体</li><li>Transformer在其他任务</li><li>预训练模型</li><li>Transformer/CNN/RNN对比及思考</li></ol><h2 id="Transformer及其变体"><a href="#Transformer及其变体" class="headerlink" title="Transformer及其变体"></a>Transformer及其变体</h2><h3 id="Transformer简单回顾"><a href="#Transformer简单回顾" class="headerlink" title="Transformer简单回顾"></a>Transformer简单回顾</h3><p>Transformer[1]采用完全的attention机制用以序列建模，序列中的每个节点都能够直接与其他节点交互，而这是通过attention机制来实现的。</p><h4 id="Transformer模型架构"><a href="#Transformer模型架构" class="headerlink" title="Transformer模型架构"></a>Transformer模型架构</h4><p>Transformer架构：<br><img src="/images/15550349717464.jpg" width="50%" height="50%"></p><p>由于Transformer最早由于翻译模型中，因此架构是由一个encoder和一个decoder组成，而encoder和decoder都是由多个基本的block堆叠而成。一个block由两部分组成，也即multi-head attention层和Position-wise Feed-Forward Networks层。</p><h5 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h5><p>对于序列中一个特定节点$x_i$，$x_i$作为query，其他节点（包括自己）作为key和value，通过向量点积计算出attention分数，进行归一化后（softmax）将value加权平均获得该节点$x_i$新的表示。<br>同时，对于每个节点，为了增强表示能力，可以将其映射到多个不同隐空间中，分别完成上述基本操作。</p><p>如下图所示：<br><img src="/images/15550350386364.jpg" width="70%" height="50%"></p><p>左图为基本操作也即scale-dot product，右图为多个scale-dot product在不同隐空间同时进行，并且将多个head的结果拼接起来作为最终结果。</p><h5 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h5><p>为了增强模型表示能力，在Multi-head attention之后，每个节点都过两层MLP以获得新的向量表示。<br>也即:</p><script type="math/tex; mode=display">\mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script><p>相比传统的RNN/CNN而言，其最大的优势是全局的感受野（Global Receptive Field）以及高度并行性（parallelization）。</p><h3 id="变体"><a href="#变体" class="headerlink" title="变体"></a>变体</h3><h4 id="Universal-Transformers"><a href="#Universal-Transformers" class="headerlink" title="Universal Transformers"></a>Universal Transformers</h4><p>提出一种新型通用的Transformer，在Transformer的基础上引入RNN的归纳偏置(inductive bias)，也即迭代学习(learning iterative)的特征。Universal Transformer[2]的主要特点有：</p><ol><li>在Transformer中每层的权重是独立的，而在Universal Transformer中，每层的权重是共享的，也即multi-head Attention与Feed-Forward在每层的权重是一致的。</li><li>在Transformer中引入自适应计算时间(Adaptive Computation Time, ACT[3])，也即对于不同的词允许迭代不同次数。这是基于有些词相比其他词词意更丰富，更难被模型学会，因此需要更多的迭代次数。与固定层数的Transformer相比有更好的通用性。</li></ol><p>因此其总体结构为：<br><img src="/images/15556585300051.jpg" width="70%" height="50%"></p><p>在这里有两个细节：</p><ol><li>加了Timestep embedding去指示当前迭代的次数</li><li>将Feedforward Function用更为通用的Transition Function，可以是普通的全连接，也可以是参数更少的Depth-wise Convolution。</li></ol><h4 id="Star-Transformer"><a href="#Star-Transformer" class="headerlink" title="Star Transformer"></a>Star Transformer</h4><p>Star Transformer[20]是一种轻量级的Transformer，通过将全连接的结构替换为星型拓扑结构，显著减小Transformer的复杂度，从$O(n^2)$减为$O(n)$。</p><p><img src="/images/15556834830687.jpg" width="50%" height="50%"></p><p>其主要思想是’Gather-Distribute’，也即每个节点不直接与其他节点交互，而是与全局节点进行交互。<br><img src="/images/15556836008776.jpg" width="50%" height="50%"></p><p>实验表明，Star Transformer不仅在多个数据集表现更优，且速度更快。</p><p><img src="/images/15557700601922.jpg" width="90%" height="50%"></p><p><img src="/images/15557701172639.jpg" width="90%" height="50%"></p><h4 id="其他小改进"><a href="#其他小改进" class="headerlink" title="其他小改进"></a>其他小改进</h4><p>接下来介绍基于Transformer的几个小改进工作。</p><p>在Convolutional Self-Attention Network[5]中，通过在self-attention层引入CNN的归纳偏置，在翻译任务上有一定的提升。具体做法：<br><img src="/images/15556604248879.jpg" width="80%" height="50%"><br>普通self-attention层，每个节点都能够直接与其他节点交互，而在1D-Convolutional的self-attention层中，每个节点只能与以该节点为中心的窗口内的节点交互。而在2D-Convolution中，对head这一维进行扩展，也即对于任意一个节点，不仅能和周围的节点交互，还可以与其他head的节点交互。<br>实验结果：<br><img src="/images/15556607769307.jpg" width="80%" height="50%"></p><p>在Multi-Head Attention with Disagreement Regularization[6]中，显式对multi-head attention添加正则化，使得不同head尽量区分开来，以使得不同head捕获到不同的特征。论文提出了三种不同位置的正则化方法：<br>①对Value：</p><script type="math/tex; mode=display">D_{\text {subpace}}=-\frac{1}{H^{2}} \sum_{i=1}^{H} \sum_{j=1}^{H} \frac{V^{i} \cdot V^{j}}{\left\|V^{i}\right\|\left\|V^{j}\right\|}</script><p>也即对不同head之间的value，计算他们之间的cos值，作为优化目标之一。</p><p>②对Attention权重：</p><script type="math/tex; mode=display">D_{\text {position}}=-\frac{1}{H^{2}} \sum_{i=1}^{H} \sum_{j=1}^{H}\left|A^{i} \odot A^{j}\right|</script><p>也即将每个head所计算得到的attention矩阵，计算他们之间的element-wise乘法，作为优化目标之一。</p><p>③对输出：</p><script type="math/tex; mode=display">D_{\text {output}}=-\frac{1}{H^{2}} \sum_{i=1}^{H} \sum_{j=1}^{H} \frac{O^{i} \cdot O^{j}}{\left\|O^{i}\right\|\left\|O^{j}\right\|}</script><p>也即对每个head的输出通过cos值进行正则化。</p><p>Modeling Localness for Self-Attention Networks[7]则是通过加强对局部信息的关注，在翻译任务上提升表现。其主要的动机是：①在Transformer中每个词都直接与所有词交互，对所有词进行线性加权导致对邻近词的关注不够（因为权重的分散）；②从直接上看，当词$i$与词$j$有对齐关系时，我们希望与词$j$周围的词也对齐，使得模型能够捕获整个语义单元的信息。其具体做法是在softmax函数内增加一个高斯偏置（Gaussian bias）去修正attention权重分布：</p><script type="math/tex; mode=display">\operatorname{ATT}(Q, K)=\operatorname{softmax}(\text {energy}+G)</script><p>$G_{ij}$是衡量词j与词i所预测的中心词之间的联系紧密程度，计算公式为：</p><script type="math/tex; mode=display">G_{i, j}=-\frac{\left(j-P_{i}\right)^{2}}{2 \sigma_{i}^{2}}</script><p>其中$\sigma_{i}=\frac{D_{i}}{2}$，$D_{i}$是窗口大小。而$P_{i}$是通过计算得出的，$P_{i}$与对应的query有关，因此可以通过$p_{i}=U_{p}^{T} \tanh \left(W_{p} Q_{i}\right)$计算得到；而窗口大小$D_{i}$可以有多种选择，①固定窗口大小；②每层特定的大小，也即将该层的key平均起来，通过$z=U_{d}^{T} \tanh \left(W_{d} \overline{\mathbf{K}}\right)$计算；③每个query都有自己的窗口大小：$z_{i}=U_{d}^{T} \tanh \left(W_{p} Q_{i}\right)$。</p><p>Self-attention with relative position representations[8]则是将Transformer中的绝对位置embedding改为相对位置embedding以提升翻译效果。</p><h2 id="Transformer在其他任务"><a href="#Transformer在其他任务" class="headerlink" title="Transformer在其他任务"></a>Transformer在其他任务</h2><h3 id="Transformer-XL"><a href="#Transformer-XL" class="headerlink" title="Transformer XL"></a>Transformer XL</h3><p>本文探索将Transformer用于语言模型(language model)，并在Transformer引入RNN的归纳偏置，也即RNN的历史信息，使得Transformer能够处理长句子。</p><p>由于Transformer的复杂度是$O(n^2)$，虽然在GPU上能够并行操作，但占用显存较大，因此在实现时，通常是将句子切分为一个一个segment，segment之间没有联系：</p><p><img src="/images/15556764280028.jpg" width="30%" height="50%"></p><p>而在测试阶段，则每生成一个词时滑动一个窗口：</p><p><img src="/images/15556765079362.jpg" width="60%" height="50%"></p><p>这样的方法显然效率很低。</p><p>而在Transformer-XL[9]中，每个segment阶段都接受前一个(甚至前L个)的历史信息：<br>因此过程如下：<br><img src="/images/15556768100801.jpg" width="60%" height="50%"></p><p>而在测试阶段，由于有历史信息，则不需要滑动窗口，因此效率更高。</p><p>具体而言：<br>$\begin{aligned} \widetilde{\mathbf{h}}_{\tau+1}^{n-1} &amp;=\left[\mathrm{SG}\left(\mathbf{h}_{\tau}^{n-1}\right) \circ \mathbf{h}_{\tau+1}^{n-1}\right] \\ \mathbf{q}_{\tau+1}^{n}, \mathbf{k}_{\tau+1}^{n}, \mathbf{v}_{\tau+1}^{n} &amp;=\mathbf{h}_{\tau+1}^{n-1} \mathbf{W}_{q}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{k}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{v}^{\top} \\ \mathbf{h}_{\tau+1}^{n} &amp;=\text { Transformer-Layer }\left(\mathbf{q}_{\tau+1}^{n}, \mathbf{k}_{\tau+1}^{n}, \mathbf{v}_{\tau+1}^{n}\right) \end{aligned}$<br>SG代表stop gradient，而历史信息与当前阶段的隐状态拼接在一起。</p><p>同时本文另一大两点是引入相对位置的encoding。如果使用绝对位置encoding，那么则会出现下述情况：<br>$\mathbf{h}_{\tau+1}=f\left(\mathbf{h}_{\tau}, \mathbf{E}_{\mathbf{s}_{\tau+1}}+\mathbf{U}_{1 : L}\right) \quad \text { and } \quad \mathbf{h}_{\tau}=f\left(\mathbf{h}_{\tau-1}, \mathbf{E}_{\mathbf{s}_{\tau}}+\mathbf{U}_{1 : L}\right)$<br>也即每个segment都会有相同的位置信息。因此在这里引入$\mathbf{R} \in \mathbb{R}^{L_{\max } \times d}$，第$i$行代表相对距离$i$的encoding。</p><p>具体而言：<br>在标准Transformer中，query $q_i$与key $k_j$所获得的attention分数可以拆解为：<br>$\mathbf{A}_{i, j}^{\mathrm{abs}}=q_{i}^{\top} k_{j}=\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(b)}+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(d)}$</p><p>对其进行改进，转化为相对位置encoding，有：<br>$\mathbf{A}_{i, j}^{\mathrm{rel}}=\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, E} \mathbf{R}_{i-j}}_{(b)}+\underbrace{u^{\top} \mathbf{W}_{k, R} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{v^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}}_{(d)}$</p><p>首先是将所有出现绝对位置的地方都改为相对位置，第二是将引入一个可学习的$u \in \mathbb{R}^{d}$和$v \in \mathbb{R}^{d}$去替代$\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top}$和$\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top}$。第三，是将同样的$\mathbf{W}_{k}$细化成两个不同的$\mathbf{W}_{k, E}$与$\mathbf{W}_{k, R}$。</p><p>Transformer-XL在不同数据集上有相当好的效果：<br><img src="/images/15556790567795.jpg" width="80%" height="50%"></p><p><img src="/images/15556790839030.jpg" width="80%" height="50%"></p><h3 id="Character-Level-Language-Modeling-with-Deeper-Self-Attention"><a href="#Character-Level-Language-Modeling-with-Deeper-Self-Attention" class="headerlink" title="Character-Level Language Modeling with Deeper Self-Attention"></a>Character-Level Language Modeling with Deeper Self-Attention</h3><p>同样是在语言模型上使用Transformer，但是character级别的语言模型。其主要思路是添加多个loss来提升其表现以及加快拟合速度。</p><p>对于传统的RNN character-level语言模型，一般做法是“truncated backpropagation through time” (TBTT)：也即每个batch预测最后一个字符，然后将该batch的隐状态传入下一个batch。</p><p>而在Transformer中也可以采用该方法。但在该基础上，引入三种loss。<br>①Multiple Positions<br>在一个batch内，每个时间步t都预测下一个字符，而不是像传统方法，只预测batch最后一个字符：</p><p><img src="/images/15556794194598.jpg" width="60%" height="50%"></p><p>②Intermediate Layer Losses<br>不仅仅最后一层要进行预测，中间层也需要预测。</p><p><img src="/images/15556795039170.jpg" width="60%" height="50%"><br>越底层的loss所分配的权重越小。</p><p>③Multiple Targets<br>每个位置不仅仅要预测下一个字符，还需要预测后几个的字符：</p><p><img src="/images/15556796677182.jpg" width="60%" height="50%"></p><p>实验表明，使用多个loss能够加速拟合，且能够获得更好的结果。</p><h2 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h2><p>自ELMo开始，预训练模型就开始受到广泛的关注，而Bert随后的问世则更是将预训练模型推上了新的阶段。因此在这里简要介绍预训练模型的历史。</p><h3 id="Non-Transformer-based"><a href="#Non-Transformer-based" class="headerlink" title="Non-Transformer-based"></a>Non-Transformer-based</h3><h4 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h4><p>词向量是最早的预训练模型，Bengio, et al.[10] 最早提出神经网络语言模型，词向量作为训练语言模型的副产品，可以用于下游任务。而随后则出现了word2vec[11],GloVe[12]，是目前最为广泛使用的词向量。</p><h4 id="CoVe-ELMo"><a href="#CoVe-ELMo" class="headerlink" title="CoVe/ELMo"></a>CoVe/ELMo</h4><p>word2vec与GloVe作为静态词向量，一大问题就是难以解决多义词，而多义词的表示可以通过上下文来推测。CoVe[13]将词向量从静态扩展为动态。通过上下文来获得特定词的动态表示，具体是通过翻译模型来达到该目的的。<br>而ELMo[14]继承了动态词向量的思想，不过是通过双向语言模型来达到这一目的的。通过双向LSTM的语言模型，将前向与后向隐状态拼接起来作为该词的表示。<br>仅仅是将传统静态词向量替换成ELMo，就能有很大的提升。自此开始，预训练模型开始受到广泛的关注。</p><h4 id="ULMFit"><a href="#ULMFit" class="headerlink" title="ULMFit"></a>ULMFit</h4><p>在上述预训练模型中，研究者的思路主要集中在预训练词向量用于下游任务。ULMFit[16]则尝试直接对分类模型进行预训练，接着再通过微调(fine-tuning)以提高分类的效果。</p><p><img src="/images/15556816887199.jpg" width="80%" height="50%"></p><p>ULMFit的成功说明直接对模型进行预训练而不是只预训练词向量用于下游任务是可行的。</p><h3 id="Transformer-based"><a href="#Transformer-based" class="headerlink" title="Transformer-based"></a>Transformer-based</h3><p>GPT[15]尝试通过探索构建一种通用模型并在其上训练语言模型，可以在多种任务上有更好的表现。其主要亮点在于①构建一种通用模型，能够处理不同任务，第二使用Transformer而不是LSTM作为其基本模型。<br>其基本模型：</p><p><img src="/images/15556818289287.jpg" width="85%" height="50%"></p><p>具体而言，主要是无监督的语言模型预训练加上有监督的微调。</p><p>而Bert[17]在GPT的基础上，引入masked语言模型，通过随机mask掉部分词，强迫模型通过上下文去预测mask掉的词，加强了模型的能力。</p><p><img src="/images/15556821324062.jpg" width="85%" height="50%"></p><p>同时引入有监督学习，也即预测下一句（Next Sentence Prediction），随机在句对中取两个句子，使得有50%可能句子对有上下文关系，50%句对没有关系，使模型去预测句对之间的关系。具体而言则是通过在句子开头加[CLS]符号，在最高层将该符号的表示通过全连接层。</p><p>Bert在11项数据集上刷新最高记录。</p><p>在此之后，MT-DNN[19]、GPT2.0[18]相继问世，通过添加更多的任务或者更多的数据使得模型表现更好。相信在接下来一段时间内，相关主题的论文也会有很多。</p><h2 id="Transformer-CNN-RNN对比及思考"><a href="#Transformer-CNN-RNN对比及思考" class="headerlink" title="Transformer/CNN/RNN对比及思考"></a>Transformer/CNN/RNN对比及思考</h2><p>上述的介绍，大概对Transformer一支有一个简单的梳理。Transformer作为与RNN/CNN并立的模型，确实值得重视。</p><p>为什么Transformer这么好，是否能够替代RNN/CNN？这也是值得所有人思考的。</p><p>正如前面介绍的那样，Transformer的一大优势是全局感受野，也即RNN/CNN每次只能‘看’到部分上下文，而Transformer则没有这个限制，每个节点都能够直接与其他节点进行交互。也可以这么说，RNN/CNN具有更强的先验(prior)。<br>但这种优势有时也会成为劣势：实践证明，Transformer在小数据集上的效果是不如RNN/CNN的。或许可以这么理解这种现象：Transformer由于不引入强的先验，因此需要大量的数据去从头学习数据所存在的某种pattern（如局部性），而引入强的先验的RNN/CNN则对小数据集更加友好一些。但当有大量训练数据时（如翻译、语言模型），Transformer则会有更高的上限，Bert/GPT也印证了这点。而这种Transformer的劣势或许也是上述几个工作（如universal transformer）的其中一个出发点，也即在Transformer内引入RNN/CNN的归纳偏置，加强对Transformer的先验知识的约束。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems. 2017.<br>[2]Dehghani, Mostafa, et al. “Universal transformers.” arXiv preprint arXiv:1807.03819 (2018).<br>[3]Graves, Alex. “Adaptive computation time for recurrent neural networks.” arXiv preprint arXiv:1603.08983 (2016).<br>[4]Ahmed, Karim, Nitish Shirish Keskar, and Richard Socher. “Weighted transformer network for machine translation.” arXiv preprint arXiv:1711.02132 (2017).<br>[5]Yang, Baosong, et al. “Convolutional Self-Attention Networks.” arXiv preprint arXiv:1904.03107 (2019).<br>[6]Li, Jian, et al. “Multi-head attention with disagreement regularization.” arXiv preprint arXiv:1810.10183 (2018).<br>[7]Yang, Baosong, et al. “Modeling localness for self-attention networks.” arXiv preprint arXiv:1810.10182 (2018).<br>[8]Shaw, Peter, Jakob Uszkoreit, and Ashish Vaswani. “Self-attention with relative position representations.” arXiv preprint arXiv:1803.02155 (2018).<br>[9]Dai, Zihang, et al. “Transformer-xl: Attentive language models beyond a fixed-length context.” arXiv preprint arXiv:1901.02860 (2019).<br>[10]Bengio, Yoshua, et al. “A neural probabilistic language model.” Journal of machine learning research 3.Feb (2003): 1137-1155.<br>[11]Mikolov, Tomas, et al. “Efficient estimation of word representations in vector space.” arXiv preprint arXiv:1301.3781 (2013).<br>[12]Pennington, Jeffrey, Richard Socher, and Christopher Manning. “Glove: Global vectors for word representation.” Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014.<br>[13]McCann, Bryan, et al. “Learned in translation: Contextualized word vectors.” Advances in Neural Information Processing Systems. 2017.<br>[14]Peters, Matthew E., et al. “Deep contextualized word representations.” arXiv preprint arXiv:1802.05365 (2018).<br>[15]Radford, Alec, et al. “Improving language understanding by generative pre-training.” URL <a href="https://s3-us-west-2" target="_blank" rel="noopener">https://s3-us-west-2</a>. amazonaws. com/openai-assets/research-covers/languageunsupervised/language understanding paper. pdf (2018).<br>[16]Universal Language Model Fine-tuning for Text Classification<br>[17]Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers for language understanding.” arXiv preprint arXiv:1810.04805 (2018).<br>[18]Radford, Alec, et al. “Language models are unsupervised multitask learners.” OpenAI Blog 1 (2019): 8.<br>[19]Multi-Task Deep Neural Networks for Natural Language Understanding<br>[20]Guo, Qipeng, et al. “Star-Transformer.” arXiv preprint arXiv:1902.09113 (2019).</p>]]></content>
      
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>无题</title>
      <link href="/2019/04/11/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BA%BA%E7%B1%BB%E6%B0%B8%E6%81%92%E7%9A%84%E6%84%9A%E8%A0%A2/"/>
      <url>/2019/04/11/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BA%BA%E7%B1%BB%E6%B0%B8%E6%81%92%E7%9A%84%E6%84%9A%E8%A0%A2/</url>
      
        <content type="html"><![CDATA[<p>人类永恒的愚蠢，是把莫名其妙的担忧，等同于智力超群。  ——美国加尔布雷斯</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文15</title>
      <link href="/2019/03/31/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8715/"/>
      <url>/2019/03/31/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8715/</url>
      
        <content type="html"><![CDATA[<p>本周论文:</p><ol><li>Selective Kernel Networks</li><li>Attentional pooling for action recognition</li></ol><h2 id="1️⃣-Selective-Kernel-Networks"><a href="#1️⃣-Selective-Kernel-Networks" class="headerlink" title="1️⃣[Selective Kernel Networks]"></a>1️⃣[Selective Kernel Networks]</h2><p>通过对不同kernel size的feature map之间进行信息筛选获得更为鲁棒的表示，能够对不同的感受野进行整合，实现动态调整感受野。其思路还挺有意思的。</p><p>Introduction将该模型与视觉神经的理论结合在一起，也即，对于人类而言，在看不同尺寸不同远近的物体时，视觉皮层神经元<strong>感受野大小</strong>是会根据刺激来进行调节的，但一般而言在CNN中卷积核的大小是固定的。该模型正是从这一现象中获得灵感。</p><p>整个模型一共分为三个步骤：split，fuse，select</p><p>split生成多个不同kernel size的feature map，也即对应不同的感受野大小；fuse将不同feature map结合起来，获得一个全局的综合的向量表示；select根据不同的weight选择不同感受野的feature map。</p><p><img src="/images/15540416698404.jpg" width="80%" height="50%"></p><p>以上图为例。</p><h3 id="SK-Net"><a href="#SK-Net" class="headerlink" title="SK-Net"></a>SK-Net</h3><h4 id="第一步split"><a href="#第一步split" class="headerlink" title="第一步split"></a>第一步split</h4><p>给定输入$\mathbf{X} \in \mathbb{R}^{H^{\prime} \times W^{\prime} \times C^{\prime}}$，通过不同的kernel size的CNN的卷积获得不同的feature map，上图是$3\times 3$与$5\times 5$的卷积核。卷积可以是传统的convolution卷积，也可以是空洞卷积（dilated convolution），或者深度卷积（depthwise convolution）。则有：<br>$\widetilde{\mathcal{F}} : \mathbf{X} \rightarrow \widetilde{\mathbf{U}} \in \mathbb{R}^{H \times W \times C}$ 与 $\widehat{\mathcal{F}} : \mathbf{X} \rightarrow \widehat{\mathbf{U}} \in \mathbb{R}^{H \times W \times C}$，其中$\widetilde{\mathcal{F}},\widehat{\mathcal{F}}$是卷积变换。</p><h4 id="第二步fuse"><a href="#第二步fuse" class="headerlink" title="第二步fuse"></a>第二步fuse</h4><p>直接将不同的feature map结合起来以获得全局信息，用以之后的动态调整。这里采用简单的求和以及global average pooling以获得channel-wise的信息$\mathbf{s} \in \mathbb{R}^{C}$：</p><script type="math/tex; mode=display">\mathbf{U}=\widetilde{\mathbf{U}}+\widehat{\mathbf{U}} \\ s_{c}=\mathcal{F}_{g p}\left(\mathbf{U}_{c}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} \mathbf{U}_{c}(i, j)</script><p>在获得$\mathbf{s}$后再通过MLP获得$\mathbf{z}$：</p><script type="math/tex; mode=display">\mathbf{z}=\mathcal{F}_{f c}(\mathbf{s})=\delta(\mathcal{B}(\mathbf{W} \mathbf{s}))</script><p>其中$\mathcal{B}$是batch normalization；$\delta$是Relu。</p><h4 id="第三步select"><a href="#第三步select" class="headerlink" title="第三步select"></a>第三步select</h4><p>使用soft attention去选择不同kernel size的feature map并结合在一起。也即：</p><script type="math/tex; mode=display">a_{c}=\frac{e^{\mathbf{A}_{c} \mathbf{z}}}{e^{\mathbf{A}_{c} \mathbf{z}}+e^{\mathbf{B}_{c} \mathbf{z}}}, b_{c}=\frac{e^{\mathbf{B}_{c} \mathbf{z}}}{e^{\mathbf{A}_{c} \mathbf{z}}+e^{\mathbf{B}_{c} \mathbf{z}}}</script><p>其中$\mathbf{A}_{c}$是对应$\widetilde{\mathbf{U}}$第$c$个channel的参数，$\mathbf{B}_{c}$是对应$\widehat{\mathbf{U}}$第$c$个channel的参数。$\mathbf{A}, \mathbf{B} \in \mathbb{R}^{C \times d}$，那么$a_{c},b_{c}$就对应不同feature map的weight。</p><p>因此，最终的feature map $\mathbf{V}$：</p><script type="math/tex; mode=display">\mathbf{V}_{c}=a_{c} \cdot \tilde{\mathbf{U}}_{c}+b_{c} \cdot \widehat{\mathbf{U}}_{c}, \quad a_{c}+b_{c}=1 \\ \mathbf{V}=\left[\mathbf{V}_{1}, \mathbf{V}_{2}, \dots, \mathbf{V}_{C}\right], \mathbf{V}_{c} \in \mathbb{R}^{H \times W}</script><h3 id="对比-amp-思考"><a href="#对比-amp-思考" class="headerlink" title="对比&amp;思考"></a>对比&amp;思考</h3><h4 id="与SE-Net"><a href="#与SE-Net" class="headerlink" title="与SE-Net"></a>与SE-Net</h4><p>SE-Net是通过不同channel之间的交互，使得channel获得全局的感受野，使用的是对channel的放缩（详见上一篇论文笔记）；而SK-Net是不同的感受野之间的同一channel在通过全局信息的指导下以soft-attention的形式加权平均，这就和论文中提到的人类视觉对不同物体进行动态调整感受野的思路一致。</p><h4 id="与dynamic-convolution"><a href="#与dynamic-convolution" class="headerlink" title="与dynamic convolution"></a>与dynamic convolution</h4><p>在论文[PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS]中，研究人员提出动态感受野的convolution，通过利用当前词预测一个卷积窗口，增加了模型的灵活性，并在机器翻译上取得了很好的结果。</p><p>虽然目的与本篇论文一致，但思路是完全不同的。一个是通过预测；另一个是在全局信息的指导下进行加权。在我的理解看来，或许本篇论文的思路更加合理一些，第一，在有了全局信息的指导下能够更好的进行加权，而通过预测，似乎有些盲目，可能需要更多的数据才能学得更好；第二，dynamic convolution论文中也提到了，如果不使用如深度可分离卷积等轻量级卷积方法，dynamic convolution不大现实（A dynamic version of standard convolutions would be impractical for current GPUs due to their large memory requirements），而SK-Net则不会有这个问题。</p><h4 id="其他思考"><a href="#其他思考" class="headerlink" title="其他思考"></a>其他思考</h4><p>从另一个角度去思考，SK-Net通过人工定义好的几种不同大小的卷积，相当于在模型中引入更强的先验（inductive bias），也即假设了数据不会超过这几种大小的卷积的处理范围，这或许比不引入先验，完全靠数据去学某种特定pattern的dynamic convolution对小数据集更友好，因此可以不需要更多的数据来使得模型表现良好。类似的理解可以在CNN/RNN与Transformer的对比中看见，因为CNN/RNN引入了较强的local bias，因此对于小数据集更友好，但同时其上限或许不如Transformer高；而Transformer一开始就是全局感受野，使得需要更多数据来帮助模型学到某种特定pattern（如某种local bias），但当数据充足时，Transformer的上限更高，近期非常火的pretrained model GPT/GPT-2.0/Bert似乎也印证了这点。</p><hr><h2 id="2️⃣-Attentional-pooling-for-action-recognition"><a href="#2️⃣-Attentional-pooling-for-action-recognition" class="headerlink" title="2️⃣[Attentional pooling for action recognition]"></a>2️⃣[Attentional pooling for action recognition]</h2><p>提出一种基于attention的pooling策略，采用低秩近似的方法，使得模型能够在计算量不增加很多的情况下达到更好的效果。可以将该方法理解成对二阶pooling的低秩近似。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="一阶pooling"><a href="#一阶pooling" class="headerlink" title="一阶pooling"></a>一阶pooling</h4><p>记$X \in R^{n \times f}$为被pooling的层，其中n为空间位置的个数，如$16\times 16$，$f$为channel个数。标准的sum/max pooling将该矩阵缩减为$R^{f \times 1}$，然后使用全连接的权重$\mathbf{w} \in R^{f \times 1}$获得一个分类的分数。这里假设的是二分类，但可以很容易推广为多分类。</p><p>上述操作形式化可以写成：</p><script type="math/tex; mode=display">\operatorname{score}_{p o o l}(X)=\mathbf{1}^{T} X \mathbf{w}, \quad \text { where } \quad X \in R^{n \times f}, \mathbf{1} \in R^{n \times 1}, \mathbf{w} \in R^{f \times 1}</script><p>其中$\mathbf{1}$为全1向量，$\mathbf{x}=\mathbf{1}^{T} X \in R^{1 \times f}$就是通过sum pooling后的feature。</p><h4 id="二阶pooling"><a href="#二阶pooling" class="headerlink" title="二阶pooling"></a>二阶pooling</h4><p>构建二阶feature $X^{T} X \in R^{f \times f}$，在获得二阶feature后，通常或向量化该矩阵，再送入全连接以做分类。也即我们会学习一个$f\times f$的全连接权重向量。若保持二阶feature与对应的全连接权重向量的形式为矩阵，矩阵相乘，其中的迹实际上就是这两个向量化后的矩阵所做内积获得的分数。形式化可以写成：</p><script type="math/tex; mode=display">\text {score}_{order2}(X)=\operatorname{Tr}\left(X^{T} X W^{T}\right), \quad \text { where } \quad X \in R^{n \times f}, W \in R^{f \times f}</script><p>这可以用迹的定义去证明：示意图<br><img src="/images/15540862875594.jpg" width="100%" height="50%"></p><h4 id="低秩二阶pooling"><a href="#低秩二阶pooling" class="headerlink" title="低秩二阶pooling"></a>低秩二阶pooling</h4><p>现尝试使用低秩去近似该二阶pooling，也即对$W$近似，将$W$写成两个向量的乘积，也即：</p><script type="math/tex; mode=display">W=\mathbf{a b}^{T} \text { where } \mathbf{a}, \mathbf{b} \in R^{f \times 1}</script><p>将上式代入二阶pooling，可获得：</p><script type="math/tex; mode=display">\begin{aligned} \text {score}_{\text {attention}}(X) &=\operatorname{Tr}\left(X^{T} X \mathbf{b a}^{T}\right), \quad \text { where } \quad X \in R^{n \times f}, \mathbf{a}, \mathbf{b} \in R^{f \times 1} \\ &=\operatorname{Tr}\left(\mathbf{a}^{T} X^{T} X \mathbf{b}\right) \\ &=\mathbf{a}^{T} X^{T} X \mathbf{b} \\ &=\mathbf{a}^{T}\left(X^{T}(X \mathbf{b})\right) \end{aligned}</script><p>第二行使用的是迹的定理：$\operatorname{Tr}(A B C)=\operatorname{Tr}(C A B)$<br>第三行使用的是标量的迹等于标量本身。<br>最后一行表明整个流程：给定一个feature map $X$，首先计算一个对所有空间位置的attentional map：$\mathbf{h}= {X \mathbf{b} \in R^{n \times 1}}$；然后根据该attentional map计算加权平均的feature：$\mathbf{x}=X^{T} \mathbf{h} \in R^{f \times 1}$。该feature再通过线性层获得最终的分数。</p><p>实际上上式还有其他理解的角度：</p><script type="math/tex; mode=display">\begin{aligned} \text {score}_{\text {attention}}(X) &=\left((X \mathbf{a})^{T} X\right) \mathbf{b} \\ &=(X \mathbf{a})^{T}(X \mathbf{b}) \end{aligned}</script><p>第一行表明attentional map也可以通过$X \mathbf{a} \in R^{n \times 1}$来计算，$\mathbf{b}$来做classifier。<br>第二行表明，该式子本质上是对称的，可以看成<strong>两个attentional heapmap的内积</strong>。</p><p>下图是整个流程：<br><img src="/images/15540869385196.jpg" width="80%" height="50%"></p><h4 id="Top-down-attention"><a href="#Top-down-attention" class="headerlink" title="Top-down attention"></a>Top-down attention</h4><p>现将二分类推广为多分类：</p><script type="math/tex; mode=display">\text {score}_{order2}(X, k)=\operatorname{Tr}\left(X^{T} X W_{k}^{T}\right), \quad \text { where } \quad X \in R^{n \times f}, W_{k} \in R^{f \times f}</script><p>也即将$W$替换成类相关的参数，仿照上面的推导，可以推出每个类都有特定的$\boldsymbol{a}_{k}$与$\boldsymbol{b}_{k}$。</p><p>但在这里通过固定其中一个参数为与类无关的参数，也即$\boldsymbol{b}_{k}=\boldsymbol{b}$。实际上就等价于一个是类相关的top-down attention；另一个是类无关的bottom-up attention。一个获得类特定的特征；另一个获得全局通用的特征。</p><p>因此最终低秩attention model为：</p><script type="math/tex; mode=display">\text {score}_{attention}(X, k)=\mathbf{t}_{k}^{T} \mathbf{h}, \quad \text { where } \quad \mathbf{t}_{k}=X \mathbf{a}_{k}, \mathbf{h}=X \mathbf{b}</script><h4 id="Average-pooling-Revisited"><a href="#Average-pooling-Revisited" class="headerlink" title="Average-pooling Revisited"></a>Average-pooling Revisited</h4><p>当然在给定了上述一系列的推导，我们对average-pooling重新进行形式化：</p><script type="math/tex; mode=display">\text {score}_{top-down}(X, k)=\mathbf{1}^{T} X \mathbf{w}_{k}=\mathbf{1}^{T} \mathbf{t}_{k} \quad \text { where } \quad \mathbf{t}_{k}=X \mathbf{w}_{k}</script><p>将$\mathbf{w}$替换成类相关的$\mathbf{w}_{k}$，实际上就是将二分类推广为多分类。但该形式赋予了average-pooling新的理解。</p><p>当然，我们还可以将rank-1推广为rank-k，实验证明对于大数据集使用大的秩会更好。</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><h4 id="与Self-attentive的联系"><a href="#与Self-attentive的联系" class="headerlink" title="与Self-attentive的联系"></a>与Self-attentive的联系</h4><p>论文[A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING]就提出了利用可学习的head对feature进行attention加权平均的方法，并且将一个head推广到多个head。<br>实际上在$\mathbf{h}= {X \mathbf{b} \in R^{n \times 1}}$我们就可以看出，$\mathbf{b}$在这里扮演的角色就是self-attentive的head的角色。对于秩为1的近似，就是head为1的情况，若将秩为1推广为秩为k，也即等价于在Self-attentive中多个head的情况。</p><p>本文巧妙的地方在于head有两个作用，一种是top-down的head，获得的是类相关的feature；另一个是bottom-up的feature，获得的是通用的feature。并且本文通过巧妙的数学推导来获得新的解释，本来仅仅是二阶feature过一个全连接，但通过公式推导赋予了attention的解释，这点让人眼前一亮。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> second-order </tag>
            
            <tag> pooling </tag>
            
            <tag> SK-Net </tag>
            
            <tag> attentional pooling </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词21</title>
      <link href="/2019/03/31/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D21/"/>
      <url>/2019/03/31/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D21/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣醉落魄-·-席上呈元素"><a href="#1️⃣醉落魄-·-席上呈元素" class="headerlink" title="1️⃣醉落魄 · 席上呈元素"></a>1️⃣醉落魄 · 席上呈元素</h3><p>[宋] 苏轼<br>分携如昨，人生到处萍飘泊。偶然相聚还离索。多病多愁，须信从来错。<br><strong>尊前一笑休辞却，天涯同是伤沦落</strong>。故山犹负平生约。西望峨嵋，长羡归飞鹤。</p><p><a href="http://lib.xcz.im/work/57c467a86be3ff0058452840" target="_blank" rel="noopener">http://lib.xcz.im/work/57c467a86be3ff0058452840</a></p><hr><h3 id="2️⃣戏为六绝句"><a href="#2️⃣戏为六绝句" class="headerlink" title="2️⃣戏为六绝句"></a>2️⃣戏为六绝句</h3><p>[唐] 杜甫<br>【其一】<br>庾信文章老更成，凌云健笔意纵横。<br>今人嗤点流传赋，不觉前贤畏后生。</p><p>【其三】<br>纵使卢王操翰墨，劣于汉魏近风骚。<br>龙文虎脊皆君驭，历块过都见尔曹。</p><p>过都历块 (guò dōu lì kuài)<br>解释：越过都市，经过山阜。意指纵横驰骋，施展才能。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文14</title>
      <link href="/2019/03/24/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8714/"/>
      <url>/2019/03/24/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8714/</url>
      
        <content type="html"><![CDATA[<p>本周论文:</p><ol><li>Is Second-order Information Helpful for Large-scale Visual Recognition?</li><li>The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classification</li></ol><h2 id="1️⃣-Is-Second-order-Information-Helpful-for-Large-scale-Visual-Recognition"><a href="#1️⃣-Is-Second-order-Information-Helpful-for-Large-scale-Visual-Recognition" class="headerlink" title="1️⃣[Is Second-order Information Helpful for Large-scale Visual Recognition?]"></a>1️⃣[Is Second-order Information Helpful for Large-scale Visual Recognition?]</h2><p>通过协方差的方法获得图像的二阶信息。<br>参考了<a href="https://zhuanlan.zhihu.com/p/46864160" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46864160</a></p><p>深度分类网络主要分为两个部分：特征提取和分类器。无论是VGG还是GoogleNet，后来的Resnet、Densenet，在连接分类器之前，一般都连接了一个Pooling层。<br>但pooling只获得了feature的一阶信息，对于细分类问题中类间差异不显著，一阶信息可能有一些不适用，因此我们可以通过一阶信息获得二阶信息，从而获取更有价值的信息。</p><p>本文通过获取<strong>特征协方差</strong>的方法，以达到该目的。</p><p>输入:$\mathbf{X} \in \mathbb{R}^{d \times N}$</p><p>则协方差矩阵为$\mathbf{X} \mapsto \mathbf{P}, \quad \mathbf{P}=\mathbf{X} \overline{\mathbf{I}} \mathbf{X}^{T}$，其中$\overline{\mathbf{I}}=\frac{1}{N}\left(\mathbf{I}-\frac{1}{N} \mathbf{1} \mathbf{1}^{T}\right)$, $\mathbf{I}$是单位阵，$\mathbf{1}$是全1的向量。</p><p>协方差矩阵是半正定矩阵，因此可写成$\mathbf{P} \mapsto(\mathbf{U}, \mathbf{\Lambda}), \quad \mathbf{P}=\mathbf{U} \mathbf{\Lambda} \mathbf{U}^{T}$，其中$\boldsymbol{\Lambda}=\operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{d}\right)$，$\mathbf{U}=\left[\mathbf{u}_{1}, \dots, \mathbf{u}_{d}\right]$，$\mathbf{U}$是对应的特征向量。</p><p>最终可获得$\mathbf{Q}$矩阵：$(\mathbf{U}, \boldsymbol{\Lambda}) \mapsto \mathbf{Q}, \mathbf{Q} \triangleq \mathbf{P}^{\alpha}=\mathbf{U F}(\mathbf{\Lambda}) \mathbf{U}^{T}$，其中$\alpha$是一个正实数，$\mathbf{F}(\boldsymbol{\Lambda})=\operatorname{diag}\left(f\left(\lambda_{1}\right), \ldots, f\left(\lambda_{d}\right)\right)$，其中$f\left(\lambda_{i}\right)=\lambda_{i}^{\alpha}$，是特征值的幂，如果要做归一化，那么可以有：</p><script type="math/tex; mode=display">f\left(\lambda_{i}\right)=\left\{\begin{array}{cc}{\lambda_{i}^{\alpha} / \lambda_{1}^{\alpha}} & {\text { for MPN+M }-\ell_{2}} \\ {\lambda_{i}^{\alpha} /\left(\sum_{k} \lambda_{k}^{2 \alpha}\right)^{\frac{1}{2}}} & {\text { for MPN+M-Fro }}\end{array}\right.</script><p>之所以取幂，是为了解决在协方差估计中小样本高维度的问题，以resnet为例，最后得到的feature为7X7X512，也就是49个512维的feature，这样估计出来的协方差矩阵是不靠谱的，而通过幂这个操作，可以解决这一问题。通过实验可以发现，当幂次为0.5也就是平方根操作时，效果最优。（似乎类似的有word2vec的平滑）</p><p>（虽然这篇有些看不大懂，但一个启发就是，可以通过协方差的方式进行特征之间的交互）</p><hr><h2 id="2️⃣-The-Treasure-beneath-Convolutional-Layers-Cross-convolutional-layer-Pooling-for-Image-Classification"><a href="#2️⃣-The-Treasure-beneath-Convolutional-Layers-Cross-convolutional-layer-Pooling-for-Image-Classification" class="headerlink" title="2️⃣[The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classification]"></a>2️⃣[The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classification]</h2><p>提出使用卷积出来后的feature经过pooling作为最后的图像特征表示而不是全连接后的特征表示。</p><p>Motivation：只使用最后一层fc的特征有一个缺点，就是丢失位置信息，而convolution layer包含了丰富的空间信息。在pooling完后每个local区域都能获得一个特征，并拼接起来作为最后的表示。</p><p><img src="/images/15533936911589.jpg" width="60%" height="50%"></p><p>prerequisite:<br>①首先有一个预训练好的模型<br>②有两层一样$H\times W$的convolution。论文以AlexNet作为例子</p><p>假设卷积后的feature map是$H × W × D$，那么可以理解成，我们将图片分为$H × W$的区域，每个区域的特征用$D$维表示。我们称每个$D$维特征一个spatial unit。当使用全连接时，这部分的空间信息就丢失了，并且无法还原。</p><p>本文提出，将每个区域提取出一个特征，然后拼起来组成一整张图的特征，如下图，每个长条（也即$1\times 1\times channel$）作为一个特征：</p><p><img src="/images/15533939765641.jpg" width="50%" height="50%"></p><p>如何判断区域？一种方法是首先检测出多个区域，每个区域对应一种object part，然后对于落入该区域的特征进行pooling，给定D种human-specified object parts，那么可以获得D个feature且拼在一起。</p><script type="math/tex; mode=display">\mathbf{P}_{k}^{t}=\sum_{i=1} \mathbf{x}_{i} I_{i, k}</script><p>具体而言，$\mathbf{x}_{i}$是特征，$I_{i, k}$是二元的indicator，表明$\mathbf{x}_{i}$是否落入该区域，每个$I$实际上定义了一个池化通道。当然，这里可以进一步将indicator从二元扩展为权重。</p><p>但在实现的过程中，并没有human-specified的区域。这里我们就借助下一层的卷积作为indicator。</p><blockquote><p>By doing so, D t+1 pooling channels are created for the local features extracted from the tth convolutional layer</p></blockquote><p>这也就被称为cross-convolutional-layer pooling。</p><p>如何做？</p><blockquote><p>the filter of a convolutional layer works as a part detector and its feature map serves a similar role as the part region indicator map.</p></blockquote><p>具体而言，有：</p><script type="math/tex; mode=display">\begin{array}{l}{\mathbf{P}^{t}=\left[\mathbf{P}_{1}^{t}, \mathbf{P}_{2}^{t}, \cdots, \mathbf{P}_{k}^{t}, \cdots, \mathbf{P}_{D_{t+1}}^{t}\right]} \\ {\text { where, } \mathbf{P}_{k}^{t}=\sum_{i=1}^{N_{t}} \mathbf{x}_{i}^{t} a_{i, k}^{t+1}}\end{array}</script><p>$\mathbf{P}^{t}$表示第t层convolution在卷积过后做cross-pooling后的特征集合，也即我们要获得的表示，该表示通过$D_{t+1}$次pooling后的结果拼接而成。$D_{t+1}$具体来说，就是第t+1层的卷积的channel维数。假设$\mathbf{a}_{i}^{t+1} \in \mathbb{R}^{D_{t+1}}$是第t+1层convolution的第i个空间单位（spatial unit）的feature vector，其中$a_{i, k}^{t+1}$是该向量的一个值，该值就作为pooling的权重。</p><p>上述有些绕口且难懂，直接看例子：<br><img src="/images/15533957004853.jpg" width="80%" height="50%"><br><img src="/images/15533957336100.jpg" width="80%" height="50%"></p><p>即，第t+1层convolution的channel维度为多少，则pooling后的特征个数即为多少。因为第t层与第t+1层的$H\times W$是一致的，那么可以用t+1层的每个slice去对第t层的convolution进行加权。</p><p>为什么这样是合理的？<br>因为第t+1层的convolution提取了$D_{t+1}$个特征，使用的是$m\times n$的kernel size，如果$x$是被$m\times n$的某个kernel提取了，那么很自然的，$x$就是对应该kernel提取出来的feature的一个spatial unit。说白了就是第t层与第t+1层的空间对应。</p>]]></content>
      
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> second-order </tag>
            
            <tag> pooling </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词20</title>
      <link href="/2019/03/23/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D20/"/>
      <url>/2019/03/23/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D20/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣杂诗七首（其四）"><a href="#1️⃣杂诗七首（其四）" class="headerlink" title="1️⃣杂诗七首（其四）"></a>1️⃣杂诗七首（其四）</h3><p>[三国] 曹植<br>南国有佳人，容华若桃李。<br>朝游江北岸，夕宿潇湘沚。<br>时俗薄朱颜，谁为发皓齿？<br>俯仰岁将暮，荣耀难久恃。</p><p><a href="http://lib.xcz.im/work/58ad4c78ac502e007e9f6f9f" target="_blank" rel="noopener">http://lib.xcz.im/work/58ad4c78ac502e007e9f6f9f</a></p><hr><h3 id="2️⃣梦江南"><a href="#2️⃣梦江南" class="headerlink" title="2️⃣梦江南"></a>2️⃣梦江南</h3><p>[唐] 温庭筠<br>千万恨，恨极在天涯。山月不知心里事，水风空落眼前花，摇曳碧云斜。</p><p><a href="http://lib.xcz.im/work/57b8d0c77db2a2005425c856" target="_blank" rel="noopener">http://lib.xcz.im/work/57b8d0c77db2a2005425c856</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文13</title>
      <link href="/2019/03/17/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8713/"/>
      <url>/2019/03/17/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8713/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-Depthwise-Separable-Convolutions-for-Neural-Machine-Translation"><a href="#1️⃣-Depthwise-Separable-Convolutions-for-Neural-Machine-Translation" class="headerlink" title="1️⃣[Depthwise Separable Convolutions for Neural Machine Translation]"></a>1️⃣[Depthwise Separable Convolutions for Neural Machine Translation]</h2><p>将depthwise separable convolution 深度可分离卷积 用于翻译任务，并在此基础上对depthwise separable进行更进一步的参数量优化，也即super-separable。（其实我觉得并没有啥创新性的感觉）</p><p><img src="/images/15528281403428.jpg" width="90%" height="50%"></p><p>首先介绍什么是depthwise separable convolution，实际上就是一个depthwise+pointwise。</p><script type="math/tex; mode=display">\operatorname{Conv}(W, y)_{(i, j)}=\sum_{k, l, m}^{K, L, M} W_{(k, l, m)} \cdot y_{(i+k, j+l, m)}</script><script type="math/tex; mode=display">\operatorname{PointwiseConv}(W, y)_{(i, j)}=\sum_{m}^{M} W_{m} \cdot y_{(i, j, m)}</script><script type="math/tex; mode=display">\text {DepthwiseConv}(W, y)_{(i, j)}=\sum_{k, l}^{K, L} W_{(k, l)} \odot y_{(i+k, j+l)}</script><script type="math/tex; mode=display">\operatorname{SepConv}\left(W_{p}, W_{d}, y\right)_{(i, j)}=\text {PointwiseConv}_{(i, j)}\left(W_{p}, \text { DepthwiseConv }_{(i, j)}\left(W_{d}, y\right)\right)</script><p>几种convolution的参数量对比：<br><img src="/images/15528283555357.jpg" width="80%" height="50%"><br>其中k是kernel size，c是channel，g是group。</p><p>g-Sub-separable是指将channel分为几个group，每个group进行常规的convolution操作；g-Super-separable，也即本文中提出的convolution，同样是将channel分为几个group，然后对每个group进行depthwise-separable的卷积。</p><hr><h2 id="2️⃣-Squeeze-and-Excitation-Networks"><a href="#2️⃣-Squeeze-and-Excitation-Networks" class="headerlink" title="2️⃣[Squeeze-and-Excitation Networks]"></a>2️⃣[Squeeze-and-Excitation Networks]</h2><p>提出一种新型的网络，能够通过建模channel之间的关系，使得每个channel能够获得全局的信息，进而提高模型的能力。<br><img src="/images/15528285519609.jpg" width="90%" height="50%"></p><p>分为两步：第一步是获得一个全局的表示，第二步是根据全局信息更新每个channel的信息。</p><h3 id="符号"><a href="#符号" class="headerlink" title="符号"></a>符号</h3><p>输入：$ \mathbf{X} \in \mathbb{R}^{H^{\prime} \times W^{\prime} \times C^{\prime}} $<br>经过特征提取后（如Convolution)：$\mathbf{U} \in \mathbb{R}^{H \times W \times C}$，也即：$\mathbf{U}=\mathbf{F}_{t r}(\mathbf{X})$<br>将$\mathbf{U}$写成：$\mathbf{U}=\left[\mathbf{u}_{1}, \mathbf{u}_{2}, \dots, \mathbf{u}_{C}\right]$<br>$\mathbf{V}$ 是可学习的卷积核参数： $\mathbf{V}=\left[\mathbf{v}_{1}, \mathbf{v}_{2}, \ldots, \mathbf{v}_{C}\right]$</p><p>则上述卷积变换可写成：$\mathbf{u}_{c}=\mathbf{v}_{c} \ast \mathbf{X}=\sum_{s=1}^{C^{\prime}} \mathbf{v}_{c}^{s} \ast \mathbf{x}^{s}$</p><h3 id="Squeeze-Global-Information-Embedding"><a href="#Squeeze-Global-Information-Embedding" class="headerlink" title="Squeeze: Global Information Embedding"></a>Squeeze: Global Information Embedding</h3><p>第一步，将所有的特征进行整合得到全局的特征：</p><script type="math/tex; mode=display">z_{c}=\mathbf{F}_{s q}\left(\mathbf{u}_{c}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} u_{c}(i, j)</script><p>论文提取全局特征的方法直接用简单的global average pooling。那么$\mathbf{z} \in \mathbb{R}^{C}$的每一维就代表每一维的channel。</p><h3 id="Excitation-Adaptive-Recalibration"><a href="#Excitation-Adaptive-Recalibration" class="headerlink" title="Excitation: Adaptive Recalibration"></a>Excitation: Adaptive Recalibration</h3><p>与attention不同的是，论文希望能够同时强调不同多个channel的重要（而不是one-hot的形式），因此使用一个简单的门控制机制，采用sigmoid激活函数：（这里的想法挺有意思，相对attention的softmax似乎确实会更好的样子）</p><script type="math/tex; mode=display">\mathbf{s}=\mathbf{F}_{ex}(\mathbf{z}, \mathbf{W})=\sigma(g(\mathbf{z}, \mathbf{W}))=\sigma\left(\mathbf{W}_{2} \delta\left(\mathbf{W}_{1} \mathbf{z}\right)\right)</script><p>为了减少参数这里的MLP采用了bottleneck的形式。亦即：<br>${\mathbf{W}_{1} \in \mathbb{R}^{\frac{C}{r} \times C}}$ $ {\mathbf{W}_{2} \in \mathbb{R}^{C \times \frac{C}{r}}}$<br>$r$是reduction ratio。</p><p>贴上作者的思路：</p><blockquote><p>To make use of the information aggregated in the squeeze operation, we follow it with a second operation which aims to fully capture channel-wise dependencies. To fulfill this objective, the function must meet two criteria: first, it must be ﬂexible (in particular, it must be capable of learning a nonlinear interaction between channels) and second, it must learn a non-mutually-exclusive relationship since we would like to ensure that multiple channels are allowed to be emphasised (rather than enforcing a one-hot activation). To meet these criteria, we opt to employ a simple gating mechanism with a sigmoid activation.</p></blockquote><p>最后对每个channel进行<strong>放缩</strong>，获得新的表示：</p><script type="math/tex; mode=display">\widetilde{\mathbf{x}}_{c}=\mathbf{F}_{\text {scale}}\left(\mathbf{u}_{c}, s_{c}\right)=s_{c} \cdot \mathbf{u}_{c}</script><hr><h2 id="3️⃣-Non-local-Neural-Networks"><a href="#3️⃣-Non-local-Neural-Networks" class="headerlink" title="3️⃣[Non-local Neural Networks]"></a>3️⃣[Non-local Neural Networks]</h2><p>提出一种新的结构，与上一篇类似，希望模型的每个位置都能感知到其他位置，从而捕获长程依赖，拥有全局信息。</p><p><img src="/images/15528297540165.jpg" width="60%" height="50%"></p><h3 id="Non-local-Network"><a href="#Non-local-Network" class="headerlink" title="Non-local Network"></a>Non-local Network</h3><p>定义non-local网络：</p><script type="math/tex; mode=display">\mathbf{y}_{i}=\frac{1}{\mathcal{C}(\mathbf{x})} \sum_{\forall j} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) g\left(\mathbf{x}_{j}\right)</script><p>其中$\mathcal{C}$是归一化函数；$f$是第$i$个位置与第$j$个位置的交互函数；$g$计算第$j$个位置的表示。</p><h4 id="g-的具体形式"><a href="#g-的具体形式" class="headerlink" title="$g$的具体形式"></a>$g$的具体形式</h4><p>一个线性函数：$g\left(\mathbf{x}_{j}\right)=W_{g} \mathbf{x}_{j}$<br>在实现的时候是一个$1\times1$或 $1\times1\times1$的convolution。</p><h4 id="f-的具体形式"><a href="#f-的具体形式" class="headerlink" title="$f$的具体形式"></a>$f$的具体形式</h4><p>①Gaussian<br>$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=e^{\mathbf{x}_{i}^{T} \mathbf{x}_{j}}$<br>则归一化定义为$\mathcal{C}(\mathbf{x})=\sum_{\forall j} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)$ 。</p><p>②Embedded Gaussian<br>$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=e^{\theta\left(\mathbf{x}_{i}\right)^{T} \phi\left(\mathbf{x}_{j}\right)}$<br>其中：$\theta\left(\mathbf{x}_{i}\right)=W_{\theta} \mathbf{x}_{i} $, $ \phi\left(\mathbf{x}_{j}\right)=W_{\phi} \mathbf{x}_{j}$<br>归一化：$\mathcal{C}(\mathbf{x})=\sum_{\forall j} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)$</p><p>可以看到self-attention是Embedded Gaussian的一种形式。虽然有这样的关系，但作者在实验中发现softmax并不是必要的。</p><p>③Dot product<br>$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\theta\left(\mathbf{x}_{i}\right)^{T} \phi\left(\mathbf{x}_{j}\right)$<br>归一化：$\mathcal{C}(\mathbf{x})=N$</p><p>④Concatenation<br>$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\operatorname{ReLU}\left(\mathbf{w}_{f}^{T}\left[\theta\left(\mathbf{x}_{i}\right), \phi\left(\mathbf{x}_{j}\right)\right]\right)$<br>$\mathcal{C}(\mathbf{x})=N$</p><p>有了上面的non-local的介绍，可以直接将其用于residual network。<br>$\mathbf{z}_{i}=W_{z} \mathbf{y}_{i}+\mathbf{x}_{i}$<br>$y$则是non-local network的输出。</p><h3 id="Non-local-block的策略-tricks"><a href="#Non-local-block的策略-tricks" class="headerlink" title="Non-local block的策略/tricks"></a>Non-local block的策略/tricks</h3><p>①设置$W_g$,$W_θ$,$W_ϕ$的channel的数目为x的channel数目的一半，这样就形成了一个bottleneck，能够减少一半的计算量。Wz再重新放大到x的channel数目，保证输入输出维度一致。</p><p>②在$\frac{1}{\mathcal{C}(\hat{\mathbf{x}})} \sum_{\forall j} f\left(\mathbf{x}_{i}, \hat{\mathbf{x}}_{j}\right) g\left(\hat{\mathbf{x}}_{j}\right)$使用下采样，如max-pooling，减少计算量。</p><hr><h2 id="4️⃣-Bilinear-CNN-Models-for-Fine-grained-Visual-Recognition"><a href="#4️⃣-Bilinear-CNN-Models-for-Fine-grained-Visual-Recognition" class="headerlink" title="4️⃣[Bilinear CNN Models for Fine-grained Visual Recognition]"></a>4️⃣[Bilinear CNN Models for Fine-grained Visual Recognition]</h2><p>提出一种双线性模型，由两个特征提取器组成，他们的输出做<strong>外积</strong>，最终获得图像描述特征。</p><p>Motivation(?不确定是不是这样)：对于细粒度物体的分类，先对局部定位，再提取特征。两个特征提取器一个是提取location，另一个提取特征。</p><p><img src="/images/15528310057673.jpg" width="60%" height="50%"></p><p>为什么用<strong>外积</strong>？</p><blockquote><p>outer product captures pairwise correlations between the feature channels</p></blockquote><p>有意思的是作者将该模型和人脑视觉处理的两个假设联系在一起(stream hypothesis)：<br>here are two main pathways, or “streams”. The ventral stream (or, “what pathway”) is involved with object identiﬁcation and recognition. The dorsal stream (or, “where pathway”) is involved with processing the object’s spatial location relative to the viewer.<br>不过看看就好，并没有什么道理。</p><p>对于一个分类的双线性模型而言，其一般形式是一个四元组：$\mathcal{B}=\left(f_{A}, f_{B}, \mathcal{P}, \mathcal{C}\right)$。其中$f$是特征函数，$\mathcal{P}$是pooling函数，$\mathcal{C}$是分类函数。具体而言，$f$是一个映射，${f : \mathcal{L} \times \mathcal{I} \rightarrow} {R^ {c\times D}} $。也即将一个image和一个location L 映射成feature。（We refer to locations generally which can include position and scale 其实这里不是很懂location的意思）</p><p>将feature a和feature b结合在一起：<br>$\text { bilinear }\left(l, \mathcal{I}, f_{A}, f_{B}\right)=f_{A}(l, \mathcal{I})^{T} f_{B}(l, \mathcal{I})$</p><p>pooling有好几种，可以直接加起来，或者使用max-pooling。这里使用直接加起来的方式，可以理解为，这些特征是无序(orderless)的叠加。</p><p>在获得输出后再做一些操作/trick能够提升表现：<br>$\begin{array}{l}{\mathbf{y} \leftarrow \operatorname{sign}(\mathbf{x}) \sqrt{|\mathbf{x}|}} \\ {\mathbf{z} \leftarrow \mathbf{y} /|\mathbf{y}|_{2}}\end{array}$</p><p>讨论：<br>①But do the networks specialize into roles of localization (“where”) and appearance modeling (“what”) when initialized asymmetrically and ﬁne-tuned?<br>通过可视化发现，并没有明确的功能分开。<br>Both these networks tend to activate strongly on highly speciﬁc semantic parts</p><p>②bilinear的好处还可以扩展成trilinear，添加更多的信息。</p>]]></content>
      
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> NMT </tag>
            
            <tag> SE-Net </tag>
            
            <tag> Non-local </tag>
            
            <tag> Bilinear </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词19</title>
      <link href="/2019/03/17/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D19/"/>
      <url>/2019/03/17/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D19/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣送灵澈上人"><a href="#1️⃣送灵澈上人" class="headerlink" title="1️⃣送灵澈上人"></a>1️⃣送灵澈上人</h3><p>[唐] 刘长卿<br>苍苍竹林寺，杳杳钟声晚。<br>荷笠带斜阳，青山独归远。</p><p>荷（hè）笠：背着斗笠。</p><p><a href="http://lib.xcz.im/work/57b90887128fe10054c9c750" target="_blank" rel="noopener">http://lib.xcz.im/work/57b90887128fe10054c9c750</a></p><hr><h3 id="2️⃣苏幕遮-·-怀旧"><a href="#2️⃣苏幕遮-·-怀旧" class="headerlink" title="2️⃣苏幕遮 · 怀旧"></a>2️⃣苏幕遮 · 怀旧</h3><p>[宋] 范仲淹<br>碧云天，黄叶地，秋色连波，波上寒烟翠。山映斜阳天接水，芳草无情，更在斜阳外。<br>黯乡魂，追旅思。夜夜除非，好梦留人睡。明月楼高休独倚，酒入愁肠，化作相思泪。</p><p><a href="http://lib.xcz.im/work/57b8ee4a128fe10054c91757" target="_blank" rel="noopener">http://lib.xcz.im/work/57b8ee4a128fe10054c91757</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文12</title>
      <link href="/2019/03/10/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8712/"/>
      <url>/2019/03/10/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8712/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-PAY-LESS-ATTENTION-WITH-LIGHTWEIGHT-AND-DYNAMIC-CONVOLUTIONS"><a href="#1️⃣-PAY-LESS-ATTENTION-WITH-LIGHTWEIGHT-AND-DYNAMIC-CONVOLUTIONS" class="headerlink" title="1️⃣[PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS]"></a>1️⃣[PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS]</h2><p>Facebook研究人员提出的两种基于卷积的方法尝试替代self-attention在transformer中的作用，拥有更少的参数以及更快的速度，并且能够达到很好的效果。</p><p><img src="/images/15521843619624.jpg" width="80%" height="80%"></p><h3 id="Lightweight-convolution"><a href="#Lightweight-convolution" class="headerlink" title="Lightweight convolution"></a>Lightweight convolution</h3><p>背景：depthwise convolution<br>每个channel独立进行卷积，注意到放到NLP任务上channel是指embedding的每一维。</p><script type="math/tex; mode=display">O_{i, c}=\text{DepthwiseConv}\left(X, W_{c, :}, i, c\right)=\sum_{j=1}^{k} W_{c, j} \cdot X_{\left(i+j-\left\lceil\frac{k+1}{2}\right]\right), c}</script><p>因此Lightweight convolution的计算方法为：</p><script type="math/tex; mode=display">\operatorname{LightConv}\left(X, W_{\left\lceil\frac{c H}{d}\right\rceil,:}, i, c\right)=\text { DepthwiseConv}\left(X, \text{softmax}(W_{\left\lceil\frac{c H}{d}\right\rceil,:}), i, c\right)</script><p>每一层都有固定的window size，这和self-attention不同，self-attention是所有的context都进行交互。</p><ul><li>Weight sharing 注意到这里讲每d/H个channel的参数进行绑定，进一步减少参数。</li><li>Softmax-normalization 对channel一维进行softmax，相当于归一化每个词的每一维的的重要性（比self-attention更精细）。实验证明，如果没有softmax没办法收敛。</li></ul><p>因此总体的架构为：<br>input—&gt;linear —&gt; GLU(gated linear unit) —&gt; lightconv/dynamicConv —&gt; linear</p><h3 id="Dynamic-convolution"><a href="#Dynamic-convolution" class="headerlink" title="Dynamic convolution"></a>Dynamic convolution</h3><p>与lightweight convolution相似，但加了一个动态的kernel size。</p><script type="math/tex; mode=display">\text { DynamicConv}( X , i , c ) = \operatorname{LightConv}\left(X, f\left(X_{i}\right)_{h,:}, i, c\right)</script><p>这里的kernel size简单使用线性映射：$f : \mathbb { R } ^ { d } \rightarrow \mathbb { R } ^ { H \times k }$<br>如：$f\left(X_{i}\right)=\sum_{c=1}^{d} W_{h, j, c}^{Q} X_{i, c}$</p><hr><h2 id="2️⃣-Joint-Embedding-of-Words-and-Labels-for-Text-Classiﬁcation"><a href="#2️⃣-Joint-Embedding-of-Words-and-Labels-for-Text-Classiﬁcation" class="headerlink" title="2️⃣[Joint Embedding of Words and Labels for Text Classiﬁcation]"></a>2️⃣[Joint Embedding of Words and Labels for Text Classiﬁcation]</h2><p>提出一种机制将label作为embedding与词一同训练，同时引入label和word的attention机制，在分类上获得效果。</p><p><img src="/images/15521854844061.jpg" width="40%" height="50%"></p><p>上图中，C是label embedding，维度为$P\times K$ ; V是句子所有词的embedding矩阵，维度为$P\times L$。<br>$\mathbf{G}$的计算公式为：</p><script type="math/tex; mode=display">\mathbf{G}=\left(\mathbf{C}^{\top} \mathbf{V}\right) \oslash \hat{\mathbf{G}}</script><p>$\oslash$表示element-wise相除。$\hat{\mathbf{G}}$表示l2 norm，也即：</p><script type="math/tex; mode=display">\hat{g}_{k l}=\left\|\boldsymbol{c}_{k}\right\|\left\|\boldsymbol{v}_{l}\right\|</script><p>因此公式的本质即在计算label与每个词的cos距离。</p><p>在获得了$\mathbf{G}$后，为了获得更高的的表示，如phrase，将一个一个block取出，并过线性层：</p><script type="math/tex; mode=display">\boldsymbol{u}_{l}=\operatorname{ReLU}\left(\mathbf{G}_{l-r : l+r} \mathbf{W}_{1}+\boldsymbol{b}_{1}\right)</script><p>接着对每个$\boldsymbol{u}_{l}$取最大值：</p><script type="math/tex; mode=display">m_{l}=\textbf{max-pooling}\left(\boldsymbol{u}_{l}\right)</script><p>此时的$\mathbf{m}$是一个长度为L的向量。最终对m做softmax获得一个分数的分布：</p><script type="math/tex; mode=display">\boldsymbol{\beta}=\operatorname{SoftMax}(\boldsymbol{m})</script><p>将该分数和每个词做加权求和，获得最终的向量表示：</p><script type="math/tex; mode=display">\boldsymbol{z}=\sum_{l} \beta_{l} \boldsymbol{v}_{l}</script><p>思考：将label与embedding放在一起训练这个思路不错。但整合的方式是否过于简单粗暴了<br>？特别是phrase的提取和随后的max-pooling的可解释性并不强的样子。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> attention </tag>
            
            <tag> Convolution </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> embedding </tag>
            
            <tag> text classification </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识18</title>
      <link href="/2019/03/10/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8618/"/>
      <url>/2019/03/10/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8618/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Depthwise-seperable-convolution"><a href="#1️⃣-Depthwise-seperable-convolution" class="headerlink" title="1️⃣[Depthwise seperable convolution]"></a>1️⃣[Depthwise seperable convolution]</h3><p>Depthwise seperable convolution = depthwise + pointwise<br>先每个卷积核独立对一个feature map进行卷积，再通过一个$1\times 1 \times n$的卷积核对feature map进行整合。</p><p><a href="https://blog.csdn.net/tintinetmilou/article/details/81607721" target="_blank" rel="noopener">https://blog.csdn.net/tintinetmilou/article/details/81607721</a></p><hr><h3 id="2️⃣-如何寻找较好的lr"><a href="#2️⃣-如何寻找较好的lr" class="headerlink" title="2️⃣[如何寻找较好的lr]"></a>2️⃣[如何寻找较好的lr]</h3><p>一种启发式的方法：</p><blockquote><p>Over an epoch begin your SGD with a very low learning rate (like 10−8) but change it (by multiplying it by a certain factor for instance) at each mini-batch until it reaches a very high value (like 1 or 10). Record the loss each time at each iteration and once you’re finished, plot those losses against the learning rate.</p></blockquote><p><a href="https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html" target="_blank" rel="noopener">https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Convolution </tag>
            
            <tag> learning rate </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词18</title>
      <link href="/2019/03/09/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D18/"/>
      <url>/2019/03/09/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D18/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣西江月-·-遣兴"><a href="#1️⃣西江月-·-遣兴" class="headerlink" title="1️⃣西江月 · 遣兴"></a>1️⃣西江月 · 遣兴</h3><p>[宋] 辛弃疾<br>醉里且贪欢笑，要愁那得工夫。近来始觉古人书，信著全无是处。<br>昨夜松边醉倒，问松「我醉何如」。只疑松动要来扶，以手推松曰「去」！</p><p><a href="http://lib.xcz.im/work/57b935bcd342d3005ac8e63f" target="_blank" rel="noopener">http://lib.xcz.im/work/57b935bcd342d3005ac8e63f</a></p><hr><h3 id="2️⃣蝶恋花"><a href="#2️⃣蝶恋花" class="headerlink" title="2️⃣蝶恋花"></a>2️⃣蝶恋花</h3><p>[宋] 晏殊<br>槛菊愁烟兰泣露，罗幕轻寒，燕子双飞去。明月不谙离恨苦，斜光到晓穿朱户。<br>昨夜西风凋碧树，独上高楼，望尽天涯路。欲寄彩笺兼尺素，山长水阔知何处？</p><p><a href="http://lib.xcz.im/work/57b318dd1532bc00618ffaff" target="_blank" rel="noopener">http://lib.xcz.im/work/57b318dd1532bc00618ffaff</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何使用fairseq复现Transformer NMT</title>
      <link href="/2019/01/28/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8fairseq%E5%A4%8D%E7%8E%B0Transformer%20NMT/"/>
      <url>/2019/01/28/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8fairseq%E5%A4%8D%E7%8E%B0Transformer%20NMT/</url>
      
        <content type="html"><![CDATA[<p>基于Transformer的NMT虽然结果好，但超参非常难调，只要有一两个参数和论文不一样，就有可能得到和论文相去甚远的结果。fairseq是现有比较完善的seq2seq库，由于是大公司出品，因此也写得较为完善，不论是代码还是文档。</p><p>本文讨论如何使用fairseq复现基于Transformer的翻译任务，也即复现Vaswani, et al. 的论文结果。本文尽量不讨论实现细节，只讨论如何复现出结果。</p><p>fairseq项目地址：<a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">https://github.com/pytorch/fairseq</a></p><h2 id="使用教程"><a href="#使用教程" class="headerlink" title="使用教程"></a>使用教程</h2><p>在这里我们参考的是18年的文章<a href="https://arxiv.org/abs/1806.00187" target="_blank" rel="noopener">Scaling Neural Machine Translation</a>，同样是基于Transformer的NMT。同时，我们使用WMT16 EN-DE而不是Vaswani, et al.论文中的WMT14 EN-DE。二者只在一个文件（commoncrawl）上有区别，其他是一样的，由于WMT16 EN-DE有预处理好的数据，为了方便起见，我们就使用该份数据（下文也有预处理WMT14数据的方法）</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><ol><li>安装fairseq，在Readme内有</li><li>阅读Readme（optional）</li><li>阅读doc（optional）</li></ol><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><h4 id="Step1"><a href="#Step1" class="headerlink" title="Step1"></a>Step1</h4><p>数据预处理主要是下载多个文件并合并—&gt;清理/tokenize数据—&gt;将数据分为train、valid—&gt;bpe(bype pair encoding)。fairseq提供了一整套处理流程的脚本，在examples/translation/prepare-wmt14en2de.sh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># Adapted from https://github.com/facebookresearch/MIXER/blob/master/prepareData.sh</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'Cloning Moses github repository (for tokenization scripts)...'</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/moses-smt/mosesdecoder.git</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'Cloning Subword NMT repository (for BPE pre-processing)...'</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/rsennrich/subword-nmt.git</span><br><span class="line"></span><br><span class="line">SCRIPTS=mosesdecoder/scripts</span><br><span class="line">TOKENIZER=<span class="variable">$SCRIPTS</span>/tokenizer/tokenizer.perl</span><br><span class="line">CLEAN=<span class="variable">$SCRIPTS</span>/training/clean-corpus-n.perl</span><br><span class="line">NORM_PUNC=<span class="variable">$SCRIPTS</span>/tokenizer/normalize-punctuation.perl</span><br><span class="line">REM_NON_PRINT_CHAR=<span class="variable">$SCRIPTS</span>/tokenizer/remove-non-printing-char.perl</span><br><span class="line">BPEROOT=subword-nmt</span><br><span class="line">BPE_TOKENS=40000</span><br><span class="line"></span><br><span class="line">URLS=(</span><br><span class="line">    <span class="string">"http://statmt.org/wmt13/training-parallel-europarl-v7.tgz"</span></span><br><span class="line">    <span class="string">"http://statmt.org/wmt13/training-parallel-commoncrawl.tgz"</span></span><br><span class="line">    <span class="string">"http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz"</span></span><br><span class="line">    <span class="string">"http://data.statmt.org/wmt17/translation-task/dev.tgz"</span></span><br><span class="line">    <span class="string">"http://statmt.org/wmt14/test-full.tgz"</span></span><br><span class="line">)</span><br><span class="line">FILES=(</span><br><span class="line">    <span class="string">"training-parallel-europarl-v7.tgz"</span></span><br><span class="line">    <span class="string">"training-parallel-commoncrawl.tgz"</span></span><br><span class="line">    <span class="string">"training-parallel-nc-v12.tgz"</span></span><br><span class="line">    <span class="string">"dev.tgz"</span></span><br><span class="line">    <span class="string">"test-full.tgz"</span></span><br><span class="line">)</span><br><span class="line">CORPORA=(</span><br><span class="line">    <span class="string">"training/europarl-v7.de-en"</span></span><br><span class="line">    <span class="string">"commoncrawl.de-en"</span></span><br><span class="line">    <span class="string">"training/news-commentary-v12.de-en"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This will make the dataset compatible to the one used in "Convolutional Sequence to Sequence Learning"</span></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1705.03122</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$1</span>"</span> == <span class="string">"--icml17"</span> ]; <span class="keyword">then</span></span><br><span class="line">    URLS[2]=<span class="string">"http://statmt.org/wmt14/training-parallel-nc-v9.tgz"</span></span><br><span class="line">    FILES[2]=<span class="string">"training-parallel-nc-v9.tgz"</span></span><br><span class="line">    CORPORA[2]=<span class="string">"training/news-commentary-v9.de-en"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ ! -d <span class="string">"<span class="variable">$SCRIPTS</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"Please set SCRIPTS variable correctly to point to Moses scripts."</span></span><br><span class="line">    <span class="built_in">exit</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">src=en</span><br><span class="line">tgt=de</span><br><span class="line">lang=en-de</span><br><span class="line">prep=wmt14_en_de</span><br><span class="line">tmp=<span class="variable">$prep</span>/tmp</span><br><span class="line">orig=orig</span><br><span class="line">dev=dev/newstest2013</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="variable">$orig</span> <span class="variable">$tmp</span> <span class="variable">$prep</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$orig</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;<span class="variable">$&#123;#URLS[@]&#125;</span>;++i)); <span class="keyword">do</span></span><br><span class="line">    file=<span class="variable">$&#123;FILES[i]&#125;</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="variable">$file</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$file</span> already exists, skipping download"</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        url=<span class="variable">$&#123;URLS[i]&#125;</span></span><br><span class="line">        wget <span class="string">"<span class="variable">$url</span>"</span></span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="variable">$file</span> ]; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"<span class="variable">$url</span> successfully downloaded."</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"<span class="variable">$url</span> not successfully downloaded."</span></span><br><span class="line">            <span class="built_in">exit</span> -1</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">if</span> [ <span class="variable">$&#123;file: -4&#125;</span> == <span class="string">".tgz"</span> ]; <span class="keyword">then</span></span><br><span class="line">            tar zxvf <span class="variable">$file</span></span><br><span class="line">        <span class="keyword">elif</span> [ <span class="variable">$&#123;file: -4&#125;</span> == <span class="string">".tar"</span> ]; <span class="keyword">then</span></span><br><span class="line">            tar xvf <span class="variable">$file</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"pre-processing train data..."</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    rm <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> <span class="string">"<span class="variable">$&#123;CORPORA[@]&#125;</span>"</span>; <span class="keyword">do</span></span><br><span class="line">        cat <span class="variable">$orig</span>/<span class="variable">$f</span>.<span class="variable">$l</span> | \</span><br><span class="line">            perl <span class="variable">$NORM_PUNC</span> <span class="variable">$l</span> | \</span><br><span class="line">            perl <span class="variable">$REM_NON_PRINT_CHAR</span> | \</span><br><span class="line">            perl <span class="variable">$TOKENIZER</span> -threads 8 -a -l <span class="variable">$l</span> &gt;&gt; <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"pre-processing test data..."</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$l</span>"</span> == <span class="string">"<span class="variable">$src</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">        t=<span class="string">"src"</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        t=<span class="string">"ref"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    grep <span class="string">'&lt;seg id'</span> <span class="variable">$orig</span>/<span class="built_in">test</span>-full/newstest2014-deen-<span class="variable">$t</span>.<span class="variable">$l</span>.sgm | \</span><br><span class="line">        sed -e <span class="string">'s/&lt;seg id="[0-9]*"&gt;\s*//g'</span> | \</span><br><span class="line">        sed -e <span class="string">'s/\s*&lt;\/seg&gt;\s*//g'</span> | \</span><br><span class="line">        sed -e <span class="string">"s/\’/\'/g"</span> | \</span><br><span class="line">    perl <span class="variable">$TOKENIZER</span> -threads 8 -a -l <span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/<span class="built_in">test</span>.<span class="variable">$l</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">""</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"splitting train and valid..."</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    awk <span class="string">'&#123;if (NR%100 == 0)  print $0; &#125;'</span> <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/valid.<span class="variable">$l</span></span><br><span class="line">    awk <span class="string">'&#123;if (NR%100 != 0)  print $0; &#125;'</span> <span class="variable">$tmp</span>/train.tags.<span class="variable">$lang</span>.tok.<span class="variable">$l</span> &gt; <span class="variable">$tmp</span>/train.<span class="variable">$l</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">TRAIN=<span class="variable">$tmp</span>/train.de-en</span><br><span class="line">BPE_CODE=<span class="variable">$prep</span>/code</span><br><span class="line">rm -f <span class="variable">$TRAIN</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    cat <span class="variable">$tmp</span>/train.<span class="variable">$l</span> &gt;&gt; <span class="variable">$TRAIN</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"learn_bpe.py on <span class="variable">$&#123;TRAIN&#125;</span>..."</span></span><br><span class="line">python <span class="variable">$BPEROOT</span>/learn_bpe.py -s <span class="variable">$BPE_TOKENS</span> &lt; <span class="variable">$TRAIN</span> &gt; <span class="variable">$BPE_CODE</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> L <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> train.<span class="variable">$L</span> valid.<span class="variable">$L</span> <span class="built_in">test</span>.<span class="variable">$L</span>; <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"apply_bpe.py to <span class="variable">$&#123;f&#125;</span>..."</span></span><br><span class="line">        python <span class="variable">$BPEROOT</span>/apply_bpe.py -c <span class="variable">$BPE_CODE</span> &lt; <span class="variable">$tmp</span>/<span class="variable">$f</span> &gt; <span class="variable">$tmp</span>/bpe.<span class="variable">$f</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">perl <span class="variable">$CLEAN</span> -ratio 1.5 <span class="variable">$tmp</span>/bpe.train <span class="variable">$src</span> <span class="variable">$tgt</span> <span class="variable">$prep</span>/train 1 250</span><br><span class="line">perl <span class="variable">$CLEAN</span> -ratio 1.5 <span class="variable">$tmp</span>/bpe.valid <span class="variable">$src</span> <span class="variable">$tgt</span> <span class="variable">$prep</span>/valid 1 250</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> L <span class="keyword">in</span> <span class="variable">$src</span> <span class="variable">$tgt</span>; <span class="keyword">do</span></span><br><span class="line">    cp <span class="variable">$tmp</span>/bpe.test.<span class="variable">$L</span> <span class="variable">$prep</span>/<span class="built_in">test</span>.<span class="variable">$L</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>如果希望使用预处理好的数据，则可以使用WMT16 EN-DE，地址为：<a href="https://drive.google.com/uc?export=download&amp;id=0B_bZck-ksdkpM25jRUN2X2UxMm8" target="_blank" rel="noopener">https://drive.google.com/uc?export=download&amp;id=0B_bZck-ksdkpM25jRUN2X2UxMm8</a><br>并解压。</p><h4 id="Step2"><a href="#Step2" class="headerlink" title="Step2"></a>Step2</h4><p>接下来对数据进行二值化(binarize):</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">TEXT=wmt16_en_de_bpe32k</span><br><span class="line">mkdir <span class="variable">$TEXT</span></span><br><span class="line">tar -xzvf wmt16_en_de.tar.gz -C <span class="variable">$TEXT</span>  <span class="comment"># 解压文件</span></span><br><span class="line">python preprocess.py --<span class="built_in">source</span>-lang en --target-lang de \</span><br><span class="line">  --trainpref <span class="variable">$TEXT</span>/train.tok.clean.bpe.32000 \</span><br><span class="line">  --validpref <span class="variable">$TEXT</span>/newstest2013.tok.bpe.32000 \</span><br><span class="line">  --testpref <span class="variable">$TEXT</span>/newstest2014.tok.bpe.32000 \</span><br><span class="line">  --destdir data-bin/wmt16_en_de_bpe32k \</span><br><span class="line">  --nwordssrc 32768 --nwordstgt 32768 \</span><br><span class="line">  --joined-dictionary</span><br></pre></td></tr></table></figure><p>到这里，麻烦的预处理就结束了。</p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>cd到fairseq目录下，执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7  python -m torch.distributed.launch --nproc_per_node 8 train.py data-bin/wmt16_en_de_bpe32k \        --arch transformer_wmt_en_de --share-all-embeddings \          --optimizer adam --adam-betas <span class="string">'(0.9, 0.98)'</span> --clip-norm 0.0 \            --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 \              --lr 0.0007 --min-lr 1e-09 \             --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --weight-decay 0.0\              --max-tokens  4096   --save-dir checkpoints/en-de-base\               --no-progress-bar --<span class="built_in">log</span>-format json --<span class="built_in">log</span>-interval 50\             --save-interval-updates  1000 --keep-interval-updates 20</span><br></pre></td></tr></table></figure><p>注意到该设置与原论文不大一致。但已证实该设置可以复现论文结果。</p><p>如果没有这么多卡，那么可以设置<code>update freq</code>以模拟8卡行为。如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=0,1,2,3  python -m torch.distributed.launch --nproc_per_node 4 \</span><br><span class="line">train.py data-bin/wmt16_en_de_bpe32k    \</span><br><span class="line"> --arch transformer_wmt_en_de --share-all-embeddings \</span><br><span class="line">--optimizer adam --adam-betas <span class="string">'(0.9, 0.98)'</span> \</span><br><span class="line">--clip-norm 0.0   --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000  \</span><br><span class="line">--lr 0.0007 --min-lr 1e-09 --criterion label_smoothed_cross_entropy \</span><br><span class="line">--label-smoothing 0.1 --weight-decay 0.0 --max-tokens  4096   \</span><br><span class="line">--save-dir checkpoints/en-de-16-base   \ </span><br><span class="line">--no-progress-bar --<span class="built_in">log</span>-format json --<span class="built_in">log</span>-interval 50 --save-interval-updates  1000 \</span><br><span class="line">--keep-interval-updates 20  --update-freq 2 |tee exp2.log</span><br></pre></td></tr></table></figure><p>4张卡则设<code>update freq=2</code>，2张卡则设<code>update freq=4</code>，以此类推。</p><p>大概在100个epoch内能够收敛(实际上应该在150-200个epoch收敛，100epoch的BLEU是27.3，150-200epoch的结果是27.67)，也即在475000个step。8张1080Ti在大概两天能够训练完成，4张1080Ti大概4天训练完成。</p><p>开始训练…<br><img src="/images/15491934129135.jpg" width="80%" height="50%"></p><p>最后则会获得checkpoint：<br><img src="/images/15491934935157.jpg" width="80%" height="50%"></p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>测试分为几个阶段：首先将几个checkpoint进行平均，实验表明，进行平均能够有一定的提升；其次，使用平均后的模型对test集的句子进行翻译；最终将生成的句子和正确的句子计算bleu值。</p><h4 id="average-checkpoint"><a href="#average-checkpoint" class="headerlink" title="average checkpoint"></a>average checkpoint</h4><p>在测试阶段，论文在Transformer-base中对最后五个checkpoint进行平均，也即对权值进行平均：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python scripts/average_checkpoints.py \</span><br><span class="line">--inputs checkpoints/en-de-base/ \</span><br><span class="line">--num-epoch-checkpoints  5 --output averaged_model.pt</span><br></pre></td></tr></table></figure><p>最终获得averaged_model.pt，我们将用该文件进行测试。</p><h4 id="generate"><a href="#generate" class="headerlink" title="generate"></a>generate</h4><p>我们采用和论文一致的超参：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 python generate.py \</span><br><span class="line">data-bin/wmt16_en_de_bpe32k/ --path /some_checkpoint \</span><br><span class="line">--remove-bpe --beam 4 --batch-size 64 --lenpen 0.6 \</span><br><span class="line">--max-len<span class="_">-a</span> 1 --max-len-b 50|tee generate.out</span><br></pre></td></tr></table></figure><p>其中lenpen是生成句子的长度惩罚系数；<code>max-len-a</code>和<code>max-len-b</code>指的是每个句子的最长长度限制，也即：假设源句子长度为x，则目标句子的长度应小于ax+b 。</p><p>最终我们翻译好的句子以及相对应的详细信息都在generate.out里面。我们需要提取源语言句子和目标语言句子，以方便后面的计算。因此：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">grep ^T generate.out | cut -f2- | perl -ple <span class="string">'s&#123;(\S)-(\S)&#125;&#123;$1 ##AT##-##AT## $2&#125;g'</span> &gt; generate.ref</span><br><span class="line"></span><br><span class="line">grep ^H generate.out |cut -f3- | perl -ple <span class="string">'s&#123;(\S)-(\S)&#125;&#123;$1 ##AT##-##AT## $2&#125;g'</span> &gt; generate.sys</span><br></pre></td></tr></table></figure><p>分别运行这两个bash命令，我们则获得了generate.ref和generate.sys，分别是目标和源语言的句子。</p><p>注意到这里有一个非常重要的小trick，也即<strong>split compound</strong>。因为一些历史原因（我也不知道为啥，tensor2tensor里面的脚本有提到），该trick已经在上面的脚本命令体现出来了。实践证明，使用该trick能够提高bleu值 0.5个点以上。</p><h4 id="score"><a href="#score" class="headerlink" title="score"></a>score</h4><p>我们此时就可以计算bleu值了，fairseq提供了该脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python score.py --sys generate.sys --ref generate.ref</span><br></pre></td></tr></table></figure><p>大功告成！我们终于复现出结果了。<br>作为参考：根据我的实验，只使用checkpoint中最好的一个checkpoint，在经过了上述的流程后，可以得到27.30的结果。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>根据我的需求，我还需要详细记录中间结果，并打印在tensorboard上方便可视化，如：<br><img src="/images/15491946401169.jpg" width="90%" height="50%"></p><p>fairseq并没有提供这种功能，因此需要自己修改部分源代码。<br>只需要修改train.py源文件即可。</p><p>①在开头加summary writer<br><img src="/images/15492019614168.jpg" width="70%" height="50%"></p><p>注意到每次实验都需要修改实验的名字。</p><p>②修改train函数<br>在训练过程中，添加记录的代码：<br><img src="/images/15492020396304.jpg" width="70%" height="50%"></p><p>在epoch结束，添加记录的代码：<br><img src="/images/15492021296611.jpg" width="70%" height="50%"></p><p>对validate的使用进行修改（添加了is_epoch）：<br><img src="/images/15492022604000.jpg" width="70%" height="50%"></p><p>train函数全部代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(args, trainer, task, epoch_itr)</span>:</span></span><br><span class="line">    <span class="string">"""Train the model for one epoch."""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update parameters every N batches</span></span><br><span class="line">    <span class="keyword">if</span> epoch_itr.epoch &lt;= len(args.update_freq):</span><br><span class="line">        update_freq = args.update_freq[epoch_itr.epoch - <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        update_freq = args.update_freq[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize data iterator</span></span><br><span class="line">    itr = epoch_itr.next_epoch_itr(fix_batches_to_gpus=args.fix_batches_to_gpus)</span><br><span class="line">    itr = iterators.GroupedIterator(itr, update_freq)</span><br><span class="line">    progress = progress_bar.build_progress_bar(</span><br><span class="line">        args, itr, epoch_itr.epoch, no_progress_bar=<span class="string">'simple'</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    extra_meters = collections.defaultdict(<span class="keyword">lambda</span>: AverageMeter())</span><br><span class="line">    first_valid = args.valid_subset.split(<span class="string">','</span>)[<span class="number">0</span>]</span><br><span class="line">    max_update = args.max_update <span class="keyword">or</span> math.inf</span><br><span class="line">    <span class="keyword">for</span> i, samples <span class="keyword">in</span> enumerate(progress, start=epoch_itr.iterations_in_epoch):</span><br><span class="line">        log_output = trainer.train_step(samples)</span><br><span class="line">        <span class="keyword">if</span> log_output <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># log mid-epoch stats</span></span><br><span class="line">        stats = get_training_stats(trainer)</span><br><span class="line">        num_updates = stats[<span class="string">'num_updates'</span>]</span><br><span class="line">        <span class="comment"># print(type(num_updates))</span></span><br><span class="line">        <span class="comment"># print(type(stats['loss']))</span></span><br><span class="line">        summary_writer.add_scalar(<span class="string">'Training/training_loss_update'</span>, float(stats[<span class="string">'loss'</span>]), num_updates)</span><br><span class="line">        summary_writer.add_scalar(<span class="string">'Training/training_nll_loss_update'</span>, float(stats[<span class="string">'nll_loss'</span>]), num_updates)</span><br><span class="line">        summary_writer.add_scalar(<span class="string">'Training/training_ppl_update'</span>, float(stats[<span class="string">'ppl'</span>]), num_updates)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ------record training metrics --- #</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> log_output.items():</span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">in</span> [<span class="string">'loss'</span>, <span class="string">'nll_loss'</span>, <span class="string">'ntokens'</span>, <span class="string">'nsentences'</span>, <span class="string">'sample_size'</span>]:</span><br><span class="line">                <span class="keyword">continue</span>  <span class="comment"># these are already logged above</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">'loss'</span> <span class="keyword">in</span> k:</span><br><span class="line">                extra_meters[k].update(v, log_output[<span class="string">'sample_size'</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                extra_meters[k].update(v)</span><br><span class="line">            stats[k] = extra_meters[k].avg</span><br><span class="line">        progress.log(stats)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ignore the first mini-batch in words-per-second calculation</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            trainer.get_meter(<span class="string">'wps'</span>).reset()</span><br><span class="line"></span><br><span class="line">        num_updates = trainer.get_num_updates()</span><br><span class="line">        <span class="keyword">if</span> args.save_interval_updates &gt; <span class="number">0</span> <span class="keyword">and</span> num_updates % args.save_interval_updates == <span class="number">0</span> <span class="keyword">and</span> num_updates &gt; <span class="number">0</span>:</span><br><span class="line">            valid_losses = validate(args, trainer, task, epoch_itr, [first_valid], is_epoch=<span class="keyword">False</span>)</span><br><span class="line">            save_checkpoint(args, trainer, epoch_itr, valid_losses[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> num_updates &gt;= max_update:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># log end-of-epoch stats</span></span><br><span class="line">    stats = get_training_stats(trainer)</span><br><span class="line">    <span class="comment"># ------record training metrics --- #</span></span><br><span class="line">    summary_writer.add_scalar(<span class="string">'Training/training_loss_epoch'</span>, float(stats[<span class="string">'loss'</span>]), epoch_itr.epoch)</span><br><span class="line">    summary_writer.add_scalar(<span class="string">'Training/training_nll_loss_epoch'</span>, float(stats[<span class="string">'nll_loss'</span>]), epoch_itr.epoch)</span><br><span class="line">    summary_writer.add_scalar(<span class="string">'Training/training_ppl_epoch'</span>, float(stats[<span class="string">'ppl'</span>]), epoch_itr.epoch)</span><br><span class="line">    <span class="keyword">for</span> k, meter <span class="keyword">in</span> extra_meters.items():</span><br><span class="line">        stats[k] = meter.avg</span><br><span class="line">    progress.print(stats)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># reset training meters</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> [</span><br><span class="line">        <span class="string">'train_loss'</span>, <span class="string">'train_nll_loss'</span>, <span class="string">'wps'</span>, <span class="string">'ups'</span>, <span class="string">'wpb'</span>, <span class="string">'bsz'</span>, <span class="string">'gnorm'</span>, <span class="string">'clip'</span>,</span><br><span class="line">    ]:</span><br><span class="line">        meter = trainer.get_meter(k)</span><br><span class="line">        <span class="keyword">if</span> meter <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            meter.reset()</span><br></pre></td></tr></table></figure><p>③修改validate函数<br>添加了一个参数<code>is_epoch</code>：<br><img src="/images/15492023879130.jpg" width="50%" height="50%"></p><p>添加记录的代码：<br><img src="/images/15492024686078.jpg" width="90%" height="50%"></p><p>validate全部代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">def validate(args, trainer, task, epoch_itr, subsets, is_epoch=True):</span><br><span class="line">    <span class="string">""</span><span class="string">"Evaluate the model on the validation set(s) and return the losses."</span><span class="string">""</span></span><br><span class="line">    valid_losses = []</span><br><span class="line">    <span class="keyword">for</span> subset <span class="keyword">in</span> subsets:</span><br><span class="line">        <span class="comment"># Initialize data iterator</span></span><br><span class="line">        itr = task.get_batch_iterator(</span><br><span class="line">            dataset=task.dataset(subset),</span><br><span class="line">            max_tokens=args.max_tokens,</span><br><span class="line">            max_sentences=args.max_sentences_valid,</span><br><span class="line">            max_positions=utils.resolve_max_positions(</span><br><span class="line">                task.max_positions(),</span><br><span class="line">                trainer.get_model().max_positions(),</span><br><span class="line">            ),</span><br><span class="line">            ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,</span><br><span class="line">            required_batch_size_multiple=8,</span><br><span class="line">            seed=args.seed,</span><br><span class="line">            num_shards=args.distributed_world_size,</span><br><span class="line">            shard_id=args.distributed_rank,</span><br><span class="line">            num_workers=args.num_workers,</span><br><span class="line">        ).next_epoch_itr(shuffle=False)</span><br><span class="line">        progress = progress_bar.build_progress_bar(</span><br><span class="line">            args, itr, epoch_itr.epoch,</span><br><span class="line">            prefix=<span class="string">'valid on \'</span>&#123;&#125;\<span class="string">' subset'</span>.format(subset),</span><br><span class="line">            no_progress_bar=<span class="string">'simple'</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reset validation loss meters</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> [<span class="string">'valid_loss'</span>, <span class="string">'valid_nll_loss'</span>]:</span><br><span class="line">            meter = trainer.get_meter(k)</span><br><span class="line">            <span class="keyword">if</span> meter is not None:</span><br><span class="line">                meter.reset()</span><br><span class="line">        extra_meters = collections.defaultdict(lambda: AverageMeter())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> sample <span class="keyword">in</span> progress:</span><br><span class="line">            log_output = trainer.valid_step(sample)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> k, v <span class="keyword">in</span> log_output.items():</span><br><span class="line">                <span class="keyword">if</span> k <span class="keyword">in</span> [<span class="string">'loss'</span>, <span class="string">'nll_loss'</span>, <span class="string">'ntokens'</span>, <span class="string">'nsentences'</span>, <span class="string">'sample_size'</span>]:</span><br><span class="line">                    <span class="built_in">continue</span></span><br><span class="line">                extra_meters[k].update(v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># log validation stats</span></span><br><span class="line">        stats = get_valid_stats(trainer)</span><br><span class="line">        <span class="comment"># ------record validate metrics --- #</span></span><br><span class="line">        <span class="keyword">if</span> is_epoch:  <span class="comment"># every epoch</span></span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_loss_epoch'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_loss'</span>]), epoch_itr.epoch)</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_nll_loss_epoch'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_nll_loss'</span>]), epoch_itr.epoch)</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_ppl_epoch'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_ppl'</span>]), epoch_itr.epoch)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># every n update</span></span><br><span class="line">            num_updates = stats[<span class="string">'num_updates'</span>]</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_loss_update'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_loss'</span>]),</span><br><span class="line">                                      num_updates / args.save_interval_updates)</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_nll_loss_update'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_nll_loss'</span>]),</span><br><span class="line">                                      num_updates / args.save_interval_updates)</span><br><span class="line">            summary_writer.add_scalar(<span class="string">'Validation/valid_ppl_update'</span>, <span class="built_in">float</span>(stats[<span class="string">'valid_ppl'</span>]),</span><br><span class="line">                                      num_updates / args.save_interval_updates)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, meter <span class="keyword">in</span> extra_meters.items():</span><br><span class="line">            stats[k] = meter.avg</span><br><span class="line">        progress.print(stats)</span><br><span class="line"></span><br><span class="line">        valid_losses.append(stats[<span class="string">'valid_loss'</span>])</span><br><span class="line">    <span class="built_in">return</span> valid_losses</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://github.com/pytorch/fairseq/tree/master/examples/translation#replicating-results-from-scaling-neural-machine-translation" target="_blank" rel="noopener">Replicating results from “Scaling Neural Machine Translation”</a></p><p><a href="https://github.com/pytorch/fairseq/issues/346" target="_blank" rel="noopener">How to reproduce the result of WMT14 en-de on transformer BASE model?</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> Transformer </tag>
            
            <tag> fairseq </tag>
            
            <tag> NMT </tag>
            
            <tag> 机器翻译 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录15</title>
      <link href="/2019/01/06/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9515/"/>
      <url>/2019/01/06/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9515/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-flatten-multi-dimentional-list"><a href="#1️⃣-flatten-multi-dimentional-list" class="headerlink" title="1️⃣[flatten multi-dimentional list]"></a>1️⃣[flatten multi-dimentional list]</h3><p>对多层嵌套的list进行展平。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 递归</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten</span><span class="params">(nestedList)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">aux</span><span class="params">(listOrItem)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(listOrItem, list):</span><br><span class="line">            <span class="keyword">for</span> elem <span class="keyword">in</span> listOrItem:</span><br><span class="line">                <span class="keyword">for</span> item <span class="keyword">in</span> aux(elem):</span><br><span class="line">                    <span class="keyword">yield</span> item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">yield</span> listOrItem</span><br><span class="line">    <span class="keyword">return</span> list(aux(nestedList))</span><br></pre></td></tr></table></figure><hr><h3 id="2️⃣-sorted-index"><a href="#2️⃣-sorted-index" class="headerlink" title="2️⃣[sorted index]"></a>2️⃣[sorted index]</h3><p>使用内置方法获得排好序的index</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sorted_index=[i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> sorted(enumerate(sent_length),</span><br><span class="line">                                    key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>],</span><br><span class="line">                                    reverse=self.reverse)]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文11</title>
      <link href="/2019/01/06/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8711/"/>
      <url>/2019/01/06/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8711/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-Multi-Head-Attention-with-Disagreement-Regularization"><a href="#1️⃣-Multi-Head-Attention-with-Disagreement-Regularization" class="headerlink" title="1️⃣[Multi-Head Attention with Disagreement Regularization]"></a>1️⃣[Multi-Head Attention with Disagreement Regularization]</h2><p>EMNLP的短文。</p><p>鼓励transformer中head与head之间的差异。</p><p>加了三种正则化方法：<br>①on subspace<br><img src="/images/15467399912055.jpg" width="40%" height="50%"></p><p>②on attention position<br><img src="/images/15467400218650.jpg" width="40%" height="50%"></p><p>③on output<br><img src="/images/15467400417247.jpg" width="40%" height="50%"></p><p>没什么亮点。</p><hr><h2 id="2️⃣-Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overﬁtting"><a href="#2️⃣-Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overﬁtting" class="headerlink" title="2️⃣[Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting]"></a>2️⃣[Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting]</h2><p>经典论文。<br>dropout方法很简单，但如何想到，其背后的intuition，以及一些现象很有启发意义。<br>仅罗列一些intuition/motivation以及现象：</p><ol><li>网络复杂关系学到很多噪声，导致overfitting</li><li>最好的regularization方法是对所有的parameter setting的结果进行average。这就是贝叶斯方法， dropout是对该方法进行近似，论文也提到了model combination</li><li>dropout能够减少unit之间复杂的co-adaptation，能够更鲁棒，也就是说，不需要依赖其他unit去纠正自己的错误。each hidden unit in a neural network trained with dropout must learn to work with a randomly chosen sample of other units. This should make each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes</li><li>dropout的特性：sparsity。标准的网络在训练过程中会固化其他unit的错误，导致复杂的co-adaptation，但这种复杂的adaptation会导致泛化性的降低，因为对于未见到的数据这种复杂的adaptation是没用的。因此dropout的网络中每个unit都要学会自己纠正自己的错误，因此每个unit能够独立学到数据的一部分特性。dropout会导致稀疏化，每次都只会有一小部分的activation高。使用dropout配合高的学习率比较好，因为dropout可能会导致gradient之间互相cancel，同时也可以使用高的momentum。</li></ol><p><img src="/images/15467404963033.jpg" width="80%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> attention </tag>
            
            <tag> dropout </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> regularization </tag>
            
            <tag> transformer </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Pytorch中index_copy_及其思考</title>
      <link href="/2018/12/31/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADindex_copy_%E5%8F%8A%E5%85%B6%E6%80%9D%E8%80%83/"/>
      <url>/2018/12/31/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADindex_copy_%E5%8F%8A%E5%85%B6%E6%80%9D%E8%80%83/</url>
      
        <content type="html"><![CDATA[<p>前几日因为in-place操作的问题，debug了好几天，最终才发现问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">output,_=pad_packed_sequence(output,batch_first=<span class="keyword">True</span>)</span><br><span class="line">output=output.index_copy(<span class="number">0</span>,torch.tensor(sorted_index),output)</span><br></pre></td></tr></table></figure><p>因为Pytorch中pack_sequence需要将batch按长度排列，我在过完GRU后需要将其顺序还原，在这边sorted_index即是记录原来index映射。</p><p>然而我在写的时候，参考的是官方的example：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]], dtype=torch.float)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>index = torch.tensor([<span class="number">0</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.index_copy_(<span class="number">0</span>, index, t)</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure><p>因此我也不假思索地写：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">output,_=pad_packed_sequence(output,batch_first=<span class="keyword">True</span>)</span><br><span class="line">output=output.index_copy_(<span class="number">0</span>,torch.tensor(sorted_index),output)</span><br></pre></td></tr></table></figure></p><p>就因为多了一个_，导致逻辑和我想象中的不一样。</p><p>一个简单的例子展示为什么这么是错的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x=torch.Tensor([<span class="number">21</span>,<span class="number">42</span>,<span class="number">45</span>,<span class="number">59</span>])</span><br><span class="line"></span><br><span class="line">print(x)  <span class="comment"># tensor([21., 42., 45., 59.])</span></span><br><span class="line"></span><br><span class="line">index=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">x=x.index_copy_(<span class="number">0</span>,index,x)</span><br><span class="line"></span><br><span class="line">print(x)  <span class="comment"># tensor([21., 21., 21., 59.])</span></span><br></pre></td></tr></table></figure><p>由于是in-place操作，第一步，将index=0的数值（也即21）复制到index=1的地方，此时变成[21,21,45,59]；接着将index=1的数值复制到index=2的位置上，注意到之前已经是in-place操作，因此此时取的不是想象中的42，而是已经被替换的21。后面的也是如此。</p><p>正确的做法只需要去掉in-place即可。</p><hr><p>已经好几次遇到in-place的问题了，在每次做in-place操作时，都要警惕。应尽可能避免in-place操作。实际上Pytorch官方也不建议使用in-place操作。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> index_coopy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录14</title>
      <link href="/2018/12/29/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9514/"/>
      <url>/2018/12/29/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9514/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-shuffle-list"><a href="#1️⃣-shuffle-list" class="headerlink" title="1️⃣[shuffle list]"></a>1️⃣[shuffle list]</h3><p>shuffle list可以使用random的shuffle函数，亦即：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">l=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">shuffle(l)  <span class="comment"># in place operation</span></span><br></pre></td></tr></table></figure><p>而想要shuffle两个对应list，也即等长且一一对应的list，则可以：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># borrow from stackoverflow</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shuffle</span><span class="params">(a,b)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(a) == len(b)</span><br><span class="line">    start_state = random.getstate()</span><br><span class="line">    random.shuffle(a)</span><br><span class="line">    random.setstate(start_state)</span><br><span class="line">    random.shuffle(b)</span><br><span class="line"></span><br><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]</span><br><span class="line">b = [<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>,<span class="number">19</span>]</span><br><span class="line">shuffle(a,b)</span><br><span class="line">print(a) <span class="comment"># [9, 7, 3, 1, 2, 5, 4, 8, 6]</span></span><br><span class="line">print(b) <span class="comment"># [19, 17, 13, 11, 12, 15, 14, 18, 16]</span></span><br></pre></td></tr></table></figure><hr><h3 id="2️⃣-inverse-tensor"><a href="#2️⃣-inverse-tensor" class="headerlink" title="2️⃣[inverse tensor]"></a>2️⃣[inverse tensor]</h3><p>Pytorch目前还不支持步进为负的情况，因此不能使用类似Python的<code>l[::-1]</code>的方法reverse tensor。<br>一种解决方案：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">inv_idx = torch.arange(tensor.size(<span class="number">0</span>)<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>).long()</span><br><span class="line"><span class="comment"># or equivalently torch.range(tensor.size(0)-1, 0, -1).long()</span></span><br><span class="line">inv_tensor = tensor.index_select(<span class="number">0</span>, inv_idx)</span><br><span class="line"><span class="comment"># or equivalently</span></span><br><span class="line">inv_tensor = tensor[inv_idx]</span><br></pre></td></tr></table></figure><hr><h3 id="3️⃣-GRU-initialization"><a href="#3️⃣-GRU-initialization" class="headerlink" title="3️⃣[GRU initialization]"></a>3️⃣[GRU initialization]</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_gru_init</span><span class="params">(self)</span>:</span>   <span class="comment"># use orthogonal seems better</span></span><br><span class="line">    nn.init.orthogonal_(self.word_RNN.weight_ih_l0.data)  <span class="comment">#没有data不行，会报leaf variable in-place错误，可能weight_ih_l0不是parameter</span></span><br><span class="line">    nn.init.orthogonal_(self.word_RNN.weight_hh_l0.data)</span><br><span class="line">    self.word_RNN.bias_ih_l0.data.zero_()</span><br><span class="line">    self.word_RNN.bias_hh_l0.data.zero_()</span><br></pre></td></tr></table></figure><hr><h3 id="4️⃣-sort-counter"><a href="#4️⃣-sort-counter" class="headerlink" title="4️⃣[sort counter]"></a>4️⃣[sort counter]</h3><p>需求：统计document的句子个数的分布，并按照长度顺序排列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n_sents=[len(sentences) <span class="keyword">for</span> sentences <span class="keyword">in</span> documents]</span><br><span class="line">n_lengths=Counter(n_sents)</span><br><span class="line">n_lengths=sorted(n_lengths.items())</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识17</title>
      <link href="/2018/12/29/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8617/"/>
      <url>/2018/12/29/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8617/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>在有RNN的代码中，如果出现</p><blockquote><p>Cuda Error : RuntimeError: CUDNN_STATUS_EXECUTION_FAILED</p></blockquote><p>那么可能的出错原因是没有将init state放入cuda中。</p><p>Reference: <a href="https://discuss.pytorch.org/t/cuda-error-runtimeerror-cudnn-status-execution-failed/17625" target="_blank" rel="noopener">https://discuss.pytorch.org/t/cuda-error-runtimeerror-cudnn-status-execution-failed/17625</a></p><hr><h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>clone() → Tensor<br>Returns a copy of the self tensor. The copy has the same size and data type as self.<br><strong>Unlike copy_(), this function is recorded in the computation graph. Gradients propagating to the cloned tensor will propagate to the original tensor.</strong></p><p>如果需要另一个相同的tensor做其他计算，则使用clone()而不是copy_()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">forward_vec=sent_vec</span><br><span class="line"><span class="comment"># backward_vec=sent_vec   wrong</span></span><br><span class="line">backward_vec=sent_vec.clone()</span><br></pre></td></tr></table></figure><p>当然也不能直接赋值，因为赋的只是指针，改变backward_vec也会改变原来的值。</p><hr><h3 id="3️⃣-Python"><a href="#3️⃣-Python" class="headerlink" title="3️⃣[Python]"></a>3️⃣[Python]</h3><p>Python中<code>==</code>和<code>is</code>的区别：<br>is表示是否是同一个object；而==表示是否是同一个值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">str=<span class="string">'GRU'</span></span><br><span class="line">str == <span class="string">'GRU'</span>  <span class="comment"># True</span></span><br><span class="line">str <span class="keyword">is</span> <span class="string">'GRU'</span>  <span class="comment"># True</span></span><br><span class="line">str=str.upper()</span><br><span class="line">str == <span class="string">'GRU'</span>  <span class="comment"># False</span></span><br><span class="line">str <span class="keyword">is</span> <span class="string">'GRU'</span>  <span class="comment"># True</span></span><br></pre></td></tr></table></figure><hr><h3 id="4️⃣-RNN"><a href="#4️⃣-RNN" class="headerlink" title="4️⃣[RNN]"></a>4️⃣[RNN]</h3><p>在RNN的初始化中，使用正交初始化会比其他方法好一些（待对比实验测验）。<br>Reference: <a href="https://smerity.com/articles/2016/orthogonal_init.html" target="_blank" rel="noopener">https://smerity.com/articles/2016/orthogonal_init.html</a></p><hr><h3 id="5️⃣-Pytorch"><a href="#5️⃣-Pytorch" class="headerlink" title="5️⃣[Pytorch]"></a>5️⃣[Pytorch]</h3><p>在提供预训练embedding作为初始化时，正确做法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> pretrained_matrix <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">    pretrained_matrix=torch.from_numpy(pretrained_matrix).type(torch.FloatTensor)</span><br><span class="line">    self.embedding.weight= nn.Parameter(pretrained_matrix,</span><br><span class="line">                                                requires_grad=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>必须要有<code>.type(torch.FloatTensor)</code>，否则会出错：CuDNN error: CUDNN_STATUS_EXECUTION_FAILED</p><hr><h3 id="6️⃣-Pytorch"><a href="#6️⃣-Pytorch" class="headerlink" title="6️⃣[Pytorch]"></a>6️⃣[Pytorch]</h3><p>Pytorch中，将初始hidden state作为可学习参数实践：<br><a href="https://discuss.pytorch.org/t/solved-train-initial-hidden-state-of-rnns/2589/9" target="_blank" rel="noopener">https://discuss.pytorch.org/t/solved-train-initial-hidden-state-of-rnns/2589/9</a><br><a href="https://discuss.pytorch.org/t/learn-initial-hidden-state-h0-for-rnn/10013/7" target="_blank" rel="noopener">https://discuss.pytorch.org/t/learn-initial-hidden-state-h0-for-rnn/10013/7</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>A Day with Google</title>
      <link href="/2018/12/23/%E8%A7%81%E9%97%BB&amp;%E6%83%B3%E6%B3%95/A%20Day%20with%20Google/"/>
      <url>/2018/12/23/%E8%A7%81%E9%97%BB&amp;%E6%83%B3%E6%B3%95/A%20Day%20with%20Google/</url>
      
        <content type="html"><![CDATA[<p>本周一乡下人终于再次进城了🙈<br><img src="/images/IMG_2243.jpg" width="70%" height="50%"></p><p><img src="/images/IMG_8631.jpg" width="70%" height="50%"></p><p>本次的目的是来参观Google。</p><p>高楼林立：<br><img src="/images/IMG_2273.jpg" width="70%" height="50%"></p><p>Here We are:<br><img src="/images/IMG_9209-1.jpg" width="70%" height="50%"></p><p>咕果是什么鬼？<br><img src="/images/IMG_3389.jpg" width="70%" height="50%"></p><p>宣讲：<br><img src="/images/IMG_1782.jpg" width="70%" height="50%"></p><p><img src="/images/IMG_1075.jpg" width="70%" height="50%"></p><p>不得不感慨食堂真好🦆，还有专门吃面的食堂。而且还都不用钱🙉，对比张江的食堂🙉：</p><p><img src="/images/IMG_0546.jpg" width="70%" height="50%"></p><p>溜了溜了：<br><img src="/images/IMG_1255.jpg" width="70%" height="50%"></p><p><img src="/images/IMG_1256.jpg" width="70%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 活动 </tag>
            
            <tag> Google </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文10</title>
      <link href="/2018/12/23/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8710/"/>
      <url>/2018/12/23/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8710/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-Regularization-of-Neural-Networks-using-DropConnect"><a href="#1️⃣-Regularization-of-Neural-Networks-using-DropConnect" class="headerlink" title="1️⃣[Regularization of Neural Networks using DropConnect]"></a>1️⃣[Regularization of Neural Networks using DropConnect]</h2><p>在dropout的基础上提出dropconnect。与dropout不同的是，dropconnect对weight进行drop而不是对layer进行drop。</p><p>创新之处在于inference的时候和dropout不同。</p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p><img src="/images/15455297378934.jpg" width="50%" height="50%"></p><h3 id="inference"><a href="#inference" class="headerlink" title="inference"></a>inference</h3><p><img src="/images/15455297645976.jpg" width="50%" height="50%"></p><p>在inference的时候通过高斯采样的方法去模拟训练时的伯努利分布。<br><strong>intuition</strong>：<br>本文对dropout在inference简单对unit进行缩放进行反思，认为这在数学上并不合理，因此提出用高斯分布去采样。<br><img src="/images/15455299241433.jpg" width="50%" height="50%"></p><p><img src="/images/15455299403032.jpg" width="50%" height="50%"></p><p><img src="/images/15455299548705.jpg" width="50%" height="50%"></p><hr><h2 id="2️⃣-Attentive-Pooling-Networks"><a href="#2️⃣-Attentive-Pooling-Networks" class="headerlink" title="2️⃣[Attentive Pooling Networks]"></a>2️⃣[Attentive Pooling Networks]</h2><p>提出attentive pooling机制，用以answer selection。<br>（什么是answer selection：给定一个问题，给定多个答案候选，要从答案选项中选择正确的答案。）</p><p>传统answer selection：<br><img src="/images/15455301265939.jpg" width="35%" height="50%"><br>首先将词转化成词向量，接着通过bi-LSTM或CNN获得一个矩阵表示，接下来对Q和A分别进行max-pooling获得固定表示，最后通过cos距离判断答案是否是正确答案，从答案候选中选择分数最高的。</p><p>但这样的问题在于Q和A之间没有交互。</p><p>本文利用attention作为Q和A的交互。<br><img src="/images/15455301891043.jpg" width="39%" height="50%"></p><p>获得Q和A矩阵的方式是一致的。<br>接下来，首先计算一个G矩阵，通过双线性attention公式获得：<br><img src="/images/15455302279543.jpg" width="20%" height="50%"></p><p>G所代表的意义是Q和A的每个词之间的对齐：对于第i行来说，代表Q的第i个词和A中所有词的一个分数；对于第j列来说，代表第j个词和Q中所有词的分数。</p><p>接下来对G的行和列分别进行max-pooling操作：<br><img src="/images/15455303089243.jpg" width="25%" height="50%"></p><p>此步代表选择与某词关系最重要的词。</p><p>接下来对g分别进行softmax，再分别进行点积以获得最终向量表示：<br><img src="/images/15455303516483.jpg" width="13%" height="50%"></p><p>同样，最终使用cos距离计算相似度。</p><hr><h2 id="3️⃣-Improved-Regularization-of-Convolutional-Neural-Networks-with-Cutout"><a href="#3️⃣-Improved-Regularization-of-Convolutional-Neural-Networks-with-Cutout" class="headerlink" title="3️⃣[Improved Regularization of Convolutional Neural Networks with Cutout]"></a>3️⃣[Improved Regularization of Convolutional Neural Networks with Cutout]</h2><p>是从数据增强和dropout的角度：</p><blockquote><p>dropout in convolutional layers simply acts to increase robustness to noisy inputs, rather than having the same model averaging effect that is observed in fully-connected layers</p></blockquote><p>某个输入被移去，所有后面相关的的feature map都被移去：</p><blockquote><p>In this sense, cutout is much closer to data augmentation than dropout, as it is not creating noise, but instead generating images that appear novel to the network</p></blockquote><p>其实只是将输入随机drop掉一块。<br><img src="/images/15455304317998.jpg" width="50%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> DropConnect </tag>
            
            <tag> Cutout </tag>
            
            <tag> Attentive Pooling </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录13</title>
      <link href="/2018/12/23/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9513/"/>
      <url>/2018/12/23/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9513/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-flatten-list"><a href="#1️⃣-flatten-list" class="headerlink" title="1️⃣[flatten list]"></a>1️⃣[flatten list]</h3><p>对二维list进行展开。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">list2d = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>], [<span class="number">8</span>,<span class="number">9</span>]]</span><br><span class="line"><span class="comment"># ①</span></span><br><span class="line">flatten = [l <span class="keyword">for</span> list <span class="keyword">in</span> list2d <span class="keyword">for</span> l <span class="keyword">in</span> list]</span><br><span class="line"><span class="comment"># ②</span></span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line">merged = list(itertools.chain(*list2d))</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">merged = list(itertools.chain.from_iterable(list2d))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识16</title>
      <link href="/2018/12/23/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8616/"/>
      <url>/2018/12/23/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8616/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Softmax"><a href="#1️⃣-Softmax" class="headerlink" title="1️⃣[Softmax]"></a>1️⃣[Softmax]</h3><p>在使用softmax的时候，要非常注意softmax的行为。应尽量控制softmax前元素的规模，否则容易出现one-hot的情况，导致训练困难。<br><img src="/images/15455275366030.jpg" width="70%" height="50%"></p><p>同时，对全-inf做softmax是未定义的，因此也会出现问题：<br><img src="/images/15455278529550.jpg" width="40%" height="50%"></p><hr><h3 id="2️⃣-slice"><a href="#2️⃣-slice" class="headerlink" title="2️⃣[slice]"></a>2️⃣[slice]</h3><p>在对tensor或array操作时，如果需要取某维的slice：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.rand(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">a[:,<span class="number">1</span>:<span class="number">3</span>]  <span class="comment"># 取第1列到第2列的slice</span></span><br><span class="line">a[:][<span class="number">1</span>:<span class="number">3</span>] <span class="comment"># wrong，获得的是第1行到第2行的slice</span></span><br></pre></td></tr></table></figure><p>原因是，<code>a[:][1:3]</code>是先做<code>a[:]</code>操作，获得了全部元素，然后再做<code>[1:3]</code>操作，也即获得第1行到第2行的元素。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> Softmax </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录12</title>
      <link href="/2018/12/16/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9512/"/>
      <url>/2018/12/16/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9512/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-CUDA-time"><a href="#1️⃣-CUDA-time" class="headerlink" title="1️⃣[CUDA time]"></a>1️⃣[CUDA time]</h3><p>正确测试代码在cuda运行时间。需要加上<code>torch.cuda.synchronize()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">a = torch.randint(high=<span class="number">1000</span>, size=(<span class="number">20</span>, <span class="number">200</span>, <span class="number">256</span>)).double().cuda()</span><br><span class="line">b = torch.randint(high=<span class="number">1000</span>, size=(<span class="number">20</span>, <span class="number">200</span>, <span class="number">256</span>)).double().cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">torch.cuda.synchronize()</span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line">M = torch.bmm(a, b.transpose(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">torch.cuda.synchronize()</span><br><span class="line">end = time.time()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"bmm"</span>, end - start)</span><br><span class="line">print(<span class="string">"max_mem"</span>, torch.cuda.max_memory_allocated())</span><br><span class="line"></span><br><span class="line">torch.cuda.synchronize()</span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line">local_a = a.unsqueeze(<span class="number">2</span>)</span><br><span class="line">local_b = b.unsqueeze(<span class="number">1</span>)</span><br><span class="line">N = (local_a*local_b).sum(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">torch.cuda.synchronize()</span><br><span class="line">end = time.time()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"element-wise"</span>, end - start)</span><br><span class="line">print(<span class="string">"max_mem"</span>, torch.cuda.max_memory_allocated())</span><br><span class="line"></span><br><span class="line">print(<span class="string">"output difference (should be 0)"</span>, (N - M).abs().max())</span><br><span class="line">print(<span class="string">"In single precision this can fail because of the size of the tensors."</span>)</span><br><span class="line">print(<span class="string">"Using double should always work"</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识15</title>
      <link href="/2018/12/16/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8615/"/>
      <url>/2018/12/16/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8615/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>在0.41的pytorch中，bernoulli的速度会比随机sample的速度慢很多；<br>在1.0中修复了该bug，但速度上还是随机sample快一点点。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Pytorch0.41</span><br><span class="line">Bernoulli  0.430371046066</span><br><span class="line">sample  0.24411702156</span><br><span class="line"></span><br><span class="line"># Pytorch1.0</span><br><span class="line">Bernoulli  0.256921529</span><br><span class="line">sample  0.25317035184</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下二者等价</span></span><br><span class="line">mask = Bernoulli(gamma).sample(x.size()) <span class="comment"># slow</span></span><br><span class="line">mask = (torch.rand_like(x)&lt;gamma).float() <span class="comment"># faster</span></span><br></pre></td></tr></table></figure><p>Reference:<br><a href="https://github.com/pytorch/pytorch/issues/6940" target="_blank" rel="noopener">https://github.com/pytorch/pytorch/issues/6940</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文9</title>
      <link href="/2018/12/16/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%879/"/>
      <url>/2018/12/16/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%879/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-Sentence-State-LSTM-for-Text-Representation"><a href="#1️⃣-Sentence-State-LSTM-for-Text-Representation" class="headerlink" title="1️⃣[Sentence-State LSTM for Text Representation]"></a>1️⃣[Sentence-State LSTM for Text Representation]</h2><p>提出一种新型的encode句子的方法。有点类似gather-distribute的想法。</p><p><img src="/images/15449283844319.jpg" width="45%" height="50%"></p><p>每个时间步t所有的h一起更新。更新方式是与其左右的点进行交互，同时与一个global representation进行交互。这样即考虑了local的信息也考虑了global的信息。每次更新都增加了信息交互，从3gram到5gram再到7gram…</p><p>具体来说：<br>①如何求$h_i$<br><img src="/images/15449285619763.jpg" width="45%" height="50%"></p><p>从公式可以看出，对于一个特定的$h_i$，同时考虑左右两点，以及global信息$g$，以及输入$x$。</p><p>②如何求g<br><img src="/images/15449286595709.jpg" width="50%" height="50%"></p><p>通过average同时考虑所有的词，同时考虑自己上一个状态。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> LSTM </tag>
            
            <tag> Encode </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python中的+=操作</title>
      <link href="/2018/12/09/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Python%E4%B8%AD%E7%9A%84+=%E6%93%8D%E4%BD%9C/"/>
      <url>/2018/12/09/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Python%E4%B8%AD%E7%9A%84+=%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>前几日在写一段Pytorch代码时，又一次遇到了in-place操作的问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output+=pos  <span class="comment"># pos是不可更新的tensor，output是可更新的tensor</span></span><br></pre></td></tr></table></figure><p>程序报错：“one of the variables needed for gradient computation has been modified by an inplace operation”。</p><p>无意中将代码改成<code>output=output+pos</code>，程序就不会报错了。</p><p>在查阅了相关资料后，将我的思考整理下来。</p><p>在Python中，<code>i=i+1</code>和<code>i+=1</code>是不同的，如果被操作数没有部署 ’<strong>iadd</strong>‘方法，则<code>i=i+1</code>和<code>i+=1</code>是等价的，’+=‘并不会产生in-place操作；当被操作数有部署该方法且正确部署，则是会产生in-place操作的。当没有in-place操作时，<code>i=i+1</code>表示对i重分配，也即i指向了另一个空间而不是原来的空间。</p><p>所以，这样的例子就能解释清楚了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A = np.arange(<span class="number">12</span>).reshape(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> A:</span><br><span class="line">    a = a + <span class="number">1</span></span><br><span class="line"><span class="comment"># A并没有被改变</span></span><br><span class="line">B = np.arange(<span class="number">12</span>).reshape(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> b <span class="keyword">in</span> B:</span><br><span class="line">    b += <span class="number">1</span></span><br><span class="line"><span class="comment"># B被改变了</span></span><br></pre></td></tr></table></figure><p>在Pytorch中，也有部署’<strong>iadd</strong>()‘操作，所以对于<code>output+=pos</code>，output内部的值被改变了，也即在计算图中引入了环，在反向求导时则会出错。</p><p>因此，在Pytorch中，应当避免in-place的操作。</p><p>Reference:<br><a href="https://stackoverflow.com/questions/41446833/what-is-the-difference-between-i-i-1-and-i-1-in-a-for-loop" target="_blank" rel="noopener">https://stackoverflow.com/questions/41446833/what-is-the-difference-between-i-i-1-and-i-1-in-a-for-loop</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文8</title>
      <link href="/2018/12/09/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%878/"/>
      <url>/2018/12/09/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%878/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-DiSAN-Directional-Self-Attention-Network-for-RNN-CNN-Free-Language-Understanding"><a href="#1️⃣-DiSAN-Directional-Self-Attention-Network-for-RNN-CNN-Free-Language-Understanding" class="headerlink" title="1️⃣[DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding]"></a>1️⃣[DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding]</h2><p>提出了两种attention机制，即 multi-dimentional attention和directional self-attention，在此基础上提出有向自注意力网络（directional self-attention network)</p><h3 id="Multi-dimensional-Attention"><a href="#Multi-dimensional-Attention" class="headerlink" title="Multi-dimensional Attention"></a>Multi-dimensional Attention</h3><p>与传统的方法不同的是，对于每个词对，attention出来的不是标量而是向量。<br><img src="/images/15443239891184.jpg" width="60%" height="50%"></p><p>计算公式：<br><img src="/images/15443240335179.jpg" width="40%" height="50%"></p><p>$f$的维度与$q$相同，每一维代表的是$x_i$在该维对$q$的重要性。也即feature-wise的attention。因此对于$q$而言，其获得的加权求和向量为：<br><img src="/images/15443241367138.jpg" width="55%" height="50%"></p><p>使用feature-wise的attention能够解决一次多义的问题，因为能够计算每一维的重要性，在不同的context下有不同的重要性。</p><p>将其应用于self-attention中，有两种变体：<br>①token2token<br><img src="/images/15443242128118.jpg" width="58%" height="50%"></p><p><img src="/images/15443242249947.jpg" width="20%" height="50%"></p><p>因此x在交互完有：<br><img src="/images/15443242733208.jpg" width="35%" height="50%"></p><p>②source2token<br><img src="/images/15443243090328.jpg" width="40%" height="50%"></p><p>也即$x_i$没有和其他元素有交互。<br>可用作获得sentence encoding：<br><img src="/images/15443243968731.jpg" width="20%" height="50%"></p><h3 id="Directional-Self-Attention"><a href="#Directional-Self-Attention" class="headerlink" title="Directional Self-Attention"></a>Directional Self-Attention</h3><p>使用mask达到有向性这一目的：<strong>通过mask矩阵将位置/方向编码进attention，解决时序丢失问题</strong>。<br>首先将x过一层获得新的h表示：<br><img src="/images/15443244421489.jpg" width="27%" height="50%"></p><p>接着使用token2token求attention，这里为了减少参数作了一定改动，将W换成c，tanh替换σ。<br><img src="/images/15443245099821.jpg" width="53%" height="50%"></p><p>$\textbf{1}$是全1的向量。M就是mask矩阵，代表i与j是否连通，Mask矩阵有：<br><img src="/images/15443248745747.jpg" width="28%" height="50%"></p><p><img src="/images/15443248908199.jpg" width="31%" height="50%"></p><p>也即：<br><img src="/images/15443249388350.jpg" width="50%" height="50%"></p><p>首先mask掉自己，第二：分别mask掉forward和backward，类似biLSTM，只和前面或后面的交互。</p><h3 id="Directional-Self-Attention-Network"><a href="#Directional-Self-Attention-Network" class="headerlink" title="Directional Self-Attention Network"></a>Directional Self-Attention Network</h3><p>在上述两个方法的基础上，此时已获得了上下文相关的$s_i$，再引入fusion gate：<br><img src="/images/15443250617220.jpg" width="45%" height="50%"></p><p>整个流程：<br><img src="/images/15443250338890.jpg" width="50%" height="50%"></p><p>将前向和反向的表示拼接起来，获得最终的表示$[u^{fw};u^{bw}]$：<br><img src="/images/15443251919096.jpg" width="50%" height="50%"></p><p>对于所获得的每一个表示，通过source2token，获得最终的句子表示。</p><p>这一点论文也提到了，非常类似bi-LSTM。</p><hr><h2 id="2️⃣-Targeted-Dropout"><a href="#2️⃣-Targeted-Dropout" class="headerlink" title="2️⃣[Targeted Dropout]"></a>2️⃣[Targeted Dropout]</h2><p>一种网络剪枝方法，想法简单易实现。<br>简单说，在每次更新时对最不重要的weight或者unit进行随机dropout。</p><h3 id="Targeted-Dropout"><a href="#Targeted-Dropout" class="headerlink" title="Targeted Dropout"></a>Targeted Dropout</h3><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>给定输入X，权重W，输出Y M为dropout的mask矩阵。<br>unit dropout：<br><img src="/images/15443218016315.jpg" width="22%" height="50%"></p><p>weight dropout：<br><img src="/images/15443218315803.jpg" width="22%" height="50%"></p><p>也即drop掉的是layer之间的connection。</p><h4 id="Magnitude-based-pruning"><a href="#Magnitude-based-pruning" class="headerlink" title="Magnitude-based pruning"></a>Magnitude-based pruning</h4><p>剪枝通常对权重最小的进行剪枝，也即保留topk个最大的权重。</p><p>Unit pruning：直接剪掉的是一整列，也即一个unit<br><img src="/images/15443218793541.jpg" width="43%" height="50%"></p><p>Weight pruning：对W的每个元素进行剪枝。注意是对每行的topk进行保留<br><img src="/images/15443219325533.jpg" width="58%" height="50%"></p><p>可以理解成对一个unit来说，保留最高的k个connection。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>结合dropout和剪枝。<br>主要思想：首先选择N-k最不重要的element，由于我们希望这些low-value的元素有机会在训练过程中变得重要，因此我们对这些element进行随机dropout。</p><p>引入targeting proportion γ和drop probability α，亦即：选择最低的γ|θ|个weight，再根据α进行dropout。<br>这样做的结果是：减少重要的子网络对不重要的子网络的依赖。</p><h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><p>①dropout的intuition：减少unit之间的相互适应。when dropout is applied to a unit, the remaining network can no longer depend on that unit’s contribution to the function and must learn to propagate that unit’s information through a more reliable channel。<br>也可以理解成：使得unit之间的交互信息达到最大，在失去某个unit的时候影响不会那么大。</p><p>②targeted dropout intuition：the important subnetwork is completely separated from the unimportant one。假设一个网络由两个不相交的子网络组成，每个都能输出正确的结果，总的网络是这两个网络的平均。我们通过对不重要的子网络进行dropout（也即往子网络里加noise，会破坏该子网络的输出，由于重要的子网络已经能够输出正确的结果，因此为了减少损失，我们需要减少不重要网络的输出到0，也即kill掉该子网络，并且加强这两个网络的分离。（为什么不直接舍弃呢？因为是在训练过程中，有可能会有变化）<br>这个解释还是没完全懂。</p><hr><h2 id="3️⃣-A2-Nets-Double-Attention-Networks"><a href="#3️⃣-A2-Nets-Double-Attention-Networks" class="headerlink" title="3️⃣[A2-Nets: Double Attention Networks]"></a>3️⃣[A2-Nets: Double Attention Networks]</h2><p>发表于NIPS2018，个人认为很有启发。提出一种新的attention机制，基于“收集-分发”的思想，能够让CNN获得更大的感受野。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>CNN本身主要是捕获局部特征与关系，但对于长距离之间的关系只能通过堆叠多几层才能实现。但这样需要更高的计算量，且容易过拟合；同时，远处的特征实际上是来自好几层的延迟，导致推理的困难。</p><p>通过将feature收集起来，然后分发下去，使得feature之间有交互，让CNN获得更大的感受野，能够捕获长距离的特征。</p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p><img src="/images/15443223814542.jpg" width="80%" height="50%"></p><p>也即：<br><img src="/images/15443224194279.jpg" width="34%" height="50%"></p><p>X是所有输入，$v_i是$local feature。</p><h4 id="The-First-Attention-Step-Feature-Gathering"><a href="#The-First-Attention-Step-Feature-Gathering" class="headerlink" title="The First Attention Step: Feature Gathering"></a>The First Attention Step: Feature Gathering</h4><p>对于两个feature map A,B，有：<br><img src="/images/15443225280678.jpg" width="40%" height="50%"></p><p>其中：<br><img src="/images/15443226145868.jpg" width="35%" height="50%"></p><p><img src="/images/15443226262919.jpg" width="35%" height="50%"></p><p>如果A、B都来自同一个X，将B归一化softmax，就类似transformer的attention。其中上式的最右边是外积的形式。</p><p>我们将G拆分成向量形式：<br><img src="/images/15443226708983.jpg" width="33%" height="50%"><br>同时将B重写成行向量形式，则有：<br><img src="/images/15443227241595.jpg" width="22%" height="50%"></p><p>则会有：<br><img src="/images/15443227868015.jpg" width="28%" height="50%"></p><p>上式让我们有一个新的理解角度：G实际上就是 a bag of visual primitives。每个$g_i$是所有local feature加权求和，其中$b_i$是求和的weight。</p><p>因此我们对B做softmax，保证权重为1：<br><img src="/images/15443228682403.jpg" width="28%" height="50%"></p><h4 id="The-Second-Attention-Step-Feature-Distribution"><a href="#The-Second-Attention-Step-Feature-Distribution" class="headerlink" title="The Second Attention Step: Feature Distribution"></a>The Second Attention Step: Feature Distribution</h4><p>在获得了全局的feature G后，现在根据local feature去获取全局feature的部分，这通过一个权重控制，也即$v_i$（local feature)的每一维作为权重。可以不将local feature $v_i$归一化，但归一化能更好地converge。</p><h4 id="The-Double-Attention-Block"><a href="#The-Double-Attention-Block" class="headerlink" title="The Double Attention Block"></a>The Double Attention Block</h4><p>最终得到double attention block：<br><img src="/images/15443230115907.jpg" width="68%" height="50%"></p><p>整个流程：<br><img src="/images/15443230641691.jpg" width="80%" height="50%"></p><p>所以其实是有三个convolution layer。</p><p>上式还可以写成：<br><img src="/images/15443232642402.jpg" width="70%" height="50%"><br>数学上等价，但计算上差很多。第一个式子会有更低的复杂度。</p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>虽然用了attention，但这里和Transformer还是有非常大的区别的。Transformer每个元素都和其他元素有交互，通过直接的计算得到权重。而这边的权重由feature本身来决定。并没有直接的交互。</p><hr>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> attention </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> Dropout </tag>
            
            <tag> DiSAN </tag>
            
            <tag> Targeted Dropout </tag>
            
            <tag> double attention </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识14</title>
      <link href="/2018/12/09/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8614/"/>
      <url>/2018/12/09/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8614/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>Pytorch的tensor和Tensor是有区别的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.tensor(<span class="number">2</span>)  <span class="comment"># 是标量，size为[]</span></span><br><span class="line">b = torch.Tensor(<span class="number">2</span>)  <span class="comment"># 是向量，size为[2]</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词17</title>
      <link href="/2018/12/09/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D17/"/>
      <url>/2018/12/09/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D17/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣虞美人"><a href="#1️⃣虞美人" class="headerlink" title="1️⃣虞美人"></a>1️⃣虞美人</h3><p>[宋] 叶梦得<br>落花已作风前舞，又送黄昏雨。晓来庭院半残红，惟有游丝，千丈袅晴空。<br>殷勤花下同携手，更尽杯中酒。美人不用敛蛾眉，<strong>我亦多情，无奈酒阑时</strong>。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识13</title>
      <link href="/2018/12/02/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8613/"/>
      <url>/2018/12/02/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8613/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-attention"><a href="#1️⃣-attention" class="headerlink" title="1️⃣[attention]"></a>1️⃣[attention]</h3><p>所有attention的总结：<br><img src="/images/15437180657954.jpg" width="70%" height="50%"><br><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank" rel="noopener">Attention? Attention!</a></p><hr><h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>①torch.no_grad能够显著减少内存使用，model.eval不能。因为eval不会关闭历史追踪。</p><blockquote><p>model.eval() will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval model instead of training mode.<br>torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script).</p></blockquote><p>Reference:<br><a href="https://discuss.pytorch.org/t/does-model-eval-with-torch-set-grad-enabled-is-train-have-the-same-effect-for-grad-history/17183/3" target="_blank" rel="noopener">Does model.eval() &amp; with torch.set_grad_enabled(is_train) have the same effect for grad history?</a></p><p><a href="https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615" target="_blank" rel="noopener">‘model.eval()’ vs ‘with torch.no_grad()’</a></p><p>②torch.full(…) returns a tensor filled with value.</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> attention </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录11</title>
      <link href="/2018/12/02/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9511/"/>
      <url>/2018/12/02/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9511/</url>
      
        <content type="html"><![CDATA[<h2 id="①"><a href="#①" class="headerlink" title="①"></a>①</h2><p>需求：对于两个向量$a$、$b$，$a,b \in R^d$，定义一种减法，有：</p><script type="math/tex; mode=display">a-b=M</script><p>其中$M \in R^{d\times d}$，$M_{ij}=a_i-b_j$</p><p>在代码中实际的维度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=torch.rand(batch_size,sequence_len,dim)</span><br><span class="line">b=torch.rand(batch_size,sequence_len,dim)</span><br></pre></td></tr></table></figure><p>方法①：for循环</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">M=torch.zeros(bz,seq_len,seq_len)</span><br><span class="line"><span class="keyword">for</span> b_i <span class="keyword">in</span> range(bz):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(seq_len):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(seq_len):</span><br><span class="line">            M_ij=torch.norm(a[b_i][i]-b[b_i][j])</span><br><span class="line">            M[b][i][j]=M_ij</span><br></pre></td></tr></table></figure><p>方法②：矩阵运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=a.unsqueeze(<span class="number">2</span>)  <span class="comment"># bz,seq_len,1,dim</span></span><br><span class="line">b=b.unsqueeze(<span class="number">1</span>)  <span class="comment"># bz,1,seq_lens,dim</span></span><br><span class="line">M=torch.norm(a-b,dim=<span class="number">-1</span>)   <span class="comment"># will broadcast</span></span><br></pre></td></tr></table></figure><hr><h2 id="②"><a href="#②" class="headerlink" title="②"></a>②</h2><p>需求，生成一个mask矩阵，每一行有一段连续的位置填充1，其中每一行填充1的开始位置和结束位置都不同。具体来说，先生成一个中心位置center，则开始位置为center-window；结束位置为center+window。其中开始位置和结束位置不能越界，也即不小于0和大于行的总长度。<br>如：<br><img src="/images/15437208061953.jpg" width="25%" height="50%"></p><p>思路：<br>①先生成n行每行对应的随机中心位置，然后再获得左和右边界</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">centers=torch.randint(low=<span class="number">0</span>,high=query_len,size=(query_len,),dtype=torch.long)</span><br><span class="line"></span><br><span class="line">left=centers-self.window</span><br><span class="line">left=torch.max(left,torch.LongTensor([<span class="number">0</span>])).unsqueeze(<span class="number">1</span>)   <span class="comment"># query_len,1</span></span><br><span class="line"></span><br><span class="line">right=centers+self.window</span><br><span class="line">right=torch.min(right,torch.LongTensor([query_len<span class="number">-1</span>])).unsqueeze(<span class="number">1</span>)  <span class="comment"># query_len,1</span></span><br></pre></td></tr></table></figure><p>②生成一个每行都用[0,n-1]填充的矩阵，[0,n-1]表示的是该元素的index，亦即：<br><img src="/images/15437212363142.jpg" width="25%" height="50%"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">range_matrix=torch.range(<span class="number">0</span>,query_len<span class="number">-1</span>,dtype=torch.long).unsqueeze(<span class="number">0</span>).expand(query_len,<span class="number">-1</span>)  <span class="comment"># query_len,query_len</span></span><br></pre></td></tr></table></figure><p>③利用&lt;=和&gt;=获得一个左边界和右边界矩阵，左边界矩阵表示在该左边界的左边都是填充的1；右边界矩阵表示在该右边界右边都是填充的1。再进行异或操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">range_matrix=torch.range(<span class="number">0</span>,query_len<span class="number">-1</span>,dtype=torch.long).unsqueeze(<span class="number">0</span>).expand(query_len,<span class="number">-1</span>)  <span class="comment"># query_len,query_len</span></span><br><span class="line">left_matrix=range_matrix&lt;=left</span><br><span class="line">right_matrix=range_matrix&lt;=right</span><br><span class="line">final_matrix=left_matrix^right_matrix</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文7</title>
      <link href="/2018/12/02/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%877/"/>
      <url>/2018/12/02/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%877/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-Convolutional-Self-Attention-Network"><a href="#1️⃣-Convolutional-Self-Attention-Network" class="headerlink" title="1️⃣[Convolutional Self-Attention Network]"></a>1️⃣[Convolutional Self-Attention Network]</h2><p>对self-attention进行改进，引入CNN的local-bias，也即对query的邻近词进行attention而不是所有词；将self-attention扩展到2D，也即让不同的head之间也有attention交互。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>1️⃣the normalization in Softmax may inhibits the attention to neighboring information 也即邻居的信息更重要，要加强邻居的重要性</p><p>2️⃣features can be better captured by modeling dependencies across different channels 对于不同的channel/head也增加他们之间的交互。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="/images/15437126353758.jpg" width="80%" height="50%"></p><p>对于1D的convolution：选取中心词周围一个window：<br><img src="/images/15437128149700.jpg" width="28%" height="50%"></p><p>对于2D的convolution，则有：<br><img src="/images/15437128476725.jpg" width="45%" height="50%"></p><p>在具体实践中，只对前三层添加local bias，这是因为modeling locality在底层更有效，对于高层应该捕获更远的信息。</p><hr><h2 id="2️⃣-Modeling-Localness-for-Self-Attention-Networks"><a href="#2️⃣-Modeling-Localness-for-Self-Attention-Networks" class="headerlink" title="2️⃣[Modeling Localness for Self-Attention Networks]"></a>2️⃣[Modeling Localness for Self-Attention Networks]</h2><p>和上文一样，引入local bias对self-attention进行改进，从而提升了翻译表现。和上文是同一作者，发在EMNLP上。</p><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>1️⃣self-attention存在的问题：虽然能够增加长程关注，但因此会导致注意力的分散，对邻居的信号会忽略。实践证明，对local bias建模在self-attention有提升。</p><p>2️⃣从直觉上来说，在翻译模型中，当目标词i与源语言词j有对齐关系时，我们希望词i能同时对词j周围的词进行对齐，使得能够捕获上下文信息，如phrase的信息。</p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>在原来的公式上添加G：<br><img src="/images/15437133932791.jpg" width="45%" height="50%"><br>也即：<br><img src="/images/15437134105761.jpg" width="70%" height="50%"></p><p>G是一个alignment position matrix（对齐位置矩阵），元素ij代表目标词i与源语言词j之间的紧密程度。<br>我们每次根据目标词i预测一个源语言的中心词，则$G_{ij}$则为：</p><p><img src="/images/15437135769000.jpg" width="23%" height="50%"></p><p>$P_i$就是对于目标词j而言源语言的中心词。 $\sigma$ 手动设定，通常是$\frac{D}{2}$，D代表窗口大小。</p><p>也即最终我们需要计算的是，中心词$P_i$和窗口$D$。</p><h4 id="计算-P-i"><a href="#计算-P-i" class="headerlink" title="计算$P_i$"></a>计算$P_i$</h4><p>利用对应的目标词i的query即可：<br><img src="/images/15437138514005.jpg" width="28%" height="50%"><br>$p_i$是一个实数。</p><h4 id="计算window-size"><a href="#计算window-size" class="headerlink" title="计算window size"></a>计算window size</h4><p>①固定窗口，将其作为一个超参。</p><p>②Layer-Speciﬁc Window<br>将该层所有的key平均，计算出一个共享的window size：<br><img src="/images/15437139914993.jpg" width="28%" height="50%"></p><p>③Query-Speciﬁc Window<br>每个query都有自己的window size<br><img src="/images/15437140367683.jpg" width="30%" height="50%"></p><h3 id="实验分析与结论"><a href="#实验分析与结论" class="headerlink" title="实验分析与结论"></a>实验分析与结论</h3><p>①将model locality用于低层效果会更好，这是因为低层对相邻建模，而越高层越关注更远的词。</p><p><img src="/images/15437141387365.jpg" width="50%" height="50%"></p><p>②将model locality放在encoder和encoder-decoder部分会更好（transformer有三个地方可以放）</p><p><img src="/images/15437141719564.jpg" width="50%" height="50%"><br>因为decoder本身就倾向关注临近的词，如果继续让其关注临近的词，那么就难以进行长程建模。</p><p>③越高层，window size（scope）越大。</p><p><img src="/images/15437142078121.jpg" width="70%" height="50%"></p><p>也即，在底层更倾向于捕获邻近词的语义；而高层倾向捕获长程依赖。但这不包括第一层，第一层是embedding，还没有上下文信息，因此倾向于捕获全局信息。</p><hr><h2 id="3️⃣-Effective-Approaches-to-Attention-based-Neural-Machine-Translation"><a href="#3️⃣-Effective-Approaches-to-Attention-based-Neural-Machine-Translation" class="headerlink" title="3️⃣[Effective Approaches to Attention-based Neural Machine Translation]"></a>3️⃣[Effective Approaches to Attention-based Neural Machine Translation]</h2><p>提出两种attention机制的翻译模型，global和local。</p><p>本文与原版的翻译模型略有不同：<br><img src="/images/15437143753188.jpg" width="40%" height="50%"><br><img src="/images/15437143893418.jpg" width="30%" height="50%"></p><p>c是context，h是decode的隐层。</p><h3 id="global-attention"><a href="#global-attention" class="headerlink" title="global attention"></a>global attention</h3><p><img src="/images/15437144396133.jpg" width="45%" height="50%"></p><p>计算attention分数：<br><img src="/images/15437145076271.jpg" width="40%" height="50%"></p><p>score有多种选择：<br><img src="/images/15437145588496.jpg" width="52%" height="50%"></p><p>注意到该模型与第一个提出attention based的模型不同之处：<br>$h_t -&gt; a_t -&gt; c_t -&gt; \tilde{h_t}$<br>原版是：<br>$h_{t-1} -&gt; a_t -&gt; c_t -&gt; h_t$</p><h3 id="local-attention"><a href="#local-attention" class="headerlink" title="local attention"></a>local attention</h3><p><img src="/images/15437147612512.jpg" width="45%" height="50%"></p><p>由于global attention计算代价高，且对于长句效果不好，我们可以选择一部分来做attention。<br>首先生成一个对齐位置$p_t$，再选择一个窗口$[p_t - D,p_t + D]$，其中D是超参。</p><p>如何获得$p_t$?<br>①直接假设$p_t=t$，也即source和target的位置大致一一对应。</p><p>②做预测：<br><img src="/images/15437150115321.jpg" width="43%" height="50%"><br>其中S是source的句子长度。</p><p>接着，以$p_t$为中心，添加一个高斯分布。最终attention计算公式：<br><img src="/images/15437150721538.jpg" width="50%" height="50%"></p><p>其中align和上面一致：<br><img src="/images/15437151043916.jpg" width="45%" height="50%"></p><p>也就是说，将位置信息也考虑进来。</p><h3 id="Input-feeding-Approach"><a href="#Input-feeding-Approach" class="headerlink" title="Input-feeding Approach"></a>Input-feeding Approach</h3><p>motivation：在下一次的alignment（也就是计算attention）之前，应当知道之前的alignment情况，所以应当作为输入信息传进下一层：<br><img src="/images/15437152269151.jpg" width="50%" height="50%"></p><p>注意这里和Bahdanau的不同。Bahdanau是直接用上下文去构造隐层。这里提出的模型相对更为通用，也可以被应用于非attention的模型中（也就是每次将encoder的最后一层作为输入在每个time step都输入）</p><hr><h2 id="4️⃣-Towards-Linear-Time-Neural-Machine-Translation-with-Capsule-Networks"><a href="#4️⃣-Towards-Linear-Time-Neural-Machine-Translation-with-Capsule-Networks" class="headerlink" title="4️⃣[Towards Linear Time Neural Machine Translation with Capsule Networks]"></a>4️⃣[Towards Linear Time Neural Machine Translation with Capsule Networks]</h2><p>思想：利用capsule提前生成source sentence的固定长度的表示，在decode的时候直接使用，而不需要attention，以达到线性时间NMT的目的。</p><p>Motivation：attention-based的NMT时间复杂度为$|S|\times |T|$，而本文希望能够将NMT减少到线性时间。而传统不加attention的NMT通常使用LSTM最后一层隐层作为源语言的encode信息传入decode，但这样的信息并不能很好地代表整个句子，因此本文使用capsule作为提取source sentence信息的方法，利用capsule生成固定长度表示，直接传入decode端，以达到线性时间的目的。</p><p><img src="/images/15437164176973.jpg" width="50%" height="50%"></p><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><p>对于embedding：<br><img src="/images/15437164440500.jpg" width="37%" height="50%"><br>希望能够转换成固定长度的表示C：<br><img src="/images/15437164654931.jpg" width="37%" height="50%"></p><p>我们首先通过一个双向的LSTM：<br><img src="/images/15437165067165.jpg" width="28%" height="50%"></p><p>一种简单的获取C的方法：<br><img src="/images/15437165382025.jpg" width="30%" height="50%"><br>其中$h_1$和$h_L$有互补关系。</p><p>本文使用capsule提取更丰富的信息。</p><p>在decode阶段，由于拥有固定表示，那么就不需要attention：</p><p><img src="/images/15437166827481.jpg" width="35%" height="50%"><br><img src="/images/15437167374470.jpg" width="37%" height="50%"></p><p>总体架构：<br><img src="/images/15437167607085.jpg" width="60%" height="50%"></p><h3 id="Aggregation-layers-with-Capsule-Networks"><a href="#Aggregation-layers-with-Capsule-Networks" class="headerlink" title="Aggregation layers with Capsule Networks"></a>Aggregation layers with Capsule Networks</h3><p><img src="/images/15437168111687.jpg" width="65%" height="50%"><br>实际上就是dynamic routing那一套，对信息进行提取（论文公式有误就不贴图了）</p><p>算法：<br><img src="/images/15437168668191.jpg" width="55%" height="50%"></p><p>最终获得了：<br><img src="/images/15437168888967.jpg" width="27%" height="50%"></p><hr><h2 id="5️⃣-DropBlock-A-regularization-method-for-convolutional-networks"><a href="#5️⃣-DropBlock-A-regularization-method-for-convolutional-networks" class="headerlink" title="5️⃣[DropBlock: A regularization method for convolutional networks]"></a>5️⃣[DropBlock: A regularization method for convolutional networks]</h2><p>重读了一遍。<br>介绍一种新型的dropout，可用于卷积层提高表现。通过大量的实验得出许多有意义的结论。本文发表于NIPS2018。</p><h3 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h3><p>由于卷积层的feature相互之间有联系，即使使用了dropout，信息也能够根据周围的feature传到下一层。因此使用dropblock，一次将一个方块内的都drop掉。</p><p><img src="/images/15437170173072.jpg" width="50%" height="50%"></p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p><img src="/images/15437170840909.jpg" width="80%" height="50%"></p><p>其中有两个超参：①block_size表示块的大小；γ表示有多少个unit要drop掉，等价传统的dropout的p。当block_size=1时等价dropout；当block size=整个feature map，等价于spatial dropout。</p><p>在实践中，通过以下公式计算γ：<br><img src="/images/15437172746112.jpg" width="55%" height="50%"></p><p>(why? 通过计算期望的方式将传统dropout的keep_prob与当前的γ联系起来，得到一个等式，整理即可获得上式）</p><p>在实验中，还可以逐渐减小keep_prob使得更加鲁棒性。</p><h3 id="实验-amp-结论"><a href="#实验-amp-结论" class="headerlink" title="实验&amp;结论"></a>实验&amp;结论</h3><p>①效果:dropout&lt; spatial dropout &lt; dropblock</p><p>②dropblock能有效去掉semantic information</p><p>③dropblock是一个更加强的regularization</p><p>④使用dropblock的模型，能够学习更多的区域，而不是只专注于一个区域<br><img src="/images/15437174940381.jpg" width="70%" height="50%"></p><p>对于resnet，直接将dropblock应用于添加完skip connection后的feature能够有更高的表现。</p><hr><h2 id="6️⃣-Contextual-String-Embeddings-for-Sequence-Labeling"><a href="#6️⃣-Contextual-String-Embeddings-for-Sequence-Labeling" class="headerlink" title="6️⃣[Contextual String Embeddings for Sequence Labeling]"></a>6️⃣[Contextual String Embeddings for Sequence Labeling]</h2><p>提出一种建立在character基础上的新型的上下文embedding(contextualized embedding）。用于sequence labeling。本文发表于coling2018。</p><h3 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h3><p>整体架构：<br><img src="/images/15437175991019.jpg" width="100%" height="50%"></p><p>首先将character作为基本单位，过一个双向LSTM，进行language model的建模。</p><p>如何提取一个词的词向量：<br><img src="/images/15437176650871.jpg" width="100%" height="50%"><br>提取前向LSTM中该词的最后一个character的后一个hidden state，以及后向LSTM中第一个词的前一个hidden state， 如上图所示。最终拼起来即可：<br><img src="/images/15437177090697.jpg" width="28%" height="50%"><br>因此该词不仅与词内部的character相关，还跟其周围的context有关。</p><p>sequence labeling我不感兴趣，该部分没看。</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>相比word level的language model，character-level独立于tokenization和fixed vocabulary，模型更容易被训练，因为词表小且训练时间短。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> attention </tag>
            
            <tag> capsule </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> NMT </tag>
            
            <tag> self-attention </tag>
            
            <tag> locality modeling </tag>
            
            <tag> dropblock </tag>
            
            <tag> contextualized embedding </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词16</title>
      <link href="/2018/12/01/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D16/"/>
      <url>/2018/12/01/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D16/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣菩萨蛮"><a href="#1️⃣菩萨蛮" class="headerlink" title="1️⃣菩萨蛮"></a>1️⃣菩萨蛮</h3><p>[五代十国] 李煜<br>人生愁恨何能免，销魂独我情何限！故国梦重归，觉来双泪垂。<br>髙楼谁与上？长记秋晴望。<strong>往事已成空，还如一梦中</strong>。</p><p>觉(jue)来：醒来。</p><hr><h3 id="2️⃣南乡子-·-和杨元素，时移守密州"><a href="#2️⃣南乡子-·-和杨元素，时移守密州" class="headerlink" title="2️⃣南乡子 · 和杨元素，时移守密州"></a>2️⃣南乡子 · 和杨元素，时移守密州</h3><p>[宋] 苏轼<br>东武望馀杭，云海天涯两杳茫。<strong>何日功成名遂了，还乡，醉笑陪公三万场</strong>。<br><strong>不用诉离觞，痛饮从来别有肠</strong>。今夜送归灯火冷，河塘，堕泪羊公却姓杨。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>无题</title>
      <link href="/2018/12/01/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BA%BA%E4%B8%8D%E5%8F%AF%E8%83%BD%E7%BB%8F%E5%8E%86%E4%B8%96%E7%95%8C%E4%B8%8A%E6%89%80%E6%9C%89%E7%83%AD%E9%97%B9/"/>
      <url>/2018/12/01/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BA%BA%E4%B8%8D%E5%8F%AF%E8%83%BD%E7%BB%8F%E5%8E%86%E4%B8%96%E7%95%8C%E4%B8%8A%E6%89%80%E6%9C%89%E7%83%AD%E9%97%B9/</url>
      
        <content type="html"><![CDATA[<p>人不可能经历世界上所有热闹，但可以用眼睛看，用心感受，用胸怀扩张。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识12</title>
      <link href="/2018/11/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8612/"/>
      <url>/2018/11/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8612/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Transformer"><a href="#1️⃣-Transformer" class="headerlink" title="1️⃣[Transformer]"></a>1️⃣[Transformer]</h3><p>对Transformer新理解：</p><ul><li>可以将Transformer理解成一张全连接图，其中每个节点与其他节点的关系通过attention权重表现。图关系是序列关系或者树关系的一般化。</li><li>为什么要有multi-head？不仅仅是论文的解释，或许还可以理解成，对一个向量的不同部分（如第1维到20维，第21维到40维等）施以不同的attention权重，如果不使用multi-head，那么对于一个query，就只会有一个权重，而不同的维度有不同的重要性。</li></ul><hr><h3 id="2️⃣-attention-amp-capsule"><a href="#2️⃣-attention-amp-capsule" class="headerlink" title="2️⃣[attention&amp;capsule]"></a>2️⃣[attention&amp;capsule]</h3><p>attention是收信息，query从value按权重获取信息，其中所有value的权重和是1。<br>capsule是发信息，对于$l-1$层的一个capsule来说，在传入到$l$层的k个capsule的信息，其权重和为1。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Transformer </tag>
            
            <tag> attention </tag>
            
            <tag> capsule </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文6</title>
      <link href="/2018/11/19/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%876/"/>
      <url>/2018/11/19/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%876/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-A-STRUCTURED-SELF-ATTENTIVE-SENTENCE-EMBEDDING"><a href="#1️⃣-A-STRUCTURED-SELF-ATTENTIVE-SENTENCE-EMBEDDING" class="headerlink" title="1️⃣[A STRUCTURED SELF ATTENTIVE SENTENCE EMBEDDING]"></a>1️⃣[A STRUCTURED SELF ATTENTIVE SENTENCE EMBEDDING]</h2><p>介绍了一种生成sentence embedding的方法。与其他sentence embedding不同的地方在于，生成的是一个矩阵而不是一个向量。通过矩阵的形式，能够关注不同部分的语义表示，类似于Transformer的multi-head。</p><p>Contribution:</p><ul><li>将sentence embedding扩展为矩阵形式，能够获得更多的信息。</li><li>引入正则化，使得sentence matrix具有更丰富的多样性。</li></ul><p><img src="/images/15425908639518.jpg" width="70%" height="50%"></p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>双向LSTM+self-attention。</p><p>双向的LSTM获得上下文的表示：</p><p><img src="/images/15425911302081.jpg" width="27%" height="50%"></p><p><img src="/images/15425911849931.jpg" width="27%" height="50%"></p><p>因此可以获得attention权重向量：<br><img src="/images/15425912555350.jpg" width="50%" height="50%"></p><p>其中$H:n\times2u,W_{s1}:d_a\times2u ,w_{s2}:d_a$ ，$d_a$是超参。</p><p>现将向量$w_{s2}$扩展为矩阵，亦即有Multi-hop attention：<br><img src="/images/15425914364548.jpg" width="50%" height="50%"></p><p>$W_{s2}$维度为$r\times d_a$，$r$代表了head的个数。</p><p>因此最终的sentence embedding矩阵为：<br><img src="/images/15425915371381.jpg" width="15%" height="50%"></p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>为了让A尽可能有多样性（因为如果都是相似的，那么则会有冗余性），引入如下的正则化：<br><img src="/images/15425915930785.jpg" width="28%" height="50%"></p><p>原因：<br>对于不同的head $a^i$与$a^j$，$A A^T$有：<br><img src="/images/15425918790543.jpg" width="31%" height="50%"></p><p>如果$a^i$与$a^j$很相似那么就会接近于1，如果非常不相似(no overlay)则会接近于0。<br>因此整个式子就是:希望对角线部分接近于0（因为减了单位阵），这就相当于尽可能focus小部分的词；同时其他部分尽可能接近于0，也即不同的head之间没有overlap。</p><h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><p>文章提到，在做分类的时候可以直接将矩阵M展开，过全连接层即可。</p><hr><h2 id="2️⃣-Attention-over-Attention-Neural-Networks-for-Reading-Comprehension"><a href="#2️⃣-Attention-over-Attention-Neural-Networks-for-Reading-Comprehension" class="headerlink" title="2️⃣[Attention-over-Attention Neural Networks for Reading Comprehension]"></a>2️⃣[Attention-over-Attention Neural Networks for Reading Comprehension]</h2><p>在完形填空任务(Cloze-style Reading Comprehension)上提出一种新的attention，即nested-attention。</p><h3 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h3><p>三元组 $ D,Q,A $，document，question，answer。其中answer一般是document的一个词。</p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>本文提出的attention机制，是通过一个新的attention去指示另一个attention的重要程度。</p><p>首先通过一层共享的embedding层，将document和query都encode成word embedding，然后通过双向的GRU，将隐层拼接起来成为新的表示。</p><p>接着获得pair-wise matching matrix：<br><img src="/images/15425993645945.jpg" width="40%" height="50%"></p><p>其中$h$代表上述提到的拼接起来的表示，$M(i,j)$代表了document的词$i$和question的词$j$之间的匹配程度。</p><p>接着对<strong>column</strong>做softmax：<br><img src="/images/15425994692189.jpg" width="50%" height="50%"><br>其代表的意义即query-to-document attention，亦即<strong>对于一个query内的词，document的每个词与其匹配的权重</strong>。</p><p>接下来，对row进行softmax操作：<br><img src="/images/15425995482827.jpg" width="50%" height="50%"><br>代表的是<strong>给定一个document的词，query的哪个词更为重要</strong>。</p><p>接下来我们将β平均起来，获得一个向量：<br><img src="/images/15425996847558.jpg" width="20%" height="50%"><br>这个向量仍有attention的性质，即所有元素加和为1。代表的是<strong>从平均来看，query词的重要性</strong>。</p><p>最后，我们对α和β做点积以获得attended document-level attention：<br><img src="/images/15425997529193.jpg" width="13%" height="50%"></p><p>其中$s$的维度是$D\times 1$。s代表的意义即“a weighted sum of each individual document-level attention α(t) when looking at query word at time t”，也就是说，对α进行加权，代表query word的平均重要程度。</p><p>最终在做完型填空的预测时：<br><img src="/images/15425999965777.jpg" width="38%" height="50%"></p><p>个人觉得这种attention-over-attention的想法还是挺有创新的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> attention </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> sentence embedding </tag>
            
            <tag> nested attention </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>网络优化与正则化总结</title>
      <link href="/2018/11/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96%E6%80%BB%E7%BB%93/"/>
      <url>/2018/11/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>大量参考自<a href="https://nndl.github.io/" target="_blank" rel="noopener">《神经网络与深度学习》</a></p><h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><p>对于标准的SGD，常见的改进算法从两个方面进行：学习率衰减&amp;梯度方向优化。<br>记$g_t$为t时刻的导数：<br><img src="/images/2018-11-13-15421196736629.jpg" width="20%" height="50%"></p><h2 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h2><h3 id="AdaGrad算法"><a href="#AdaGrad算法" class="headerlink" title="AdaGrad算法"></a>AdaGrad算法</h3><p>通过计算历次的梯度平方累计值进行学习率衰减。<br>$G_t$是累计值：<br><img src="/images/2018-11-13-15421189802198.jpg" width="20%" height="50%"></p><p>更新值则为：<br><img src="/images/2018-11-13-15421190100615.jpg" width="30%" height="50%"></p><p>缺点：随着迭代次数的增加学习率递减。在经过一定次数的迭代依然没有找到最优点时，由于这时的学习率已经非常小，很难再继续找到最优点。</p><h3 id="RMSprop算法"><a href="#RMSprop算法" class="headerlink" title="RMSprop算法"></a>RMSprop算法</h3><p>对AdaGrad的改进，唯一的区别在于$G_t$的计算，将历史信息和当前信息进行线性加权，使得学习率可以动态改变而不是单调递减：<br><img src="/images/2018-11-13-15421192344025.jpg" width="40%" height="50%"></p><p>β为衰减率，通常取0.9。也即历史信息占主导。</p><h3 id="AdaDelta算法"><a href="#AdaDelta算法" class="headerlink" title="AdaDelta算法"></a>AdaDelta算法</h3><p>同样是对AdaGrad的改进。<br>每次计算：<br><img src="/images/2018-11-13-15421195264173.jpg" width="50%" height="50%"></p><p>也即历史更新差和上一时刻的更新差的加权（RMSprop是历史梯度和当前梯度）。</p><p>最终更新差值为：<br><img src="/images/2018-11-13-15421197355615.jpg" width="30%" height="50%"></p><p>其中$G_t$计算方法和RMSprop一致。</p><h2 id="梯度方向优化"><a href="#梯度方向优化" class="headerlink" title="梯度方向优化"></a>梯度方向优化</h2><p>利用历史的梯度（方向）调整当前时刻的梯度。</p><h3 id="动量（Momentum）法"><a href="#动量（Momentum）法" class="headerlink" title="动量（Momentum）法"></a>动量（Momentum）法</h3><p>动量法（Momentum Method）是用之前积累动量来替代真正的梯度。每次迭代的梯度可以看作是加速度。</p><p><img src="/images/2018-11-13-15421199473226.jpg" width="28%" height="50%"></p><p>也即上一时刻的更新差值和当前梯度共同决定当前的更新差值。$ρ$为动量因子，通常为0.9。也即动量占了主导。</p><p>当某个参数在最近一段时间内的梯度方向不一致时，其真实的参数更新幅度变小；相反，当在最近一段时间内的梯度方向都一致时，其真实的参数更新幅度变大，起到加速作用。一般而言，在迭代初期，梯度方法都比较一致，动量法会起到加速作用，可以更快地到达最优点。在迭代后期，梯度方法会取决不一致，在收敛值附近震荡，动量法会起到减速作用，增加稳定性。</p><h3 id="Nesterov加速梯度"><a href="#Nesterov加速梯度" class="headerlink" title="Nesterov加速梯度"></a>Nesterov加速梯度</h3><p>动量法的改进版本。</p><p>前面提到的动量法，是上一步的更新方向$\Delta \theta_{t-1}$与当前梯度$-g_t$的加和。因此可以理解成，先根据$∆θ_{t−1}$更新一次得到参数θ，再用$g_t$进行更新。亦即：<br><img src="/images/2018-11-13-15421202426163.jpg" width="27%" height="50%"><br>上式的第二步中，$g_t$是在$ \theta_{t-1}$上的梯度。我们将该步改为在$\theta_{t}$的梯度。<br>因此，有：<br><img src="/images/2018-11-13-15421203421465.jpg" width="50%" height="50%"></p><p>和动量法相比，相当于提前走了一步。<br><img src="/images/2018-11-13-15421203910771.jpg" width="70%" height="50%"></p><h3 id="Adam-amp-Nadam"><a href="#Adam-amp-Nadam" class="headerlink" title="Adam&amp;Nadam"></a>Adam&amp;Nadam</h3><p>Adam一方面计算梯度平方的加权，同时还计算梯度的加权：<br><img src="/images/2018-11-13-15421205162558.jpg" width="40%" height="50%"><br>通常$β_1=0.9$，$β_2=0.99$<br>也即历史信息占了主导。</p><p>在初期$M_t$与$G_t$会比真实均值和方差要小（想象$M_0=0$，$G_0=0$时）。因此对其进行修正，即：<br><img src="/images/2018-11-13-15421207635850.jpg" width="18%" height="50%"><br>因此最终有：<br><img src="/images/2018-11-13-15421207966341.jpg" width="26%" height="50%"></p><p>同理有Nadam。</p><p>Adam = Momentum + RMSprop<br>Nadam = Nesterov + RMSprop</p><h3 id="梯度截断-gradient-clipping"><a href="#梯度截断-gradient-clipping" class="headerlink" title="梯度截断 gradient clipping"></a>梯度截断 gradient clipping</h3><p>分为按值截断与按模截断。</p><h1 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h1><p>初始值选取很关键。假设全部初始化为0，则后续更新导致所有的激活值相同，也即对称权重现象。</p><p>原则：不能过大，否则激活值会变得饱和，如sigmoid；不能过小，否则经过多层信号会逐渐消失，并且导致sigmoid丢失非线性的能力（在0附近基本近似线性）。如果一个神经元的输入连接很多，它的每个输入连接上的权重就应该小一些，这是为了避免输出过大。</p><h2 id="Gaussian分布初始化"><a href="#Gaussian分布初始化" class="headerlink" title="Gaussian分布初始化"></a>Gaussian分布初始化</h2><p>同时考虑输入输出，可以按 $N(0,\sqrt{\frac{2}{n_{in} + n_{out}}})$ 高斯分布来初始化。</p><h2 id="均匀分布初始化"><a href="#均匀分布初始化" class="headerlink" title="均匀分布初始化"></a>均匀分布初始化</h2><p>在$[-r,r]$区间均匀分布初始化，其中r可以按照神经元数量自适应调整。</p><h3 id="Xavier初始化方法"><a href="#Xavier初始化方法" class="headerlink" title="Xavier初始化方法"></a>Xavier初始化方法</h3><p>自动计算超参r。r的公式为：<br><img src="/images/2018-11-14-15421648119504.jpg" width="22%" height="50%"><br>其中$n^l$代表第$l$层的神经元个数。</p><p>为什么是这个式子（推导见参考资料）：综合考虑了①输入输出的方差要一致；②反向传播中误差信号的方差不被放大或缩小。</p><h1 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h1><p>将数据分布归一化，使得分布保持稳定。<br><img src="/images/2018-11-14-15421656553319.jpg" width="100%" height="50%"><br>假设数据有四维(N,C,H,W)。N代表batch；C代表channel；H,W代表height和width。</p><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>沿着通道进行归一化，亦即每个通道都有自己的均值和方差。<br><img src="/images/2018-11-14-15421657694248.jpg" width="70%" height="50%"><br>其中缩放平移变量是可学习的。</p><p>缺点：<br>①对batch size敏感，batch size太小则方差均值不足以代表数据分布<br>②对于不等长的输入如RNN来说，每一个timestep都需要保存不同的特征。</p><h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><p>对一个输入进行正则化，亦即每个输入都有自己的方差、均值。这样不依赖于batch大小和输入sequence的深度。</p><p>对RNN效果比较明显，但CNN中不如BN</p><h2 id="Instance-Normalization"><a href="#Instance-Normalization" class="headerlink" title="Instance Normalization"></a>Instance Normalization</h2><p>对HW进行归一化</p><h2 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h2><p>将channel分为多个group，每个group内做归一化</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://nndl.github.io/" target="_blank" rel="noopener">《神经网络与深度学习》</a><br><a href="https://blog.csdn.net/liuxiao214/article/details/81037416" target="_blank" rel="noopener">https://blog.csdn.net/liuxiao214/article/details/81037416</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习🤖 </tag>
            
            <tag> 优化算法 </tag>
            
            <tag> 参数初始化 </tag>
            
            <tag> Normalization </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录10</title>
      <link href="/2018/11/11/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9510/"/>
      <url>/2018/11/11/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9510/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-get-sinusoid-encoding-table"><a href="#1️⃣-get-sinusoid-encoding-table" class="headerlink" title="1️⃣[get_sinusoid_encoding_table]"></a>1️⃣[get_sinusoid_encoding_table]</h3><p>Transformer绝对位置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sinusoid_encoding_table</span><span class="params">(n_position, d_hid, padding_idx=None)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_angle</span><span class="params">(position, hid_idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> position / np.power(<span class="number">10000</span>, <span class="number">2</span> * (hid_idx // <span class="number">2</span>) / d_hid)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_posi_angle_vec</span><span class="params">(position)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [cal_angle(position, hid_j) <span class="keyword">for</span> hid_j <span class="keyword">in</span> range(d_hid)]</span><br><span class="line"></span><br><span class="line">    sinusoid_table = np.array([get_posi_angle_vec(pos_i) <span class="keyword">for</span> pos_i <span class="keyword">in</span> range(n_position)])</span><br><span class="line"></span><br><span class="line">    sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line">    sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>] = np.cos(sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> padding_idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        sinusoid_table[padding_idx] = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.FloatTensor(sinusoid_table)  <span class="comment"># n_position,embed_dim</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识11</title>
      <link href="/2018/11/11/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8611/"/>
      <url>/2018/11/11/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8611/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Optimizer"><a href="#1️⃣-Optimizer" class="headerlink" title="1️⃣[Optimizer]"></a>1️⃣[Optimizer]</h3><p><a href="https://zhuanlan.zhihu.com/p/32262540" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32262540</a><br><a href="https://zhuanlan.zhihu.com/p/32338983" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32338983</a></p><p>Adam等自适应学习率算法对于稀疏数据具有优势，且收敛速度很快；但精调参数的SGD（+Momentum）往往能够取得更好的最终结果。</p><p>建议：<br>前期用Adam，享受Adam快速收敛的优势；后期切换到SGD，慢慢寻找最优解。<br>什么时候从Adam切换到SGD？当SGD的相应学习率的移动平均值基本不变的时候。</p><hr><h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>LongTensor除以浮点数，会对除数进行取整，再做除法。<br><img src="/images/2018-11-11-15419055399325.jpg" width="30%" height="50%"></p><hr><h3 id="3️⃣-Pytorch"><a href="#3️⃣-Pytorch" class="headerlink" title="3️⃣[Pytorch]"></a>3️⃣[Pytorch]</h3><p>使用Pytorch的DataParallel</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda:'</span> + str(</span><br><span class="line">    config.CUDA_VISIBLE_DEVICES[<span class="number">0</span>]) <span class="keyword">if</span> config.use_cuda <span class="keyword">else</span> <span class="string">'cpu'</span>)   <span class="comment"># 指定第一个设备</span></span><br><span class="line"></span><br><span class="line">model = ClassifyModel(</span><br><span class="line">    vocab_size=len(vocab), max_seq_len=config.max_sent_len,</span><br><span class="line">    embed_dim=config.embed_dim, n_layers=config.n_layers,</span><br><span class="line">    n_head=config.n_head, d_k=config.d_k,</span><br><span class="line">    d_v=config.d_v,</span><br><span class="line">    d_model=config.d_model, d_inner=config.d_inner_hid,</span><br><span class="line">    n_label=config.n_label,</span><br><span class="line">    dropout=config.dropout</span><br><span class="line">).to(device)</span><br><span class="line"></span><br><span class="line">model = DataParallel(model, device_ids=config.CUDA_VISIBLE_DEVICES)  <span class="comment"># 显式定义device_ids</span></span><br></pre></td></tr></table></figure><p>注意到：device_ids的起始编号要与之前定义的device中的“cuda:0”相一致，不然会报错。</p><p>如果不显式在代码中的DataParallel指定设备，那么需要在命令行内指定。如果是在命令行里面运行的，且device不是从0开始，应当显式设置GPU_id，否则会出错‘AssertionError: Invalid device id’，正确的命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=4,5  python -u classify_main.py --gpu_id 0,1</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Optimizer </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于sparse gradient</title>
      <link href="/2018/11/11/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8Esparse%20gradient/"/>
      <url>/2018/11/11/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8Esparse%20gradient/</url>
      
        <content type="html"><![CDATA[<p>前几天在看AllenAI在EMNLP的ppt时，有一页写道：<br><img src="/images/2018-11-11-15419037448379.jpg" width="70%" height="50%"></p><p>为什么会出现这种情况？</p><p>Embedding是一个很大的矩阵，每一次其实都只有一个小部分进行了更新，对于一些词来说，出现的频率不高，或者说，其实大部分的词在一个loop/epoch中，被更新的次数是较少的。但是，注意到一般的optimizer算法，是以matrix为单位进行更新的，也就是每一次都是$W^{t+1}=W^{t}-\eta \frac{\partial L}{\partial{W}}$</p><p>而Adam算法：<br><img src="/images/2018-11-11-15419038346958.jpg" width="70%" height="50%"></p><p>动量占了主导。但这样，每次batch更新，那些没被更新的词（也即gradient=0）的动量仍然会被衰减，所以这样当到这个词更新的时候，他的动量已经被衰减完了，所以更新的gradient就很小。</p><p>解决方案：</p><p>①在PyTorch中，Embedding的API：<br><code>torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False, _weight=None)</code></p><p>其中sparse (bool, optional) – if True, gradient w.r.t. weight matrix will be a sparse tensor.</p><p>将sparse设为True即可。</p><p>②针对sparse矩阵，使用不同的optimizer，如torch.optim.SparseAdam：</p><blockquote><p>Implements lazy version of Adam algorithm suitable for sparse tensors.<br>In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> sparse gradient </tag>
            
            <tag> 代码实践 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文5</title>
      <link href="/2018/11/10/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%875/"/>
      <url>/2018/11/10/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%875/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-Neural-Turing-Machine"><a href="#1️⃣-Neural-Turing-Machine" class="headerlink" title="1️⃣[Neural Turing Machine]"></a>1️⃣[Neural Turing Machine]</h2><p>通过模仿冯诺依曼机，引入外部内存(externel memory)。<br><img src="/images/2018-11-10-15418626837026.jpg" width="70%" height="50%"></p><p>和普通神经网络一样，与外界交互，获得一个输入，产生一个输出。但不同的是，内部还有一个memory进行读写。<br>假设memory是一个N × M的矩阵，N是内存的位置数量。</p><h3 id="读写memory"><a href="#读写memory" class="headerlink" title="读写memory"></a>读写memory</h3><p>①读<br><img src="/images/2018-11-10-15418627403268.jpg" width="25%" height="50%"><br>其中读的时候对各内存位置线性加权。w是归一化权重。</p><p>②写<br>$e_t$是擦除向量（erase vector）<br><img src="/images/2018-11-10-15418627941358.jpg" width="35%" height="50%"></p><p>$a_t$是加和向量(add vector)<br><img src="/images/2018-11-10-15418628323343.jpg" width="30%" height="50%"></p><p>具体如何获得权重就不说了。</p><h3 id="Controller-network"><a href="#Controller-network" class="headerlink" title="Controller network"></a>Controller network</h3><p>中间的controller network可以是一个普通的feed forward或者RNN。</p><p>在实际中NTM用得并不多。</p><hr><h2 id="2️⃣-Efficient-Contextualized-Representation-Language-Model-Pruning-for-Sequence-Labeling"><a href="#2️⃣-Efficient-Contextualized-Representation-Language-Model-Pruning-for-Sequence-Labeling" class="headerlink" title="2️⃣[Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling]"></a>2️⃣[Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling]</h2><p>ELMo的精简版，通过即插即用的方法来压缩语言模型，对特定任务剪枝不同的层，使得能够减少inference的时间。<br>这篇的idea挺有创新的，但似乎有些trivial的感觉。</p><p><img src="/images/2018-11-11-15418977712883.jpg" width="70%" height="50%"></p><h3 id="RNN-and-Dense-Connectivity"><a href="#RNN-and-Dense-Connectivity" class="headerlink" title="RNN and Dense Connectivity"></a>RNN and Dense Connectivity</h3><p>每一层的输出都会传到所有层作为输入，因此对于L层的输入：<br><img src="/images/2018-11-11-15418979328482.jpg" width="35%" height="50%"></p><p>这样我们就能够随意地去掉任意中间层了。同时一些语言信息也分散到各个层，即使去掉某些层也没有关系。</p><p>则最终的output为：<br><img src="/images/2018-11-11-15418991345288.jpg" width="33%" height="50%"></p><p>最终作projection到正常维度（在每层都会这么做，将输入降维到正常维度再输入）：<br><img src="/images/2018-11-11-15418992517849.jpg" width="37%" height="50%"></p><p>再做一个softmax：<br><img src="/images/2018-11-11-15418993146246.jpg" width="32%" height="50%"></p><p>由于 $h^{※}$ 用于softmax，所以可能和target word，也即下一个词比较相似，<strong>因此可能没有很多的上下文信息</strong>。</p><p>所以最终我们使用$h_t$，以及反向的$h_t^r$，再过一层线性层获得最终的embedding（和ELMo有些不同，ELMo是直接拼起来）：<br><img src="/images/2018-11-11-15418994541442.jpg" width="40%" height="50%"></p><h3 id="Layer-Selection"><a href="#Layer-Selection" class="headerlink" title="Layer Selection"></a>Layer Selection</h3><p>我们在每层的output都加一个权重系数。<br><img src="/images/2018-11-11-15418996081852.jpg" width="30%" height="50%"></p><p>我们希望在target task上用的时候，部分z能够变成0，达到layer selection的效果，加快inference的速度。</p><p>亦即：<br><img src="/images/2018-11-11-15418996697208.jpg" width="20%" height="50%"></p><p>一种理想的方法是L0正则化：<br><img src="/images/2018-11-11-15418997294756.jpg" width="17%" height="50%"></p><p>但由于没办法求导，因此，采用L1正则化：<br><img src="/images/2018-11-11-15418997747882.jpg" width="15%" height="50%"><br>但使用L1正则化有一定的风险，因为如果让所有z都远离1，那么会影响performance。</p><p>引入新的正则化方法$R_2 =\delta(|z|_0&gt;\lambda_1) |z|_1$<br>亦即，只有在非零z的个数大于某个阈值时，才能有正则化效果，保证非零的个数。’it can be “turned-off” after achieving a satisfying sparsity’.</p><p>进一步引入$R_3=\delta(|z|_0&gt;\lambda_1) |z|_1 + |z(1-z)|_1$<br>其中第二项为了鼓励z向0或1走。</p><h3 id="Layer-wise-Dropout"><a href="#Layer-wise-Dropout" class="headerlink" title="Layer-wise Dropout"></a>Layer-wise Dropout</h3><p>随机删除部分layer，这些layer的输出不会传入之后的层，但仍然会参与最后的representation计算。<br><img src="/images/2018-11-11-15419000928057.jpg" width="70%" height="50%"></p><p>这种dropout会让perplexity更高，但对生成更好的representation有帮助。</p><hr><h2 id="3️⃣-Constituency-Parsing-with-a-Self-Attentive-Encoder"><a href="#3️⃣-Constituency-Parsing-with-a-Self-Attentive-Encoder" class="headerlink" title="3️⃣[Constituency Parsing with a Self-Attentive Encoder]"></a>3️⃣[Constituency Parsing with a Self-Attentive Encoder]</h2><p>其中的positional encoding我比较感兴趣。<br>原版的positional encoding是直接和embedding相加的。<br>亦即：<br><img src="/images/2018-11-11-15419002563338.jpg" width="22%" height="50%"><br>那么在selt-attention时，有：<br><img src="/images/2018-11-11-15419002855901.jpg" width="45%" height="50%"><br>这样会有交叉项：<br><img src="/images/2018-11-11-15419003111684.jpg" width="13%" height="50%"><br>该项没有什么意义，且可能会带来过拟合。</p><p>因此在这边将positional encoding和embedding拼起来，亦即：<br><img src="/images/2018-11-11-15419003740409.jpg" width="23%" height="50%"></p><p>并且，在进入multi-head时的线性层也做改变：<br><img src="/images/2018-11-11-15419004269693.jpg" width="24%" height="50%"></p><p>这样在相乘的时候就不会有交叉项了。</p><p>实验证明，该方法有一定的提升。</p><hr><h2 id="4️⃣-DropBlock-A-regularization-method-for-convolutional-networks"><a href="#4️⃣-DropBlock-A-regularization-method-for-convolutional-networks" class="headerlink" title="4️⃣[DropBlock: A regularization method for convolutional networks]"></a>4️⃣[DropBlock: A regularization method for convolutional networks]</h2><p>大致翻了一下。<br>Motivation:在CNN中，dropout对convolutional layer的作用不大，一般都只用在全连接层。作者推测，因为每个feature map都有一个感受野范围，仅仅对单个像素进行dropout并不能降低feature map学习的特征范围，亦即网络仍可以通过该位置的相邻位置元素去学习对应的语义信息，也就不会促使网络去学习更加鲁棒的特征。</p><p>因此作者的做法是，dropout一整块位置。<br><img src="/images/2018-11-11-15419007355875.jpg" width="80%" height="50%"></p><hr><h2 id="5️⃣-Accelerating-Neural-Transformer-via-an-Average-Attention-Network"><a href="#5️⃣-Accelerating-Neural-Transformer-via-an-Average-Attention-Network" class="headerlink" title="5️⃣[Accelerating Neural Transformer via an Average Attention Network]"></a>5️⃣[Accelerating Neural Transformer via an Average Attention Network]</h2><p>提出了AAN(average attention network)，对transformer翻译模型的decode部分进行改进，加速了过程。</p><p>由于Transformer在decode阶段需要用到前面所有的y，也即自回归(auto-regressive)的性质，所以无法并行：</p><p><img src="/images/2018-11-11-15419009098650.jpg" width="50%" height="50%"></p><h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><p>给定y：<br><img src="/images/2018-11-11-15419010325049.jpg" width="27%" height="50%"></p><p>首先将他们加起来，过一层全连接：<br><img src="/images/2018-11-11-15419010603010.jpg" width="27%" height="50%"><br>这也相当于就是让所有的y有相同的权重，此时g就是上下文相关的表示。</p><p>接下来添加一个gating：<br><img src="/images/2018-11-11-15419011154221.jpg" width="27%" height="50%"><br>控制了从过去保存多少信息和获取多少新的信息。</p><p>和Transformer原版论文一样，添加一个residual connection：<br><img src="/images/2018-11-11-15419011595237.jpg" width="30%" height="50%"></p><p>如图整个过程：<br><img src="/images/2018-11-11-15419011840751.jpg" width="55%" height="50%"></p><p>总结：AAN=average layer+gating layer</p><h3 id="加速"><a href="#加速" class="headerlink" title="加速"></a>加速</h3><p>①考虑到加和操作是序列化的，只能一个一个来，不能并行，在这里使用一个mask的trick，使得在训练时也能够并行：<br><img src="/images/2018-11-11-15419013219526.jpg" width="60%" height="50%"></p><p>②在inference时的加速：<br><img src="/images/2018-11-11-15419019335926.jpg" width="20%" height="50%"></p><p>这样Transformer就能够类似RNN，只考虑前一个的state，而不是前面所有的state。</p><p>最终的模型：<br><img src="/images/2018-11-11-15419023032628.jpg" width="60%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Transformer </tag>
            
            <tag> dropout </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> self-attention </tag>
            
            <tag> NTM </tag>
            
            <tag> ELMo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词15</title>
      <link href="/2018/11/10/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D15/"/>
      <url>/2018/11/10/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D15/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣蜀相"><a href="#1️⃣蜀相" class="headerlink" title="1️⃣蜀相"></a>1️⃣蜀相</h3><p>[唐] 杜甫<br>丞相祠堂何处寻，锦官城外柏森森。<br>映阶碧草自春色，隔叶黄鹂空好音。<br>三顾频烦天下计，两朝开济老臣心。<br><strong>出师未捷身先死，长使英雄泪满襟</strong>。</p><p><a href="http://m.xichuangzhu.com/work/57b93410a633bd00665efd4a" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b93410a633bd00665efd4a</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文4</title>
      <link href="/2018/11/04/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%874/"/>
      <url>/2018/11/04/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%874/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-Character-Level-Language-Modeling-with-Deeper-Self-Attention"><a href="#1️⃣-Character-Level-Language-Modeling-with-Deeper-Self-Attention" class="headerlink" title="1️⃣[Character-Level Language Modeling with Deeper Self-Attention]"></a>1️⃣[Character-Level Language Modeling with Deeper Self-Attention]</h2><p>将transformer用于character-level的语言模型中，通过添加多个loss来提高其表现以及加快拟合速度，同时加深transformer的层数，极大提升表现，12层的transformer layer能达到SOTA，而64层则有更多的提升。</p><p>普通RNN用于character-level language model：<br>将句子按character为单位组成多个batch，每个batch预测最后一个词，然后将该batch的隐状态传入下一个batch。也即“truncated backpropagation through time” (TBTT)。</p><p>如果用在Transformer，如下图，我们只预测$t_4$。<br><img src="/images/2018-11-04-15412915431327.jpg" width="90%" height="50%"></p><p>本文的一大贡献是多加了三种loss，并且有些loss的权值会随着训练的过程而逐渐减小，每个loss都会自己的schedule。这些loss加快了拟合速度，同时也提升了表现。</p><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><h4 id="Multiple-Positions"><a href="#Multiple-Positions" class="headerlink" title="Multiple Positions"></a>Multiple Positions</h4><p>对于batch内而言，每个时间步t都要预测下一个词。<br><img src="/images/2018-11-04-15412916429104.jpg" width="90%" height="50%"></p><h4 id="Intermediate-Layer-Losses"><a href="#Intermediate-Layer-Losses" class="headerlink" title="Intermediate Layer Losses"></a>Intermediate Layer Losses</h4><p>要求中间层也做出预测：<br><img src="/images/2018-11-04-15412916704097.jpg" width="95%" height="50%"></p><p>在这里，越底层的layer其loss权值越低。</p><h4 id="Multiple-Targets"><a href="#Multiple-Targets" class="headerlink" title="Multiple Targets"></a>Multiple Targets</h4><p>每一个position，不仅仅要预测下一个词，还要预测下几个词，预测下一个词和预测下几个词的分类器是独立的。</p><p><img src="/images/2018-11-04-15412917374689.jpg" width="70%" height="50%"></p><h3 id="Positional-embedding"><a href="#Positional-embedding" class="headerlink" title="Positional embedding"></a>Positional embedding</h3><p>每一层的都添加一个不共享的可学习的positional embedding。</p><hr><h2 id="2️⃣-Self-Attention-with-Relative-Position-Representations"><a href="#2️⃣-Self-Attention-with-Relative-Position-Representations" class="headerlink" title="2️⃣[Self-Attention with Relative Position Representations]"></a>2️⃣[Self-Attention with Relative Position Representations]</h2><p>提出使用相对位置替代Transformer的绝对位置信息，并在NMT上有一定的提升。</p><p>分解：<br>在原先的self-attention中，输出为：<br><img src="/images/2018-11-04-15412923510664.jpg" width="25%" height="50%"></p><p>其中：<br><img src="/images/2018-11-04-15412923744647.jpg" width="25%" height="50%"><br><img src="/images/2018-11-04-15412923773686.jpg" width="25%" height="50%"></p><p>现在我们考虑添加相对位置，其中相对位置信息在各层都是共享的：<br><img src="/images/2018-11-04-15412924279426.jpg" width="30%" height="50%"><br><img src="/images/2018-11-04-15412924396468.jpg" width="30%" height="50%"></p><p>$a_{ij}^K$的具体形式：<br><img src="/images/2018-11-04-15412925792994.jpg" width="40%" height="50%"><br><img src="/images/2018-11-04-15412925910424.jpg" width="55%" height="50%"><br>上式为了降低复杂度，不考虑长于k的相对位置信息。</p><p>考虑到transformer的并行性，为了并行性，我们考虑如下式子：<br><img src="/images/2018-11-04-15412926687951.jpg" width="50%" height="50%"><br>其中，第一项和原来的Transformer一致；第二项，通过reshape可以达到并行的效果，然后两项直接加起来。</p><p>实验证明，使用相对位置效果是有一定的提升的，而同时使用绝对位置和相对位置并没有提升。<br><img src="/images/2018-11-04-15412930642978.jpg" width="90%" height="50%"></p><hr><h2 id="3️⃣-WEIGHTED-TRANSFORMER-NETWORK-FOR-MACHINE-TRANSLATION"><a href="#3️⃣-WEIGHTED-TRANSFORMER-NETWORK-FOR-MACHINE-TRANSLATION" class="headerlink" title="3️⃣[WEIGHTED TRANSFORMER NETWORK FOR MACHINE TRANSLATION]"></a>3️⃣[WEIGHTED TRANSFORMER NETWORK FOR MACHINE TRANSLATION]</h2><p>这篇被ICLR拒了，但有审稿人打了9分的高分。</p><p>对Transformer进行改进，拥有更好的效果和更小的计算代价。</p><p>传统的Transformer：</p><script type="math/tex; mode=display">Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><script type="math/tex; mode=display">head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)</script><script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat_i (head_i)W^O</script><script type="math/tex; mode=display">FFN(x)=max(0,xW_1+b_1)W_2 + b_2</script><p>在本文中，先对head进行升维并乘以权重，过了FNN后，再乘以另一个权重。其中权重$\alpha$ $ \kappa$为可学习参数：</p><script type="math/tex; mode=display">head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)</script><script type="math/tex; mode=display">\overline{head_i}=head_i W^{O_i} \times \kappa_i</script><script type="math/tex; mode=display">BranchedAttention(Q,K,V)=\sum_{i=1}^{M} \alpha_i FFN(\overline{head}_i)</script><p>其中要求权重之和为1。即$\sum_{i=1}^{M}\alpha_i=1$,$\sum_{i=1}^{M}\kappa_i=1$。</p><p><img src="/images/2018-11-04-15412939412047.jpg" width="90%" height="50%"></p><p>文中对$\kappa$和$\alpha$作了解释。</p><blockquote><p>κ can be interpreted as a learned concatenation weight and α as the learned addition weight</p></blockquote><p>通过实验，发现该模型会有更好的正则化特性。同时效果也有一定提升，收敛速度更快：<br><img src="/images/2018-11-04-15412940966579.jpg" width="80%" height="50%"></p><hr><h2 id="4️⃣-You-May-Not-Need-Attention"><a href="#4️⃣-You-May-Not-Need-Attention" class="headerlink" title="4️⃣[You May Not Need Attention]"></a>4️⃣[You May Not Need Attention]</h2><p>粗略地过了一遍，一些细节没有弄明白。</p><p>提出一种将encoder-decoder融合起来的模型，也即eager translation model，不需要attention，能够实现即时的翻译，也即读入一个词就能翻译一个词，同时不需要记录encoder的所有输出，因此需要很少的内存。</p><p><img src="/images/2018-11-04-15412942175720.jpg" width="50%" height="50%"></p><p>分为三步：<br>①pre-processing<br>进行预处理，使得源句子和目标句子满足<strong>eager feasible</strong> for every aligned pair of words $(s_i , t_j ), i ≤ j$。</p><p>首先通过现成的工具进行对齐操作(alignment)，然后对于那些不符合eager feasible的有具体算法（没认真看）进行补padding。如图<br><img src="/images/2018-11-04-15412945231042.jpg" width="60%" height="50%"></p><p>我们还可以在target sentence的开头添加b个padding，使得模型能够在开始预测之前获取更多的source sentence的词。</p><p>②模型<br>两层的LSTM，输入是上一次的y和当前的x拼接起来直接传进去。</p><p>③post processing<br>在最终结果之前，将padding去掉。</p><p>在inference（也即beam search）时，还有几个操作/trick：</p><ul><li>Padding limit</li><li>Source padding injection SPI</li></ul><p>实验表明，eager model在长的句子表现超过传统带attention的NMT，而长句子的建模正是attention-based 的模型的一大挑战；而在短句子上就不如attention-based的NMT。<br><img src="/images/2018-11-04-15412946442983.jpg" width="50%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Transformer </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> NMT </tag>
            
            <tag> Language Modeling </tag>
            
            <tag> self-attention </tag>
            
            <tag> relative position </tag>
            
            <tag> positional encoding </tag>
            
            <tag> eager translation model </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词14</title>
      <link href="/2018/11/04/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D14/"/>
      <url>/2018/11/04/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D14/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣鹤冲天"><a href="#1️⃣鹤冲天" class="headerlink" title="1️⃣鹤冲天"></a>1️⃣鹤冲天</h3><p>[宋] 柳永<br>黄金榜上，偶失龙头望。明代暂遗贤，如何向？未遂风云便，争不恣游狂荡。何须论得丧？才子词人，自是白衣卿相。<br>烟花巷陌，依约丹靑屛障。幸有意中人，堪寻访。且恁偎红倚翠，风流事，平生畅。靑春都一饷。<strong>忍把浮名，换了浅斟低唱</strong>！</p><p>恣（zì）：放纵，随心所欲。<br>恁（nèn）：如此。</p><p><a href="http://m.xichuangzhu.com/work/57aeff68a633bd0057f7d406" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57aeff68a633bd0057f7d406</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录9</title>
      <link href="/2018/11/04/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%959/"/>
      <url>/2018/11/04/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%959/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-collate-fn"><a href="#1️⃣-collate-fn" class="headerlink" title="1️⃣[collate_fn]"></a>1️⃣[collate_fn]</h3><p>将不等长句子组合成batch。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(insts)</span>:</span></span><br><span class="line">    <span class="string">''' Pad the instance to the max seq length in batch '''</span></span><br><span class="line"></span><br><span class="line">    max_len = max(len(inst) <span class="keyword">for</span> inst <span class="keyword">in</span> insts)</span><br><span class="line"></span><br><span class="line">    batch_seq = np.array([</span><br><span class="line">        inst + [Constants.PAD] * (max_len - len(inst))</span><br><span class="line">        <span class="keyword">for</span> inst <span class="keyword">in</span> insts])</span><br><span class="line"></span><br><span class="line">    batch_pos = np.array([</span><br><span class="line">        [pos_i + <span class="number">1</span> <span class="keyword">if</span> w_i != Constants.PAD <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">         <span class="keyword">for</span> pos_i, w_i <span class="keyword">in</span> enumerate(inst)] <span class="keyword">for</span> inst <span class="keyword">in</span> batch_seq]) <span class="comment"># 位置信息</span></span><br><span class="line"></span><br><span class="line">    batch_seq = torch.LongTensor(batch_seq)</span><br><span class="line">    batch_pos = torch.LongTensor(batch_pos)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> batch_seq, batch_pos</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>“莱斯杯”挑战赛有感</title>
      <link href="/2018/10/30/%E8%A7%81%E9%97%BB&amp;%E6%83%B3%E6%B3%95/%E2%80%9C%E8%8E%B1%E6%96%AF%E6%9D%AF%E2%80%9D%E6%8C%91%E6%88%98%E8%B5%9B%E6%9C%89%E6%84%9F/"/>
      <url>/2018/10/30/%E8%A7%81%E9%97%BB&amp;%E6%83%B3%E6%B3%95/%E2%80%9C%E8%8E%B1%E6%96%AF%E6%9D%AF%E2%80%9D%E6%8C%91%E6%88%98%E8%B5%9B%E6%9C%89%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>历时三个月的“莱斯杯”全国第一届“军事智能·机器阅读”挑战赛终于落下帷幕，前几日（10.26-10.28）有幸在南京青旅宾馆参与决赛，体验多多，收获满满，心中亦有一些感想。</p><p>一个是南京总带给我一种回家的感觉，对南京的事物总有亲切感。第一次来南京是一年半前，也是来参加比赛。周五晚上的夜游秦淮，让我感受到许久未曾感受到的烟火气息。</p><p><img src="/images/2018-10-30-511540860855_.pic_hd.jpg" width="90%" height="50%"></p><p>第二个是此次主办方提供的食宿令人惊喜。一开始听到青旅宾馆，我已经做好了艰苦奋战的准备了，然而酒店是星级酒店的，吃方面直接到楼下的自助。可以看出主办方此次确实用心在举办这次比赛。</p><p><img src="/images/2018-10-30-15408644190676.jpg" width="100%" height="50%"></p><p>第三点是关于比赛的，关于比赛的整个历程我还是颇有感触。<br>我们是以第9名的成绩挺进决赛，其实在后期比赛中，我们都有所懈怠了，几乎没有花时间在这上面，10月初发布决赛的数据集，而我们在10月20日才得知这一事情，此时离决赛只剩一周时间。因此我们确实准备不足。当然我们也没有预料到我们的决赛成绩会这么靠前，否则我们肯定会更加充分去准备。这确实是我们的失误。</p><p>我们在比赛过程中，一直尝试在使用ELMo，这正是我负责的部分。一开始使用官方TensorFlow的代码，费了九牛二虎之力我才跑通代码，但因为队长使用的是pytorch，而二者在cuda版本上不兼容，因此在初赛我们没有使用ELMo。而在最后几天，我尝试使用哈工大的pytorch训练代码，但因为inference速度实在太慢，我们最终还是弃用了这个方案。而在决赛现场，我们发现也确实是因为速度和资源的原因，大家都没有使用ELMo，除了一组。该组正是凭借了ELMo弯道超车从第7升到了第一，拿走了20万大奖。这也是我们非常遗憾的一个地方，我们在遇到困难时没有尝试解决，而是直接弃用，最终没有取得更好的成绩。</p><p>此次我们的成绩排名第4(三等奖)，是有一定的进步的，但有一点遗憾的是，我们仅差0.18百分点，就能超过第三名拿到5万的奖金了。后面我们分析了一下，还是因为我们对比赛懈怠的态度，其他组都对数据进行了分析并有针对性的改进，而我们并没有做这一步。</p><p>Anyway，第一次组队参加比赛就有收获，增长了见识，从交流中也获得了许多。这个比赛之后，就得好好看paper了。 __(:з」∠)_</p><p><img src="/images/2018-10-30-521540861008_.pic_hd.jpg" width="80%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 有感 </tag>
            
            <tag> 莱斯杯 </tag>
            
            <tag> 比赛 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词13</title>
      <link href="/2018/10/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D13/"/>
      <url>/2018/10/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D13/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣行路难三首"><a href="#1️⃣行路难三首" class="headerlink" title="1️⃣行路难三首"></a>1️⃣行路难三首</h3><p>[唐] 李白<br>【其一】<br>金樽清酒斗十千，玉盘珍羞直万钱。<br><strong>停杯投箸不能食，拔剑四顾心茫然</strong>。<br>欲渡黄河冰塞川，将登太行雪满山。<br>闲来垂钓碧溪上，忽复乘舟梦日边。<br>行路难，行路难，多歧路，今安在？<br><strong>长风破浪会有时，直挂云帆济沧海</strong>！</p><p><strong>注释</strong>：<br>「闲来垂钓碧溪上，忽复乘舟梦日边。」句：暗用典故：姜太公吕尚曾在渭水的磻溪上钓鱼，得遇周文王，助周灭商；伊尹曾梦见自己乘船从日月旁边经过，后被商汤聘请，助商灭夏。这两句表示诗人自己对从政仍有所期待。碧，一作「坐」。</p><hr><h3 id="2️⃣登科后"><a href="#2️⃣登科后" class="headerlink" title="2️⃣登科后"></a>2️⃣登科后</h3><p>[唐] 孟郊<br>昔日龌龊不足夸，今朝放荡思无涯。<br><strong>春风得意马蹄疾，一日看尽长安花</strong>。</p><p><strong>注释</strong>：<br>龌龊（wò chuò）：原意是肮脏，这里指不如意的处境。不足夸：不值得提起。<br>放荡（dàng）：自由自在，不受约束。<br>思无涯：兴致高涨。</p><p><a href="http://m.xichuangzhu.com/work/57add198a633bd0057eefa8a" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57add198a633bd0057eefa8a</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文3</title>
      <link href="/2018/10/29/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%873/"/>
      <url>/2018/10/29/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%873/</url>
      
        <content type="html"><![CDATA[<h2 id="1️⃣-A-Neural-Probabilistic-Language-Model"><a href="#1️⃣-A-Neural-Probabilistic-Language-Model" class="headerlink" title="1️⃣[A Neural Probabilistic Language Model]"></a>1️⃣[A Neural Probabilistic Language Model]</h2><p>第一篇使用神经网络获得词向量的paper。</p><p>通过对language model建模，将词映射到低维表示，在训练过程中同时训练语言模型以及每个词的词向量。</p><p><img src="/images/2018-10-29-15407808716787.jpg" width="50%" height="50%"></p><p>将中心词的前n个拼接起来 $x=(C(w_{t-1},C(w_{t-2}),…,C(w_{t-n+1}))$<br>将$x$送入神经网络中获得$y=b+Wx+Utanh(d+Hx)$，最后做一个softmax即可。</p><hr><h2 id="2️⃣-Adaptive-Computation-Time-for-Recurrent-Neural-Networks"><a href="#2️⃣-Adaptive-Computation-Time-for-Recurrent-Neural-Networks" class="headerlink" title="2️⃣[Adaptive Computation Time for Recurrent Neural Networks]"></a>2️⃣[Adaptive Computation Time for Recurrent Neural Networks]</h2><p>一种允许RNN动态堆叠层数的算法。</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>证据证明，RNN的堆叠层数多，效果会有提升。但是，对于不同的任务，要求不同的计算复杂度。我们需要先验来决定特定任务的计算复杂度。当然我们可以粗暴地直接堆叠深层的网络。ACT(Adaptive Computation Time)能够动态决定每个输入t所需的计算次数。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>将RNN每一步的输出过一个网络+sigmoid层，获得一个概率分布，也即什么时候应当停止不再继续往上堆叠，直到概率加和为1。同时为了尽可能抑制层数的无限增长，在loss添加一项惩罚。</p><h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>对于普通的RNN：<br><img src="/images/2018-10-29-15408103221289.jpg" width="30%" height="50%"></p><p>s是隐藏层；y是输出。</p><p>对于ACT的RNN，有：<br><img src="/images/2018-10-29-15408103823681.jpg" width="40%" height="50%"></p><p>上标n是指的t时刻的层数；其中：<br><img src="/images/2018-10-29-15408104201718.jpg" width="20%" height="50%"></p><p>$δ$是flat，指示x是第几次输入。</p><p>引入新的网络，输入时隐状态，输出是一个概率分布：<br><img src="/images/2018-10-29-15408105451770.jpg" width="30%" height="50%"></p><p>那么每一层的概率是：<br><img src="/images/2018-10-29-15408105687677.jpg" width="35%" height="50%"></p><p>其中$R(t)$是在每一层概率求和超过1时的剩余概率（为了保证概率和为1，可以试着举一个例子来证明）<br><img src="/images/2018-10-29-15408106099743.jpg" width="45%" height="50%"></p><p><img src="/images/2018-10-29-15408106125837.jpg" width="25%" height="50%"></p><p>ε是为了解决第一次输出时就超过1-ε的情况，ε一般取很小。</p><p>最终，加权求和，作为最终的结果，传入下一个时间步：<br><img src="/images/2018-10-29-15408106649319.jpg" width="45%" height="50%"></p><p>普通RNN与ACT的RNN对比：<br><img src="/images/2018-10-29-15408106950342.jpg" width="90%" height="50%"></p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>为了防止模型层数无限增长，添加一项惩罚项以抑制。</p><p>记每一步的惩罚项为：<br><img src="/images/2018-10-29-15408107184035.jpg" width="23%" height="50%"></p><p>总的惩罚项则为：<br><img src="/images/2018-10-29-15408107351871.jpg" width="19%" height="50%"></p><p>Loss function则为：<br><img src="/images/2018-10-29-15408108024183.jpg" width="35%" height="50%"></p><p>因为N(t)是不可导的，我们在实际过程中只去最小化R(t)  （<del>我觉得不甚合理</del>，一种解读是如果我们不断最小化R(t)直到变成0，那么相当于N(t)少了一层，接着R(t)就会变得很大，然后又继续最小化R(t)…）</p><hr><h2 id="3️⃣-Universal-Transformers"><a href="#3️⃣-Universal-Transformers" class="headerlink" title="3️⃣[Universal Transformers]"></a>3️⃣[Universal Transformers]</h2><p>提出一种新型通用的transformer。</p><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>Transformer的问题：RNN的归纳偏置(inductive bias)在一些任务上很重要，也即RNN的循环学习的过程；Transformer在一些问题上表现不好，可能是归纳偏置的原因。</p><blockquote><p>Notably, however, the Transformer foregoes the RNN’s inductive bias towards learning iterative or recursive transformations.Our experiments indicate that this inductive bias may be crucial for several algorithmic and language understanding tasks of varying complexity: in contrast to models such as the Neural Turing Machine [13], the Neural GPU [17] or Stack RNNs [16], the Transformer does not generalize well to input lengths not encountered during training.</p></blockquote><p>因此在Transformer内引入归纳偏置</p><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li>每一层的权重是共享的，也即multi-head上的权重以及transition function在每一层是一致的。这一点和RNN、CNN一致。</li><li>动态层数（ACT mechanism ）：对于每个词都会有不同的循环次数；也即有些词需要更多的refine；而有些词不需要。和固定层数的transformer相比，会有更好的通用性。</li></ul><h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><h4 id="总体架构"><a href="#总体架构" class="headerlink" title="总体架构"></a>总体架构</h4><p><img src="/images/2018-10-29-15408125098915.jpg" width="90%" height="50%"></p><p>过程：<br><img src="/images/2018-10-29-15408125469899.jpg" width="45%" height="50%"></p><p><img src="/images/2018-10-29-15408125685038.jpg" width="70%" height="50%"></p><p><img src="/images/2018-10-29-15408125974795.jpg" width="70%" height="50%"></p><p><img src="/images/2018-10-29-15408126143943.jpg" width="60%" height="50%"></p><p>和普通Transformer不同的地方在于：</p><ul><li>加了一层Transition层，Transition可以是depth-wise separable convolution（<a href="https://www.cnblogs.com/adong7639/p/7918527.html" target="_blank" rel="noopener">是什么？</a>）或者全连接层。</li><li>每层都添加了position embedding；以及timestep embedding，用以指示层数。</li></ul><h4 id="ACT"><a href="#ACT" class="headerlink" title="ACT"></a>ACT</h4><p>由于一个句子中间，有些词比其他词更难学会，需要更多计算量，但堆叠太多层会大大增加计算量，为了节省计算量，我们可以引入ACT来动态分配计算量。</p><p>ACT原来用于RNN，在Transformer中，当halting unit指示词t应当停止时，直接讲该词的状态复制到下一个time step，直到所有的词都停止。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Embedding </tag>
            
            <tag> Paper </tag>
            
            <tag> Transformer </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> ACT </tag>
            
            <tag> Language Modeling </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词12</title>
      <link href="/2018/10/21/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D12/"/>
      <url>/2018/10/21/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D12/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣望海潮"><a href="#1️⃣望海潮" class="headerlink" title="1️⃣望海潮"></a>1️⃣望海潮</h3><p>[宋] 柳永<br>东南形胜，三吴都会，钱塘自古繁华。烟柳画桥，风帘翠幕，参差十万人家。云树绕堤沙，怒涛卷霜雪，天堑无涯。市列珠玑，户盈罗绮，竞豪奢。<br>重湖叠巘清嘉，有三秋桂子，十里荷花。羌管弄晴，菱歌泛夜，嬉嬉钓叟莲娃。千骑拥高牙，乘醉听箫鼓，吟赏烟霞。异日图将好景，归去凤池夸。</p><p>叠巘（yǎn）：层层叠叠的山峦。</p><p><a href="http://m.xichuangzhu.com/work/57b318228ac247005f2223db" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b318228ac247005f2223db</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录8</title>
      <link href="/2018/10/21/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%958/"/>
      <url>/2018/10/21/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%958/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-batchify"><a href="#1️⃣-batchify" class="headerlink" title="1️⃣[batchify]"></a>1️⃣[batchify]</h3><p>快速将数据分成batch。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchify</span><span class="params">(data, bsz)</span>:</span></span><br><span class="line">    <span class="comment"># Work out how cleanly we can divide the dataset into bsz parts.</span></span><br><span class="line">    nbatch = data.size(<span class="number">0</span>) // bsz</span><br><span class="line">    <span class="comment"># Trim off any extra elements that wouldn't cleanly fit (remainders).</span></span><br><span class="line">    data = data.narrow(<span class="number">0</span>, <span class="number">0</span>, nbatch * bsz)</span><br><span class="line">    <span class="comment"># Evenly divide the data across the bsz batches.</span></span><br><span class="line">    data = data.view(bsz, <span class="number">-1</span>).t().contiguous()</span><br><span class="line">    <span class="keyword">return</span> data.to(device)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PRML第四章 分类的线性模型</title>
      <link href="/2018/10/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%88%86%E7%B1%BB%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
      <url>/2018/10/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%88%86%E7%B1%BB%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="判别函数"><a href="#判别函数" class="headerlink" title="判别函数"></a>判别函数</h1><p><img src="/images/2018-10-21-Xnip2018-10-21_09-26-42.jpg" alt="0"></p><hr><p><img src="/images/2018-10-21-Xnip2018-10-21_09-27-57.jpg" alt="1"></p><p>—-未完—-</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> PRML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文2</title>
      <link href="/2018/10/20/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%872/"/>
      <url>/2018/10/20/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%872/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-An-Empirical-Evaluation-of-Generic-Convolutional-and-Recurrent-Networks-for-Sequence-Modeling"><a href="#1️⃣-An-Empirical-Evaluation-of-Generic-Convolutional-and-Recurrent-Networks-for-Sequence-Modeling" class="headerlink" title="1️⃣[An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling]"></a>1️⃣[An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling]</h3><p>本文贡献：提出一种新的模型<strong>TCN（Temporal Convolutional Networks）</strong>进行language model建模。</p><h4 id="Dilated-convolution"><a href="#Dilated-convolution" class="headerlink" title="Dilated convolution"></a>Dilated convolution</h4><p>每一层的感受野都可以是不同的，也即，同样的kernel size，高层的可以跳着看。<br><img src="/images/2018-10-20-15400016170606.jpg" width="60%" height="50%"></p><p>每层的d逐渐增大（也即跳的步数），一般按指数增大。（我觉得这样很有道理，如果每一层的d都是一样的，那capture到的信息就会有重复，能看到的视野也不如逐渐增大的多）</p><h4 id="Residual-block"><a href="#Residual-block" class="headerlink" title="Residual block"></a>Residual block</h4><p><img src="/images/2018-10-20-15400017320092.jpg" width="70%" height="50%"></p><p>这边的residual block比较复杂；一个值得主意的细节是，因为感受野的不同，上层的感受野总是比下层的大很多，因此不应该直接将下层的加到上层，而是可以使用一个1*1的convolution对下层的x进行卷积，这就类似scale对输入进行放缩。</p><hr><h3 id="2️⃣-Dissecting-Contextual-Word-Embeddings：-Architecture-and-Representation"><a href="#2️⃣-Dissecting-Contextual-Word-Embeddings：-Architecture-and-Representation" class="headerlink" title="2️⃣[Dissecting Contextual Word Embeddings： Architecture and Representation]"></a>2️⃣[Dissecting Contextual Word Embeddings： Architecture and Representation]</h3><p>一篇分析的文章。ELMo作者的又一篇文章。</p><p>对比三种不同的建模方式（LSTM/GCNN/Transformer）获得的词向量，以及在不同任务上的表现；以及不同层获得的不同信息…获得了不同的结论。</p><p>①biLM 专注于word morphology词的形态；底层的LM关注local syntax；而高层的LM关注semantic content；</p><p>②不同的任务会有不同的正则化s的倾向。</p><hr><h3 id="3️⃣-Transformer-XL-Language-modeling-with-longer-term-dependency"><a href="#3️⃣-Transformer-XL-Language-modeling-with-longer-term-dependency" class="headerlink" title="3️⃣[Transformer-XL: Language modeling with longer-term dependency]"></a>3️⃣[Transformer-XL: Language modeling with longer-term dependency]</h3><p>利用Transformer进行language model，与普通的Transformer建模不同的是，Transformer-XL添加了历史信息，能够显著提升表现。这篇还在ICLR2019审稿中。</p><p>贡献：本文提出了能够进行长程依赖的基于Transformer的语言模型 Transformer-XL；引入相对位置的positional encoding。</p><h4 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h4><p>原先的transformer language model是将句子分为一个一个segment。segment之间是没有联系的。（为什么不直接按原版的Transformer那样所有的词都相互做self-attention？因为考虑到效率问题，句子长度可能会很长）</p><p>训练阶段：<br><img src="/images/2018-10-20-15400023645268.jpg" width="35%" height="50%"></p><p>而在测试阶段，每次向右滑动一格：<br><img src="/images/2018-10-20-15400024115822.jpg" width="80%" height="50%"><br>这样每一个时间步都要重新计算一遍，历史信息没有利用到。显然速度很慢。</p><p>在Transformer引入recurrence，也即引入历史信息。基于这样的想法，提出的新模型Transformer-XL。在结构上同样分为每个segment，但在每个阶段都接收上一个（甚至上L个）历史信息。</p><p>训练阶段：<br><img src="/images/2018-10-20-15400026059113.jpg" width="80%" height="50%"></p><p>而在测试阶段，同样分为segment，但因为接收了历史信息，不需要每次滑动一格也能获得大量信息。<br><img src="/images/2018-10-20-15400027040526.jpg" width="45%" height="50%"></p><p>具体来说：<br><img src="/images/2018-10-20-15400027302545.jpg" width="120%" height="50%"><br>SG代表stop gradient，和该阶段的hidden state进行拼接。</p><h4 id="RELATIVE-POSITIONAL-ENCODINGS"><a href="#RELATIVE-POSITIONAL-ENCODINGS" class="headerlink" title="RELATIVE POSITIONAL ENCODINGS"></a>RELATIVE POSITIONAL ENCODINGS</h4><p>如果我们使用了absolute positional encodings（也即原版的positional encodings）那么会出现这种情况</p><p><img src="/images/2018-10-20-15400027991211.jpg" width="70%" height="50%"></p><p>在同一层之间的前一个segment和后一个segment使用了同样的绝对位置信息，对于当前segment的高层，对于同一个位置i，无法区分该位置信息是来自当前segment的还是上一个segment的（因为都是同样的绝对位置）。</p><p>因此我们引入相对位置信息R，其中第i行代表相对距离i的encoding。</p><p>具体来说：</p><p>首先我们在传统的计算$query_i$和$key_j$的attention分数时，可以拆解成：</p><p><img src="/images/2018-10-20-15400030310583.jpg" width="80%" height="50%"><br>（因为query=(embedding E +positional embedding U），key也一样，将式子拆开就能获得上述式子)</p><p>我们将该式子进行修改：</p><p><img src="/images/2018-10-20-15400031662378.jpg" width="80%" height="50%"></p><p>第一，将出现了absolute positional embedding $U$的地方，统统改成$R_{i-j}$，也即在b和d项。其中这里的R和原版的Transformer的位置计算公式相同。</p><p>第二，在c项中，使用一个$u$替代了$U_i W_q$，这一项原本的意义在于，$query_i$的positional encoding对$key_j$的embedding进行attention，也就是说，该项表现了$query_i$位置对哪些$key_j$的内容有兴趣，作者认为query不管在哪个位置上都是一样的，也就是说query的位置信息应当没影响，所以统统替换成一个可学习的$u$。基于类似的理由d项换成了$v$。</p><p>第三，将$W_k$细分成了两个$W_{k,E}$和$W_{k,R}$。这是根据query是Embedding还是positional encoding来区分的。for producing the content-based key vectors and location-based key vectors respectively</p><p>每一项现在都有了不同的意义：</p><blockquote><p>Under the new parameterization, each term has an intuitive meaning: term (a) represents contentbased addressing, term (b) captures a content-dependent positional bias, term (c) governs a global content bias, and (d) encodes a global positional bias.</p></blockquote><p>最后总结一下整个结构：</p><p><img src="/images/2018-10-20-15400040342520.jpg" width="120%" height="50%"></p><p>与原版Transformer不同的是，Transformer-XL在每一层都添加了位置信息。</p><hr><h3 id="4️⃣-Trellis-Networks-for-Sequence-Modeling"><a href="#4️⃣-Trellis-Networks-for-Sequence-Modeling" class="headerlink" title="4️⃣[Trellis Networks for Sequence Modeling]"></a>4️⃣[Trellis Networks for Sequence Modeling]</h3><p>一种结合RNN和CNN的语言建模方式。</p><p>最小的单元结构：</p><p><img src="/images/2018-10-21-15400858162232.jpg" width="40%" height="50%"></p><p>也即：<br><img src="/images/2018-10-21-15400860605560.jpg" width="40%" height="50%"></p><p>接下来再处理非线性：<br><img src="/images/2018-10-21-15400861618655.jpg" width="30%" height="50%"></p><p>因为每层都要输入x，且W是共享的，所以我们可以提前计算好这一项，后面直接用即可。<br><img src="/images/2018-10-21-15400861870898.jpg" width="35%" height="50%"></p><p>最终在实现的时候是：<br><img src="/images/2018-10-21-15400862184335.jpg" width="40%" height="50%"></p><p><img src="/images/2018-10-21-15400862303741.jpg" width="40%" height="50%"></p><p>总体框架：<br><img src="/images/2018-10-21-15400874987498.jpg" width="70%" height="50%"></p><p>与TCN（temporal convolution network）不同之处：①filter weight不仅在time step之间共享，在不同层之间也共享；②在每一层都添加了输入</p><p>优点：共享了W，显著减少了参数；‘Weight tying can be viewed as a form of regularization that can stabilize training’</p><p>我们还可以扩展该网络，引入gate：<br><img src="/images/2018-10-21-15400875805208.jpg" width="40%" height="50%"></p><hr><h3 id="5️⃣-Towards-Decoding-as-Continuous-Optimisation-in-Neural-Machine-Translation"><a href="#5️⃣-Towards-Decoding-as-Continuous-Optimisation-in-Neural-Machine-Translation" class="headerlink" title="5️⃣[Towards Decoding as Continuous Optimisation in Neural Machine Translation]"></a>5️⃣[Towards Decoding as Continuous Optimisation in Neural Machine Translation]</h3><p>一篇很有意思的paper。用于NMT decode的inference阶段。这篇有一定的难度，以下只是我的理解。</p><h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><p>Motivation：<br>NMT中的decode inference阶段，通常都是从左到右的，这样有个缺点，就是整体的target之间的依赖是没有被充分利用到的，比如说生成的词的右边是没有用到的。那么我们为什么不直接全部生成呢？然后不断更新。也就是说我们将离散（discrete）的decode过程变成一个连续的过程（continuous optimization）。</p><p>假设我们已经训练好模型，给定一个句子，我们要翻译成目标句子，且假设我们已知要生成的句子长度是l，那么我们有：<br><img src="/images/2018-10-21-15400876953609.jpg" width="45%" height="50%"><br>我们要找到一个最优的序列$y$，使得$-log$最小。</p><p>等价于：<br><img src="/images/2018-10-21-15400877226851.jpg" width="55%" height="50%"><br>其中$\widetilde{y}_i$是one-hot。其实这里就是假设有这么一个ground truth，但实际上是没有的。</p><p>我们将$\widetilde{y}_i$是one-hot这个条件放宽一些，变成是一个概率单纯型（其实就是所有元素加起来是1，且都大于等于0）。</p><p>那么就变成了：<br><img src="/images/2018-10-21-15400879019592.jpg" width="50%" height="50%"></p><p>这个改变的本质是：<br><img src="/images/2018-10-21-15400879379023.jpg" width="50%" height="50%"></p><p>就是说原来one-hot的$\widetilde{y}_i$生成后丢到下一个时间步，取了一个词向量，接着计算。现在是一个概率分布$\hat{y}_i$丢进来，就相当于取了多个词向量的加权求和。</p><p>在利用下述的更新算法更新完$\hat{y}_i$之后，对于每个时间步t，我们找$\hat{y}_i$中元素最大的值对应的词作为生成的词。</p><p>有两种方法Exponentiated Gradient 和 SGD。实际上方法倒在其次了，主要是前面所述的continuous optimization这种思想。</p><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><h5 id="Exponentiated-Gradient"><a href="#Exponentiated-Gradient" class="headerlink" title="Exponentiated Gradient"></a>Exponentiated Gradient</h5><p><img src="/images/2018-10-21-15400881918713.jpg" width="80%" height="50%"><br>具体见论文</p><h5 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h5><p>因为我们要保证单纯形的约束不变，因此我们引入一个r，然后做一个softmax<br><img src="/images/2018-10-21-15400882306948.jpg" width="80%" height="50%"></p><h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>这种连续decode可以用在哪？</p><h5 id="Bidirectional-Ensemble"><a href="#Bidirectional-Ensemble" class="headerlink" title="Bidirectional Ensemble"></a>Bidirectional Ensemble</h5><p>可以很方便地进行双向的生成：</p><p><img src="/images/2018-10-21-15400883321474.jpg" width="45%" height="50%"><br>而在传统的方法中没办法（很难）做到</p><h5 id="Bilingual-Ensemble"><a href="#Bilingual-Ensemble" class="headerlink" title="Bilingual Ensemble"></a>Bilingual Ensemble</h5><p>我们希望源语言到目标语言和目标到源语言都生成得好</p><p><img src="/images/2018-10-21-15400883583228.jpg" width="50%" height="50%"></p><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>$\hat{y}_i$的初始化很重要，一不小心就会陷入local minima；生成的速度慢</p><hr><h3 id="6️⃣-Universal-Language-Model-Fine-tuning-for-Text-Classiﬁcation"><a href="#6️⃣-Universal-Language-Model-Fine-tuning-for-Text-Classiﬁcation" class="headerlink" title="6️⃣[Universal Language Model Fine-tuning for Text Classiﬁcation]"></a>6️⃣[Universal Language Model Fine-tuning for Text Classiﬁcation]</h3><p>和ELMo、OpenAI GPT一样，都是预训练语言模型，迁移到其他任务上（这里是分类任务）。可以在非常小的数据集上有很好的效果。</p><p>贡献：</p><ol><li>迁移学习模型ULMFiT</li><li>提出几种trick：discriminative ﬁne-tuning, slanted triangular learning rates,gradual unfreezing ，最大保证知识的保留。</li></ol><h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p><img src="/images/2018-10-21-15401043145373.jpg" width="90%" height="50%"></p><p>三部曲：</p><ol><li>通用语言模型预训练</li><li>目标任务的语言模型fine-tuning</li><li>目标任务的分类fine-tuning</li></ol><h4 id="trick"><a href="#trick" class="headerlink" title="trick"></a>trick</h4><h5 id="Discriminative-ﬁne-tuning"><a href="#Discriminative-ﬁne-tuning" class="headerlink" title="Discriminative ﬁne-tuning"></a>Discriminative ﬁne-tuning</h5><p>Motivation：不同层有不同的信息；应当fine-tune 不同程度，也即使用不同的learning rate。</p><p><img src="/images/2018-10-21-15401044160803.jpg" width="35%" height="50%"></p><p>作者发现上一层的学习率是下一层的2.6倍时效果比较好。</p><h5 id="Slanted-triangular-learning-rates-STLR"><a href="#Slanted-triangular-learning-rates-STLR" class="headerlink" title="Slanted triangular learning rates (STLR)"></a>Slanted triangular learning rates (STLR)</h5><p><img src="/images/2018-10-21-15401045153164.jpg" width="60%" height="50%"></p><p>具体公式：<br><img src="/images/2018-10-21-15401045316305.jpg" width="50%" height="50%"></p><h5 id="Gradual-unfreezing"><a href="#Gradual-unfreezing" class="headerlink" title="Gradual unfreezing"></a>Gradual unfreezing</h5><p>从顶层到底层，一步一步unfreeze，也即从上到下fine-tune。这是因为最上一层有最少的general knowledge。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> TCN </tag>
            
            <tag> Transformer-XL </tag>
            
            <tag> Trellis Networks </tag>
            
            <tag> continuous decoding </tag>
            
            <tag> ULMFiT </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周论文1</title>
      <link href="/2018/10/14/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%871/"/>
      <url>/2018/10/14/%E8%AE%BA%E6%96%87/%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%871/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Learned-in-Translation-Contextualized-Word-Vectors"><a href="#1️⃣-Learned-in-Translation-Contextualized-Word-Vectors" class="headerlink" title="1️⃣[Learned in Translation: Contextualized Word Vectors]"></a>1️⃣[Learned in Translation: Contextualized Word Vectors]</h3><p>CoVe是第一个引入动态词向量的模型。<br>Motivation：翻译模型能够保存最多的信息，因为如果保存信息不够多，decoder接收到的信息不足，翻译效果就不会好。（但实际上，我个人认为，decoder的表现还和language model有关，如果decoder是一个好的language model，也有可能翻译出不错的结果）</p><p>做法：使用传统NMT的encoder-decoder的做法翻译模型，只是将(bi)LSTM所得到的隐层状态表示取出来和embedding拼接起来，作为一个词的表示：</p><script type="math/tex; mode=display">w=[GloVe(w); CoVe(w)]</script><hr><h3 id="2️⃣-Language-Modeling-with-Gated-Convolutional-Networks"><a href="#2️⃣-Language-Modeling-with-Gated-Convolutional-Networks" class="headerlink" title="2️⃣[Language Modeling with Gated Convolutional Networks]"></a>2️⃣[Language Modeling with Gated Convolutional Networks]</h3><p>使用CNN对语言模型进行建模，提高并行性。</p><p>贡献：使用了CNN进行language model建模；提出了简化版的gate机制应用在CNN中。</p><p>做法：<br><img src="/images/2018-10-14-15394870930257.jpg" width="50%" height="50%"></p><p>实际上就是一个输入两个filter，卷积出来的做一个gate的操作$H_0 = A⊗σ(B)$，控制流向下一层的数据。</p><p>一个小细节是，为了不让language model看到下一个词，每一层在开始卷积的时候会在左边添加kernel_size-1个padding。</p><p>扩展：因为CNN的并行性高，可以使用CNN来对language model建模替代ELMo，同样可以获得动态词向量。这个想法已经由提出ELMo的团队做出来并进行对比了。论文：Dissecting Contextual Word Embeddings: Architecture and Representation</p><p>目前正在<a href="https://github.com/linzehui/Gated-Convolutional-Networks" target="_blank" rel="noopener">复现</a>该论文 。</p><hr><h3 id="3️⃣-Attention-is-All-you-need"><a href="#3️⃣-Attention-is-All-you-need" class="headerlink" title="3️⃣[Attention is All you need]"></a>3️⃣[Attention is All you need]</h3><p>非常经典的论文。提出了Transformer。为了读BERT重温了一遍。<br><img src="/images/2018-10-14-15394876881322.jpg" width="70%" height="50%"></p><p><img src="/images/2018-10-14-15394877200390.jpg" width="70%" height="50%"></p><p><img src="/images/2018-10-14-15394877478814.jpg" width="70%" height="50%"></p><hr><h3 id="4️⃣-Improving-Language-Understanding-by-Generative-Pre-Training"><a href="#4️⃣-Improving-Language-Understanding-by-Generative-Pre-Training" class="headerlink" title="4️⃣[Improving Language Understanding by Generative Pre-Training]"></a>4️⃣[Improving Language Understanding by Generative Pre-Training]</h3><p>BERT就是follow这篇文章的工作。<br>使用Transformer预训练一个language model进行迁移学习。</p><p>训练过程分为两步：①使用未标记数据训练language model；②使用有标记数据进行fine-tune</p><p>Motivation：ELMo是训练好language model，然后获得动态词向量再用到其他任务上，这样就会多了很多参数。和ELMo不同的是，这里使用一个Transformer模型解决多种任务（利用迁移学习）。</p><p>贡献：使用Transformer进行language model建模；尝试利用language model进行迁移学习而不是另一种思路（ELMo）只提取词向量。</p><p>①无监督学习language model<br><img src="/images/2018-10-14-15395044176746.jpg" width="40%" height="50%"></p><p>具体到Transformer就是：<br><img src="/images/2018-10-14-15395044608239.jpg" width="50%" height="50%"></p><p>②监督学习（fine-tune）<br>根据输入预测标签<br><img src="/images/2018-10-14-15395045508824.jpg" width="35%" height="50%"></p><p>具体就是：<br><img src="/images/2018-10-14-15395045734859.jpg" width="40%" height="50%"></p><p>将两个任务一起训练，则有：<br><img src="/images/2018-10-14-15395045932795.jpg" width="30%" height="50%"></p><p>对于不同任务，对输入进行一定的改动以适应Transformer结构：<br><img src="/images/2018-10-14-15395046364928.jpg" width="90%" height="50%"></p><hr><h3 id="5️⃣-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding"><a href="#5️⃣-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding" class="headerlink" title="5️⃣[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]"></a>5️⃣[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]</h3><p>刷爆各榜单的一篇神文。使用Transformer预训练一个language model进行迁移学习。</p><p>Motivation：之前的language model只能根据前面的词来预测下一个（即使ELMo是双向的LSTM，也是分别训练一个前向和一个后向的），限制了双向的context；因此提出了双向的language model。</p><h4 id="做法："><a href="#做法：" class="headerlink" title="做法："></a>做法：</h4><p>模型分为两个部分：<br>①masked LM：因为使用了两边的context，而language model的目的是预测下一个词，这样模型会提前看到下一个词，为了解决该问题，训练的时候讲部分词mask掉，最终只预测被mask掉的词。</p><p>②Next Sentence Prediction：随机50%生成两个句子是有上下句关系的，50%两个句子是没有关系的，然后做分类；具体来说是拿第一个词[CLS]（这是手动添加的）的表示，过一个softmax层得到。<br><img src="/images/2018-10-14-15394891973653.jpg" width="50%" height="50%"></p><p>联合训练这两个任务。</p><p>接下来是通过具体的任务进行fine-tune。一个模型解决多种问题：<br><img src="/images/2018-10-14-15395038593955.jpg" width="80%" height="50%"></p><p>本文贡献：使用Transformer进行双向的language model建模。论文提到的一些细节/tricks非常值得讨论，比如对token embedding添加了许多信息，非常简单粗暴。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Transformer </tag>
            
            <tag> 每周论文阅读 </tag>
            
            <tag> CoVe </tag>
            
            <tag> GCNN </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 18:Deep Reinforcement Learning</title>
      <link href="/2018/10/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2018:%20Deep%20Reinforcement%20Learning/"/>
      <url>/2018/10/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2018:%20Deep%20Reinforcement%20Learning/</url>
      
        <content type="html"><![CDATA[<p>记号： $a$是action，$s$即外部状态state，$\pi_{\theta}(s)$也即从$s$映射到$a$的函数；$r$是reward，每采取一个动作，会有一个reward，则总的reward为</p><script type="math/tex; mode=display">R_\theta = \sum_{t=1}^{T} r_t</script><p>我们使用神经网络来拟合$\pi$，一个eposide $\tau$是一个流程下来的的所有state、action和reward的集合。</p><script type="math/tex; mode=display">\tau = \{s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T \}</script><p>如果我们使用相同的actor运行n次，则每个$\tau$会有一定的概率被采样到，采样概率记为$P(\tau|\theta)$，则我们可以通过采样的方式来对期望reward进行估计：</p><script type="math/tex; mode=display">\overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta) ≈ \frac{1}{N} \sum_{n=1}^{N} R(\tau^n)</script><p>那么我们接下来的<strong>目标</strong>就是最大化期望reward，其中期望reward是：</p><script type="math/tex; mode=display">\overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta)</script><p>我们同样使用梯度上升：其中与$θ$相关的是$P$，则可以写成：</p><script type="math/tex; mode=display">\nabla \overline{R}_\theta = \sum_\tau R(\tau) \nabla P(\tau|\theta)= \sum_\tau R(\tau) P(\tau|\theta) \frac{\nabla P(\tau|\theta)}{P(\tau|\theta)}</script><p>由于$\dfrac {d\log \left( f\left( x\right) \right) }{dx}=\dfrac {1}{f\left( x\right) }\dfrac {df(x)}{dx}$，则前式可写成：</p><script type="math/tex; mode=display">\nabla \overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta) \nabla log P(\tau | \theta) ≈ \frac{1}{N} \sum_{n=1}^{N} R(\tau^n) log P(\tau ^n| \theta)</script><p>如何求梯度？<br>由于：</p><script type="math/tex; mode=display">P(\tau | \theta)=p(s_1)p(a_1|s_1,\theta)p(r_1,s_2|s_1,a_1)p(a_2|s_2,\theta)p(r_2,s_3|s_2,a_2)...\\=p(s_1)\prod_{t=1}^{T}p(a_t|s_t,\theta)p(r_t , s_{t+1}| s_t,a_t)</script><p>实际上，其中与梯度相关的只有中间项$p(a_t|s_t,\theta)$，该项也即$π$函数，从state到action的映射。<br>取log并求导，有：</p><script type="math/tex; mode=display">\nabla log P(\tau | \theta)= \sum_{t=1}^{T} \nabla log p(a_t|s_t,\theta)</script><p>代回，因此最终$\overline{R}_\theta$的梯度为：</p><script type="math/tex; mode=display">\nabla \overline{R}_\theta = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^n) \nabla log p(a_{t}^n | s_t^n,\theta)</script><p>注意到该式子告诉我们，应考虑整体的reward而不应该只考虑每一步的reward；并且取log的原因可以理解成是对action取归一化，因为：</p><script type="math/tex; mode=display">\frac{\nabla p(a_t^n | s_t^n,\theta)}{p(a_t^n | s_t^n,\theta)}</script><p>也就是说对于那些出现次数较多的action，要衡量他们对reward的真正影响，应当对他们归一化。</p><p>为了让那些出现可能性较低的action不会因为没被sample到而在更新后被降低他们的概率，可以添加一个baseline，只有超过$b$的reward才会增加他们出现的概率。</p><script type="math/tex; mode=display">\nabla \overline{R}_\theta = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} (R(\tau^n)-b) \nabla log p(a_{t}^n | s_t^n,\theta)</script>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> Deep Reinforcement Learning </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 17:Ensemble</title>
      <link href="/2018/10/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2017:%20Ensemble/"/>
      <url>/2018/10/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2017:%20Ensemble/</url>
      
        <content type="html"><![CDATA[<h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>对于复杂模型，往往variance会大，通过对多个模型的平均，能够减小variance：<br><img src="/images/2018-10-14-15394832736339.jpg" width="50%" height="50%"></p><p>bagging的思想是多次有放回地采样N’个点（通常N’=N），然后对采样的几个数据集分别训练一个模型<br><img src="/images/2018-10-14-15394833007835.jpg" width="50%" height="50%"></p><p>测试的时候再对几个模型进行平均或投票<br><img src="/images/2018-10-14-15394833255306.jpg" width="50%" height="50%"></p><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>基本思想是对几个弱分类器线性加权，得到强分类器。分类器按先后顺序训练，每次训练完，对新模型分类错误的数据进行调高权重，而正确的数据则降低权重。</p><p>可以保证：只要分类器的错误率小于50%，在boosting后能够有100%的正确率（在训练集）。</p><p>证明过程略。</p><h2 id="Ensemble-Stacking"><a href="#Ensemble-Stacking" class="headerlink" title="Ensemble: Stacking"></a>Ensemble: Stacking</h2><p>基本思想：使用训练数据训练多个初级分类器，将初级分类器的输出作为次级分类器的输入，获得最终的输出。我们应当使用不同的训练数据来训练次级分类器<br><img src="/images/2018-10-14-15394833971028.jpg" width="50%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Ensemble </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>浅谈mask矩阵</title>
      <link href="/2018/10/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%B5%85%E8%B0%88mask%E7%9F%A9%E9%98%B5/"/>
      <url>/2018/10/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%B5%85%E8%B0%88mask%E7%9F%A9%E9%98%B5/</url>
      
        <content type="html"><![CDATA[<p>个人目前对mask矩阵的一点理解。</p><hr><h2 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h2><p>mask矩阵是什么？是一个由0和1组成的矩阵。一个例子是，在自然语言处理(NLP)中，句子的长度是不等长的，但因为我们经常将句子组成mini-batch用以训练，因此那些长度较短的句子都会在句尾进行填充0，也即padding的操作。一个mask矩阵即用以指示哪些是真正的数据，哪些是padding。如：<br><img src="/images/2018-10-12-15393574958961.jpg" width="50%" height="50%"><br>图片来源：<a href="https://www.cnblogs.com/neopenx/p/4806006.html" target="_blank" rel="noopener">Theano：LSTM源码解析</a></p><p>其中mask矩阵中1代表真实数据；0代表padding数据。</p><h2 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h2><p>为什么要使用mask矩阵？使用mask矩阵是为了让那些被mask掉的tensor不会被更新。考虑一个tensor T的size(a,b)，同样大小的mask矩阵M，相乘后，在反向回传的时候在T对应mask为0的地方，0的梯度仍为0。因此不会被更新。</p><h2 id="怎么做"><a href="#怎么做" class="headerlink" title="怎么做"></a>怎么做</h2><p>接下来介绍几种（可能不全）使用mask的场景。</p><h3 id="对输入进行mask"><a href="#对输入进行mask" class="headerlink" title="对输入进行mask"></a>对输入进行mask</h3><p>考虑NLP中常见的句子不等长的情况。设我们的输入的batch I:(batch_size,max_seqlen)，我们在过一层Embedding层之前，<br>在过了一层Embedding层，则有 E:(batch_size,max_seqlen,embed_dim)，如果我们希望Embedding是更新的(比如我们的Embedding是随机初始化的，那当然Embedding需要更新)，但我们又不希望padding更新。<br>一种方法即令E与M相乘。其中M是mask矩阵(batch_size,max_seqlen,1) (1是因为要broadcast），这样在Embedding更新梯度时，因为mask矩阵的关系，padding位置上的梯度就是0。<br>当然在Pytorch中还可以直接显式地写：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.embedding = nn.Embedding(vocab_size, embed_dim,padding_idx=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><p>而此时应当将padding显式添加到词典的第一个。</p><h3 id="对模型中间进行mask"><a href="#对模型中间进行mask" class="headerlink" title="对模型中间进行mask"></a>对模型中间进行mask</h3><p>一个很经典的场景就是dropout。<br>对于参数矩阵W:(h,w)，同样大小的mask矩阵M，在前向传播时令W’=W*M，则在反向传播时，M中为0的部分不被更新。<br>当然，我们可以直接调用PyTorch中的包<code>nn.Dropout()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Dropout(p=<span class="number">0.2</span>)</span><br><span class="line">input = torch.randn(<span class="number">20</span>, <span class="number">16</span>)</span><br><span class="line">output = m(input)</span><br></pre></td></tr></table></figure><h3 id="对loss进行mask"><a href="#对loss进行mask" class="headerlink" title="对loss进行mask"></a>对loss进行mask</h3><p>考虑NLP中的language model，每个词都需要预测下一个词，在一个batch中句子总是有长有短，对于一个短句，此时在计算loss的时候，会出现这样的场景：<code>&lt;pad&gt;</code>词要预测下一个<code>&lt;pad&gt;</code>词。举个例子：三个句子[a,b,c,d],[e,f,g],[h,i]，在组成batch后，会变成<br>X：</p><div class="table-container"><table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>a</td><td>b</td><td>c</td><td>d</td></tr><tr><td>e</td><td>f</td><td>g</td><td><code>&lt;pad&gt;</code></td></tr><tr><td>h</td><td>i</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr></tbody></table></div><p>Y：</p><div class="table-container"><table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>b</td><td>c</td><td>d</td><td><code>&lt;pad&gt;</code></td></tr><tr><td>f</td><td>g</td><td><code>&lt;eos&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr><tr><td>i</td><td><code>&lt;eos&gt;</code></td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr></tbody></table></div><p>X是输入，Y是预测。那么从第三行可以看出，<code>&lt;pad&gt;</code>在预测下一个<code>&lt;pad&gt;</code>。这显然是有问题的。<br>一种解决方案就是使用mask矩阵，在loss的计算时，将那些本不应该计算的mask掉，使得其loss为0，这样就不会反向回传了。<br>具体实践：在PyTorch中，以CrossEntropy为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">nn</span>.<span class="title">CrossEntropyLoss</span><span class="params">(weight=None, size_average=None, ignore_index=<span class="number">-100</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">reduce=None, reduction=’elementwise_mean’</span></span></span><br></pre></td></tr></table></figure><p>如果<code>reduction=None</code>则会返回一个与输入同样大小的矩阵。在与mask矩阵相乘后，再对新矩阵进行mean操作。<br>在PyTorch实践上还可以可以这么写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">masked_outputs = torch.masked_select(dec_outputs, mask)</span><br><span class="line">masked_targets = torch.masked_select(targets, mask)</span><br><span class="line">loss = my_criterion(masked_outputs, masked_targets)</span><br></pre></td></tr></table></figure><p>另一种更为简单的解决方案是，直接在CrossEntropy中设<code>ignore_index=0</code>，这样，在计算loss的时候，发现target=0时，会自动不对其进行loss的计算。其本质和mask矩阵是一致的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>mask矩阵可以用在任何地方，只要希望与之相乘的tensor相对应的地方不更新就可以进行mask操作。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 代码实践 </tag>
            
            <tag> mask矩阵 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>深度炼丹tricks合集</title>
      <link href="/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%B7%B1%E5%BA%A6%E7%82%BC%E4%B8%B9tricks%E5%90%88%E9%9B%86/"/>
      <url>/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%B7%B1%E5%BA%A6%E7%82%BC%E4%B8%B9tricks%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p>—-Deprecated—-</p><h2 id="调参技巧"><a href="#调参技巧" class="headerlink" title="调参技巧"></a>调参技巧</h2><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><h4 id="1️⃣zero-center"><a href="#1️⃣zero-center" class="headerlink" title="1️⃣zero-center"></a>1️⃣zero-center</h4><p>[9]将数据中心化</p><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><h4 id="1️⃣Xavier-initialization-7-方法"><a href="#1️⃣Xavier-initialization-7-方法" class="headerlink" title="1️⃣Xavier initialization[7]方法"></a>1️⃣Xavier initialization[7]方法</h4><p>适用[9]于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)</p><h4 id="2️⃣He-initialization-8-方法"><a href="#2️⃣He-initialization-8-方法" class="headerlink" title="2️⃣He initialization[8]方法"></a>2️⃣He initialization[8]方法</h4><p>适用[9]于ReLU：scale = np.sqrt(6/n)</p><h4 id="3️⃣Batch-normalization-10"><a href="#3️⃣Batch-normalization-10" class="headerlink" title="3️⃣Batch normalization[10]"></a>3️⃣Batch normalization[10]</h4><h4 id="4️⃣RNN-LSTM-init-hidden-state"><a href="#4️⃣RNN-LSTM-init-hidden-state" class="headerlink" title="4️⃣RNN/LSTM init hidden state"></a>4️⃣RNN/LSTM init hidden state</h4><p>Hinton[3]提到将RNN/LSTM的初始hidden state设置为可学习的weight</p><h3 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h3><h4 id="1️⃣Gradient-Clipping-5-6"><a href="#1️⃣Gradient-Clipping-5-6" class="headerlink" title="1️⃣Gradient Clipping[5,6]"></a>1️⃣Gradient Clipping[5,6]</h4><h4 id="2️⃣learning-rate"><a href="#2️⃣learning-rate" class="headerlink" title="2️⃣learning rate"></a>2️⃣learning rate</h4><p>原则：当validation loss开始上升时，减少学习率。<br>[1]Time/Drop-based/Cyclical Learning Rate</p><h4 id="3️⃣batch-size"><a href="#3️⃣batch-size" class="headerlink" title="3️⃣batch size"></a>3️⃣batch size</h4><p>[2]中详细论述了增加batch size而不是减小learning rate能够提升模型表现。保持学习率不变，提高batch size，直到batch size~训练集/10，接下来再采用学习率下降的策略。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]<a href="https://blog.goodaudience.com/how-to-make-your-model-happy-again-part-1-40d94a9ffb41" target="_blank" rel="noopener">How to make your model happy again — part 1</a></p><p>[2]<a href="https://arxiv.org/abs/1711.00489" target="_blank" rel="noopener">Don’t Decay the Learning Rate, Increase the Batch Size</a></p><p>[3]<a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf" target="_blank" rel="noopener">CSC2535 2013: Advanced Machine Learning Lecture 10 Recurrent neural networks</a></p><p>[4]<a href="https://zhuanlan.zhihu.com/p/25110150" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25110150</a></p><p>[5]<a href="https://arxiv.org/abs/1211.5063" target="_blank" rel="noopener">On the difficulty of training Recurrent Neural Networks</a></p><p>[6]<a href="https://arxiv.org/abs/1612.08083" target="_blank" rel="noopener">Language Modeling with Gated Convolutional Networks</a></p><p>[7]<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Understanding the difficulty of training deep feedforward neural networks</a></p><p>[8]<a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></p><p>[9]<a href="https://www.zhihu.com/question/41631631" target="_blank" rel="noopener">知乎：你有哪些deep learning（rnn、cnn）调参的经验？</a></p><p>[10]<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 调参 </tag>
            
            <tag> tricks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词11</title>
      <link href="/2018/10/07/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D11/"/>
      <url>/2018/10/07/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D11/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣赋得古原草送别"><a href="#1️⃣赋得古原草送别" class="headerlink" title="1️⃣赋得古原草送别"></a>1️⃣赋得古原草送别</h3><p>[唐] 白居易<br>离离原上草，一岁一枯荣。<br><strong>野火烧不尽，春风吹又生</strong>。<br>远芳侵古道，晴翠接荒城。<br>又送王孙去，萋萋满别情。</p><p>萋萋（qī）：形容草木长得茂盛的样子。</p><p><a href="http://m.xichuangzhu.com/work/57b8a5371532bc005b99da51" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8a5371532bc005b99da51</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PRML第三章 回归的线性模型</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%9B%9E%E5%BD%92%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%9B%9E%E5%BD%92%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="线性基函数模型"><a href="#线性基函数模型" class="headerlink" title="线性基函数模型"></a>线性基函数模型</h1><p><img src="/images/2018-10-07-Xnip2018-10-07_15-51-50.jpg" alt="0"></p><p><img src="/images/2018-10-07-Xnip2018-10-07_15-53-54.jpg" alt="1"></p><h1 id="偏置-⽅差分解"><a href="#偏置-⽅差分解" class="headerlink" title="偏置-⽅差分解"></a>偏置-⽅差分解</h1><p><img src="/images/2018-10-07-Xnip2018-10-07_15-56-30.jpg" alt="0"></p><h1 id="贝叶斯线性回归"><a href="#贝叶斯线性回归" class="headerlink" title="贝叶斯线性回归"></a>贝叶斯线性回归</h1><p><img src="/images/2018-10-07-Xnip2018-10-07_16-13-57.jpg" alt="1"></p><h1 id="贝叶斯模型⽐较"><a href="#贝叶斯模型⽐较" class="headerlink" title="贝叶斯模型⽐较"></a>贝叶斯模型⽐较</h1><p><img src="/images/2018-10-07-Xnip2018-10-07_23-30-33.jpg" alt="1"></p><h1 id="证据近似"><a href="#证据近似" class="headerlink" title="证据近似"></a>证据近似</h1><p><img src="/images/2018-10-09-Xnip2018-10-09_22-06-22.jpg" alt="1"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> PRML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 16:SVM</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2016:%20SVM/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2016:%20SVM/</url>
      
        <content type="html"><![CDATA[<p><strong>Hinge Loss+kernel method = SVM</strong></p><h3 id="Hinge-Loss"><a href="#Hinge-Loss" class="headerlink" title="Hinge Loss"></a>Hinge Loss</h3><p>SVM与logistic regression的区别即在于loss function的不同，logistic是cross entropy，而SVM是hinge loss<br><img src="/images/2018-10-07-15388878731499.jpg" width="50%" height="50%"></p><p>也即如果分类间隔大于1，则 $L(m_i)=max(0,1−m_i(w))$，则损失为0。因此SVM更具鲁棒性，因为对离群点不敏感。</p><p>对于linear SVM：</p><ul><li>定义函数 $f(x)=\sum_i w_i x_i +b=w^T x$</li><li>定义损失函数  $L(f)=\sum_n l(f(x^n),\hat{y}^n)+\lambda ||w||_2$，其中$l(f(x^n),\hat{y}^n)=max(0,1-\hat{y}^n f(x))$</li><li><p>梯度下降求解（省略了正则化）</p><script type="math/tex; mode=display">\frac{\partial{l(f(x^n),\hat{y}^n})}{\partial{w_i}}=  \frac{\partial{l(f(x^n),\hat{y}^n})}{\partial{f(x^n)}}  \frac{\partial{f(x^n)}}{\partial{w_i}} x_i^n</script><p>  而</p><script type="math/tex; mode=display">f(x^n)=w^T \cdot x^n</script></li></ul><script type="math/tex; mode=display">\frac{\partial{max(0,1-\hat{y}^n f(x^n)})}{\partial{f(x^n)}}=\left\{               \begin{array}{**lr**}                -\hat{y}^n & if  \hat{y}^n f(x^n)<1 \\                 0  & otherwise &                 \end{array}  \right.</script><p>因此最终有：<br><img src="/images/2018-10-07-15388891611785.jpg" width="55%" height="50%"><br>我们接下来用$c^n(w)$替代$-\delta(\hat{y}^n f(x^n)&lt;1) \hat{y}^n$</p><h3 id="Kernel-Method"><a href="#Kernel-Method" class="headerlink" title="Kernel Method"></a>Kernel Method</h3><p>一个事实：$w$是$x$的线性加和，其中$α$不等于0对应的$x$就是support vectors</p><p>证明：<br>我们前面说过，更新过程：<br><img src="/images/2018-10-07-15388894194698.jpg" width="30%" height="50%"></p><p>将其组织成向量形式：<br><img src="/images/2018-10-07-15388894627632.jpg" width="25%" height="50%"></p><p><strong>如果我们将$w$初始化成0向量</strong>，那么$w$最终就是$x$的线性组合。证毕</p><p>因为$c(w)$是hinge loss，因此大多数的值是0，会造成$α$稀疏。<br>如果我们将训练数据$x$组织成一个矩阵，那么有：<br><img src="/images/2018-10-07-15388895570090.jpg" width="25%" height="50%"><br>也即：<br><img src="/images/2018-10-07-15388895870336.jpg" width="40%" height="50%"></p><p>所以对于$f(x)$，有：<br><img src="/images/2018-10-07-15388896378645.jpg" width="50%" height="50%"></p><p>实际上$X^Tx$就是每个训练数据和$x$进行点积的结果，但实际上线性函数往往表达能力不强，我们希望$x$能够变成非线性的。如果我们引入kernel，将点积换成kernel，则会有：</p><script type="math/tex; mode=display">f(x)=\sum_n \alpha_n (x_n\cdot x)=\sum_n \alpha_n K(x_n,x)</script><p>所以我们的问题就变成了：</p><ul><li>定义函数 $f(x)=\sum_n \alpha_n K(x_n,x)$</li><li>找到最佳的α，最小化loss function：$L(f)=\sum_n l(f(x^n),\hat{y}^n)=\sum_n l(\sum_{n’} \alpha_{n’} K(x^{n^{‘}},x^n),\hat{y}^n)$</li></ul><p>实际上我们不需要真的知道$x$的非线性的具体形式，我们只需要会算$K$就行，这种绕过$x$的具体形式的方法就是<strong>kernel trick</strong>。直接计算$K$，比先将$x$非线性转化再做点积来得高效。甚至有时候，我们对$x$做的非线性是无穷多维的，是无法直接做非线性化的。比如RBF核:</p><script type="math/tex; mode=display">K(x,z)=exp(-\frac{1}{2}||x-z||_2)</script><p>通过泰勒展开可以知道，RBF核是无穷维的。</p><p>另一个kernel的例子是sigmoid kernel：</p><script type="math/tex; mode=display">K(x,z)=tanh(x\cdot z)</script><p>当我们使用sigmoid kernel时，就相当于一层hidden layer的神经网络，如图：<br><img src="/images/2018-10-07-15388901736757.jpg" width="40%" height="50%"></p><p>给定一个输入，共有n个neuron，其中的weight就是每个训练数据的向量值，然后再将这些neuron加和得到输出。当然大部分的α的值是0，因此实质上神经元的个数和support vector的个数一致。</p><p>我们可以直接设计kernel，而不需要考虑x的非线性变换的形式，只要kernel符合mercer’s theory即可。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 15:Transfer Learning</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2015:%20Transfer%20Learning/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2015:%20Transfer%20Learning/</url>
      
        <content type="html"><![CDATA[<h3 id="Model-Fine-tuning"><a href="#Model-Fine-tuning" class="headerlink" title="Model Fine-tuning"></a>Model Fine-tuning</h3><p>假设我们有很多的source data $(x^s,y^s )$，与任务相关的target data $(x^t,y^t )$  很少。<br>我们利用source data训练一个模型，然后用target data来fine tune模型。</p><h4 id="conservative-training"><a href="#conservative-training" class="headerlink" title="conservative training"></a>conservative training</h4><p><img src="/images/2018-10-07-15388871902739.jpg" width="50%" height="50%"></p><p>我们可以用source data训练好的模型的weight作为新的模型的weight，然后设定一些限制，比如source data作为输入的output应和target data作为输入的output尽量相似，或者参数尽量相似等。</p><h4 id="layer-transfer"><a href="#layer-transfer" class="headerlink" title="layer transfer"></a>layer transfer</h4><p>也就是新模型有几层是直接copy旧模型的，只训练其它层。注意到不同任务所应copy的层是不同的，语音任务最后几层效果好，图像识别前面几层效果好</p><h3 id="Multitask-Learning"><a href="#Multitask-Learning" class="headerlink" title="Multitask Learning"></a>Multitask Learning</h3><p>不同任务之间共享相同的中间层，如：<br><img src="/images/2018-10-07-15388872452545.jpg" width="30%" height="50%"><br><img src="/images/2018-10-07-15388872627707.jpg" width="30%" height="50%"></p><p>还有一种progressive neural networks：<br><img src="/images/2018-10-07-15388872920224.jpg" width="50%" height="50%"><br>首先训练好第一个任务的模型，然后在训练第二个模型的时候将第一个模型的隐层加入到第二个模型的隐层中；训练第三个模型则将第二个和第一个模型的隐层加入到第三个模型的隐层中，以此类推</p><h3 id="Domain-adversarial-training"><a href="#Domain-adversarial-training" class="headerlink" title="Domain-adversarial training"></a>Domain-adversarial training</h3><p>source data是有标签的，而target data是无标签的，都属于同一个任务，但数据是mismatch的，如：<br><img src="/images/2018-10-07-15388873325090.jpg" width="50%" height="50%"></p><p>因为NN的隐层可以理解成是在抽取图像的特征，我们希望能够在训练NN的过程中去掉source data的一些domain specific的特性，这样就可以用在target data上了。因此我们在feature exactor后面连接两个模块：<br><img src="/images/2018-10-07-15388873772888.jpg" width="50%" height="50%"></p><p>一方面我们希望抽取的特征能够使得分类器正确地分类，另一方面我们希望这些特征能够让domain classifier能够无法识别特征是从哪些data抽取得到的，这样得到的特征就是被去掉domain specific特征的。</p><p>具体训练：<br><img src="/images/2018-10-07-15388874447304.jpg" width="50%" height="50%"></p><h3 id="Zero-shot-Learning"><a href="#Zero-shot-Learning" class="headerlink" title="Zero-shot Learning"></a>Zero-shot Learning</h3><p>source data有标签，target data无标签，但任务不同，如：<br><img src="/images/2018-10-07-15388874838165.jpg" width="50%" height="50%"></p><h4 id="Representing-each-class-by-its-attributes"><a href="#Representing-each-class-by-its-attributes" class="headerlink" title="Representing each class by its attributes"></a>Representing each class by its attributes</h4><p>一种方法是将每一个类都用特征表示，但特征要足够丰富：<br><img src="/images/2018-10-07-15388875088114.jpg" width="50%" height="50%"></p><p>在训练的时候，输入是图片，输出则是这些特征：<br><img src="/images/2018-10-07-15388875558853.jpg" width="40%" height="50%"><br>这样在将target data放入训练好的NN后也会得到一个这样的attribute，查表即可找到最相似的特征对应的类。</p><h4 id="Attribute-embedding"><a href="#Attribute-embedding" class="headerlink" title="Attribute embedding"></a>Attribute embedding</h4><p>如果特征维度太高，也可以将特征压缩成一个向量表示，这样在训练的时候，输出则是这样的向量特征，输入target data，输出向量特征，找到最近的特征对应的类即可<br><img src="/images/2018-10-07-15388875888699.jpg" width="50%" height="50%"></p><h4 id="Attribute-embedding-word-embedding"><a href="#Attribute-embedding-word-embedding" class="headerlink" title="Attribute embedding + word embedding"></a>Attribute embedding + word embedding</h4><p>如果没有attribute数据，利用word embedding也可以达到不错的效果。<br>在zero-shot learning中，光是让相同类的f和g相似是不够的，还应该让不同的f和g尽量远。</p><script type="math/tex; mode=display">f^∗,g^∗=arg min_{(f,g)}⁡∑_nmax(0,k−f(x^n )\cdot g(y^n )+max_{(m≠n)} ⁡f(x^m )\cdot g(x^m ) )</script>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Transfer Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 14:Unsupervised Learning:Generation</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2014:%20Unsupervised%20Learning:%20Generation/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2014:%20Unsupervised%20Learning:%20Generation/</url>
      
        <content type="html"><![CDATA[<h3 id="Component-by-component"><a href="#Component-by-component" class="headerlink" title="Component-by-component"></a>Component-by-component</h3><p>对于图像来说，每次生成一个pixel：PixelRNN</p><h3 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h3><p>架构：<br><img src="/images/2018-10-07-15388837191574.jpg" width="50%" height="50%"></p><p>其中e是噪声，σ是方差，目标是最小化reconstruction error，以及一个限制。该限制的目的即防止σ=0，m是正则化项。</p><p><del>中间的推导以及为什么是这样的架构我还不是很懂，之后再更新。</del><br>实际上可以这么理解，有几个要点：</p><ul><li>首先我们是基于这么一个假设：中间的code应当是服从正态分布的，而encoder的作用即在于拟合该正态分布的均值与方差的对数（因为方差应当恒为正，但神经网络的输出可能有正有负）</li><li>如果生成出来的code不符合正态分布，会有一个惩罚项，也就是上图的constraint（可以通过KL散度推导获得）</li><li>按理说，应当是在生成了均值和方差后，定义好该正态分布，然后再从中采样，但是这样没办法回传更新梯度，因此这里使用重参数技巧(Reparameterization Trick)，也即从$N(\mu,\sigma^2)$中采样$Z$，相当于从$N(0,I)$中采样$\varepsilon$，然后让$Z=\mu + \varepsilon \times \mu$</li></ul><p><img src="/images/2018-10-08-15389638077301.jpg" width="70%" height="50%"></p><p><strong>Reference</strong>:<br><a href="https://www.sohu.com/a/226209674_500659" target="_blank" rel="noopener">https://www.sohu.com/a/226209674_500659</a></p><p>VAE的主要问题在于，网络只试图去记住见过的图像，但没法真正去生成没见过的图像。</p><h3 id="Generative-Adversarial-Network-GAN"><a href="#Generative-Adversarial-Network-GAN" class="headerlink" title="Generative Adversarial Network (GAN)"></a>Generative Adversarial Network (GAN)</h3><p>GAN包含一个discriminator和一个generator，generator试图生成能够骗过discriminator的样本，而generator试图能够将generator生成的样本和真实的样本区分。</p><p>之后会有详细的介绍。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Unsupervised Learning </tag>
            
            <tag> Generation </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 13:Unsupervised Learning:Auto-encoder</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2013:%20Unsupervised%20Learning:%20Auto-encoder/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2013:%20Unsupervised%20Learning:%20Auto-encoder/</url>
      
        <content type="html"><![CDATA[<h3 id="Auto-encoder"><a href="#Auto-encoder" class="headerlink" title="Auto-encoder"></a>Auto-encoder</h3><p>由一个encoder和一个decoder组成，encoder负责将输入转成一个向量表示（维度通常小于输入），decoder负责将这段向量表示恢复成原来的输入。那么中间的code就可以作为输入的一个低维表示：<br><img src="/images/2018-10-07-15388832782913.jpg" width="50%" height="50%"></p><h3 id="Auto-encoder-for-CNN"><a href="#Auto-encoder-for-CNN" class="headerlink" title="Auto-encoder for CNN"></a>Auto-encoder for CNN</h3><p><img src="/images/2018-10-07-15388833149617.jpg" width="50%" height="50%"></p><h4 id="Unpooling"><a href="#Unpooling" class="headerlink" title="Unpooling"></a>Unpooling</h4><p>有两种方法，一种在pooling的时候记录最大值的位置，在unpooling时在相对位置填充最大值，其他位置填充0；另一种不记录最大值位置，直接在pooling区域全部填充最大值。<br><img src="/images/2018-10-07-15388833530548.jpg" width="50%" height="50%"></p><h4 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h4><p>其实本质就是convolution。</p><p>这是convolution:</p><p><img src="/images/2018-10-07-15388834044149.jpg" width="10%" height="50%"></p><p>我们期待的convolution：<br><img src="/images/2018-10-07-15388834434741.jpg" width="15%" height="50%"></p><p>实际上就等价在两边做padding，然后直接convolution：<br><img src="/images/2018-10-07-15388834751493.jpg" width="15%" height="50%"></p><h3 id="Auto-encoder的用处"><a href="#Auto-encoder的用处" class="headerlink" title="Auto-encoder的用处"></a>Auto-encoder的用处</h3><p>可以预训练每一层的DNN：<br><img src="/images/2018-10-07-15388835335550.jpg" width="50%" height="50%"></p><p>同理其它层也是一样，每次fix住其他层然后做Auto-encoder。那么在bp的时候只需要fine-tune就行。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Unsupervised Learning </tag>
            
            <tag> Auto-encoder </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 12:Unsupervised Learning:Neighbor Embedding</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2012:%20Unsupervised%20Learning:%20Neighbor%20Embedding/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2012:%20Unsupervised%20Learning:%20Neighbor%20Embedding/</url>
      
        <content type="html"><![CDATA[<h3 id="Locally-Linear-Embedding-LLE"><a href="#Locally-Linear-Embedding-LLE" class="headerlink" title="Locally Linear Embedding (LLE)"></a>Locally Linear Embedding (LLE)</h3><p>一种降维方法<br>思想：假设每个点可以由其周围的点来表示<br><img src="/images/2018-10-07-15388822769215.jpg" width="25%" height="50%"></p><p>我们需要找到这样的$w_{ij}$，使得：</p><script type="math/tex; mode=display">∑_i‖x^i−∑_j w_{ij} x^j ‖_2</script><p>这样在降维的时候，我们仍然保持x之间的这样的关系:<br><img src="/images/2018-10-07-15388823792351.jpg" width="50%" height="50%"></p><h3 id="Laplacian-Eigenmaps"><a href="#Laplacian-Eigenmaps" class="headerlink" title="Laplacian Eigenmaps"></a>Laplacian Eigenmaps</h3><p>一种降维方法<br>基本思想：如果$x^1$与$x^2$在高维空间中相近，则降维后也应该接近：</p><script type="math/tex; mode=display">S=1/2 ∑_{i,j} w_{i,j} (z^i−z^j )^2</script><p>其中：<br><img src="/images/2018-10-07-15388824984809.jpg" width="30%" height="50%"></p><p>如果将z全设为0，显然S最小，因此我们需要给z一个限制：z应当充满空间，也即假如z是M维，那么$\{z^1,z^2…,z^N\}$的秩应该等于M</p><h3 id="T-distributed-Stochastic-Neighbor-Embedding-t-SNE"><a href="#T-distributed-Stochastic-Neighbor-Embedding-t-SNE" class="headerlink" title="T-distributed Stochastic Neighbor Embedding (t-SNE)"></a>T-distributed Stochastic Neighbor Embedding (t-SNE)</h3><p>也是一种降维方法<br>前面提到的方法有一个问题：同一类的点确实聚在一起，但不同类的点并没有尽量分开<br><img src="/images/2018-10-07-15388826477983.jpg" width="50%" height="50%"></p><p>t-SNE的主要思想：将数据点映射到概率分布，我们希望降维前和降维后，数据分布的概率应当尽可能一致。<br>t-SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。t-SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。</p><p>如何做？<br>在高维空间中，我们定义：</p><script type="math/tex; mode=display">P(x^j |x^i )=\frac{S(x^i,x^j )}{∑_{k≠i}S(x^i,x^k )}</script><p>其中S表示i与j之间的相似度。</p><p>在低维空间中，同样有：</p><script type="math/tex; mode=display">Q(z^j |z^i )=\frac{S′(z^i,z^j )}{∑_{k≠i}S′(z^i,z^k )}</script><p>使用KL散度去计算两个分布之间的差异：</p><script type="math/tex; mode=display">L=∑_i KL(P(∗|x^i )||Q(∗|z^i )) =∑_i∑_j P(x^j |x^i )\frac{log P(x^j |x^i )}{Q(z^j |z^i )}</script><p>t-SNE中，高维空间和低维空间计算相似度的公式不大一样：</p><script type="math/tex; mode=display">S(x^i,x^j )=exp(−‖x^i−x^j ‖_2 )</script><script type="math/tex; mode=display">S′(z^i,z^j )=\frac{1}{(1+‖z^i−z^j ‖_2)}</script><p>两个公式的图示：<br><img src="/images/2018-10-07-15388830652023.jpg" width="70%" height="50%"></p><p>也即<strong>低维空间会拉长距离，使得距离远的点尽可能被拉开</strong>。</p><p>t-SNE的问题在于：t-SNE无法对新的数据点进行降维。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Unsupervised Learning </tag>
            
            <tag> Neighbor Embedding </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 11:Unsupervised Learning:Linear Dimension Reduction</title>
      <link href="/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2011:%20Unsupervised%20Learning:%20Linear%20Dimension%20Reduction/"/>
      <url>/2018/10/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2011:%20Unsupervised%20Learning:%20Linear%20Dimension%20Reduction/</url>
      
        <content type="html"><![CDATA[<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><h3 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h3><p>算法步骤：<br><img src="/images/2018-10-07-15388800377875.jpg" width="70%" height="50%"></p><p>迭代更新使得最后聚类中心收敛。但事先需要定好有多少类。</p><h3 id="Hierarchical-Agglomerative-Clustering-HAC"><a href="#Hierarchical-Agglomerative-Clustering-HAC" class="headerlink" title="Hierarchical Agglomerative Clustering (HAC)"></a>Hierarchical Agglomerative Clustering (HAC)</h3><p>自下而上，每次选两个最近的聚为一类，直到所有的都分成一类<br>最后选择一个阈值划分，如蓝色绿色和红色的线<br><img src="/images/2018-10-07-15388801021791.jpg" width="50%" height="50%"></p><h2 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h2><p>找到一个映射，使得x能够映射到低维z</p><h3 id="Principle-Component-Analysis-PCA"><a href="#Principle-Component-Analysis-PCA" class="headerlink" title="Principle Component Analysis (PCA)"></a>Principle Component Analysis (PCA)</h3><p>目的是找到一个维度，使得投影得到的variance最大，也即最大程度保留数据的差异性。<br><img src="/images/2018-10-07-15388801830659.jpg" width="50%" height="50%"></p><p>形式化可以写成（一维情形）：</p><script type="math/tex; mode=display">Var(z_1 )=\frac{1}{N} ∑_{z_1}(z_1−\overline{z_1} )^2</script><p>其中：</p><script type="math/tex; mode=display">‖w^1 ‖_2=1</script><script type="math/tex; mode=display">z_1=w^1 \cdot x</script><p>$\overline{z_1}$表示z的均值</p><p>假如我们要投影到多维，其他维度也有同样的目标。其中每个维度之间都应该是相互正交的。<br><img src="/images/2018-10-07-15388804752506.jpg" width="20%" height="50%"></p><h4 id="如何做？"><a href="#如何做？" class="headerlink" title="如何做？"></a>如何做？</h4><p>找到$ \frac{1}{N}∑(x−\overline{x} ) (x−\overline{x})^T$的前k个最大的特征值对应的特征向量，组合起来即是我们要找的$W$</p><h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><p>—-Warning of Math—-<br>目的：$Var(z_1 )=\frac{1}{N} ∑_{z_1}(z_1−\overline{z_1} )^2 $<br>其中 $\overline{z_1} =\frac{1}{N} ∑{z_1} = \frac{1}{N} ∑ w^1 \cdot x=w^1\cdot \overline{x}$</p><p>推导：<br><img src="/images/2018-10-07-15388811276042.jpg" width="35%" height="50%"><br>改变符号 $S=Cov(x)$</p><p>利用拉格朗日乘数法，有：<br>$Sw^1=αw^1$<br>等式两边各左乘$(w^1)^T$，有：<br>$(w^1 )^T Sw^1=α(w^1 )^T w^1=α$</p><p>也即，$α$是$S$的特征值，选择最大的特征值，就能够最大化我们的目标。</p><p>同理，我们要找$w^2$，最大化$(w^2 )^T Sw^2$，其中有：<br>$(w^2 )^T w^2=1$<br>$(w^2 )^T w^1=0$ （与第一维正交）</p><p>因此利用拉格朗日乘数法：</p><script type="math/tex; mode=display">g(w^2 )= (w^2 )^T Sw^2−α((w^2 )^T w^2−1)−β((w^2 )^T w^1−0)</script><p>最终得到，w2对应第二大的特征值的特征向量。</p><p>以此类推，其他维也同理。<br>—-End of Math—-</p><h4 id="PCA的其他"><a href="#PCA的其他" class="headerlink" title="PCA的其他"></a>PCA的其他</h4><p>实际上最终得到的z，每一维之间的协方差都为0<br><img src="/images/2018-10-07-15388815546680.jpg" width="50%" height="50%"></p><p>证明如下：<br><img src="/images/2018-10-07-15388815837458.jpg" width="50%" height="50%"></p><p>PCA也可以用SVD来做：<br><img src="/images/2018-10-07-15388816250075.jpg" width="60%" height="50%"></p><p>U中保存了K个特征向量。</p><p>从另一种角度理解PCA，也可以认为PCA是一种autoencoder：<br><img src="/images/2018-10-07-15388816896369.jpg" width="50%" height="50%"></p><h4 id="PCA的问题"><a href="#PCA的问题" class="headerlink" title="PCA的问题"></a>PCA的问题</h4><p>PCA是无监督学习，如果有标签，则无法按照类别来进行正确降维，如：<br><img src="/images/2018-10-07-15388817393283.jpg" width="30%" height="50%"></p><p>第二就是PCA是线性变换，对于一些需要非线性变换的无能为力<br><img src="/images/2018-10-07-15388817566149.jpg" width="28%" height="50%"></p><h3 id="Matrix-Factorization"><a href="#Matrix-Factorization" class="headerlink" title="Matrix Factorization"></a>Matrix Factorization</h3><p>定义：矩阵分解，就是将一个矩阵D分解为U和V的乘积，即对于一个特定的规模为m*n的矩阵D，估计出规模分别为m*k和n*k的矩阵U和V，使得$UV^T$的值尽可能逼近矩阵D。常用于推荐系统。</p><p>思想：<br>假如有一个矩阵：<br><img src="/images/2018-10-07-15388819053983.jpg" width="60%" height="50%"></p><p>假设横轴和纵轴每一维都有一个向量代表该维，矩阵的每个元素就是横轴和纵轴对应维的点积。我们的目的是尽可能减小：</p><script type="math/tex; mode=display">L=\sum_{(i,j)} (r^i \cdot r^j -n_{ij})^2</script><p>其中$r_i$ $r_j$就是向量表示，$n_{ij}$就是矩阵的内容。</p><p>可以使用SVD求解上式：<br><img src="/images/2018-10-07-15388820642382.jpg" width="50%" height="50%"></p><p>实际上，考虑每一行或列本身的特性，我们对Loss进行扩展：</p><script type="math/tex; mode=display">Minimizing \ \ L=\sum_{(i,j)} (r^i \cdot r^j +b_i+b_j-n_{ij})^2</script><p>使用SGD可以求解。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Unsupervised Learning </tag>
            
            <tag> Linear Dimension Reduction </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>梯度消失与梯度爆炸的推导</title>
      <link href="/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E6%8E%A8%E5%AF%BC/"/>
      <url>/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E6%8E%A8%E5%AF%BC/</url>
      
        <content type="html"><![CDATA[<p> 记RNN中每一步的损失为$E_t$，则损失对$h_{t-1}$的权重$W$的导数有：</p><script type="math/tex; mode=display">\frac{\partial{E_t}}{\partial{W}}=\sum_{k=1}^{t}    \frac{\partial{E_t}}{\partial{y_t}} \frac{\partial{y_t}}{\partial{h_t}} \frac{\partial{h_t}}{\partial{h_k}} \frac{\partial{h_k}}{\partial{W}}</script><p>其中$\frac{\partial{h_t}}{\partial{h_k}}$使用链式法则有：</p><script type="math/tex; mode=display">\frac{\partial{h_t}}{\partial{h_k}} =     \prod_{j=k+1}^{t} \frac{\partial{h_j}}{\partial{h_{j-1}}} =    \prod_{j=k+1}^{t} W^T \times diag[f^{\prime}(h_{j-1})]</script><p>其中$\frac{\partial{h_j}}{\partial{h_{j-1}}}$ 是雅克比矩阵。对其取模(norm)，有：</p><script type="math/tex; mode=display">\rVert \frac{\partial{h_j}}{\partial{h_{j-1}}}\rVert ≤ \rVert W^T \rVert \rVert diag[f^{\prime}(h_{j-1})] \rVert ≤ \beta_W \beta_h</script><p>当$f$为sigmoid时，$f^{\prime}(h_{j-1})$最大值为1。</p><p>最终我们有：</p><script type="math/tex; mode=display">\rVert \frac{\partial{h_t}}{\partial{h_{k}}}\rVert ≤ \rVert \prod_{j=k+1}^{t} \frac{\partial{h_j}}{\partial{h_{j-1}}} \rVert ≤ (\beta_W \beta_h)^{t-k}</script><p>从上式可以看出，当t-k足够大时，如果$(\beta_W \beta_h)$小于1则$(\beta_W \beta_h)^{t-k}$则会变得非常小，相反，若$(\beta_W \beta_h)$大于1则$(\beta_W \beta_h)^{t-k}$则会变得非常大。</p><p>在计算机中，当梯度值很大时，会造成上溢(NaN)，也即梯度爆炸问题，当梯度值很小时，会变成0，也即梯度消失。注意到，t-k的损失实际上评估的是一个较远的词对当前t的贡献，梯度消失也即意味着对当前的贡献消失。</p><p>Reference:<br>CS224d: Deep Learning for NLP Lecture4</p>]]></content>
      
      
      
        <tags>
            
            <tag> 梯度消失 </tag>
            
            <tag> 梯度爆炸 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识10</title>
      <link href="/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8610/"/>
      <url>/2018/10/07/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8610/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-正态分布"><a href="#1️⃣-正态分布" class="headerlink" title="1️⃣[正态分布]"></a>1️⃣[正态分布]</h3><p>高维正态分布是从一维发展而来的：<br><img src="/images/2018-10-07-15388761009977.jpg" width="70%" height="50%"></p><p><a href="https://www.zhihu.com/question/36339816" target="_blank" rel="noopener">https://www.zhihu.com/question/36339816</a></p><hr><h3 id="2️⃣-RNN"><a href="#2️⃣-RNN" class="headerlink" title="2️⃣[RNN]"></a>2️⃣[RNN]</h3><p>from <a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf" target="_blank" rel="noopener">https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf</a></p><p>通常而言，我们都会将RNN的initial state设为全0，但在Hinton的slide中提到，我们可以将初始状态作为可学习的变量，和我们在学习权重矩阵一样。</p><p><img src="/images/2018-10-07-15388770544817.jpg" width="80%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 正态分布 </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PRML第二章 概率分布</title>
      <link href="/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83/"/>
      <url>/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83/</url>
      
        <content type="html"><![CDATA[<h1 id="二元变量"><a href="#二元变量" class="headerlink" title="二元变量"></a>二元变量</h1><p><img src="/images/2018-09-30-Xnip2018-09-30_14-25-21.jpg" alt="二元变量1"></p><p><img src="/images/2018-09-30-Xnip2018-09-30_14-26-46.jpg" alt="贝塔分布"></p><h1 id="多项式分布"><a href="#多项式分布" class="headerlink" title="多项式分布"></a>多项式分布</h1><p><img src="/images/2018-09-30-Xnip2018-09-30_14-31-55.jpg" alt="多项式分布"></p><h1 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h1><p><img src="/images/2018-09-30-Xnip2018-09-30_14-40-32.jpg" alt="高斯分布"></p><p><img src="/images/2018-09-30-Xnip2018-09-30_14-43-09.jpg" alt="2"></p><p><img src="/images/2018-09-30-Xnip2018-09-30_14-45-15.jpg" alt="3"></p><p><img src="/images/2018-09-30-Xnip2018-09-30_14-48-58.jpg" alt="4"></p><h1 id="指数族分布"><a href="#指数族分布" class="headerlink" title="指数族分布"></a>指数族分布</h1><p><img src="/images/2018-09-30-Xnip2018-09-30_14-51-25.jpg" alt="1"></p><p><img src="/images/2018-10-03-Xnip2018-10-03_10-03-54.jpg" alt="2"></p><h1 id="非参数优化"><a href="#非参数优化" class="headerlink" title="非参数优化"></a>非参数优化</h1><p><img src="/images/2018-10-03-Xnip2018-10-03_10-05-24.jpg" alt="1"></p><p><img src="/images/2018-10-03-Xnip2018-10-03_10-06-55.jpg" alt="2"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> PRML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 10:Semi-supervised learning</title>
      <link href="/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2010:%20Semi-supervised/"/>
      <url>/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%2010:%20Semi-supervised/</url>
      
        <content type="html"><![CDATA[<p>什么是semi-supervised learning</p><p>给定数据${(x^r,\hat{y}^r)}_{r=1}^{R},{(x_u)}_{u=R}^{R+U}$，其中未标记数据远远多于标记数据 $U&gt;&gt;R$</p><p>为什么半监督学习有用？<br>因为未标记数据的分布可能能够给我们一些信息。</p><h3 id="生成模型的半监督学习"><a href="#生成模型的半监督学习" class="headerlink" title="生成模型的半监督学习"></a>生成模型的半监督学习</h3><p>给定两类$C_1$、$C_2$，要求得到后验概率分布</p><script type="math/tex; mode=display">P(C_1 |x)=\frac{P(x|C_1 )P(C_1 )}{(P(x|C_1 )P(C_1 )+P(x|C_2 )P(C_2 ) )}</script><p>其中联合概率分布服从高斯分布。未标记数据此时的作用即帮我们重新估计$P(C_1),P(C_2),\mu,\Sigma$</p><p><img src="/images/2018-09-30-15382829251218.jpg" width="50%" height="50%"></p><p>如何做?<br>先初始化$P(C_1),P(C_2),\mu,\Sigma$，通常可以先用有标记数据进行估计</p><ol><li>计算每个未标记数据的后验概率分布</li><li>以该概率分布更新模型<br>不断重复直至拟合</li></ol><p><img src="/images/2018-09-30-15382829987091.jpg" width="70%" height="50%"></p><p>原因：<br>当我们在做监督学习时，使用最大似然求解：</p><script type="math/tex; mode=display">logL(θ)=∑_{x^r,\hat{y}^r} logP_θ (x^r |\hat{y}^r )</script><p>加上了未标记数据后，同样也要做最大似然：</p><script type="math/tex; mode=display">logL(θ)=∑_{(x^r,\hat{y}^r)} logP_θ (x^r |\hat{y}^r )+∑_{x^u} logP_θ (x^u)</script><h3 id="Low-density-Separation"><a href="#Low-density-Separation" class="headerlink" title="Low-density Separation"></a>Low-density Separation</h3><p>假设不同类别之间有一条明显的分界线，也即存在一个区域，其密度比其他区域小</p><h4 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h4><p>如何做?</p><ol><li>先用有标签数据训练一个模型$f$；</li><li>利用模型对未标记数据进行标记，这些标签称为伪标签（pseudo-label）</li><li>将部分有伪标签的数据放入有标签数据中，重新训练<br>重复直到拟合</li></ol><p>这种方式和生成模型的区别：该方法使用的是hard label而生成模型使用的是soft label</p><h4 id="Entropy-based-Regularization"><a href="#Entropy-based-Regularization" class="headerlink" title="Entropy-based Regularization"></a>Entropy-based Regularization</h4><p>将未标记数据充当正则化的效果，我们希望模型预测标签的概率较为集中，也即熵应该尽可能小。也就是说，未标记数据使得分类边界尽可能划在低密度区域。<br><img src="/images/2018-09-30-15382837957204.jpg" width="30%" height="50%"></p><h3 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h3><p>假设：位于稠密数据区域的两个距离很近的样例的类标签相似，通过high density path连接。</p><p><img src="/images/2018-09-30-15382840889274.jpg" width="40%" height="50%"><br>x1与x2之间较为稠密，因此x2与x1比x2与x3更为接近。</p><p><strong>如何知道x1与x2通过high density path连接？</strong><br><img src="/images/2018-09-30-15382841851160.jpg" width="50%" height="50%"></p><p>基于图的方法：</p><ol><li>定义xi与xj之间的相似度$s(x^i,x^j)$</li><li>添加边，有两种选择<ol><li>k nearest neighbor</li><li>e-neighborhood<br><img src="/images/2018-09-30-15382842669412.jpg" width="50%" height="50%"></li></ol></li><li>边之间的权重通过相似度来衡量。如： $s(x^i,x^j )=exp(−γ‖x^i−x^j‖^2)$</li></ol><p>该方法本质即利用有标签数据去影响未标记数据，通过图的传播。但一个问题是如果数据不够多，就可能没办法传播。如：<br><img src="/images/2018-09-30-15382844101208.jpg" width="30%" height="50%"></p><p>在建立好图后，如何使用?</p><ul><li>定义图的平滑程度，$y$表示标签。$S$越小表示越平滑。<script type="math/tex; mode=display">S=1/2∑_{i,j} w_{i,j} (y^i−y^j )^2=y^T Ly</script><script type="math/tex; mode=display">y=[⋯y^i⋯y^j⋯]^T</script><script type="math/tex; mode=display">L=D−W</script></li></ul><p>D是邻接矩阵，第ij个元素即xi与xj之间的weight，W是对角矩阵，ii个元素是D的第i行的加和；L称为Graph Laplacian<br><img src="/images/2018-09-30-15382847006975.jpg" width="50%" height="50%"></p><ul><li>我们最终在计算Loss的时候要加上这项正则项<script type="math/tex; mode=display">L=∑_{x^r}C(y^r,\hat{y}^r ) +λS</script></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> Semi-supervised learning </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 7:Tips for DL</title>
      <link href="/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%207:%20Tips%20for%20DL/"/>
      <url>/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%207:%20Tips%20for%20DL/</url>
      
        <content type="html"><![CDATA[<p>大纲<br><img src="/images/2018-09-30-15382757690955.jpg" width="50%" height="50%"></p><h2 id="new-activation-function"><a href="#new-activation-function" class="headerlink" title="new activation function"></a>new activation function</h2><p>梯度消失问题：由于sigmoid会将值压缩，所以在反向传播时，越到后面值越小。</p><p><img src="/images/2018-09-30-15382758841507.jpg" width="30%" height="50%"><br>所以后层的更新会比前层的更新更快，导致前层还没converge，后层就根据前层的数据（random）达到converge了</p><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p><img src="/images/2018-09-30-15382759503401.jpg" width="30%" height="50%"><br>能够快速计算，且能够解决梯度消失问题。</p><p>因为会有部分neuron的值是0，所以相当于每次训练一个瘦长的神经网络。<br><img src="/images/2018-09-30-15382760030965.jpg" width="50%" height="50%"></p><h4 id="ReLU的变体"><a href="#ReLU的变体" class="headerlink" title="ReLU的变体"></a>ReLU的变体</h4><p><img src="/images/2018-09-30-15382928024258.jpg" width="50%" height="50%"></p><h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><p>首先将几个neuron归为一组，然后每次前向传播时取最大的作为输出。<br><img src="/images/2018-09-30-15382761367509.jpg" width="50%" height="50%"></p><p>实际上ReLU是maxout的一种特殊形式：<br><img src="/images/2018-09-30-15382761741870.jpg" width="40%" height="50%"></p><p>更一般的，有：<br><img src="/images/2018-09-30-15382762237369.jpg" width="40%" height="50%"></p><p>因为w和b的变化，所以该activation function实际上就是一个learnable activation function</p><p>这样一个learnable activation function有这样的特点：</p><blockquote><p>Activation function in maxout network can be any piecewise linear convex function<br>How many pieces depending on how many elements in a group</p></blockquote><p>如：<br><img src="/images/2018-09-30-15382763537888.jpg" width="60%" height="50%"></p><p>maxout应如何训练？</p><p><img src="/images/2018-09-30-15382764343880.jpg" width="50%" height="50%"></p><p>实际上就是一个普通的瘦长network，常规训练即可。<br><img src="/images/2018-09-30-15382764564859.jpg" width="50%" height="50%"></p><h2 id="Adaptive-learning-rate"><a href="#Adaptive-learning-rate" class="headerlink" title="Adaptive learning rate"></a>Adaptive learning rate</h2><p>在adagrad中:<br><img src="/images/2018-09-30-15382765516383.jpg" width="30%" height="50%"></p><p>越到后面learning rate越来越小，但实际上在dl里面，error surface是非常复杂的，越来越小的learning rate可能不适用于dl。如：<br><img src="/images/2018-09-30-15382765821828.jpg" width="50%" height="50%"></p><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p><img src="/images/2018-09-30-15382766372355.jpg" width="60%" height="50%"><br>$σ^t$是历史信息，也就是说$σ^t$参考了过去的梯度和当前的梯度获得一个新的放缩大小</p><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>引入惯性作为参考，也即参考了上一次梯度的方向。引入惯性后，可能有机会越过local minimum。<br>普通的gradient descent:<br><img src="/images/2018-09-30-15382769712428.jpg" width="40%" height="50%"><br>每次朝着梯度的反方向走。</p><p>Momentum:<br><img src="/images/2018-09-30-15382770120104.jpg" width="40%" height="50%"></p><p>考虑了上一步走的方向。</p><p>具体算法：<br><img src="/images/2018-09-30-15382771516587.jpg" width="30%" height="50%"></p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>结合了RMSprop和Momentum，也即综合考虑了历史信息决定当前步长；考虑了上一步的方向决定当前走的方向。<br>具体算法：<br><img src="/images/2018-09-30-15382772986250.jpg" width="60%" height="50%"></p><h2 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h2><p>就是在validation set的loss不再减小时停止<br><img src="/images/2018-09-30-15382814784406.jpg" width="50%" height="50%"></p><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p><img src="/images/2018-09-30-15382815175056.jpg" width="50%" height="50%"><br>其中<br><img src="/images/2018-09-30-15382815336088.jpg" width="30%" height="50%"><br>因此更新公式为：<br>    <img src="/images/2018-09-30-15382815641437.jpg" width="50%" height="50%"></p><p>也即每次以$1-\eta \lambda$对w进行放缩，使w更接近0<br>正则化在DL中也称为weight decay</p><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p><img src="/images/2018-09-30-15382816962676.jpg" width="25%" height="50%"></p><p><img src="/images/2018-09-30-15382817122102.jpg" width="25%" height="50%"><br><img src="/images/2018-09-30-15382817409633.jpg" width="25%" height="50%"></p><p>则更新公式为：<br><img src="/images/2018-09-30-15382817897319.jpg" width="50%" height="50%"></p><p>也即每次以$ηλsgn(w)$ 使w往0靠（sgn表示符号函数）</p><p>可以看出，L1每次都加减相同的值，而L2按比例进行缩放。因此L1更为稀疏(sparse)。</p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>训练的时候每一层采样p%的神经元设为0，让其不工作<br><img src="/images/2018-09-30-15382819376579.jpg" width="50%" height="50%"></p><p>实际上就是每个batch改变了网络结构，使得网络更细长<br><img src="/images/2018-09-30-15382819772611.jpg" width="50%" height="50%"></p><p>测试的时候所有的weight都乘以1-p%</p><p>从ensemble的角度看待dropout：<br>在训练的时候训练一堆不同结构的network，最多有$2^N$种组合，N为neuron个数，可以称为终极的ensemble方法了。而在测试的时候对这些不同的网络进行平均。</p><p><img src="/images/2018-09-30-15382821089494.jpg" width="50%" height="50%"></p><p><img src="/images/2018-09-30-15382821379688.jpg" width="50%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Tips for DL </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>采样浅析</title>
      <link href="/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/%E9%87%87%E6%A0%B7%E6%B5%85%E6%9E%90/"/>
      <url>/2018/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/%E9%87%87%E6%A0%B7%E6%B5%85%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>总结在NLP中的采样方法（持续更新）。</p><h2 id="采样方法"><a href="#采样方法" class="headerlink" title="采样方法"></a>采样方法</h2><h3 id="1️⃣逆变换采样-Inverse-Sampling"><a href="#1️⃣逆变换采样-Inverse-Sampling" class="headerlink" title="1️⃣逆变换采样(Inverse Sampling)"></a>1️⃣逆变换采样(Inverse Sampling)</h3><p>目的：已知任意概率分布的<strong>累积分布函数</strong>时，用于从该分布中生成随机样本。</p><p>—-什么是累积分布函数(CDF)—-<br>是概率密度函数(PDF)的积分，定义：</p><script type="math/tex; mode=display">F_X(x)=P(X≤x)=\int_{-∞}^{x}f_X(t)dt</script><p>—-END—-</p><p>想象我们知道高斯分布的概率密度函数，我们应该如何采样？本质上我们只能对均匀分布进行直接采样（高斯分布有<a href="https://www.zhihu.com/question/29971598" target="_blank" rel="noopener">算法</a>可以生成采样，但无法一般化）。对于这种连续的随机变量，我们只能通过间接的方法进行采样。</p><p>逆变换采样即是通过累积分布函数的反函数来采样。因为累积分布函数的值域为$[0,1]$，因此我们通过在$[0,1]$上进行采样，再映射到原分布。<br>例子:<br><img src="/images/2018-09-30-15382714567064.jpg" width="80%" height="50%"><br>映射关系如图：<br><img src="/images/2018-09-30-15382715821631.jpg" width="50%" height="50%"></p><h3 id="2️⃣重要性采样-Importance-Sampling"><a href="#2️⃣重要性采样-Importance-Sampling" class="headerlink" title="2️⃣重要性采样(Importance Sampling)"></a>2️⃣重要性采样(Importance Sampling)</h3><p>目的：已知某个分布$P$，希望能估计$f(x)$的期望。亦即：</p><script type="math/tex; mode=display">E[f(x)]=\int_{x}f(x)p(x)dx≈\frac{1}{n}\sum_{i=1}^{n}f(x_i)</script><p>其中$x\sim p$。<br>假设$p(x)$的分布复杂或样本不好生成，另一分布$q(x)$方便生成样本。因此我们引入$q(x)$对原先分布进行估计。</p><script type="math/tex; mode=display">E[f(x)]=\int_{x}f(x)p(x)dx=\int_{x}f(x)\frac{p(x)}{q(x)}q(x)dx≈\frac{1}{n}\sum_{i=1}^{n}f(x_i)\frac{p(x_i)}{q(x_i)}</script><p>其中，$x \sim q$。$w(x)=\frac{p(x)}{q(x)}$称为Importance Weight</p><p>根据上式，实际上就是每次采样的加权求和。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>逆变换采样<br><a href="https://zh.wikipedia.org/wiki/%E9%80%86%E5%8F%98%E6%8D%A2%E9%87%87%E6%A0%B7" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E9%80%86%E5%8F%98%E6%8D%A2%E9%87%87%E6%A0%B7</a></p><p>重要性采样<br><a href="https://www.youtube.com/watch?v=S3LAOZxGcnk" target="_blank" rel="noopener">https://www.youtube.com/watch?v=S3LAOZxGcnk</a></p><p>——持续更新——</p>]]></content>
      
      
      
        <tags>
            
            <tag> 采样 </tag>
            
            <tag> sampling </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识9</title>
      <link href="/2018/09/30/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%869/"/>
      <url>/2018/09/30/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%869/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>Pytorch中保存checkpoint是一个dict形式，可以保存任意多个模型到一个checkpoint中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#save</span></span><br><span class="line">torch.save(&#123;            <span class="string">'epoch'</span>: epoch,            <span class="string">'model_state_dict'</span>: model.state_dict(),            <span class="string">'optimizer_state_dict'</span>: optimizer.state_dict(),            <span class="string">'loss'</span>: loss,            ...            &#125;, PATH)</span><br><span class="line"><span class="comment">#load</span></span><br><span class="line">model = TheModelClass(*args, **kwargs)optimizer = TheOptimizerClass(*args, **kwargs)</span><br><span class="line">checkpoint = torch.load(PATH)model.load_state_dict(checkpoint[<span class="string">'model_state_dict'</span>])optimizer.load_state_dict(checkpoint[<span class="string">'optimizer_state_dict'</span>])epoch = checkpoint[<span class="string">'epoch'</span>]loss = checkpoint[<span class="string">'loss'</span>]</span><br><span class="line">model.eval()<span class="comment"># - or -</span>model.train()</span><br></pre></td></tr></table></figure></p><hr><h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>Pytorch可以load部分模型，也就是只load进来部分我们需要的层，这在transfer learning中用到。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">torch.save(modelA.state_dict(), PATH)</span><br><span class="line">modelB = TheModelBClass(*args, **kwargs)modelB.load_state_dict(torch.load(PATH), strict=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>no title</title>
      <link href="/2018/09/30/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%9C%89%E5%8A%9B%E9%87%8F%E7%9A%84%E6%96%87%E5%AD%97/"/>
      <url>/2018/09/30/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%9C%89%E5%8A%9B%E9%87%8F%E7%9A%84%E6%96%87%E5%AD%97/</url>
      
        <content type="html"><![CDATA[<p>每当我遇到自己不敢直视的困难时，我就会闭上双眼，想象自己是一个80岁的老人，为人生中曾放弃和逃避过的无数困难而懊悔不已，我会对自己说，能再年轻一次该有多好，然后我睁开眼睛：砰！我又年轻一次了！</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词10</title>
      <link href="/2018/09/30/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D10/"/>
      <url>/2018/09/30/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D10/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣次北固山下"><a href="#1️⃣次北固山下" class="headerlink" title="1️⃣次北固山下"></a>1️⃣次北固山下</h3><p>[唐] 王湾<br>客路青山外，行舟绿水前。<br>潮平两岸阔，风正一帆悬。<br><strong>海日生残夜，江春入旧年。</strong><br>乡书何处达，归雁洛阳边。</p><p>次：旅途中暂时停宿，这里是停泊的意思。</p><p><a href="http://m.xichuangzhu.com/work/57b95de92e958a005fa8919e" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b95de92e958a005fa8919e</a></p><hr><h3 id="2️⃣将赴吴兴登乐游原"><a href="#2️⃣将赴吴兴登乐游原" class="headerlink" title="2️⃣将赴吴兴登乐游原"></a>2️⃣将赴吴兴登乐游原</h3><p>[唐] 杜牧<br>清时有味是无能，闲爱孤云静爱僧。<br><strong>欲把一麾江海去，乐游原上望昭陵。</strong></p><p>无能：无所作为。</p><p><a href="http://m.xichuangzhu.com/work/57b99db9165abd005a6da742" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b99db9165abd005a6da742</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PRML第一章 绪论</title>
      <link href="/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E7%BB%AA%E8%AE%BA/"/>
      <url>/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/PRML%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E7%BB%AA%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<p>记录PRML学习过程。<br>笔记共享链接：<a href="https://1drv.ms/u/s!Apsp2510NHF6rIRjMclFB16v7B0FWg" target="_blank" rel="noopener">https://1drv.ms/u/s!Apsp2510NHF6rIRjMclFB16v7B0FWg</a></p><hr><h1 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h1><p><img src="/images/2018-09-23-Xnip2018-09-23_10-27-21.jpg" alt="概率论"></p><h1 id="决策论"><a href="#决策论" class="headerlink" title="决策论"></a>决策论</h1><p><img src="/images/2018-09-23-Xnip2018-09-23_10-36-52.jpg" alt="决策论"></p><h1 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h1><p><img src="/images/2018-09-23-Xnip2018-09-23_10-38-46.jpg" alt="信息论"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> PRML </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 6:Backpropagation</title>
      <link href="/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%206:%20Backpropagation/"/>
      <url>/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%206:%20Backpropagation/</url>
      
        <content type="html"><![CDATA[<h3 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h3><p>基本公式<br><img src="/images/2018-09-23-15376684598125.jpg" width="50%" height="50%"><br><img src="/images/2018-09-23-15376684674095.jpg" width="50%" height="50%"></p><h3 id="forward-pass和backward-pass"><a href="#forward-pass和backward-pass" class="headerlink" title="forward pass和backward pass"></a>forward pass和backward pass</h3><p>可以将backpropagation分为两步</p><h4 id="forward-pass"><a href="#forward-pass" class="headerlink" title="forward pass"></a>forward pass</h4><p>在前向传播的时候提前计算/保存好，因为该梯度很简单<br><img src="/images/2018-09-23-15376686358832.jpg" width="50%" height="50%"></p><p>比如z对w1的梯度就是x1，就是和w1相连的项<br><img src="/images/2018-09-23-15376687025289.jpg" width="20%" height="50%"></p><h4 id="backward-pass"><a href="#backward-pass" class="headerlink" title="backward pass"></a>backward pass</h4><p>回传的时候逐层相乘下去，类似动态规划，获得了后一层的梯度才能求出前一层的梯度。<br><img src="/images/2018-09-23-15376687488493.jpg" width="50%" height="50%"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/images/2018-09-23-15376687824826.jpg" width="50%" height="50%"></p><p>先前向，提前算出最邻近的梯度，直到output layer，计算完该梯度，再不断回传逐层相乘获得output对各层的梯度。</p><h3 id="代码实现例子"><a href="#代码实现例子" class="headerlink" title="代码实现例子"></a>代码实现例子</h3><p>relu实现forward pass和backward pass<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return</span></span><br><span class="line"><span class="string">        a Tensor containing the output. ctx is a context object that can be used</span></span><br><span class="line"><span class="string">        to stash information for backward computation. You can cache arbitrary</span></span><br><span class="line"><span class="string">        objects for use in the backward pass using the ctx.save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ctx.save_for_backward(input)  <span class="comment">#为了之后的backward计算</span></span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Backpropagation </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 5:Classification:Logistic Regression</title>
      <link href="/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%205%20Classification:%20Logistic%20Regression/"/>
      <url>/2018/09/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%205%20Classification:%20Logistic%20Regression/</url>
      
        <content type="html"><![CDATA[<h3 id="logistic-regression如何做？"><a href="#logistic-regression如何做？" class="headerlink" title="logistic regression如何做？"></a>logistic regression如何做？</h3><p>step1: 定义function set<br><img src="/images/2018-09-23-15376666391912.jpg" width="30%" height="50%"></p><p>step2: 更新<br>使用最大似然更新</p><script type="math/tex; mode=display">L(w,b)=f_{w,b}(x^1 )f_{w,b}(x^2 )(1−f_{w,b} (x^3 ))⋯f_{w,b} (x^N )</script><p>找到w，b使得L最大</p><p>对似然函数取负对数，则有：<br><img src="/images/2018-09-23-15376667791380.jpg" width="60%" height="50%"></p><p>将式子的每个元素写成伯努利分布形式：<br><img src="/images/2018-09-23-15376669013511.jpg" width="60%" height="50%"></p><p>上式就是cross-entropy损失函数。</p><p>求导该式子可得：<br><img src="/images/2018-09-23-15376669743224.jpg" width="30%" height="50%"><br>更新公式：<br><img src="/images/2018-09-23-15376669980138.jpg" width="40%" height="50%"><br>可以看出上式很直观：和答案差距越大，更新步伐越大。</p><p>同时发现上式和linear regression的更新公式是一致的。</p><h3 id="为什么不像linear-regression那样设loss为square？"><a href="#为什么不像linear-regression那样设loss为square？" class="headerlink" title="为什么不像linear regression那样设loss为square？"></a>为什么不像linear regression那样设loss为square？</h3><p>假设我们使用square loss，则求导得到的梯度：<br><img src="/images/2018-09-23-15376671202521.jpg" width="50%" height="50%"><br>上式可以看出，当接近target时，梯度小；远离target时，梯度也小。难以达到全局最小<br><img src="/images/2018-09-23-15376672527230.jpg" width="60%" height="50%"></p><p>下图是cross entropy和square error的图像示意：<br><img src="/images/2018-09-23-15376672892502.jpg" width="60%" height="50%"></p><p>如图，square loss难以到达全局最小。</p><h3 id="生成式模型与判别式模型的区别"><a href="#生成式模型与判别式模型的区别" class="headerlink" title="生成式模型与判别式模型的区别"></a>生成式模型与判别式模型的区别</h3><p>生成式对联合概率分布进行建模，再通过贝叶斯定理获得后验概率；而判别式模型直接对后验概率建模。<br><img src="/images/2018-09-23-15376674213503.jpg" width="60%" height="50%"><br>二者所定义的function set是一致的，但同一组数据可能会得到不同的w和b。</p><p>二者优劣对比：</p><ul><li>数据量多时，一般来说判别式模型会更好。因为判别式模型没有先验假设，完全依赖于数据。但如果数据有噪声，容易受影响。</li><li>生成式模型是有一定的假设的，当假设错误，会影响分类效果。</li><li>正因为有一定的先验假设，当数据量很少时，可能效果会不错；对于噪声更具有鲁棒性。</li><li>先验可以从其他数据源获得来帮助特定任务，如语音识别问题。</li></ul><h3 id="logistic的局限"><a href="#logistic的局限" class="headerlink" title="logistic的局限"></a>logistic的局限</h3><p>本质仍是一个线性分类器，没办法分类非线性的数据。<br>如何解决该问题?<br><strong>将logistic regression model拼接起来</strong>，前面的model对数据进行feature transformation，然后再对新的feature进行分类。<br><img src="/images/2018-09-23-15376677559470.jpg" width="70%" height="50%"></p><p>logistic与deep learning的联系：<br>如果将logistic regression的一个单元称为neuron，拼起来就是neural network了！！！</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Classification </tag>
            
            <tag> Logistic Regression </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识8</title>
      <link href="/2018/09/23/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%868/"/>
      <url>/2018/09/23/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%868/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>torch.max()有两种不同写法。<br>torch.max(input) → Tensor 返回其中最大的元素<br>torch.max(input, dim, keepdim=False, out=None) → (Tensor, LongTensor) 返回该维度上最大值，以及对应的index</p><hr><h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>将模型同时部署到多张卡上训练，本质就是将一个batch的数据split，送到各个model，然后合并结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model)</span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> rand_loader:</span><br><span class="line">    input = data.to(device)</span><br><span class="line">    output = model(input)</span><br></pre></td></tr></table></figure><hr><h3 id="3️⃣-求导"><a href="#3️⃣-求导" class="headerlink" title="3️⃣[求导]"></a>3️⃣[求导]</h3><p>标量、向量、矩阵之间的求导有两种布局，即分子布局和分母布局。分子布局和分母布局只差一个转置。<br>我的记法：在求导过程中，假设分母为m*n，分子为 k*n，则导数矩阵应该为 k*m 。一些特殊的如标量对矩阵求导等除外。<br>具体直接查表：<a href="https://en.m.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">https://en.m.wikipedia.org/wiki/Matrix_calculus</a></p><p>按位计算求导：<br>假设一个函数$f(x)$的输入是标量$x$。对于一组K个标量$x_1,··· ,x_K$，我们<br>可以通过$f(x)$得到另外一组K个标量$z_1,··· ,z_K$，<br>$z_k = f(x_k),∀k = 1,··· ,K$<br>其中，$f(x)$是按位运算的，即$[f(x)]_i = f(x_i)$<br>其导数是一个对角矩阵：<br><img src="/images/2018-09-23-15376727095200.jpg" width="50%" height="50%"></p><p><strong>Reference</strong>：<br><a href="https://en.m.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">https://en.m.wikipedia.org/wiki/Matrix_calculus</a><br><a href="https://blog.csdn.net/uncle_gy/article/details/78879131" target="_blank" rel="noopener">https://blog.csdn.net/uncle_gy/article/details/78879131</a><br><a href="https://nndl.github.io/chap-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.pdf" target="_blank" rel="noopener">https://nndl.github.io/chap-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.pdf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> 求导 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录7</title>
      <link href="/2018/09/23/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%957/"/>
      <url>/2018/09/23/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%957/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣softmax的numpy实现"><a href="#1️⃣softmax的numpy实现" class="headerlink" title="1️⃣softmax的numpy实现"></a>1️⃣softmax的numpy实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x,axis=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Compute softmax values for each sets of scores in x."""</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(x) / np.sum(np.exp(x), axis=axis)</span><br></pre></td></tr></table></figure><hr><h3 id="2️⃣numpy-手动求导relu"><a href="#2️⃣numpy-手动求导relu" class="headerlink" title="2️⃣numpy 手动求导relu"></a>2️⃣numpy 手动求导relu</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.dot(w1)</span><br><span class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.dot(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = np.square(y_pred - y).sum()</span><br><span class="line">    print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><hr><h3 id="3️⃣Pytorch实现relu"><a href="#3️⃣Pytorch实现relu" class="headerlink" title="3️⃣Pytorch实现relu"></a>3️⃣Pytorch实现relu</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return</span></span><br><span class="line"><span class="string">        a Tensor containing the output. ctx is a context object that can be used</span></span><br><span class="line"><span class="string">        to stash information for backward computation. You can cache arbitrary</span></span><br><span class="line"><span class="string">        objects for use in the backward pass using the ctx.save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ctx.save_for_backward(input)  <span class="comment">#为了之后的backward计算</span></span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br></pre></td></tr></table></figure><hr><h3 id="4️⃣Pytorch在多张卡上部署"><a href="#4️⃣Pytorch在多张卡上部署" class="headerlink" title="4️⃣Pytorch在多张卡上部署"></a>4️⃣Pytorch在多张卡上部署</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model)</span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> rand_loader:</span><br><span class="line">    input = data.to(device)</span><br><span class="line">    output = model(input)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>听达观杯现场答辩有感</title>
      <link href="/2018/09/19/%E8%A7%81%E9%97%BB&amp;%E6%83%B3%E6%B3%95/%E5%90%AC%E8%BE%BE%E8%A7%82%E6%9D%AF%E7%8E%B0%E5%9C%BA%E7%AD%94%E8%BE%A9%E6%9C%89%E6%84%9F/"/>
      <url>/2018/09/19/%E8%A7%81%E9%97%BB&amp;%E6%83%B3%E6%B3%95/%E5%90%AC%E8%BE%BE%E8%A7%82%E6%9D%AF%E7%8E%B0%E5%9C%BA%E7%AD%94%E8%BE%A9%E6%9C%89%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<p>前几日（周日）去了达观杯答辩现场听了前10名做了报告，有了一些感想，但一直没有抽出时间写一下自己的感想（懒）。</p><p>自己大概花了十来天做了一下比赛，实际上也就是一个文本分类的<a href="http://www.dcjingsai.com/common/cmpt/%E2%80%9C%E8%BE%BE%E8%A7%82%E6%9D%AF%E2%80%9D%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98%E8%B5%9B_%E8%B5%9B%E4%BD%93%E4%B8%8E%E6%95%B0%E6%8D%AE.html" target="_blank" rel="noopener">比赛</a>，因为没有比赛经验的缘故，走了很多弯路。不过也学到了一些东西。</p><p>现记录前十名的一些idea/trick：</p><ul><li>数据增强<ul><li>因为给的句子长度很长，因此在做截断的时候后面的就没法训练到了，可以将文本倒序作为新的数据训练模型。可以充分利用到数据</li><li>将数据打乱、随机删除，实际上就是对一个句子的词进行sample再组合</li><li>打乱词序以增加数据量</li><li>使用pseudo labeling，但有的队伍使用这个做出效果了，但有的没有</li></ul></li><li>特征工程<ul><li>假设开头中间结尾的信息对分类有帮助，因此截取该部分信息做训练</li><li>改进baseline的tfidf的特征工程方法，使用基于熵的词权重计算</li><li>降维，留下最重要的特征。先用卡方分布降到20万，再用SVD降到8000</li><li>将word2vec和GloVe拼接起来作为deep learning模型的输入</li><li>将文章分段，每段取前20后20拼起来</li></ul></li><li>模型融合<br>  所有队伍都无一例外使用了模型融合，stacking或者简单的投票<ul><li>DL+ML —&gt; lgbm model —&gt; voting</li><li>深度模型+传统模型，在深度模型最后一层加入传统模型的信息/feature</li><li>后向选择剔除冗余模型</li></ul></li><li>DL&amp;其他<ul><li>HAN，选择10个attention vector</li><li>对易错类增加权重，通过改变损失函数来增加权重</li><li>CNN, [1,2,3,4,5,6]*600</li><li>提出新的模型（第一名）</li></ul></li></ul><p>其实除了一些trick，我还是有些失望的，因为都是用模型融合堆出来的，这也让我对比赛失去了一些兴趣。虽然能理解现在的比赛都是这样的，但感觉实在太暴力了。<br>当然，其中还是有一些亮点的，有一支队伍立意很高，从理解业务的角度出发而不是堆模型，也取得了很好的效果；还有一个使用了最新论文中的特征工程改进方法，令我耳目一新；以及第一名在比赛过程中提出来三个新的模型。</p><p>Anyway，我目前还是太菜了，还是安心搞科研吧。_(:з」∠)</p>]]></content>
      
      
      
        <tags>
            
            <tag> 有感 </tag>
            
            <tag> 比赛 </tag>
            
            <tag> 达观杯 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识7</title>
      <link href="/2018/09/16/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%867/"/>
      <url>/2018/09/16/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%867/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>只有一个元素的tensor，可用.item()来获取元素</p><p>tensor &lt;—&gt; numpy 相互转化会共享内部数据，因此改变其中一个会改变另一个</p><p>可用使用 .to 来移动到设备<br><img src="/images/2018-09-16-15370671350548.jpg" alt=""></p><p>.detech()  detach it from the computation history, and to prevent future computation from being tracked. 将其从计算图中分离，变为叶子节点，并且requires_grad=False</p><p>Function 记录了这个tensor是怎么来的，所有的tensor都有，除非是用户自定义的：<br><img src="/images/2018-09-16-15370672806253.jpg" width="65%" height="50%"></p><hr><h3 id="2️⃣-协方差"><a href="#2️⃣-协方差" class="headerlink" title="2️⃣[协方差]"></a>2️⃣[协方差]</h3><p>关于协方差的理解，x与y关于某个自变量的变化程度，即度量了x与y之间的联系。<br><a href="https://www.zhihu.com/question/20852004" target="_blank" rel="noopener">https://www.zhihu.com/question/20852004</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> 协方差 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 4:Classification:Probabilistic Generative Model</title>
      <link href="/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%204%20Classification%20%20Probabilistic%20Generative%20Model/"/>
      <url>/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%204%20Classification%20%20Probabilistic%20Generative%20Model/</url>
      
        <content type="html"><![CDATA[<h3 id="为什么不使用regression来分类？"><a href="#为什么不使用regression来分类？" class="headerlink" title="为什么不使用regression来分类？"></a>为什么不使用regression来分类？</h3><p>1️⃣如果使用regression的思想来分类，会对离边界较远的点进行惩罚：<br><img src="/images/2018-09-16-15370662150910.jpg" width="70%" height="50%"></p><p>2️⃣如果多分类使用regression，如class 1, class 2, class 3；则隐式地假设了class 1 和 class 2较为接近，如果没有这种接近关系，则分类会不正确。</p><h3 id="问题描述与定义"><a href="#问题描述与定义" class="headerlink" title="问题描述与定义"></a>问题描述与定义</h3><p><img src="/images/2018-09-16-15370662762802.jpg" width="70%" height="50%"></p><p>当P大于0.5则是C1类，反之是C2类<br>先验P(C1)和P(C2)都好计算，计算C1占总的比例即可<br>因此，我们需要计算的就是p(x|C)</p><p>这一想法，本质是得到了生成式模型：<br><img src="/images/2018-09-16-15370663099515.jpg" width="70%" height="50%"></p><h3 id="原理概述"><a href="#原理概述" class="headerlink" title="原理概述"></a>原理概述</h3><p>现<strong>假设训练数据点的分布服从高斯分布</strong>：（显然可以自己设任何分布）<br>即数据从高斯分布采样得到：<br><img src="/images/2018-09-16-15370663870205.jpg" width="55%" height="50%"></p><p>根据最大似然估计，可以获得每个类别的μ和Σ：<br><img src="/images/2018-09-16-15370664041246.jpg" width="70%" height="50%"></p><p>得到了参数后，即可代入得到P(C|x) ：<br><img src="/images/2018-09-16-15370664355955.jpg" width="80%" height="50%"></p><p>刚刚假设$Σ$对于不同类别不同，现我们<strong>令不同类别共享相同$Σ$</strong>：<br>（因为协方差代表的是不同feature之间的联系，可以认为是和类别无关的）</p><p>$Σ$的计算公式是加权求和：<br><img src="/images/2018-09-16-15370665362081.jpg" width="24%" height="50%"></p><p>在使用了相同的协方差矩阵后，边界就是线性的（后面会提到为什么是这样）：<br><img src="/images/2018-09-16-15370665553962.jpg" alt=""></p><p> 总结：<br> 三步走，定义function set，计算μ和协方差矩阵，得到best function：<br><img src="/images/2018-09-16-15370665888683.jpg" width="60%" height="50%"></p><p>注意到，如果我们认为，不同feature之间没有关系，每个feature符合特定的高斯分布，则该分类器则是朴素贝叶斯分类器：<br><img src="/images/2018-09-16-15370666092459.jpg" width="60%" height="50%"></p><h3 id="分类与logistics-regression"><a href="#分类与logistics-regression" class="headerlink" title="分类与logistics regression"></a>分类与logistics regression</h3><p>现推导，该分类问题与logistics regression之间的联系：<br>即：<br><img src="/images/2018-09-16-15370666567324.jpg" width="60%" height="50%"></p><h4 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h4><p>数据服从高斯分布，共享$Σ$</p><h4 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h4><p>①总框架：<br><img src="/images/2018-09-16-15370667000974.jpg" width="50%" height="50%"></p><p>令<br><img src="/images/2018-09-16-15370667135868.jpg" width="24%" height="50%"></p><p>则有：<br><img src="/images/2018-09-16-15370667376312.jpg" width="50%" height="50%"></p><p>②z的进一步推导与简化：<br><img src="/images/2018-09-16-15370667582564.jpg" width="30%" height="50%"></p><p>将z展开：<br><img src="/images/2018-09-16-15370667763267.jpg" width="50%" height="50%"></p><p>而第一部分有：<br><img src="/images/2018-09-16-15370667880942.jpg" width="50%" height="50%"></p><p>第一部分相除，有：<br><img src="/images/2018-09-16-15370668274551.jpg" width="50%" height="50%"></p><p>再进行展开，有：<br><img src="/images/2018-09-16-15370668394919.jpg" width="50%" height="50%"></p><p>最终z的公式为：<br><img src="/images/2018-09-16-15370668522531.jpg" width="50%" height="50%"></p><p>由于共享协方差矩阵，则可以消去部分，得到：<br><img src="/images/2018-09-16-15370668957564.jpg" width="50%" height="50%"></p><p>替换成w和b：<br><img src="/images/2018-09-16-15370669103562.jpg" width="50%" height="50%"></p><p>③最终，将z带回到原式：<br><img src="/images/2018-09-16-15370669221472.jpg" width="25%" height="50%"></p><p>所以我们不需要再估计N1,N2,μ和Σ，直接计算w和b即可。也因此，分界线是线性的。</p><p>全过程：<br><img src="/images/2018-09-16-15370669594744.jpg" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Classification </tag>
            
            <tag> Probabilistic Generative Model </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 3:Gradient Descent</title>
      <link href="/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%203%20Gradient%20Descent/"/>
      <url>/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%203%20Gradient%20Descent/</url>
      
        <content type="html"><![CDATA[<h2 id="Gradient-Descent-tips"><a href="#Gradient-Descent-tips" class="headerlink" title="Gradient Descent tips"></a>Gradient Descent tips</h2><h3 id="tip-1：Adaptive-Learning-Rates"><a href="#tip-1：Adaptive-Learning-Rates" class="headerlink" title="tip 1：Adaptive Learning Rates"></a>tip 1：Adaptive Learning Rates</h3><h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><h5 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h5><p><img src="/images/2018-09-16-15370654467771.jpg" width="30%" height="50%"></p><p><img src="/images/2018-09-16-15370654502774.jpg" width="20%" height="50%"></p><p>其中σ是之前所有的梯度的平方根<br><img src="/images/2018-09-16-15370654728457.jpg" width="30%" height="50%"></p><p>化简形式：<br><img src="/images/2018-09-16-15370654853172.jpg" width="50%" height="50%"></p><h5 id="为什么要怎么做？"><a href="#为什么要怎么做？" class="headerlink" title="为什么要怎么做？"></a>为什么要怎么做？</h5><p>考虑一个开口向上的二次函数<br><img src="/images/2018-09-16-15370655091732.jpg" width="50%" height="50%"></p><p>也即，最好的步长是一次导除以二次导，但二次导计算量大，因此使用近似的方式：<br><strong>对一次导作多次的sample</strong>。<br>下图显示，如果二次导小，那么多次sample获得的一次导也小，反之则大，也就是说，一次导在某种程度上可以反映二次导的大小，所以直接用一次导近似，可以减少计算量。</p><p><img src="/images/2018-09-16-15370655850876.jpg" width="50%" height="50%"></p><h3 id="tip-2：feature-scaling"><a href="#tip-2：feature-scaling" class="headerlink" title="tip 2：feature scaling"></a>tip 2：feature scaling</h3><p><img src="/images/2018-09-16-15370656836724.jpg" width="50%" height="50%"></p><p>能够改变loss的分布，上图1中w2对loss的影响较大，则较陡峭，参数更新就较困难，需要adaptive learning rate；如果进行feature scaling，能够更好达到local optimal</p><h2 id="Gradient-Descent-Theory"><a href="#Gradient-Descent-Theory" class="headerlink" title="Gradient Descent Theory"></a>Gradient Descent Theory</h2><p>另一种角度看gradient descent：</p><p>基本思想：<br>我们希望每一次都在当前点附近找到一个最小的点，即在一个范围内：<br><img src="/images/2018-09-16-15370657785697.jpg" width="40%" height="50%"></p><p>应该如何找到该最小点？</p><p>我们知道，泰勒级数的形式：<br><img src="/images/2018-09-16-15370658066669.jpg" width="50%" height="50%"></p><p>当x接近x0时，会有如下近似：<br><img src="/images/2018-09-16-15370658167935.jpg" width="30%" height="50%"></p><p>推广到多元泰勒级数则有：<br><img src="/images/2018-09-16-15370658315314.jpg" width="60%" height="50%"></p><p>那么，如前所述，x接近x0，对于图中，即圆圈足够小时：<br><img src="/images/2018-09-16-15370658494091.jpg" width="50%" height="50%"></p><p>简化符号：<br><img src="/images/2018-09-16-15370658736683.jpg" width="12%" height="50%"></p><p><img src="/images/2018-09-16-15370658623258.jpg" width="30%" height="50%"></p><p>所以可以简写成：<br><img src="/images/2018-09-16-15370658855882.jpg" width="30%" height="50%"></p><p>由于s,u,v都是常数，在圆圈范围内寻找最小值对应的参数可以简化成：<br><img src="/images/2018-09-16-15370658981519.jpg" width="40%" height="50%"></p><p><img src="/images/2018-09-16-15370659061025.jpg" width="30%" height="50%"></p><p>再度简化，可以表达成：<br><img src="/images/2018-09-16-15370659680601.jpg" width="40%" height="50%"></p><p><img src="/images/2018-09-16-15370659747339.jpg" width="30%" height="50%"></p><p>在图中可以画为两个向量的点积<br><img src="/images/2018-09-16-15370660126195.jpg" width="40%" height="50%"></p><p>显然，当反方向时，最小：<br><img src="/images/2018-09-16-15370660243469.jpg" width="40%" height="50%"></p><p>也即：<br><img src="/images/2018-09-16-15370660628961.jpg" width="50%" height="50%"></p><p>最终完整的式子：<br><img src="/images/2018-09-16-15370660794436.jpg" width="55%" height="50%"></p><p>因此，当learning rate不够小时，是不满足泰勒级数近似的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> Gradient Descent </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lecture 2:Bias and Variance</title>
      <link href="/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%202%20Bias%20and%20Variance/"/>
      <url>/2018/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86/Lecture%202%20Bias%20and%20Variance/</url>
      
        <content type="html"><![CDATA[<h3 id="如何理解bias-amp-variance"><a href="#如何理解bias-amp-variance" class="headerlink" title="如何理解bias&amp;variance"></a>如何理解bias&amp;variance</h3><p><img src="/images/2018-09-16-15370650140053.jpg" width="40%" height="50%"><br>bias是function space中心离optimal model的差距，variance是某次实验所得模型离function space中心的距离。</p><p>比如说，简单地模型的function space小，随机性小，因此variance小，但也因为function space小，表示能力有限，因此bias大。</p><p>如图：<br><img src="/images/2018-09-16-15370651353167.jpg" width="70%" height="50%"><br>该图中蓝色圈代表模型所能表达的范围。</p><h3 id="如何解决variance大的问题"><a href="#如何解决variance大的问题" class="headerlink" title="如何解决variance大的问题"></a>如何解决variance大的问题</h3><p>①更多的data<br>②regularization：强迫function更平滑，因此减小variance，但因为调整了function space，可能会增加bias。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 机器学习🤖 </tag>
            
            <tag> 李宏毅机器学习课程 </tag>
            
            <tag> bias&amp;variance </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词9</title>
      <link href="/2018/09/16/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D9/"/>
      <url>/2018/09/16/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D9/</url>
      
        <content type="html"><![CDATA[<h3 id="白雪歌送武判官归京"><a href="#白雪歌送武判官归京" class="headerlink" title="白雪歌送武判官归京"></a>白雪歌送武判官归京</h3><p>[唐] 岑参<br>北风卷地白草折，胡天八月即飞雪。<br><strong>忽如一夜春风来，千树万树梨花开</strong>。<br>散入珠帘湿罗幕，狐裘不暖锦衾薄。<br>将军角弓不得控，都护铁衣冷难着。<br>瀚海阑干百丈冰，愁云惨淡万里凝。<br>中军置酒饮归客，胡琴琵琶与羌笛。<br>纷纷暮雪下辕门，风掣红旗冻不翻。<br><strong>轮台东门送君去，去时雪满天山路。<br>山回路转不见君，雪上空留马行处。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290</a></p><hr><h3 id="绝命诗"><a href="#绝命诗" class="headerlink" title="绝命诗"></a>绝命诗</h3><p>谭嗣同<br>望门投止思张俭，<br>忍死须臾待杜根。<br><strong>我自横刀向天笑，<br>去留肝胆两昆仑！</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pytorch backward()浅析</title>
      <link href="/2018/09/16/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Pytorch%20backward()%E6%B5%85%E6%9E%90/"/>
      <url>/2018/09/16/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Pytorch%20backward()%E6%B5%85%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>最近在看pytorch文档的时候，看到backward内有一个参数gradient，在经过查阅了相关资料和进行了实验后，对backward有了更深的认识。</p><h2 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h2><p>1️⃣如果调用backward的是一个标量，如：<code>loss.backward()</code><br>则gradient不需要手动传入，会自动求导。<br>例子:<br>$a=[x_1,x_2],b=\frac{x_1+x_2}{2}$<br>则b对a求导，有：<br>$\dfrac {\partial b}{\partial x_{1}}=\frac{1}{2}，\dfrac {\partial b}{\partial x_{2}}=\frac{1}{2}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a=torch.Tensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">a.requires_grad=<span class="keyword">True</span></span><br><span class="line">b=torch.mean(a)  <span class="comment">#tensor(2.5000, grad_fn=&lt;MeanBackward1&gt;)</span></span><br><span class="line">b.backward()</span><br><span class="line">a.grad   <span class="comment">#tensor([0.5000, 0.5000])</span></span><br></pre></td></tr></table></figure><p>gradient此时只是在缩放原grad的大小，也即不指定gradient和gradient=1是等价的</p><p>当然，也可以指定gradient，其中指定gradient的shape必须和b的维度相同<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gradient=torch.tensor(<span class="number">10.0</span>)</span><br><span class="line">b.backward(gradient)</span><br><span class="line">a.grad   <span class="comment">#tensor([5., 5.])</span></span><br></pre></td></tr></table></figure></p><p>2️⃣如果调用backward的是一个向量<br>例子：<br>$a=[x_1,x_2],b=[b_1,b_2]$, 其中 $b_1=x_1+x_2,b_2=x_1*x_2$<br>b对a求导，有：<br>$\dfrac {\partial b_1}{\partial x_{1}}=1,\dfrac {\partial b_1}{\partial x_{2}}=1$</p><p>$\dfrac {\partial b_2}{\partial x_{1}}=x_2,\dfrac {\partial b_2}{\partial x_{2}}=x_1$</p><p>在backward的时候则必须指定gradient。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a=torch.FloatTensor([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">a.requires_grad=<span class="keyword">True</span></span><br><span class="line">b=torch.zeros(<span class="number">2</span>)</span><br><span class="line">b[<span class="number">0</span>]=a[<span class="number">0</span>]+a[<span class="number">1</span>]</span><br><span class="line">b[<span class="number">1</span>]=a[<span class="number">0</span>]*a[<span class="number">1</span>]    <span class="comment"># b=tensor([5., 6.], grad_fn=&lt;CopySlices&gt;)</span></span><br><span class="line">gradient=torch.tensor([<span class="number">1.0</span>,<span class="number">0.0</span>])</span><br><span class="line">b.backward(gradient,retain_graph=<span class="keyword">True</span>)</span><br><span class="line">a.grad   <span class="comment">#tensor([1., 1.])，说明是对b_1进行求导</span></span><br><span class="line">a.grad.zero_()  <span class="comment">#将梯度清空，否则会叠加</span></span><br><span class="line"><span class="comment">#-------------- #</span></span><br><span class="line">gradient=torch.tensor([<span class="number">0.0</span>,<span class="number">1.0</span>])</span><br><span class="line">b.backward(gradient,retain_graph=<span class="keyword">True</span>)</span><br><span class="line">a.grad  <span class="comment"># tensor([3., 2.])，说明对b_2进行求导</span></span><br><span class="line">a.grad.zero_()</span><br><span class="line"><span class="comment"># ------------- #</span></span><br><span class="line">gradient=torch.tensor([<span class="number">1.0</span>,<span class="number">1.0</span>])</span><br><span class="line">b.backward(gradient,retain_graph=<span class="keyword">True</span>)</span><br><span class="line">a.grad   <span class="comment"># tensor([4., 3.])，即b_1,b_2的导数的叠加</span></span><br><span class="line">a.grad.zero_()</span><br></pre></td></tr></table></figure><p>注意到b.backward()时需要retain_graph设为True，否则在计算完后会自动释放计算图的内存，这样就没法进行二次反向传播了。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.pytorchtutorial.com/pytorch-backward/" target="_blank" rel="noopener">https://www.pytorchtutorial.com/pytorch-backward/</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> backward </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词8</title>
      <link href="/2018/09/09/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D8/"/>
      <url>/2018/09/09/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D8/</url>
      
        <content type="html"><![CDATA[<h3 id="望月怀远"><a href="#望月怀远" class="headerlink" title="望月怀远"></a>望月怀远</h3><p>[唐] 张九龄<br><strong>海上生明月，天涯共此时。</strong><br>情人怨遥夜，竟夕起相思。<br>灭烛怜光满，披衣觉露滋。<br>不堪盈手赠，还寝梦佳期。</p><p>遥夜，长夜。</p><p><a href="http://m.xichuangzhu.com/work/57aca120a341310060e2a09f" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57aca120a341310060e2a09f</a></p><hr><h3 id="无题"><a href="#无题" class="headerlink" title="无题"></a>无题</h3><p>萨镇冰<br>五十七载犹如梦，举国沦亡缘汉城。<br><strong>龙游浅水勿自弃，终有扬眉吐气天。</strong></p><p>1951年，中国人民志愿军在抗美援朝战争第三次战役后打进了汉城，萨镇冰得知此事，回想起57年前的甲午悲歌，当即作诗一首。</p><hr><h3 id="白雪歌送武判官归京"><a href="#白雪歌送武判官归京" class="headerlink" title="白雪歌送武判官归京"></a>白雪歌送武判官归京</h3><p>[唐] 岑参<br>北风卷地白草折，胡天八月即飞雪。<br><strong>忽如一夜春风来，千树万树梨花开。</strong><br>散入珠帘湿罗幕，狐裘不暖锦衾薄。<br>将军角弓不得控，都护铁衣冷难着。<br>瀚海阑干百丈冰，愁云惨淡万里凝。<br>中军置酒饮归客，胡琴琵琶与羌笛。<br>纷纷暮雪下辕门，风掣红旗冻不翻。<br><strong>轮台东门送君去，去时雪满天山路。<br>山回路转不见君，雪上空留马行处</strong>。</p><p><a href="http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词7</title>
      <link href="/2018/09/02/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D7/"/>
      <url>/2018/09/02/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D7/</url>
      
        <content type="html"><![CDATA[<h3 id="滕王阁序"><a href="#滕王阁序" class="headerlink" title="滕王阁序"></a>滕王阁序</h3><p>遥襟甫畅，逸兴遄飞。爽籁发而清风生，纤歌凝而白云遏。睢园绿竹，气凌彭泽之樽；邺水朱华，光照临川之笔。四美具，二难并。穷睇眄于中天，极娱游于暇日。天高地迥，觉宇宙之无穷；兴尽悲来，识盈虚之有数。望长安于日下，目吴会于云间。地势极而南溟深，天柱高而北辰远。关山难越，谁悲失路之人；萍水相逢，尽是他乡之客。怀帝阍而不见，奉宣室以何年？</p><hr><p><strong>注释：</strong><br>遥襟甫畅，逸兴遄（chuán）飞：登高望远的胸怀顿时舒畅，飘欲脱俗的兴致油然而生。</p><p>爽籁（lài）发而清风生，纤歌凝而白云遏：宴会上，排箫响起，好像清风拂来；柔美的歌声缭绕不散，遏止了白云飞动。爽：形容籁的发音清脆。籁：排箫，一种由多根竹管编排而成的管乐器。</p><p>睢（suī）园绿竹，气凌彭泽之樽：今日的宴会，好比当年睢园竹林的聚会，在座的文人雅士，豪爽善饮的气概超过了陶渊明。睢园：西汉梁孝王在睢水旁修建的竹园，他常和一些文人在此饮酒赋诗。</p><p>邺（yè）水朱华，光照临川之笔：这是借诗人曹植、谢灵运来比拟参加宴会的文人。邺：今河北临漳，是曹魏兴起的地方。曹植曾在这里作过《公宴诗》，诗中有“朱华冒绿池”的句子。临川之笔：指谢灵运，他曾任临川（今属江西）内史。</p><p>四美：指良辰、美景、赏心、乐事。</p><p>二难：贤主、嘉宾。</p><p>地势极而南溟深，天柱高而北辰远：地势偏远，南海深邃；天柱高耸，北极星远悬。</p><p>帝阍（hūn）：原指天帝的守门者。这里指皇帝的宫门。</p><p>奉宣室以何年：什么时候才能像贾谊那样去侍奉君王呢</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识6</title>
      <link href="/2018/09/02/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%866/"/>
      <url>/2018/09/02/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%866/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-dropout"><a href="#1️⃣-dropout" class="headerlink" title="1️⃣[dropout]"></a>1️⃣[dropout]</h3><p>dropout形式:<br><img src="/images/2018-09-02-15358857481798.jpg" width="70%" height="50%"><br>RNN的形式有多种：</p><ul><li><p>recurrent dropout<br>RNN: $h_t=f(W_h ⊙ [x_t,h_{t-1}]+b_h)$<br>加上dropout的RNN：$h_t=f(W_h ⊙ [x_t,d(h_{t-1})]+b_h)$，其中$d(\cdot)$为dropout函数<br>同理：<br>LSTM:$c_t=f_t ⊙c_{t-1} + i_t ⊙ d(g_t)$<br>GRU:$h_t=(1-z_t)⊙c_{t-1}+z_t⊙d(g_t)$</p></li><li><p>垂直连接的dropout<br>dropout的作用即是否允许L层某个LSTM单元的隐状态信息流入L+1层对应单元。<br><img src="/images/2018-09-02-15358866404870.jpg" width="50%" height="50%"></p></li></ul><p>Reference:<br><a href="https://blog.csdn.net/falianghuang/article/details/72910161" target="_blank" rel="noopener">https://blog.csdn.net/falianghuang/article/details/72910161</a></p><hr><h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>pack_padded_sequence用于RNN中，将padding矩阵压缩:<br><img src="/images/2018-09-02-15358868858836.jpg" width="60%" height="50%"><br>这样就可以实现在RNN传输过程中短句提前结束。</p><p>pad_packed_sequence是pack_padded_sequence的逆运算。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> dropout </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>我没有说话</title>
      <link href="/2018/08/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%88%91%E6%B2%A1%E6%9C%89%E8%AF%B4%E8%AF%9D/"/>
      <url>/2018/08/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%88%91%E6%B2%A1%E6%9C%89%E8%AF%B4%E8%AF%9D/</url>
      
        <content type="html"><![CDATA[<p>《我没有说话》</p><p>纳粹杀共产党时，<br>我没有出声<br>——因为我不是共产党员；<br>接着他们迫害犹太人，<br>我没有出声<br>——因为我不是犹太人；<br>然后他们杀工会成员，<br>我没有出声<br>——因为我不是工会成员；<br>后来他们迫害天主教徒，<br>我没有出声<br>——因为我是新教徒；<br>最后当他们开始对付我的时候，<br>已经没有人能站出来为我发声了</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Deep Learning NLP best practices笔记</title>
      <link href="/2018/08/26/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Deep%20Learning%20NLP%20best%20practices%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/08/26/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Deep%20Learning%20NLP%20best%20practices%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>博客地址：<a href="http://ruder.io/deep-learning-nlp-best-practices/index.html" target="_blank" rel="noopener">http://ruder.io/deep-learning-nlp-best-practices/index.html</a><br>个人觉得这篇文章写得很好，有许多实践得到的经验，通过这篇可以避免走一些弯路。</p><h2 id="Practices"><a href="#Practices" class="headerlink" title="Practices"></a>Practices</h2><h3 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h3><blockquote><p>The optimal dimensionality of word embeddings is mostly task-dependent: a smaller dimensionality works better for more syntactic tasks such as named entity recognition or part-of-speech (POS) tagging, while a larger dimensionality is more useful for more semantic tasks such as sentiment analysis.</p></blockquote><p>对于偏向语法的，使用维度低一些的词向量；而对于偏向语义内容的，使用维度大一些的词向量，如情感分析。</p><h3 id="LSTM-Depth"><a href="#LSTM-Depth" class="headerlink" title="LSTM Depth"></a>LSTM Depth</h3><blockquote><p>performance improvements of making the model deeper than 2 layers are minimal </p></blockquote><p>LSTM深度最好不要超过两层。</p><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><blockquote><p>It is often thought that Adam clearly outperforms vanilla stochastic gradient descent (SGD). However, while it converges much faster than SGD, it has been observed that SGD with learning rate annealing slightly outperforms Adam. Recent work furthermore shows that SGD with properly tuned momentum outperforms Adam .</p></blockquote><p>Adam可以更早拟合，而SGD效果可能会更好一些。</p><p>可以采用优化策略，比如说使用Adam训练直到拟合，然后将学习率减半，并重新导入之前训练好的最好的模型。这样Adam能够忘记之前的信息并重新开始训练。</p><blockquote><p>Denkowski &amp; Neubig (2017) show that Adam with 2 restarts and learning rate annealing is faster and performs better than SGD with annealing</p></blockquote><h3 id="Ensembling"><a href="#Ensembling" class="headerlink" title="Ensembling"></a>Ensembling</h3><blockquote><p>Combining multiple models into an ensemble by averaging their predictions is a proven strategy to improve model performance.</p></blockquote><p>Ensembling很重要的一点是需要保证多样性：</p><blockquote><p>Ensembling is an important way to ensure that results are still reliable if the diversity of the evaluated models increases (Denkowski &amp; Neubig, 2017). While ensembling different checkpoints of a model has been shown to be effective (Jean et al., 2015; Sennrich et al., 2016) [51, 52], it comes at the cost of model diversity. Cyclical learning rates can help to mitigate this effect</p></blockquote><h3 id="LSTM-tricks"><a href="#LSTM-tricks" class="headerlink" title="LSTM tricks"></a>LSTM tricks</h3><ul><li><p>在initial state中我们常常使用全0向量，实际上可以将其作为参数学习。</p><blockquote><p>Instead of fixing the initial state, we can learn it like any other parameter, which can improve performance</p></blockquote></li><li><p>将input和output embedding的参数共享，如果是做language model或者机器翻译之类的，可以让他们共享。</p></li><li><p>Gradient Norm Clipping</p><blockquote><p>Rather than clipping each gradient independently, clipping the global norm of the gradient yields more significant improvements</p></blockquote></li></ul><p>这点我没看懂。</p><h3 id="Classification-practices"><a href="#Classification-practices" class="headerlink" title="Classification practices"></a>Classification practices</h3><p>关于CNN</p><blockquote><p>CNN filters:Combining filter sizes near the optimal filter size, e.g. (3,4,5) performs best (Kim, 2014; Kim et al., 2016). The optimal number of feature maps is in the range of 50-600 (Zhang &amp; Wallace, 2015) [59].</p><p>Aggregation function:1-max-pooling outperforms average-pooling and k-max pooling (Zhang &amp; Wallace, 2015).</p></blockquote><p>这在我之前的关于CNN文本分类指南中有更详尽的分析。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>这是一篇干货满满的博客，实际上我还是有许多地方没有读懂，这适合多看几遍，慢慢理解。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 指南 </tag>
            
            <tag> 调参 </tag>
            
            <tag> NLP🤖 </tag>
            
            <tag> 笔记📒 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识5</title>
      <link href="/2018/08/26/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%865/"/>
      <url>/2018/08/26/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%865/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[Paper]<br>Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components</p><p>基本框架和CBOW一致，主要贡献在于针对中文词向量添加了偏旁、字的组件作为训练信息。</p><p><img src="/images/2018-08-26-15352530482345.jpg" width="50%" height="50%"></p><hr><p>2️⃣[Paper]<br>Highway Networks</p><p>为了解决神经网络深度过深时导致的反向传播困难的问题。<br>前向传播的公式：</p><script type="math/tex; mode=display">y=H(x,W_H)</script><p>而论文所做的改进：</p><script type="math/tex; mode=display">y=H(x,W_H) \cdot T(x,W_T)+ x \cdot C(x,W_C)</script><p>其中$T$是transform gate，$C$是carry gate。方便起见，可以将 $C=1-T$，最终有：</p><script type="math/tex; mode=display">y=H(x,W_H) \cdot T(x,W_T)+ x \cdot (1-T(x,W_T))</script><p>可以看出思想和LSTM很类似，都是gate的思想。</p><hr><p>3️⃣[调参方法]<br>博客：<a href="https://blog.goodaudience.com/how-to-make-your-model-happy-again-part-1-40d94a9ffb41" target="_blank" rel="noopener">https://blog.goodaudience.com/how-to-make-your-model-happy-again-part-1-40d94a9ffb41</a></p><ul><li><strong>学习率</strong>：</li></ul><p>一条原则：当validation loss开始上升时，减少学习率。</p><p>如何减少？</p><p><img src="/images/2018-08-26-15352871548330.jpg" width="50%" height="50%"></p><p>或者：</p><p><img src="/images/2018-08-26-15352873283890.jpg" width="50%" height="50%"><br>设定一定的epoch作为一个stepsize，在训练过程中线性增加学习率，然后在到达最大值后再线性减小。<br>实验表明，使用该方法可以在一半的epoch内达到相同的效果。</p><ul><li><strong>batch size</strong>：</li></ul><p><img src="/images/2018-08-26-15352891425729.jpg" width="50%" height="50%"></p><p>由于batch size和学习率的强相关性，<a href="https://arxiv.org/pdf/1711.00489.pdf" target="_blank" rel="noopener">相关论文</a>提出提高batch size而不是降低学习率的方法来提升模型表现。</p><blockquote><p>increasing the batch size during training, instead of decaying learning rate. — L. Smith<br><a href="https://arxiv.org/pdf/1711.00489.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1711.00489.pdf</a></p></blockquote><p>一个trick：保持学习率不变，提高batch size，直到batch size~训练集/10，接下来再采用学习率下降的策略。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Paper </tag>
            
            <tag> 调参方法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录6</title>
      <link href="/2018/08/26/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB6/"/>
      <url>/2018/08/26/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB6/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣将数据整理成batch"><a href="#1️⃣将数据整理成batch" class="headerlink" title="1️⃣将数据整理成batch"></a>1️⃣将数据整理成batch</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_iter_batch</span><span class="params">(paras,labels,batch_size,shuffle=True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param paras:</span></span><br><span class="line"><span class="string">    :param labels:</span></span><br><span class="line"><span class="string">    :param batch_size:</span></span><br><span class="line"><span class="string">    :param shuffle:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">assert</span> len(paras)==len(labels)</span><br><span class="line">    paras_size=len(paras)</span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        indices=np.arange(paras_size)</span><br><span class="line">        np.random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> start_idx <span class="keyword">in</span> range(<span class="number">0</span>,paras_size-batch_size+<span class="number">1</span>,batch_size):</span><br><span class="line">        <span class="keyword">if</span> shuffle:</span><br><span class="line">            excerpt=indices[start_idx:start_idx+batch_size]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            excerpt=slice(start_idx,start_idx+batch_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> paras[excerpt],labels[excerpt]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词6</title>
      <link href="/2018/08/26/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D6/"/>
      <url>/2018/08/26/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D6/</url>
      
        <content type="html"><![CDATA[<p>1️⃣</p><h3 id="戏为六绝句"><a href="#戏为六绝句" class="headerlink" title="戏为六绝句"></a>戏为六绝句</h3><p>[唐] 杜甫<br>【其二】<br>王杨卢骆当时体，轻薄为文哂未休。<br><strong>尔曹身与名俱灭，不废江河万古流</strong>。</p><p>哂（shěn）：讥笑。</p><p><a href="http://m.xichuangzhu.com/work/57b9658c0a2b58005c95d2a7" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b9658c0a2b58005c95d2a7</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CNN文本分类任务指南</title>
      <link href="/2018/08/25/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/CNN%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%8C%87%E5%8D%97/"/>
      <url>/2018/08/25/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/CNN%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p>最近因为比赛的缘故对文本分类有一定的了解。其中使用CNN方法做情感分析任务存在着许多优势。虽然模型简单，但如何设置超参有时候对结果有很大的影响。本文记录了关于CNN文本分类的一些学习历程和指南，基本参考了论文。</p><h2 id="做法"><a href="#做法" class="headerlink" title="做法"></a>做法</h2><p>基本上目前较为浅层的CNN文本分类的做法都是如下图：<br><img src="/images/2018-08-25-15351860103617.jpg" alt=""></p><p>将词向量堆积成为二维的矩阵，通过CNN的卷积单元对矩阵进行卷积处理，同时使用pooling（通常是1max-pooling）操作，将不等长的卷积结果变为等长，对不同的卷积单元的结果进行拼接后生成单个向量，最后再通过线性层转化成类别概率分布。</p><p>另一张图也说明了该流程。</p><p><img src="/images/2018-08-25-15351863867337.jpg" alt=""></p><h2 id="建议与指导"><a href="#建议与指导" class="headerlink" title="建议与指导"></a>建议与指导</h2><h3 id="超参及其对结果的影响"><a href="#超参及其对结果的影响" class="headerlink" title="超参及其对结果的影响"></a>超参及其对结果的影响</h3><p>接下来的内容参考了论文<a href="https://arxiv.org/pdf/1510.03820.pdf" target="_blank" rel="noopener">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional<br>Neural Networks for Sentence Classification</a></p><p>CNN文本分类的超参：</p><ul><li>输入向量</li><li>卷积大小</li><li>输出通道（feature maps）</li><li>激活函数</li><li>池化策略</li><li>正则化</li></ul><h4 id="输入向量的影响"><a href="#输入向量的影响" class="headerlink" title="输入向量的影响"></a>输入向量的影响</h4><p>实验表明，使用word2vec和GloVe不分伯仲，但将word2vec和GloVe简单拼接在一起并不能带来提升。</p><blockquote><p>unfortunately, simply concatenating these representations does necessarily seem helpful</p></blockquote><p>当句子长度很长（document classification）时，使用one-hot可能会有效果，但在句子长度不是很长时，效果不好。</p><h5 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h5><p>对于新任务，可以word2vec或GloVe或者其他词向量都试一下，如果句子长，可以试着使用one-hot。</p><h4 id="卷积大小"><a href="#卷积大小" class="headerlink" title="卷积大小"></a>卷积大小</h4><p>由于卷积的长度是固定的，也就是词向量的长度，因此只需讨论宽度。<br>实验表明，不同的数据集会有不同的最佳大小，但似乎对于长度越长的句子，最佳大小有越大的趋势。</p><blockquote><p>However, for datasets comprising longer sentences, such as CR (maximum sentence length is 105, whereas it ranges from 36-56 on the other sentiment datasets used here), the optimal region size may be larger.</p></blockquote><p>同时，当增加不同卷积大小作为组合时，如果组合的卷积核大小接近于最佳大小（optimal region size），有助于结果的提升；相反，如果卷积核大小离最佳大小很远时，反而会产生负面影响。</p><h5 id="建议-1"><a href="#建议-1" class="headerlink" title="建议"></a>建议</h5><p>首先试着找到最优的卷积核大小，然后在这个基础上添加和该卷积核大小类似的卷积核。</p><h4 id="feature-maps"><a href="#feature-maps" class="headerlink" title="feature maps"></a>feature maps</h4><p>也就是输出通道（out channel），表明该卷积核大小的卷积核有多少个。</p><p>实验表明，最佳的feature maps和数据集相关，但一般不超过600。</p><blockquote><p>it would seem that increasing the number of maps beyond 600 yields at best very marginal returns, and often hurts performance.</p></blockquote><h5 id="建议-2"><a href="#建议-2" class="headerlink" title="建议"></a>建议</h5><p>在600内搜索最优，如果在600的边缘还没有明显的效果下降，那么可以尝试大于600的feature maps。</p><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>实验结果：<br><img src="/images/2018-08-25-15351889835594.jpg" alt=""></p><p>结果表明，tanh、ReLU和不使用激活函数效果较好。tanh的优点是以0为中心，ReLU能够加速拟合，至于为什么不使用的效果会好，可能是因为模型较为简单：</p><blockquote><p>This indicates that on some datasets, a linear transformation is enough to capture the<br>correlation between the word embedding and the output label.</p></blockquote><h5 id="建议-3"><a href="#建议-3" class="headerlink" title="建议"></a>建议</h5><p>使用tanh、ReLU或者干脆不使用。但如果模型更为复杂，有多层的结构，还是需要使用激活函数的。</p><h4 id="pooling策略"><a href="#pooling策略" class="headerlink" title="pooling策略"></a>pooling策略</h4><p>所有的实验都表明了，1-max pooling的效果比其他好，如k-max pooling。在pooling这一步可以直接选择1-max pooling。</p><blockquote><p>This may be because the location of predictive contexts does not matter, and certain n-grams in the sentence can be more predictive on their own than the entire sentence considered jointly.</p></blockquote><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>主要是dropout和l2 norm constraint。<br>dropout就是随机将一些神经元置为0，l2 norm constraint是对参数矩阵W进行整体缩放，使其不超过一定阈值。（与通常的l2 regularization不同，最早可追溯到Hinton的<a href="https://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="noopener">Improving neural networks by preventing<br>co-adaptation of feature detectors</a>）</p><blockquote><p>the l2 norm of a weight vector is linearly scaled to a constraint c when it exceeds this threshold, so a smaller c implies stronger regularization</p></blockquote><p>实验表明，dropout起的作用很小，l2 norm没有提升甚至还会导致下降。可能是因为模型参数不多，因此过拟合的可能性较低。</p><h5 id="建议-4"><a href="#建议-4" class="headerlink" title="建议"></a>建议</h5><p>设置较小的dropout和较大的l2 norm，当feature maps增大时，可以试着调节较大的dropout以避免过拟合。</p><h3 id="建议及结论"><a href="#建议及结论" class="headerlink" title="建议及结论"></a>建议及结论</h3><ul><li>刚开始的使用使用word2vec或者GloVe，如果数据量够大，可以尝试one-hot</li><li>线性搜索最佳的卷积核大小，如果句子够长，那么可以扩大搜索范围。一旦确定了最佳卷积核大小，尝试在该卷积核大小的附近进行组合，如最佳卷积核宽度是5，那么尝试[3,4,5]或者[2,3,4,5]等</li><li>使用较小的dropout和较大的max norm constraint，然后在[100,600]范围内搜索feature maps，如果最佳的feature maps在600附近，可以试着选择比600更大的范围</li><li>尝试不同的激活函数，通常tanh和ReLU是较好的，但也可以尝试什么都不加。</li><li>使用1-max pooling。</li><li>如果模型复杂，比如feature maps很大，那么可以尝试更为严格的正则化，如更大的dropout rate和较小的max norm constraint。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="http://www.aclweb.org/anthology/D14-1181" target="_blank" rel="noopener">Convolutional Neural Networks for Sentence Classification</a></p><p><a href="https://arxiv.org/pdf/1510.03820.pdf" target="_blank" rel="noopener">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional<br>Neural Networks for Sentence Classification</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> 情感分析 </tag>
            
            <tag> 指南 </tag>
            
            <tag> 调参 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Pytorch中的inplace的操作</title>
      <link href="/2018/08/20/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADin-place%E7%9A%84%E6%93%8D%E4%BD%9C/"/>
      <url>/2018/08/20/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADin-place%E7%9A%84%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>最近在写Hierarchical attention network的时候遇到了如下的bug：</p><blockquote><p>one of the variables needed for gradient computation has been modified by an inplace operation</p></blockquote><p>在查阅了文档和请教了其他人之后，最终找到了bug。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(seq_len):</span><br><span class="line">    h_i = rnn_outputs[i]  <span class="comment"># batch,hidden*2</span></span><br><span class="line">    a_i = attn_weights[i].unsqueeze_(<span class="number">1</span>)  <span class="comment"># take in-place opt may cause an error</span></span><br><span class="line">    a_i = a_i.expand_as(h_i)  <span class="comment"># batch,hidden*2</span></span><br></pre></td></tr></table></figure><p>这是我原来的逻辑，我在无意中做了inplace操作，导致了bug的发生。正确的做法应该是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(seq_len):</span><br><span class="line">    h_i = rnn_outputs[i]  <span class="comment"># batch,hidden*2</span></span><br><span class="line">    <span class="comment"># a_i = attn_weights[i].unsqueeze_(1)  # take in-place opt may cause an error</span></span><br><span class="line">    a_i = attn_weights[i].unsqueeze(<span class="number">1</span>)  <span class="comment"># batch,1</span></span><br><span class="line">    a_i = a_i.expand_as(h_i)  <span class="comment"># batch,hidden*2</span></span><br></pre></td></tr></table></figure><p>实际上，在实践过程中应当尽量避免inplace操作，在官方文档中也提到了（存疑）这点，虽然提供了inplace操作，但并不推荐使用。</p><p>具体的原因是，在Pytorch构建计算图的过程中，会记录每个节点是怎么来的，但inplace会破坏这种关系，使得在回传的时候没法正常求导。</p><p>特别地，有两种情况不应该使用inplace操作（摘自知乎）：</p><ol><li>对于requires_grad=True的叶子张量(leaf tensor)不能使用inplace operation</li><li>对于在求梯度阶段需要用到的张量不能使用inplace operation</li></ol><p>Reference:<br><a href="https://zhuanlan.zhihu.com/p/38475183" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38475183</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>愿中国青年都摆脱冷气</title>
      <link href="/2018/08/19/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%84%BF%E4%B8%AD%E5%9B%BD%E9%9D%92%E5%B9%B4%E9%83%BD%E6%91%86%E8%84%B1%E5%86%B7%E6%B0%94/"/>
      <url>/2018/08/19/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%84%BF%E4%B8%AD%E5%9B%BD%E9%9D%92%E5%B9%B4%E9%83%BD%E6%91%86%E8%84%B1%E5%86%B7%E6%B0%94/</url>
      
        <content type="html"><![CDATA[<p>近期的新闻常让人感到愤怒以致绝望…</p><hr><p>愿中国青年都摆脱冷气，只是向上走，不必听自暴自弃者流的话。能做事的做事，能发声的发声。有一分热，发一分光。就令萤火一般，也可以在黑暗里发一点光，不必等候炬火。</p><p>—鲁迅《热风》</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录5</title>
      <link href="/2018/08/19/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB5/"/>
      <url>/2018/08/19/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB5/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣sklearn模型的保存与恢复"><a href="#1️⃣sklearn模型的保存与恢复" class="headerlink" title="1️⃣sklearn模型的保存与恢复"></a>1️⃣sklearn模型的保存与恢复</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">clf = svm.SVC()</span><br><span class="line">clf.fit(X, y)  </span><br><span class="line">clf.fit(train_X,train_y)</span><br><span class="line">joblib.dump(clf, <span class="string">"train_model.m"</span>)</span><br><span class="line">clf = joblib.load(<span class="string">"train_model.m"</span>)</span><br><span class="line">clf.predit(test_X)</span><br></pre></td></tr></table></figure><hr><h3 id="2️⃣Dictionary类"><a href="#2️⃣Dictionary类" class="headerlink" title="2️⃣Dictionary类"></a>2️⃣Dictionary类</h3><p>在构造字典时需要用到<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dictionary</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.word2idx = &#123;&#125;</span><br><span class="line">        self.idx2word = []</span><br><span class="line">        self.__vocab_size = <span class="number">0</span></span><br><span class="line">        self.add_word(<span class="string">'&lt;pad&gt;'</span>)</span><br><span class="line">        self.add_word(<span class="string">'&lt;UNK&gt;'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_word</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.word2idx:</span><br><span class="line">            self.idx2word.append(word)</span><br><span class="line">            self.word2idx[word] = self.__vocab_size</span><br><span class="line">            self.__vocab_size += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_index</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> self.word2idx:</span><br><span class="line">            <span class="keyword">return</span> self.word2idx[word]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.word2idx[<span class="string">'&lt;UNK&gt;'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.idx2word[idx]</span><br></pre></td></tr></table></figure></p><hr><h3 id="3️⃣对dict按元素排序的三种方法"><a href="#3️⃣对dict按元素排序的三种方法" class="headerlink" title="3️⃣对dict按元素排序的三种方法"></a>3️⃣对dict按元素排序的三种方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">d=&#123;<span class="string">'apple'</span>:<span class="number">10</span>,<span class="string">'orange'</span>:<span class="number">20</span>,<span class="string">'banana'</span>:<span class="number">5</span>,<span class="string">'watermelon'</span>:<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#法1</span></span><br><span class="line">print(sorted(d.items(),key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])) <span class="comment">#[('watermelon', 1), ('banana', 5), ('apple', 10), ('orange', 20)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#法2</span></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"></span><br><span class="line">print(sorted(d.items(),key=itemgetter(<span class="number">1</span>))) <span class="comment">#[('watermelon', 1), ('banana', 5), ('apple', 10), ('orange', 20)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#法3</span></span><br><span class="line"></span><br><span class="line">print(sorted(d,key=d.get))  <span class="comment">#['watermelon', 'banana', 'apple', 'orange'] 没有value了</span></span><br></pre></td></tr></table></figure><hr><h3 id="4️⃣合并dict的三种方法"><a href="#4️⃣合并dict的三种方法" class="headerlink" title="4️⃣合并dict的三种方法"></a>4️⃣合并dict的三种方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d1=&#123;<span class="string">'a'</span>:<span class="number">1</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d2=&#123;<span class="string">'b'</span>:<span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#法1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d=&#123;**d1,**d2&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d</span><br><span class="line">&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#法2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dd=dict(d1.items()|d2.items())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dd</span><br><span class="line">&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#法3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d1.update(d2)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d1</span><br><span class="line">&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="5️⃣找到list最大最小值的index"><a href="#5️⃣找到list最大最小值的index" class="headerlink" title="5️⃣找到list最大最小值的index"></a>5️⃣找到list最大最小值的index</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">lst = [<span class="number">40</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minIndex</span><span class="params">(lst)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> min(range(len(lst)),key=lst.__getitem__)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxIndex</span><span class="params">(lst)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> max(range(len(lst)),key=lst.__getitem__)</span><br><span class="line">    </span><br><span class="line">print(minIndex(lst))</span><br><span class="line">print(maxIndex(lst))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Pytorch中的Embedding padding</title>
      <link href="/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADEmbedding%E7%9A%84padding/"/>
      <url>/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADEmbedding%E7%9A%84padding/</url>
      
        <content type="html"><![CDATA[<p>在Pytorch中，nn.Embedding()代表embedding矩阵，其中有一个参数<code>padding_idx</code>指定用以padding的索引位置。所谓padding，就是在将不等长的句子组成一个batch时，对那些空缺的位置补0，以形成一个统一的矩阵。</p><p>用法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.embedding = nn.Embedding(vocab_size, embed_dim,padding_idx=<span class="number">0</span>) <span class="comment">#也可以是别的数值</span></span><br></pre></td></tr></table></figure></p><p>在显式设定<code>padding_idx=0</code>后，在自定义的词典内也应当在相应位置添加<code>&lt;pad&gt;</code>作为一个词。如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dictionary</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.word2idx = &#123;&#125;</span><br><span class="line">        self.idx2word = []</span><br><span class="line">        self.__vocab_size = <span class="number">0</span></span><br><span class="line">        self.add_word(<span class="string">'&lt;pad&gt;'</span>)  <span class="comment"># should add &lt;pad&gt; first</span></span><br><span class="line">        self.add_word(<span class="string">'&lt;UNK&gt;'</span>)</span><br></pre></td></tr></table></figure><p>那么对于<code>padding_idx</code>，内部是如何操作的呢？</p><p>在查看了Embedding的源码后，发现设置了<code>padding_idx</code>，类内部会有如下操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-----Embedding __init__ 内部--------------</span></span><br><span class="line"><span class="keyword">if</span> _weight <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">    self.weight = Parameter(torch.Tensor(num_embeddings, embedding_dim))</span><br><span class="line">    self.reset_parameters()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#---------reset_parameters()--------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.weight.data.normal_(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> self.padding_idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        self.weight.data[self.padding_idx].fill_(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>也就是说，当Embedding是随机初始化的矩阵时，会对<code>padding_idx</code>所在的行进行填0。保证了padding行为的正确性。</p><p>那么，还需要保证一个问题，就是在反向回传的时候，<code>padding_idx</code>是不会更新的.</p><p>在查看了源码后发现在Embedding类内有如下注释：</p><blockquote><p>.. note::<br>        With :attr:<code>padding_idx</code> set, the embedding vector at<br>        :attr:<code>padding_idx</code> is initialized to all zeros. However, note that this<br>        vector can be modified afterwards, e.g., using a customized<br>        initialization method, and thus changing the vector used to pad the<br>        output. The gradient for this vector from :class:<code>~torch.nn.Embedding</code><br>        is always zero.</p></blockquote><p>并且在查阅了其他资料后，发现该行确实会不更新。有意思的是，查阅源码并没有找到如何使其不更新的机制，因为在F.embedding函数中，返回：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)</span><br></pre></td></tr></table></figure><p>但我并不能跳转到torch.embedding中，大概是因为这部分被隐藏了吧。我也没有再深究下去。我猜测有可能是在autograd内部有对该部分进行单独的处理，用mask屏蔽这部分的更新；或者一个更简单的方法，就是任其更新，但每一次都reset，将第一行手动设为全0。</p><p><strong>附记</strong>：</p><p>假如说没有显式设置该行，是否padding就没有效果呢？<br>我认为是的。</p><p>一般来说，我们都是以0作为padding的填充，如：</p><div class="table-container"><table><thead><tr><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>12</td><td>44</td><td>22</td><td>67</td><td>85</td></tr><tr><td>12</td><td>13</td><td>534</td><td>31</td><td>0</td></tr><tr><td>87</td><td>23</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div><p>每一行代表一个句子，其中0作为填充。然后将该矩阵送入到embedding_lookup中，获得三维的tensor，那么0填充的部分，所获得的embedding表示应当是要全0。</p><p>假如不显式设置<code>padding_idx=0</code>，就可能会出现两个结果（个人推测)：</p><p>①本应该全0的地方，被词典中第一个词的词向量表示给替代了，因为将0作为索引去embedding矩阵获取到的词向量，就是第一个词的词向量，而该词并不全0。</p><p>②词典的最后一个词被全0覆盖。F.embedding中有如下片段：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> padding_idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">    <span class="keyword">if</span> padding_idx &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">assert</span> padding_idx &lt; weight.size(<span class="number">0</span>), <span class="string">'Padding_idx must be within num_embeddings'</span></span><br><span class="line">    <span class="keyword">elif</span> padding_idx &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">assert</span> padding_idx &gt;= -weight.size(<span class="number">0</span>), <span class="string">'Padding_idx must be within num_embeddings'</span></span><br><span class="line">        padding_idx = weight.size(<span class="number">0</span>) + padding_idx</span><br><span class="line"><span class="keyword">elif</span> padding_idx <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        padding_idx = <span class="number">-1</span></span><br></pre></td></tr></table></figure><p>上面片段显示，<code>padding_idx</code>被设置为-1，也就是最后一个单词。做完这步紧接着就返回：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)</span><br></pre></td></tr></table></figure><p>还是由于torch.embedding无法查看的原因，我不知道内部是如何实现的，但应该来说，最后一个词就是被覆盖了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Embedding </tag>
            
            <tag> padding </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python Tricks[转]</title>
      <link href="/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Python%20Tricks%5B%E8%BD%AC%5D/"/>
      <url>/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Python%20Tricks%5B%E8%BD%AC%5D/</url>
      
        <content type="html"><![CDATA[<p>原文地址:<a href="https://hackernoon.com/python-tricks-101-2836251922e0" target="_blank" rel="noopener">https://hackernoon.com/python-tricks-101-2836251922e0</a></p><p>我觉得这个介绍Python一些tricks的文章很好，能够更加熟悉Python的一些非常方便的用法。<br>以下是我觉得有用的几个点。</p><p>1️⃣Reverse a String/List</p><p><img src="/images/2018-08-19-15346465152976.jpg" width="70%" height="50%"></p><p><img src="/images/2018-08-19-15346467215597.jpg" width="70%" height="50%"></p><p>[::-1]解释：<br>[:]表示取所有的元素，-1表示步进。[1:5:2]表示的就是从元素1到元素5，每2个距离取一个。</p><hr><p>2️⃣transpose 2d array</p><p><img src="/images/2018-08-19-15346470165919.jpg" width="70%" height="50%"></p><p>zip()相当于压缩，zip(*)相当于解压。</p><hr><p>3️⃣Chained function call</p><p><img src="/images/2018-08-19-15346471756442.jpg" width="70%" height="50%"></p><p>非常简洁的写法。</p><hr><p>4️⃣Copy List</p><p><img src="/images/2018-08-19-15346472744350.jpg" width="50%" height="50%"></p><p>之前谈过的Python的赋值、浅拷贝、深拷贝。</p><hr><p>5️⃣Dictionary get</p><p><img src="/images/2018-08-19-15346473929918.jpg" width="70%" height="50%"></p><p>避免了dict不存在该元素的问题。</p><hr><p>6️⃣✨Sort Dictionary by Value</p><p><img src="/images/2018-08-19-15346475170316.jpg" width="90%" height="50%"></p><p>其中第三种返回的是[‘watermelon’, ‘banana’, ‘apple’, ‘orange’]，没有value了。</p><hr><p>7️⃣For…else</p><p><img src="/images/2018-08-19-15346481408714.jpg" width="90%" height="50%"></p><p>注意到如果for在中途break了，就不会进入到else了；只有顺利循环完才会进入到else。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> e <span class="keyword">in</span> a:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">if</span> e==<span class="number">0</span>:</span><br><span class="line"><span class="meta">... </span>            <span class="keyword">break</span></span><br><span class="line"><span class="meta">... </span><span class="keyword">else</span>:</span><br><span class="line"><span class="meta">... </span>    print(<span class="string">'hello'</span>)</span><br><span class="line"><span class="meta">... </span><span class="comment">#什么都没有print</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> e <span class="keyword">in</span> a:</span><br><span class="line"><span class="meta">... </span>    print(e)</span><br><span class="line"><span class="meta">... </span><span class="keyword">else</span>:</span><br><span class="line"><span class="meta">... </span>    print(<span class="string">'hello'</span>)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line">hello</span><br></pre></td></tr></table></figure><hr><p>8️⃣Merge dict’s</p><p><img src="/images/2018-08-19-15346483785515.jpg" width="90%" height="50%"></p><p>合并dict的方法。</p><hr><p>9️⃣Min and Max index in List</p><p><img src="/images/2018-08-19-15346487918895.jpg" width="80%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> Python tricks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识4</title>
      <link href="/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%864/"/>
      <url>/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%864/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[概率校准(Probability Calibration)]<br>一种对机器学习算法输出结果的校准，通过几个实验可以发现，概率校准能够一定程度提高表现。<br>几个参考资料：<br>直观理解:  <a href="http://www.bubuko.com/infodetail-2133893.html" target="_blank" rel="noopener">http://www.bubuko.com/infodetail-2133893.html</a><br>SVC的概率校准在sklearn上的应用: <a href="https://blog.csdn.net/ericcchen/article/details/79337716" target="_blank" rel="noopener">https://blog.csdn.net/ericcchen/article/details/79337716</a><br>✨完全手册: <a href="http://users.dsic.upv.es/~flip/papers/BFHRHandbook2010.pdf" target="_blank" rel="noopener">Calibration of Machine Learning Models</a></p><hr><p>2️⃣[Paper]<br><a href="https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf" target="_blank" rel="noopener">Hierarchical Attention Networks for Document Classification</a></p><p>亮点在使用层次的RNN结构，以及使用了attention方法。<br><img src="/images/2018-08-19-15346447273228.jpg" width="50%" height="50%"></p><p>参考了其他人的代码自己也试着实现了一个，GitHub地址：<a href="https://github.com/linzehui/pytorch-hierarchical-attention-network" target="_blank" rel="noopener">https://github.com/linzehui/pytorch-hierarchical-attention-network</a></p><hr><p>3️⃣[XGBoost]<br>kaggle神器XGBoost，一篇原理的详细介绍：<br><a href="http://www.cnblogs.com/willnote/p/6801496.html" target="_blank" rel="noopener">http://www.cnblogs.com/willnote/p/6801496.html</a><br>虽然还是有好些地方没搞懂，有必要从头学起。</p><hr><p>4️⃣[Python]<br>关于函数列表中单星号(*)和双星号(**)<br>单星号：</p><ul><li>代表接收任意多个非关键字参数，将其转换成元组：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one</span><span class="params">(a,*b)</span>:</span></span><br><span class="line">    <span class="string">"""a是一个普通传入参数，*b是一个非关键字星号参数"""</span></span><br><span class="line">    print(b)</span><br><span class="line">one(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)  <span class="comment">#输出：(2, 3, 4, 5, 6)</span></span><br></pre></td></tr></table></figure><ul><li>对一个普通变量使用单星号，表示对该变量拆分成单个元素</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(a,b)</span>:</span></span><br><span class="line">    print(a,b)</span><br><span class="line">l=[<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">fun(*l)  <span class="comment">#输出 1,2</span></span><br></pre></td></tr></table></figure><p>双星号：</p><ul><li>获得字典值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two</span><span class="params">(a=<span class="number">1</span>,**b)</span>:</span></span><br><span class="line">    <span class="string">"""a是一个普通关键字参数，**b是一个关键字双星号参数"""</span></span><br><span class="line">    print(b)</span><br><span class="line">two(a=<span class="number">1</span>,b=<span class="number">2</span>,c=<span class="number">3</span>,d=<span class="number">4</span>,e=<span class="number">5</span>,f=<span class="number">6</span>)  <span class="comment">#输出&#123;'b': 2, 'c': 3, 'e': 5, 'f': 6, 'd': 4&#125;</span></span><br></pre></td></tr></table></figure><hr><p>5️⃣[Pytorch]<br>在Pytorch中，只要一个tensor的requires_grad是true，那么两个tensor的加减乘除后的结果的requires_grad也会是true。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Paper </tag>
            
            <tag> 概率校准 </tag>
            
            <tag> Probability Calibration </tag>
            
            <tag> HAN </tag>
            
            <tag> XGBoost </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词5</title>
      <link href="/2018/08/19/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D5/"/>
      <url>/2018/08/19/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D5/</url>
      
        <content type="html"><![CDATA[<p>本周太忙了，没背什么诗词，只背（复习）了部分的《滕王阁序》。</p><p>1️⃣</p><h3 id="滕王阁序"><a href="#滕王阁序" class="headerlink" title="滕王阁序"></a>滕王阁序</h3><p>嗟乎！时运不齐，命途多舛。冯唐易老，李广难封。屈贾谊于长沙，非无圣主；窜梁鸿于海曲，岂乏明时？所赖<strong>君子见机，达人知命</strong>。老当益壮，宁移白首之心？<strong>穷且益坚，不坠青云之志</strong>。酌贪泉而觉爽，处涸辙以犹欢。<strong>北海虽赊，扶摇可接；东隅已逝，桑榆非晚。</strong>孟尝高洁，空馀报国之情；阮籍猖狂，岂效穷途之哭！</p><p>勃，三尺微命，一介书生。无路请缨，等终军之弱冠；有怀投笔，慕宗慤之长风。舍簪笏于百龄，奉晨昏于万里。非谢家之宝树，接孟氏之芳邻。他日趋庭，叨陪鲤对；今兹捧袂，喜托龙门。杨意不逢，抚凌云而自惜；锺期既遇，奏流水以何惭？</p><hr><p><strong>注释：</strong><br>冯唐：西汉人，有才能却一直不受重用。汉武帝时选求贤良，有人举荐冯唐，可是他已九十多岁，难再做官了。李广：汉武帝时的名将，多年抗击匈奴，军功很大，却终身没有封侯。</p><p>贾谊：汉文帝本想任贾谊为公卿，但因朝中权贵反对，就疏远了贾谊，任他为长沙王太傅。梁鸿：东汉人，因作诗讽刺君王，得罪了汉章帝，被迫逃到齐鲁一带躲避。</p><p>酌（zhuó）贪泉而觉爽：喝下贪泉的水，仍觉得心境清爽。古代传说广州有水名贪泉，人喝了这里的水就会变得贪婪。这句是说有德行的人在污浊的环境中也能保持纯正，不被污染。处涸辙以犹欢：处在奄奄待毙的时候，仍然乐观开朗。处河辙：原指鲋鱼处在干涸的车辙旦。比喻人陷入危急之中。</p><p>孟尝：东汉人，为官清正贤能，但不被重用，后来归田。阮籍：三国魏诗人，他有时独自驾车出行，到无路处便恸哭而返，借此宣泄不满于现实的苦闷心情。</p><p>终军：《汉书·终军传》记载，汉武帝想让南越（今广东、广西一带）王归顺，派终军前往劝说，终军请求给他长缨，必缚住南越王，带回到皇宫门前（意思是一定完成使命）。后来用“请缨”指投军报国。</p><p>宗悫（què）：南朝宋人，少年时很有抱负，说“愿乘长风破万里浪”。</p><p>簪（zān）笏（hù）：这里代指官职。晨昏：晨昏定省，出自 《礼记·曲礼上》，释义为旧时侍奉父母的日常礼节。</p><p>非谢家之宝树，接孟氏之芳邻：自己并不是像谢玄那样出色的人才，却能在今日的宴会上结识各位名士。谢家之宝树：指谢玄。《晋书·谢玄传》记载，晋朝谢安曾问子侄们：为什么人们总希望自己的子弟好？侄子谢玄回答：“譬如芝兰玉树，欲使其生于庭阶耳。”后来就称谢玄为谢家宝树。孟氏之芳邻：这里借孟子的母亲为寻找邻居而三次搬家的故事，来指赴宴的嘉宾。</p><p>他日趋庭，叨陪鲤对：过些时候自己将到父亲那里陪侍和聆听教诲。趋庭：快步走过庭院，这是表示对长辈的恭敬。叨：惭愧地承受，表示自谦。鲤对：孔鲤是孔子的儿子，鲤对指接受父亲教诲。事见《论语·季氏》：（孔子）尝独立，（孔）鲤趋而过庭。（子）曰：“学诗乎？”对曰：“未也。”“不学诗，无以言。”鲤退而学诗。他日，又独立，鲤趋而过庭。（子）曰：“学礼乎？”对曰：‘未也。”“不学礼，无以立。”鲤退而学礼。</p><p>捧袂（mèi）：举起双袖作揖，指谒见阎公。喜托龙门：（受到阎公的接待）十分高兴，好像登上龙门一样。</p><p>杨意：即蜀人杨得意，任掌管天子猎犬的官，西汉辞赋家司马相如是由他推荐给汉武帝的。凌云：这里指司马相如的赋，《史记·司马相如传》说，相如献《大人赋》，“天子大悦，飘飘有凌云之气，似游天地之间”。钟期：即钟子期。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python中的拷贝</title>
      <link href="/2018/08/18/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Python%E4%B8%AD%E7%9A%84%E6%8B%B7%E8%B4%9D/"/>
      <url>/2018/08/18/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Python%E4%B8%AD%E7%9A%84%E6%8B%B7%E8%B4%9D/</url>
      
        <content type="html"><![CDATA[<p>Python的拷贝和C/C++的差别很大，很经常就容易搞混，因此记录一下。</p><h3 id="赋值、拷贝"><a href="#赋值、拷贝" class="headerlink" title="赋值、拷贝"></a>赋值、拷贝</h3><ul><li>赋值：实际上就是对象的引用，没有开辟新的内存空间<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lst=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">l=lst</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>浅拷贝:创建了新对象，但是<strong>内容是对原对象的引用</strong>，有三种形式</p><ol><li><p>切片  </p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">l=lst[:]</span><br><span class="line">l=[i <span class="keyword">for</span> i <span class="keyword">in</span> lst]</span><br></pre></td></tr></table></figure></li><li><p>工厂</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l=list(lst)</span><br></pre></td></tr></table></figure></li><li><p>copy </p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">l=copy.copy(lst)</span><br></pre></td></tr></table></figure></li></ol></li><li><p>深拷贝:copy中的deepcopy，生成一个全新的对象，与原来的对象无关</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">l=copy.deepcopy(lst)</span><br></pre></td></tr></table></figure></li></ul><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 引用https://www.cnblogs.com/huangbiquan/p/7795152.html 的例子###</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> copy</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,[<span class="string">'a'</span>,<span class="string">'b'</span>]] <span class="comment">#定义一个列表a</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a <span class="comment">#赋值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = copy.copy(a) <span class="comment">#浅拷贝</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = copy.deepcopy(a) <span class="comment">#深拷贝</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.append(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(a)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>], <span class="number">5</span>] <span class="comment">#a添加一个元素5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(b) </span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>], <span class="number">5</span>] <span class="comment">#b跟着添加一个元素5 </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(c)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>]] <span class="comment">#c保持不变</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(d)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>]] <span class="comment">#d保持不变</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a[<span class="number">4</span>].append(<span class="string">'c'</span>) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(a)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>], <span class="number">5</span>] <span class="comment">#a中的list(即a[4])添加一个元素c</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(b)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>], <span class="number">5</span>] <span class="comment">#b跟着添加一个元素c</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(c)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]] <span class="comment">#c跟着添加一个元素c</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(d)</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">'a'</span>, <span class="string">'b'</span>]] <span class="comment">#d保持不变</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#说明如下：</span></span><br><span class="line"><span class="comment">#1.外层添加元素时， 浅拷贝c不会随原列表a变化而变化；内层list添加元素时，浅拷贝c才会变化。</span></span><br><span class="line"><span class="comment">#2.无论原列表a如何变化，深拷贝d都保持不变。</span></span><br><span class="line"><span class="comment">#3.赋值对象随着原列表一起变化</span></span><br></pre></td></tr></table></figure><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.cnblogs.com/huangbiquan/p/7795152.html" target="_blank" rel="noopener">https://www.cnblogs.com/huangbiquan/p/7795152.html</a><br><a href="https://www.cnblogs.com/xueli/p/4952063.html" target="_blank" rel="noopener">https://www.cnblogs.com/xueli/p/4952063.html</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> 拷贝 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何将ELMo词向量用于中文</title>
      <link href="/2018/08/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E5%B0%86ELMo%E8%AF%8D%E5%90%91%E9%87%8F%E7%94%A8%E4%BA%8E%E4%B8%AD%E6%96%87/"/>
      <url>/2018/08/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E5%B0%86ELMo%E8%AF%8D%E5%90%91%E9%87%8F%E7%94%A8%E4%BA%8E%E4%B8%AD%E6%96%87/</url>
      
        <content type="html"><![CDATA[<p>10.10更新：ELMo已经由哈工大组用PyTorch重写了，并且提供了中文的预训练好的language model，可以直接使用。</p><p>2019.4.7更新：年代过于久远，本人于细节方面早已记不大清楚了。遇到bug或问题烦请自行查阅解决，请不必在评论区提问或邮件提问，不会再回复。</p><hr><p>ELMo于今年二月由AllenNLP提出，与word2vec或GloVe不同的是其动态词向量的思想，其本质即通过训练language model，对于一句话进入到language model获得不同的词向量。根据实验可得，使用了Elmo词向量之后，许多NLP任务都有了大幅的提高。</p><p>论文:<a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">Deep contextualized word representations</a></p><p>AllenNLP一共release了两份ELMo的代码，一份是Pytorch版本的，另一份是Tensorflow版本的。Pytorch版本的只开放了使用预训练好的词向量的接口，但没有给出自己训练的接口，因此无法使用到中文语料中。Tensorflow版本有提供训练的代码，因此本文记录如何将ELMo用于中文语料中，但本文只记录使用到的部分，而不会分析全部的代码。</p><p>需求:<br>使用预训练好的词向量作为句子表示直接传入到RNN中(也就是不使用代码中默认的先过CNN)，在训练完后，将模型保存，在需要用的时候load进来，对于一个特定的句子，首先将其转换成预训练的词向量，传入language model之后最终得到ELMo词向量。</p><p>准备工作:</p><ol><li>将中文语料分词</li><li>训练好GloVe词向量或者word2vec</li><li>下载<a href="https://github.com/allenai/bilm-tf" target="_blank" rel="noopener">bilm-tf代码</a></li><li>生成词表 vocab_file （训练的时候要用到）</li><li>optional:阅读Readme</li><li>optional:通读bilm-tf的代码，对代码结构有一定的认识</li></ol><p>思路:</p><ol><li>将预训练的词向量读入</li><li>修改bilm-tf代码<ol><li>option部分</li><li>添加给embedding weight赋初值</li><li>添加保存embedding weight的代码</li></ol></li><li>开始训练，获得checkpoint和option文件</li><li>运行脚本，获得language model的weight文件</li><li>将embedding weight保存为hdf5文件形式</li><li>运行脚本，将语料转化成ELMo embedding。</li></ol><h3 id="训练GloVe或word2vec"><a href="#训练GloVe或word2vec" class="headerlink" title="训练GloVe或word2vec"></a>训练GloVe或word2vec</h3><p>可参见我以前的博客或者网上的教程。<br>注意到，如果要用gensim导入GloVe训好的词向量，需要在开头添加num_word embedding_dim。 如：<br><img src="/images/2018-08-10-15338861462682.jpg" width="70%" height="50%"></p><h3 id="获得vocab词表文件"><a href="#获得vocab词表文件" class="headerlink" title="获得vocab词表文件"></a>获得vocab词表文件</h3><p>注意到，词表文件的开头必须要有<code>&lt;S&gt; &lt;/S&gt; &lt;UNK&gt;</code>，且大小写敏感。并且应当按照单词的词频降序排列。可以通过手动添加这三个特殊符号。<br>如：<br><img src="/images/2018-08-11-15339757184030.jpg" width="10%" height="50%"></p><p>代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model=gensim.models.KeyedVectors.load_word2vec_format(</span><br><span class="line">    fname=<span class="string">'/home/zhlin/GloVe/vectors.txt'</span>,binary=<span class="keyword">False</span></span><br><span class="line">)</span><br><span class="line">words=model.vocab</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'vocab.txt'</span>,<span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'&lt;S&gt;'</span>)</span><br><span class="line">    f.write(<span class="string">'\n'</span>）</span><br><span class="line">    f.write(<span class="string">'&lt;/S&gt;'</span>)</span><br><span class="line">    f.write(<span class="string">'\n'</span>)</span><br><span class="line">    f.write(<span class="string">'&lt;UNK&gt;'</span>)</span><br><span class="line">    f.write(<span class="string">'\n'</span>)    <span class="comment"># bilm-tf 要求vocab有这三个符号，并且在最前面</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        f.write(word)</span><br><span class="line">        f.write(<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure></p><h3 id="修改bilm-tf代码"><a href="#修改bilm-tf代码" class="headerlink" title="修改bilm-tf代码"></a>修改bilm-tf代码</h3><p>注意到，在使用该代码之前，需要安装好相应的环境。</p><p><img src="/images/2018-08-10-15338879402377.jpg" width="50%" height="50%"></p><p>如果使用的是conda作为默认的Python解释器，强烈建议使用conda安装，否则可能会出现一些莫名的错误。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install tensorflow-gpu=1.4</span><br><span class="line">conda install h5py</span><br><span class="line">python setup.py install <span class="comment">#应在bilm-tf的文件夹下执行该指令</span></span><br></pre></td></tr></table></figure></p><p>然后再运行测试代码，通过说明安装成功。</p><h4 id="修改train-elmo-py"><a href="#修改train-elmo-py" class="headerlink" title="修改train_elmo.py"></a>修改train_elmo.py</h4><p>bin文件夹下的train_elmo.py是程序的入口。<br>主要修改的地方：</p><ol><li>load_vocab的第二个参数应该改为None</li><li>n_gpus CUDA_VISIBLE_DEVICES 根据自己需求改</li><li>n_train_tokens 可改可不改，影响的是输出信息。要查看自己语料的行数，可以通过<code>wc -l corpus.txt</code> 查看。</li><li><strong>option的修改</strong>，将char_cnn部分都注释掉，其他根据自己需求修改</li></ol><p>如：<br><img src="/images/2018-08-10-15338888745894.jpg" width="70%" height="50%"></p><h4 id="修改LanguageModel类"><a href="#修改LanguageModel类" class="headerlink" title="修改LanguageModel类"></a>修改LanguageModel类</h4><p>由于我需要传入预训练好的GloVe embedding，那么还需要修改embedding部分，这部分在bilm文件夹下的training.py，进入到LanguageModel类中_build_word_embeddings函数中。注意到，由于前三个是<code>&lt;S&gt; &lt;/S&gt; &lt;UNK&gt;</code>，而这三个字符在GloVe里面是没有的，因此这三个字符的embedding应当在训练的时候逐渐学习到，而正因此 <code>embedding_weights</code>的<code>trainable</code>应当设为<code>True</code></p><p>如:</p><p><img src="/images/2018-08-12-15340585073779.jpg" alt=""></p><h4 id="修改train函数"><a href="#修改train函数" class="headerlink" title="修改train函数"></a>修改train函数</h4><p>添加代码，使得在train函数的最后保存embedding文件。<br><img src="/images/2018-08-12-15340607132103.jpg" alt=""></p><h3 id="训练并获得weights文件"><a href="#训练并获得weights文件" class="headerlink" title="训练并获得weights文件"></a>训练并获得weights文件</h3><p>训练需要语料文件corpus.txt，词表文件vocab.txt。</p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>cd到bilm-tf文件夹下，运行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=4</span><br><span class="line">nohup python -u bin/train_elmo.py \</span><br><span class="line">--train_prefix=<span class="string">'/home/zhlin/bilm-tf/corpus.txt'</span> \</span><br><span class="line">--vocab_file /home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab.txt \</span><br><span class="line">--save_dir /home/zhlin/bilm-tf/try &gt;bilm_out.txt 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p><p>根据实际情况设定不同的值和路径。</p><p>运行情况：<br><img src="/images/2018-08-10-15339015862848.jpg" width="50%" height="50%"></p><p>PS:运行过程中可能会有warning:</p><blockquote><p>‘list’ object has no attribute ‘name’<br>WARNING:tensorflow:Error encountered when serializing lstm_output_embeddings.<br>Type is unsupported, or the types of the items don’t match field type in CollectionDef.</p></blockquote><p>应该不用担心，还是能够继续运行的，后面也不受影响。</p><p>在等待了相当长的时间后，在save_dir文件夹内生成了几个文件，其中checkpoint和options是关键，checkpoint能够进一步生成language model的weights文件，而options记录language model的参数。</p><p><img src="/images/2018-08-11-15339734319058.jpg" alt=""></p><h4 id="获得language-model的weights"><a href="#获得language-model的weights" class="headerlink" title="获得language model的weights"></a>获得language model的weights</h4><p>接下来运行bin/dump_weights.py将checkpoint转换成hdf5文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nohup python -u  /home/zhlin/bilm-tf/bin/dump_weights.py  \</span><br><span class="line">--save_dir /home/zhlin/bilm-tf/try  \</span><br><span class="line">--outfile /home/zhlin/bilm-tf/try/weights.hdf5 &gt;outfile.txt 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>其中save_dir是checkpoint和option文件保存的地址。</p><p>接下来等待程序运行：</p><p><img src="/images/2018-08-11-15339740970081.jpg" width="70%" height="50%"></p><p><img src="/images/2018-08-11-15339745511775.jpg" width="70%" height="50%"></p><p>最终获得了想要的weights和option：<br><img src="/images/2018-08-11-15339978499136.jpg" alt=""></p><h3 id="将语料转化成ELMo-embedding"><a href="#将语料转化成ELMo-embedding" class="headerlink" title="将语料转化成ELMo embedding"></a>将语料转化成ELMo embedding</h3><p>由于我们有了vocab_file、与vocab_file一一对应的embedding h5py文件、以及language model的weights.hdf5和options.json。<br>接下来参考usage_token.py将一句话转化成ELMo embedding。</p><p>参考代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> bilm <span class="keyword">import</span> TokenBatcher, BidirectionalLanguageModel, weight_layers, \</span><br><span class="line">    dump_token_embeddings</span><br><span class="line"></span><br><span class="line"><span class="comment"># Our small dataset.</span></span><br><span class="line">raw_context = [</span><br><span class="line">    <span class="string">'这 是 测试 .'</span>,</span><br><span class="line">    <span class="string">'好的 .'</span></span><br><span class="line">]</span><br><span class="line">tokenized_context = [sentence.split() <span class="keyword">for</span> sentence <span class="keyword">in</span> raw_context]</span><br><span class="line">tokenized_question = [</span><br><span class="line">    [<span class="string">'这'</span>, <span class="string">'是'</span>, <span class="string">'什么'</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">vocab_file=<span class="string">'/home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab.txt'</span></span><br><span class="line">options_file=<span class="string">'/home/zhlin/bilm-tf/try/options.json'</span></span><br><span class="line">weight_file=<span class="string">'/home/zhlin/bilm-tf/try/weights.hdf5'</span></span><br><span class="line">token_embedding_file=<span class="string">'/home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab_embedding.hdf5'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Now we can do inference.</span></span><br><span class="line"><span class="comment"># Create a TokenBatcher to map text to token ids.</span></span><br><span class="line">batcher = TokenBatcher(vocab_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Input placeholders to the biLM.</span></span><br><span class="line">context_token_ids = tf.placeholder(<span class="string">'int32'</span>, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>))</span><br><span class="line">question_token_ids = tf.placeholder(<span class="string">'int32'</span>, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the biLM graph.</span></span><br><span class="line">bilm = BidirectionalLanguageModel(</span><br><span class="line">    options_file,</span><br><span class="line">    weight_file,</span><br><span class="line">    use_character_inputs=<span class="keyword">False</span>,</span><br><span class="line">    embedding_weight_file=token_embedding_file</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get ops to compute the LM embeddings.</span></span><br><span class="line">context_embeddings_op = bilm(context_token_ids)</span><br><span class="line">question_embeddings_op = bilm(question_token_ids)</span><br><span class="line"></span><br><span class="line">elmo_context_input = weight_layers(<span class="string">'input'</span>, context_embeddings_op, l2_coef=<span class="number">0.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">''</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">    <span class="comment"># the reuse=True scope reuses weights from the context for the question</span></span><br><span class="line">    elmo_question_input = weight_layers(</span><br><span class="line">        <span class="string">'input'</span>, question_embeddings_op, l2_coef=<span class="number">0.0</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elmo_context_output = weight_layers(</span><br><span class="line">    <span class="string">'output'</span>, context_embeddings_op, l2_coef=<span class="number">0.0</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">''</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">    <span class="comment"># the reuse=True scope reuses weights from the context for the question</span></span><br><span class="line">    elmo_question_output = weight_layers(</span><br><span class="line">        <span class="string">'output'</span>, question_embeddings_op, l2_coef=<span class="number">0.0</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># It is necessary to initialize variables once before running inference.</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create batches of data.</span></span><br><span class="line">    context_ids = batcher.batch_sentences(tokenized_context)</span><br><span class="line">    question_ids = batcher.batch_sentences(tokenized_question)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute ELMo representations (here for the input only, for simplicity).</span></span><br><span class="line">    elmo_context_input_, elmo_question_input_ = sess.run(</span><br><span class="line">        [elmo_context_input[<span class="string">'weighted_op'</span>], elmo_question_input[<span class="string">'weighted_op'</span>]],</span><br><span class="line">        feed_dict=&#123;context_token_ids: context_ids,</span><br><span class="line">                   question_token_ids: question_ids&#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">print(elmo_context_input_,elmo_context_input_)</span><br></pre></td></tr></table></figure></p><p>可以修改代码以适应自己的需求。</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://github.com/allenai/bilm-tf" target="_blank" rel="noopener">https://github.com/allenai/bilm-tf</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 教程 </tag>
            
            <tag> ELMo </tag>
            
            <tag> 词向量 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录4</title>
      <link href="/2018/08/12/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB4/"/>
      <url>/2018/08/12/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB4/</url>
      
        <content type="html"><![CDATA[<p>本周没有什么代码要记录的。</p><h3 id="1️⃣sklearn之Pipeline例子"><a href="#1️⃣sklearn之Pipeline例子" class="headerlink" title="1️⃣sklearn之Pipeline例子"></a>1️⃣sklearn之Pipeline例子</h3><p>用机器学习解决问题的流程：<br>(去掉部分数据）—&gt; 获取feature（Tf-idf等） —&gt; （feature selection，chi2、互信息等） —&gt; （缩放/正则化） —&gt; 分类器 —&gt; GridSearch/RandomizedSearch调参</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pipe=Pipeline([     <span class="comment">#建立pipeline</span></span><br><span class="line">    (<span class="string">'vect'</span>,TfidfVectorizer()),</span><br><span class="line">    (<span class="string">'select'</span>,SelectKBest(chi2),</span><br><span class="line">    (<span class="string">'norm'</span>,MaxAbsScaler()),   </span><br><span class="line">    (<span class="string">'svm'</span>,svm.LinearSVC())</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">parameters=&#123;</span><br><span class="line">    <span class="string">'vect__ngram_range'</span>:[(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">3</span>),(<span class="number">2</span>,<span class="number">3</span>)],</span><br><span class="line">    <span class="string">'vect__max_df'</span>:[<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>],</span><br><span class="line">    <span class="string">'vect__min_df'</span>:[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>],</span><br><span class="line">    <span class="string">'vect__norm'</span>:[<span class="string">'l1'</span>,<span class="string">'l2'</span>],</span><br><span class="line">    <span class="string">'svm__penalty'</span>:[<span class="string">'l1'</span>,<span class="string">'l2'</span>],</span><br><span class="line">    <span class="string">'svm__loss'</span>:[<span class="string">'squared_hinge'</span>],  </span><br><span class="line">    <span class="string">'svm__dual'</span>:[<span class="keyword">False</span>,<span class="keyword">True</span>],</span><br><span class="line">    <span class="string">'svm__tol'</span>:[<span class="number">1e-5</span>,<span class="number">1e-4</span>],</span><br><span class="line">    <span class="string">'svm__C'</span>:[<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>,<span class="number">1.0</span>,<span class="number">1.1</span>],</span><br><span class="line">    <span class="string">'svm__class_weight'</span>:[<span class="keyword">None</span>,<span class="string">'balanced'</span>],</span><br><span class="line">    <span class="string">'svm__max_iter'</span>:[<span class="number">1000</span>,<span class="number">5000</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">grid_search_model=GridSearchCV(pipe,parameters,error_score=<span class="number">0</span>,n_jobs=<span class="number">5</span>)</span><br><span class="line">grid_search_model.fit(train[column],train[<span class="string">'class'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> para_name <span class="keyword">in</span> sorted(parameters.keys()):</span><br><span class="line">    print(para_name,grid_search_model.best_params_[para_name])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"cv_result:"</span>)</span><br><span class="line">print(grid_search_model.cv_results_)</span><br></pre></td></tr></table></figure><hr>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识3</title>
      <link href="/2018/08/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%863/"/>
      <url>/2018/08/12/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%863/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[Python]<br>在服务器上跑代码时，如 <code>python project/folder1/a.py</code>，如果a.py引用了一个自定义的模块但又不在folder1内，此时interpreter就会报错，提示找不到该模块。这是因为解释器默认只会在同一个folder下查找。解决方案是在运行前显式添加查找范围。如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PYTHONPATH=/home/zhlin/bilm-tf:<span class="variable">$PYTHONPATH</span></span><br></pre></td></tr></table></figure></p><p>那么python解释器就会到该目录下去找。</p><hr><p>2️⃣[度量标准]<br><img src="/images/2018-08-12-15340420442670.jpg" alt=""></p><ul><li>准确率(accuracy):  $ACC=\frac{TP+TN}{TP+TN+FP+FN}$<br> 衡量的是分类器预测准确的比例</li><li>召回率(recall): $Recall=\frac{TP}{TP+FN}$<br>  正例中被分对的比例，衡量了分类器对正例的识别能力。</li><li>精确率(Precision): $P=\frac{TP}{TP+FP}$<br>度量了被分为正例的示例中实际为正例的比例。</li><li>F-Measure: $F=\frac{(\alpha^2 +1)P*R}{\alpha^2 (P+R)}$<br>  其中P是Precision,R是Recall。综合考量了两种度量。<br>  当$\alpha=1$时，称为F1值 $F1=\frac{2PR}{P+R}$</li></ul><hr><p>3️⃣[调参技巧]<br>在google发布的一份关于text-classification的<a href="https://developers.google.com/machine-learning/guides/text-classification/" target="_blank" rel="noopener">guide</a>中，提到了几个调参的trick。</p><ol><li>在feature selection步骤中，卡方检验chi2和方差分析的F值 f_classif的表现相当，在大约选择20k的feature时，准确率达到顶峰，当feature越多，效果并没有提升甚至会下降。<br><img src="/images/2018-08-12-15340434326365.jpg" width="90%" height="50%"></li><li>在文本分类中，似乎使用normalization并没有多少用处，建议跳过。<blockquote><p>Normalization converts all feature/sample values to small and similar values. This simplifies gradient descent convergence in learning algorithms. From what we have seen, normalization during data preprocessing does not seem to add much value in text classification problems; we recommend skipping this step.</p></blockquote></li></ol><p>实际上我也测试过，发现确实normalization对于准确率的提高没什么帮助，甚至还有一点下降。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> 度量标准 </tag>
            
            <tag> 调参技巧 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词4</title>
      <link href="/2018/08/12/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D4/"/>
      <url>/2018/08/12/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D4/</url>
      
        <content type="html"><![CDATA[<p>1️⃣</p><h3 id="灞上秋居"><a href="#灞上秋居" class="headerlink" title="灞上秋居"></a>灞上秋居</h3><p>[唐] 马戴<br>灞原风雨定，晚见雁行频。<br>落叶他乡树，寒灯独夜人。<br>空园白露滴，孤壁野僧邻。<br><strong>寄卧郊扉久，何年致此身。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b8a4080a2b58005c9108d9" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8a4080a2b58005c9108d9</a></p><hr><p>2️⃣</p><h3 id="唐多令"><a href="#唐多令" class="headerlink" title="唐多令"></a>唐多令</h3><p>[宋] 刘过<br>芦叶满汀洲，寒沙带浅流。二十年重过南楼。柳下系船犹未稳，能几日，又中秋。<br>黄鹤断矶头，故人今在否？旧江山浑是新愁。<strong>欲买桂花同载酒，终不似、少年游。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b922e7c4c9710055904842" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b922e7c4c9710055904842</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Vim常用快捷键</title>
      <link href="/2018/08/10/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Vim%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
      <url>/2018/08/10/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Vim%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/</url>
      
        <content type="html"><![CDATA[<p>在服务器经常要用到Vim，因此记录常用的快捷键并熟悉之。</p><h3 id="退出"><a href="#退出" class="headerlink" title="退出"></a>退出</h3><p>:q 退出<br>:wq 写入并退出<br>:q! 退出并忽略所有更改<br>:e! 放弃修改并打开原来的文件</p><h3 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h3><p>i 在当前位置前插入<br>a 在当前位置后插入</p><h3 id="撤销"><a href="#撤销" class="headerlink" title="撤销"></a>撤销</h3><p>:u 撤销<br>:U 撤销整行操作<br>Ctrl+r 重做</p><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p>:md 删除第m行<br>nd 删除当前行开始的n行(一共n+1行)<br>dd 删除当前行<br>D 删除当前字符至行尾<br>:m,nd 删除从m到n行的内容，如: <code>:100,10000d</code><br>:m,$d 删除m行及以后所有的行<br>:10d</p><h3 id="移动"><a href="#移动" class="headerlink" title="移动"></a>移动</h3><p>:n 跳转到行号  如， :100<br>gg 跳到行首<br>G(shift+g)移动到文件尾</p><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><p>/text 搜索text，n搜索下一个，N搜索上一个<br>?text 反向查找<br>:set ignorecase 忽略大小写查找<br>:set noignorecase 不忽略大小写查找<br>*或# 对光标处的单词搜索</p><h3 id="复制粘贴"><a href="#复制粘贴" class="headerlink" title="复制粘贴"></a>复制粘贴</h3><p>v 从当前位置开始，光标经过的地方被选中，再按一下v结束</p><h3 id="环境设置"><a href="#环境设置" class="headerlink" title="环境设置"></a>环境设置</h3><p>:set nu 显示行号<br>:set nonu 隐藏行号<br>:set hlsearch 设置搜索结果高亮</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.cnblogs.com/wangrx/p/5907013.html" target="_blank" rel="noopener">https://www.cnblogs.com/wangrx/p/5907013.html</a><br><a href="https://www.cnblogs.com/yangjig/p/6014198.html" target="_blank" rel="noopener">https://www.cnblogs.com/yangjig/p/6014198.html</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 技巧 </tag>
            
            <tag> 杂七杂八 </tag>
            
            <tag> 快捷键 </tag>
            
            <tag> Vim </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pycharm常用技巧</title>
      <link href="/2018/08/10/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Pycharm%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
      <url>/2018/08/10/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Pycharm%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<p>记录Pycharm的一些技巧，让Pycharm更顺手</p><h3 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h3><p>0️⃣Double Shift 万能搜索<br>可以搜索<strong>文件名、类名、方法名、目录名</strong>（在关键字前面加/ ），并不能用来搜索任意关键字</p><p>1️⃣ Command+F 在页面搜索</p><p>2️⃣ Ctrl+Shift+F Find in Path 在路径下搜索</p><p>3️⃣✨Command+E 快速查找文件<br>显示最近打开的文件</p><p>4️⃣ Shift+Enter 任意位置换行<br>无论光标在何处都可以直接另起一行</p><p>5️⃣ Option+Enter 自动导入模块；万能提示键<br>自动导入如何设置见小技巧#0️⃣</p><p>6️⃣ Ctrl+F10 运行<br>我已经添加了Ctrl+R作为另一对运行快捷键</p><p>7️⃣ Command+Shift+ +/-  展开/收缩代码 </p><p>8️⃣ Option+F 在Dash中搜索</p><p>9️⃣ Ctrl+J 不跳转查看代码</p><h3 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h3><p>0️⃣ Pycharm自动导入模块<br><a href="https://blog.csdn.net/lantian_123/article/details/78094148" target="_blank" rel="noopener">https://blog.csdn.net/lantian_123/article/details/78094148</a></p><p>1️⃣ ✨远程部署工程 强烈推荐<br>两步走：配置服务器映射+配置服务器解释器</p><p>2️⃣跳转后如何回退<br>开启toolbar即可<br><a href="https://segmentfault.com/a/1190000010205945" target="_blank" rel="noopener">https://segmentfault.com/a/1190000010205945</a></p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://foofish.net/pycharm-tips.html" target="_blank" rel="noopener">https://foofish.net/pycharm-tips.html</a><br><a href="https://blog.csdn.net/lantian_123/article/details/78094148" target="_blank" rel="noopener">https://blog.csdn.net/lantian_123/article/details/78094148</a><br><a href="https://segmentfault.com/a/1190000010205945" target="_blank" rel="noopener">https://segmentfault.com/a/1190000010205945</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 技巧 </tag>
            
            <tag> Pycharm </tag>
            
            <tag> 杂七杂八 </tag>
            
            <tag> 快捷键 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>无题</title>
      <link href="/2018/08/06/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%97%A0%E9%A2%98/"/>
      <url>/2018/08/06/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%97%A0%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>人这辈子一共会死三次。</p><p>第一次是你的心脏停止跳动，那么从生物的角度来说，你死了；</p><p>第二次是在葬礼上，认识你的人都来祭奠，那么你在社会关系上的事实存在就死了；</p><p>第三次是在最后一个记得你的人死后，那你就真的死了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>人可以卑微如尘土,不可扭曲如蛆虫</title>
      <link href="/2018/08/05/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BA%BA%E5%8F%AF%E4%BB%A5%E5%8D%91%E5%BE%AE%E5%A6%82%E5%B0%98%E5%9C%9F,%E4%B8%8D%E5%8F%AF%E6%89%AD%E6%9B%B2%E5%A6%82%E8%9B%86%E8%99%AB/"/>
      <url>/2018/08/05/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E4%BA%BA%E5%8F%AF%E4%BB%A5%E5%8D%91%E5%BE%AE%E5%A6%82%E5%B0%98%E5%9C%9F,%E4%B8%8D%E5%8F%AF%E6%89%AD%E6%9B%B2%E5%A6%82%E8%9B%86%E8%99%AB/</url>
      
        <content type="html"><![CDATA[<p>如果天总也不亮，那就摸黑过生活; </p><p>如果发出声音是危险的，那就保持沉默; </p><p>如果自觉无力发光，那就别去照亮别人。 </p><p>但是——不要习惯了黑暗就为黑暗辩护; </p><p>不要为自己的苟且而得意洋洋; </p><p>不要嘲讽那些比自己更勇敢、更有热量的人们。 </p><p><strong>可以卑微如尘土，不可扭曲如蛆虫</strong>。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 佳句分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python惯例[转]</title>
      <link href="/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Python%E6%83%AF%E4%BE%8B/"/>
      <url>/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/Python%E6%83%AF%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<p>fork from <a href="https://github.com/jackfrued/Python-100-Days/blob/master/Python惯例.md" target="_blank" rel="noopener">https://github.com/jackfrued/Python-100-Days/blob/master/Python惯例.md</a></p><h2 id="Python惯例"><a href="#Python惯例" class="headerlink" title="Python惯例"></a>Python惯例</h2><p>“惯例”这个词指的是“习惯的做法，常规的办法，一贯的做法”，与这个词对应的英文单词叫“idiom”。由于Python跟其他很多编程语言在语法和使用上还是有比较显著的差别，因此作为一个Python开发者如果不能掌握这些惯例，就无法写出“Pythonic”的代码。下面我们总结了一些在Python开发中的惯用的代码。</p><ol><li><p>让代码既可以被导入又可以被执行。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br></pre></td></tr></table></figure></li><li><p>用下面的方式判断逻辑“真”或“假”。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> x:</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> x:</span><br></pre></td></tr></table></figure><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">'jackfrued'</span></span><br><span class="line">fruits = [<span class="string">'apple'</span>, <span class="string">'orange'</span>, <span class="string">'grape'</span>]</span><br><span class="line">owners = &#123;<span class="string">'1001'</span>: <span class="string">'骆昊'</span>, <span class="string">'1002'</span>: <span class="string">'王大锤'</span>&#125;</span><br><span class="line"><span class="keyword">if</span> name <span class="keyword">and</span> fruits <span class="keyword">and</span> owners:</span><br><span class="line">    print(<span class="string">'I love fruits!'</span>)</span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">'jackfrued'</span></span><br><span class="line">fruits = [<span class="string">'apple'</span>, <span class="string">'orange'</span>, <span class="string">'grape'</span>]</span><br><span class="line">owners = &#123;<span class="string">'1001'</span>: <span class="string">'骆昊'</span>, <span class="string">'1002'</span>: <span class="string">'王大锤'</span>&#125;</span><br><span class="line"><span class="keyword">if</span> name != <span class="string">''</span> <span class="keyword">and</span> len(fruits) &gt; <span class="number">0</span> <span class="keyword">and</span> owners != &#123;&#125;:</span><br><span class="line">    print(<span class="string">'I love fruits!'</span>)</span><br></pre></td></tr></table></figure></li><li><p>善于使用in运算符。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> x <span class="keyword">in</span> items: <span class="comment"># 包含</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> items: <span class="comment"># 迭代</span></span><br></pre></td></tr></table></figure><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">'Hao LUO'</span></span><br><span class="line"><span class="keyword">if</span> <span class="string">'L'</span> <span class="keyword">in</span> name:</span><br><span class="line">    print(<span class="string">'The name has an L in it.'</span>)</span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">'Hao LUO'</span></span><br><span class="line"><span class="keyword">if</span> name.find(<span class="string">'L'</span>) != <span class="number">-1</span>:</span><br><span class="line">    print(<span class="string">'This name has an L in it!'</span>)</span><br></pre></td></tr></table></figure></li><li><p>不使用临时变量交换两个值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a, b = b, a</span><br></pre></td></tr></table></figure></li><li><p><strong>用序列构建字符串</strong>。</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chars = [<span class="string">'j'</span>, <span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'k'</span>, <span class="string">'f'</span>, <span class="string">'r'</span>, <span class="string">'u'</span>, <span class="string">'e'</span>, <span class="string">'d'</span>]</span><br><span class="line">name = <span class="string">''</span>.join(chars)</span><br><span class="line">print(name)  <span class="comment"># jackfrued</span></span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">chars = [<span class="string">'j'</span>, <span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'k'</span>, <span class="string">'f'</span>, <span class="string">'r'</span>, <span class="string">'u'</span>, <span class="string">'e'</span>, <span class="string">'d'</span>]</span><br><span class="line">name = <span class="string">''</span></span><br><span class="line"><span class="keyword">for</span> char <span class="keyword">in</span> chars:</span><br><span class="line">    name += char</span><br><span class="line">print(name)  <span class="comment"># jackfrued</span></span><br></pre></td></tr></table></figure></li><li><p><strong>EAFP优于LBYL</strong>。</p><p>EAFP - <strong>E</strong>asier to <strong>A</strong>sk <strong>F</strong>orgiveness than <strong>P</strong>ermission.</p><p>LBYL - <strong>L</strong>ook <strong>B</strong>efore <strong>Y</strong>ou <strong>L</strong>eap.</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'x'</span>: <span class="string">'5'</span>&#125;</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    value = int(d[<span class="string">'x'</span>])</span><br><span class="line">    print(value)</span><br><span class="line"><span class="keyword">except</span> (KeyError, TypeError, ValueError):</span><br><span class="line">    value = <span class="keyword">None</span></span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'x'</span>: <span class="string">'5'</span>&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="string">'x'</span> <span class="keyword">in</span> d <span class="keyword">and</span> isinstance(d[<span class="string">'x'</span>], str) \</span><br><span class="line"><span class="keyword">and</span> d[<span class="string">'x'</span>].isdigit():</span><br><span class="line">    value = int(d[<span class="string">'x'</span>])</span><br><span class="line">    print(value)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    value = <span class="keyword">None</span></span><br></pre></td></tr></table></figure></li><li><p>使用enumerate进行迭代。</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fruits = [<span class="string">'orange'</span>, <span class="string">'grape'</span>, <span class="string">'pitaya'</span>, <span class="string">'blueberry'</span>]</span><br><span class="line"><span class="keyword">for</span> index, fruit <span class="keyword">in</span> enumerate(fruits):</span><br><span class="line">print(index, <span class="string">':'</span>, fruit)</span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fruits = [<span class="string">'orange'</span>, <span class="string">'grape'</span>, <span class="string">'pitaya'</span>, <span class="string">'blueberry'</span>]</span><br><span class="line">index = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> fruit <span class="keyword">in</span> fruits:</span><br><span class="line">    print(index, <span class="string">':'</span>, fruit)</span><br><span class="line">    index += <span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p>用生成式生成列表。</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = [<span class="number">7</span>, <span class="number">20</span>, <span class="number">3</span>, <span class="number">15</span>, <span class="number">11</span>]</span><br><span class="line">result = [num * <span class="number">3</span> <span class="keyword">for</span> num <span class="keyword">in</span> data <span class="keyword">if</span> num &gt; <span class="number">10</span>]</span><br><span class="line">print(result)  <span class="comment"># [60, 45, 33]</span></span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = [<span class="number">7</span>, <span class="number">20</span>, <span class="number">3</span>, <span class="number">15</span>, <span class="number">11</span>]</span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">    <span class="keyword">if</span> i &gt; <span class="number">10</span>:</span><br><span class="line">        result.append(i * <span class="number">3</span>)</span><br><span class="line">print(result)  <span class="comment"># [60, 45, 33]</span></span><br></pre></td></tr></table></figure></li><li><p>用zip组合键和值来创建字典。</p><p><strong>好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keys = [<span class="string">'1001'</span>, <span class="string">'1002'</span>, <span class="string">'1003'</span>]</span><br><span class="line">values = [<span class="string">'骆昊'</span>, <span class="string">'王大锤'</span>, <span class="string">'白元芳'</span>]</span><br><span class="line">d = dict(zip(keys, values))</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure><p><strong>不好</strong>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keys = [<span class="string">'1001'</span>, <span class="string">'1002'</span>, <span class="string">'1003'</span>]</span><br><span class="line">values = [<span class="string">'骆昊'</span>, <span class="string">'王大锤'</span>, <span class="string">'白元芳'</span>]</span><br><span class="line">d = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i, key <span class="keyword">in</span> enumerate(keys):</span><br><span class="line">    d[key] = values[i]</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure></li></ol><blockquote><p><strong>说明</strong>：这篇文章的内容来自于网络，有兴趣的读者可以阅读<a href="http://safehammad.com/downloads/python-idioms-2014-01-16.pdf" target="_blank" rel="noopener">原文</a>。</p></blockquote><p>注：<br>许多原则我认为非常有意义，能够摆脱C/C++的风格，真正写出Pythonic的代码。让我有很大感触的是1、3、8，能够写出非常简洁优雅的代码。同时6我之前从没注意过，习惯了C/C++风格之后总是会在执行之前考虑所有情况，但确实不够优雅，今后可以尝试EAFP风格（<a href="https://stackoverflow.com/questions/11360858/what-is-the-eafp-principle-in-python" target="_blank" rel="noopener">什么是EAFP</a>）。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> 杂七杂八 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何训练GloVe中文词向量</title>
      <link href="/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83GloVe%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F/"/>
      <url>/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83GloVe%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<h3 id="准备语料"><a href="#准备语料" class="headerlink" title="准备语料"></a>准备语料</h3><p>准备好自己的语料，保存为txt，每行一个句子或一段话，注意要分好词。</p><p><img src="/images/2018-08-05-15334388069130.jpg" width="60%" height="60%"></p><h3 id="准备源码"><a href="#准备源码" class="headerlink" title="准备源码"></a>准备源码</h3><p>从GitHub下载代码，<a href="https://github.com/stanfordnlp/GloVe" target="_blank" rel="noopener">https://github.com/stanfordnlp/GloVe</a><br>将语料corpus.txt放入到Glove的主文件夹下。</p><h3 id="修改bash"><a href="#修改bash" class="headerlink" title="修改bash"></a>修改bash</h3><p>打开demo.sh，修改相应的内容</p><ol><li>因为demo默认是下载网上的语料来训练的，因此如果要训练自己的语料，需要注释掉</li></ol><p><img src="/images/2018-08-05-15334390298383.jpg" width="70%" height="50%"></p><ol><li>修改参数设置，将CORPUS设置成语料的名字</li></ol><p><img src="/images/2018-08-05-15334391029224.jpg" width="50%" height="50%"></p><h3 id="执行bash文件"><a href="#执行bash文件" class="headerlink" title="执行bash文件"></a>执行bash文件</h3><p>进入到主文件夹下</p><ol><li>make</li></ol><p><img src="/images/2018-08-05-15334392348665.jpg" width="70%" height="50%"></p><ol><li>bash demo.sh</li></ol><p><img src="/images/2018-08-05-15334392595148.jpg" width="70%" height="70%"></p><p>注意，如果训练数据较大，则训练时间较长，那么建议使用nohup来运行程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup bash demo.sh &gt;output.txt 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>坐等训练，最后会得到vectors.txt 以及其他的相应的文件。如果要用gensim的word2vec load进来，那么需要在vectors.txt的第一行加上vacob_size vector_size，第一个数指明一共有多少个向量，第二个数指明每个向量有多少维。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.cnblogs.com/echo-cheng/p/8561171.html" target="_blank" rel="noopener">https://www.cnblogs.com/echo-cheng/p/8561171.html</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> GloVe </tag>
            
            <tag> 教程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识2</title>
      <link href="/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%862/"/>
      <url>/2018/08/05/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%862/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[Pytorch]<br>避免写出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.zeros(...), requires_grad=<span class="keyword">True</span>).cuda()</span><br></pre></td></tr></table></figure><p>而是应该要：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.zeros(...).cuda(), requires_grad=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>Reference:<br><a href="https://discuss.pytorch.org/t/variable-grad-is-always-none-when-extending-autograd/12187" target="_blank" rel="noopener">https://discuss.pytorch.org/t/variable-grad-is-always-none-when-extending-autograd/12187</a></p><hr><p>2️⃣[Tf-idf]<br>本周因为比赛的原因了解了一下各种文本建模的方法。Tf-idf能够取得不错的成绩，但有一定的缺陷。</p><blockquote><p>TF-IDF用于向量空间模型，进行文档相似度计算是相当有效的。但在文本分类中单纯使用TF-IDF来判断一个特征是否有区分度是不够的。</p><ol><li>它仅仅综合考虑了该词在文档中的重要程度和文档区分度。</li><li>它没有考虑特征词在类间的分布。特征选择所选择的特征应该在某类出现多，而其它类出现少，即考察各类的文档频率的差异。如果一个特征词，在各个类间分布比较均匀，这样的词对分类基本没有贡献；但是如果一个特征词比较集中的分布在某个类中，而在其它类中几乎不出现，这样的词却能够很好代表这个类的特征，而TF-IDF不能区分这两种情况。</li><li>它没有考虑特征词在类内部文档中的分布情况。在类内部的文档中，如果特征词均匀分布在其中，则这个特征词能够很好的代表这个类的特征，如果只在几篇文档中出现，而在此类的其它文档中不出现，显然这样的特征词不能够代表这个类的特征。</li></ol></blockquote><p>Reference:<br><a href="https://blog.csdn.net/mmc2015/article/details/46771791" target="_blank" rel="noopener">https://blog.csdn.net/mmc2015/article/details/46771791</a></p><hr><p>3️⃣[卡方检验CHI]<br>在文本分类中，用于选择最相关的特征。</p><p>Reference:<br><a href="https://blog.csdn.net/blockheadls/article/details/49977361" target="_blank" rel="noopener">https://blog.csdn.net/blockheadls/article/details/49977361</a></p><hr><p>4️⃣[文本分类]<br>各种文本分类方法的简单介绍。</p><p>Reference:<br><a href="https://github.com/wangjiang0624/Note/blob/master/MachineLearning/文本分类.md" target="_blank" rel="noopener">https://github.com/wangjiang0624/Note/blob/master/MachineLearning/文本分类.md</a></p><hr><p>5️⃣[Python]<br>collections的两个有用的类</p><ol><li>named_tuple：快速建立一个类，使得可以使用属性来访问而非索引，提高了代码可读性</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line">Point = namedtuple(<span class="string">'Point'</span>,[<span class="string">'x'</span>,<span class="string">'y'</span>])</span><br><span class="line">p = Point(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">print(p.x)  <span class="comment"># 1</span></span><br><span class="line">print(p.y)  <span class="comment"># 2</span></span><br></pre></td></tr></table></figure><ol><li>Counter：统计字符出现的次数</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">count = Counter([...]).most_commom()  <span class="comment">#会按照出现的次数排序，通常可用于构建词典</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> count:     <span class="comment"># c是一个tuple，c[0]是词，c[1]是频率</span></span><br><span class="line">    <span class="keyword">if</span> c[<span class="number">1</span>]&gt;= threshold:</span><br><span class="line">        vocab.add_word(c[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>Counter用法：<br><a href="https://blog.csdn.net/u014755493/article/details/69812244" target="_blank" rel="noopener">https://blog.csdn.net/u014755493/article/details/69812244</a></p><hr><p>6️⃣[nohup]<br>本周在服务器上跑代码的时候遇到一个问题，使用nohup执行python程序时，发现输出文件没有显示。以为是代码的问题，但经过排查并非是代码的问题。通过查阅资料，发现问题所在：<br>因为python输出有缓冲，导致output不能<strong>马上</strong>看到输出。实际上，在等待了一段时间后，输出文件终于显示出来了。</p><p>解决方案：使用python的参数 -u 使得python不启用缓冲。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python -u test.py &gt; nohup.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>Reference:<br><a href="https://blog.csdn.net/sunlylorn/article/details/19127107" target="_blank" rel="noopener">https://blog.csdn.net/sunlylorn/article/details/19127107</a></p><hr><p>7️⃣[hexo配置]</p><ol><li>mathjax配置: <a href="https://www.jianshu.com/p/7ab21c7f0674" target="_blank" rel="noopener">https://www.jianshu.com/p/7ab21c7f0674</a></li><li>配置域名:<a href="https://www.zhihu.com/question/31377141" target="_blank" rel="noopener">https://www.zhihu.com/question/31377141</a></li><li>配置sitemap:<a href="http://www.yuan-ji.me/Hexo-优化：提交sitemap及解决百度爬虫抓取-GitHub-Pages-问题/" target="_blank" rel="noopener">http://www.yuan-ji.me/Hexo-优化：提交sitemap及解决百度爬虫抓取-GitHub-Pages-问题/</a></li></ol><hr><p>8️⃣[Paper]<br><a href="http://aclweb.org/anthology/D17-1025" target="_blank" rel="noopener">Learning Chinese Word Representations From Glyphs Of Characters</a></p><p>使用图像的卷积来生成词向量:<br><img src="/images/2018-08-05-15334453515509.jpg" width="60%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Paper </tag>
            
            <tag> Tf-idf </tag>
            
            <tag> 文本分类 </tag>
            
            <tag> hexo </tag>
            
            <tag> nohup </tag>
            
            <tag> CHI </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录3</title>
      <link href="/2018/08/05/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB3/"/>
      <url>/2018/08/05/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB3/</url>
      
        <content type="html"><![CDATA[<p>本周只有简单的代码。</p><h3 id="1️⃣使用gensim训练word2vec"><a href="#1️⃣使用gensim训练word2vec" class="headerlink" title="1️⃣使用gensim训练word2vec"></a>1️⃣使用gensim训练word2vec</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#encoding=utf-8</span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> word2vec</span><br><span class="line">sentences=word2vec.Text8Corpus(<span class="string">u'分词后的爽肤水评论.txt'</span>)   <span class="comment">#sentence:[ [ a b ],[c d]... ]</span></span><br><span class="line">model=word2vec.Word2Vec(sentences, size=<span class="number">50</span>)  <span class="comment">#size:dim </span></span><br><span class="line"></span><br><span class="line">y2=model.similarity(<span class="string">u"好"</span>, <span class="string">u"还行"</span>)  <span class="comment">#计算相似度</span></span><br><span class="line">print(y2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> model.most_similar(<span class="string">u"滋润"</span>):</span><br><span class="line">    <span class="keyword">print</span> i[<span class="number">0</span>],i[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment">#保存</span></span><br><span class="line">model.save(<span class="string">'/model/word2vec_model'</span>)</span><br><span class="line"></span><br><span class="line">new_model=gensim.models.Word2Vec.load(<span class="string">'/model/word2vec_model'</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="2️⃣使用Counter建立词表"><a href="#2️⃣使用Counter建立词表" class="headerlink" title="2️⃣使用Counter建立词表"></a>2️⃣使用Counter建立词表</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dict</span><span class="params">(dataset,min_freq=<span class="number">5</span>)</span>:</span></span><br><span class="line">    dictionary=Dictionary() </span><br><span class="line">    count=Counter(flat(dataset)).most_common()  </span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> count:</span><br><span class="line">        <span class="keyword">if</span> c[<span class="number">1</span>]&gt;=min_freq:</span><br><span class="line">            dictionary.add_word(c[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> dictionary</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词3</title>
      <link href="/2018/08/05/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D3/"/>
      <url>/2018/08/05/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D3/</url>
      
        <content type="html"><![CDATA[<p>本周背的都是比较简单的。</p><p>1️⃣</p><h3 id="把酒问月"><a href="#把酒问月" class="headerlink" title="把酒问月"></a>把酒问月</h3><p>[唐] 李白<br>青天有月来几时？我今停杯一问之。<br>人攀明月不可得，月行却与人相随。<br>皎如飞镜临丹阙，绿烟灭尽清辉发。<br>但见宵从海上来，宁知晓向云间没。<br>白兔捣药秋复春，嫦娥孤栖与谁邻？<br><strong>今人不见古时月，今月曾经照古人。<br>古人今人若流水，共看明月皆如此。</strong><br><strong>唯愿当歌对酒时，月光长照金樽里。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b8f6f4165abd0054bf8c13" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8f6f4165abd0054bf8c13</a></p><hr><p>2️⃣</p><h3 id="金缕衣"><a href="#金缕衣" class="headerlink" title="金缕衣"></a>金缕衣</h3><p>[唐] 杜秋娘<br>劝君莫惜金缕衣，劝君惜取少年时。<br><strong>花开堪折直须折，莫待无花空折枝。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b92bdca633bd00665eb99e" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b92bdca633bd00665eb99e</a></p><hr><p>3️⃣</p><h3 id="北青萝"><a href="#北青萝" class="headerlink" title="北青萝"></a>北青萝</h3><p>[唐] 李商隐<br>残阳西入崦，茅屋访孤僧。<br>落叶人何在，寒云路几层。<br>独敲初夜磬，闲倚一枝藤。<br><strong>世界微尘里，吾宁爱与憎。</strong></p><p>崦（yān）：即“崦嵫（zī）”，山名，在甘肃。古时常用来指太阳落山的地方。<br><strong>磬（qìng）</strong>：古代打击乐器，形状像曲尺，用玉、石制成，可悬挂。</p><p><a href="http://m.xichuangzhu.com/work/57b8ee240a2b58005c91c99e" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8ee240a2b58005c91c99e</a></p><hr><p>4️⃣</p><h3 id="夏日绝句"><a href="#夏日绝句" class="headerlink" title="夏日绝句"></a>夏日绝句</h3><p>[宋] 李清照<br>生当作人杰，死亦为鬼雄。<br><strong>至今思项羽，不肯过江东。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b911dac4c97100558fb30e" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b911dac4c97100558fb30e</a></p><hr><p>5️⃣</p><h3 id="雨霖铃"><a href="#雨霖铃" class="headerlink" title="雨霖铃"></a>雨霖铃</h3><p>[宋] 柳永<br>寒蝉凄切，对长亭晚，骤雨初歇。都门帐饮无绪，留恋处，兰舟催发。执手相看泪眼，竟无语凝噎。念去去，千里烟波，暮霭沉沉楚天阔。<br><strong>多情自古伤离别</strong>，更那堪、冷落清秋节。今宵酒醒何处？杨柳岸，晓风残月。此去经年，应是良辰好景虚设。<strong>便纵有千种风情，更与何人说</strong>？</p><p><a href="http://m.xichuangzhu.com/work/57ad742f5bbb500062bc7c9c" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57ad742f5bbb500062bc7c9c</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Pytorch中grad的理解</title>
      <link href="/2018/08/03/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADgrad%E7%9A%84%E7%90%86%E8%A7%A3/"/>
      <url>/2018/08/03/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADgrad%E7%9A%84%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>事情起源于我写了一个CNN用于文本分类，但loss一直没降，因此我尝试<code>print(loss.grad)</code>的grad，发现神奇的是loss grad显示为None，接着尝试<code>print(y_pred.grad)</code>，同样是None，但再print loss和y_pred的requires_grad发现是正常的True。</p><p>在查阅了资料，以及问了学长之后发现原来并不是bug，而是因为，Pytorch默认不会保存中间节点(intermediate variable)的grad，此举是为了节省内存。</p><blockquote><p>By default, gradients are only retained for leaf variables. non-leaf variables’ gradients are not retained to be inspected later. This was done by design, to save memory.</p></blockquote><p><a href="https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94" target="_blank" rel="noopener">https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94</a></p><p>实际上可以通过retain_grad()或者hook来查看中间节点的grad。</p><p>我后面尝试print了叶子节点，如 <code>print(CNN_model.fc.weight.grad)</code>，最终获得了正确的grad。</p><p>ps：所谓中间节点，是<strong>由其他节点计算所得</strong>的tensor，而叶子节点则是<strong>自己定义</strong>出来的。</p><p>最后我发现，原来loss一直没降的原因是因为我定义的CNN过于复杂，并且数据集偏小，无法快速收敛导致的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Pytorch </tag>
            
            <tag> grad </tag>
            
            <tag> bug </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Linux常用命令</title>
      <link href="/2018/08/03/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2018/08/03/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h3 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h3><p>记录自己常用的命令。</p><p>1️⃣ls：显式当前目录下的文件和目录<br>    -a 包括隐藏文件<br>    -h 将文件的容量以易读方式列出（配合-s使用）<br>    -s 以块数形式显示每个文件分配的尺寸<br>    -l 以较长格式列出信息，可以直接写成 <code>ll</code><br><img src="/images/2018-08-10-15339070264416.jpg" width="70%" height="50%"></p><hr><p>2️⃣cd 到达指定地址</p><hr><p>3️⃣kill 杀死程序<br>    -l 信息编号。<strong>当l=9时，无条件终止，其他信号可能忽略</strong><br>    killall -u <user_name> 杀死该用户全部进程</user_name></p><p><img src="/images/2018-08-03-15332830170623.jpg" width="50%" height="50%"></p><hr><p>4️⃣ps 报告当前系统的进程状态<br>    -a 所有<br>    -p 指定程序<br>    -u 指定用户<br>    -x 列出该用户的进程的详细信息(我的理解应该是)<br>    如：<br><img src="/images/2018-08-10-15339036207642.jpg" width="70%" height="50%"></p><hr><p>5️⃣htop 比top更优，交互更好，同时可以直观看到资源占用情况<br>基本命令与top一致<br><img src="/images/2018-08-04-15333484387393.jpg" width="70%" height="50%"></p><hr><p>6️⃣top：动态查看系统运行状态<br>    -u 指定用户名<br>    -p 指定进程</p><p>7️⃣nvidia-smi 查看显卡状态<br>watch nvidia-smi 实时查看显卡状态，定时刷新</p><hr><p>8️⃣tail 显示指定文件的末尾若干行<br>    -f 显示文件最新追加的内容<br>    -n 显示文件尾部n行内容<br>    -c 显示文件尾部最后c个字符</p><p>如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tail file 显示最后10行</span><br><span class="line">tail -n +20 file 显示从第20行至末尾</span><br><span class="line">tail -c 10 file 显示文件file的最后10个字符</span><br></pre></td></tr></table></figure><pre><code>-------</code></pre><p>9️⃣echo 用于打印指定的字符串<br><img src="/images/2018-08-04-15333497266470.jpg" width="50%" height="50%"></p><hr><p>🔟which 用于查找并显示给定命令的绝对路径，which指令会在环境变量$PATH设置的目录里查找符合条件的文件。使用which命令，可以看到某个系统命令是否存在，以及执行的是哪个位置的命令。如：</p><p><img src="/images/2018-08-04-15333500837235.jpg" width="50%" height="50%"></p><hr><p>1️⃣1️⃣nohup 将程序以忽略挂起信号的方式运行，经常用于在服务器跑代码<br>如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python xxx.py &gt;output.txt 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p><p>即，将输出重定向到output.txt ；最后一个<code>&amp;</code>表示后台挂起</p><hr><p>1️⃣2️⃣cp 复制文件   cp [文件] [目标文件夹]<br>    -r 递归复制，用于目录的复制</p><hr><p>1️⃣3️⃣mv 移动文件、目录或更名  mv [文件/文件夹] [文件夹]<br>    -f 强制，当目标文件存在，直接覆盖<br>    -i 会询问</p><hr><p>1️⃣4️⃣rm 删除文件或目录<br>    -f 强制删除<br>    -r 递归删除，用于目录删除</p><hr><p>1️⃣5️⃣file 用于判断文件的基本数据<br>如：</p><p><img src="/images/2018-08-05-15334547949248.jpg" width="80%" height="50%"></p><hr><p>1️⃣6️⃣tar 对文件打包/压缩<br>    -t 查看打包文件的内容含有哪些文件名<br>    -x 解压缩<br>    -c 新建打包文件<br>    -C 指定压缩/解压目录<br>    -v 解压/压缩过程中将处理的文件名显示出来<br>常用的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">压缩：tar -jcv -f filename.tar.bz2 要被处理的文件或目录名称</span><br><span class="line">查询：tar -jtv -f filename.tar.bz2</span><br><span class="line">解压：tar -jxv -f filename.tar.bz2 -C 欲解压缩的目录</span><br></pre></td></tr></table></figure><hr><p>1️⃣7️⃣wc word count 统计文件内容信息，如行数、字符数<br>    -l 显示文件行数<br>    -c 显示字节数<br>    -m 显示字符数<br>    -w 显示字数  字被定义为由空白、跳格、换行字符分隔的字符串<br>    -L 显示最长行的长度<br>    不加参数，所有的都显示，依次是行数、单词数、字节数、文件名</p><hr><p>1️⃣8️⃣df 显示磁盘相关信息<br>    -h 以可读性较高的方式显示信息</p><hr><p>1️⃣9️⃣scp 服务器之间的文件复制<br>    如:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r /test1 zhlin@123.12.1.12:/home/zhlin</span><br></pre></td></tr></table></figure><h3 id="✨快捷键"><a href="#✨快捷键" class="headerlink" title="✨快捷键"></a>✨快捷键</h3><p>Ctrl+a 跳到行首<br>Ctrl+c 退出当前进程<br>Ctrl+e 跳到页尾<br>Ctrl+k 删除当前光标后面的文字<br>Ctrl+l 清屏，等价于clear<br>Ctrl+r 搜索之前打过的命令<br>Ctrl+u 删除当前光标前面的文字<br>✨Ctrl+左右键 单词之间跳转 在Mac上可以使用option+左右键<br>Ctrl+y 进行恢复删除<br>Ctrl+z 将当前进程转到后台，使用fg恢复</p><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://blog.csdn.net/leo_618/article/details/53003111" target="_blank" rel="noopener">https://blog.csdn.net/leo_618/article/details/53003111</a></p><p>———-持续更新———-</p>]]></content>
      
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 碎片知识 </tag>
            
            <tag> 技巧 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>一个关于yield的重新认识</title>
      <link href="/2018/07/31/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E4%B8%80%E4%B8%AA%E5%85%B3%E4%BA%8Eyield%E7%9A%84%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86/"/>
      <url>/2018/07/31/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E4%B8%80%E4%B8%AA%E5%85%B3%E4%BA%8Eyield%E7%9A%84%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<p>今天遇到了一个神奇的”bug”，让我对yield的理解更深一步。</p><p>这是一个函数，我本来打算试着print一下line内部的格式和内容。<br><img src="/images/2018-07-31-15330184801079.jpg" width="40%" height="40%"></p><p>这是调用的主函数：<br><img src="/images/2018-07-31-15330185414299.jpg" width="50%" height="50%"></p><p>结果跑出的结果是：<br><img src="/images/2018-07-31-15330185762441.jpg" width="90%" height="90%"></p><p>？？？<br><img src="/images/2018-07-31-15330186003254.jpg" alt=""></p><p>我尝试在函数的开头添加print：<br><img src="/images/2018-07-31-15330186689312.jpg" width="40%" height="50%"></p><p>结果仍然没有任何的输出。</p><p>我试着在main函数添加print：<br><img src="/images/2018-07-31-15330187249603.jpg" width="50%" height="50%"></p><p>结果：</p><p><img src="/images/2018-07-31-15330187445810.jpg" width="50%" height="50%"></p><p>也就是说，根本没有进入到get_dataset_from_txt函数啊。</p><p>我以为是pycharm的问题还重启了一遍，然而并没有任何作用。问了其他人，他们也觉得很神奇。最后一个同学看了一下函数，发现了问题所在：<strong>yield</strong></p><p>我突然想起来，<strong>yield返回的是一个generator，只有在对generator进行遍历时，才会开始运行</strong>…</p><p>于是，我试着这么写，试着对generator遍历：</p><p><img src="/images/2018-07-31-15330189280379.jpg" width="50%" height="50%"></p><p>虽然报错了，但函数终于是进去了…</p><p><img src="/images/2018-07-31-15330189613535.jpg" width="70%" height="50%"></p><p><strong>结论：有yield的函数会返回一个generator，当对其进行遍历时，函数才会开始运行。</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> 遇到的问题 </tag>
            
            <tag> Python </tag>
            
            <tag> yield </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录2</title>
      <link href="/2018/07/29/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB2/"/>
      <url>/2018/07/29/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB2/</url>
      
        <content type="html"><![CDATA[<p>本周主要看了<a href="https://github.com/allenai/bilm-tf" target="_blank" rel="noopener">AllenNLP/ELMO</a>的代码，但并没有找到很多可复用的代码。本周也没有比较有意义的代码。</p><hr><h3 id="1️⃣get-time-diff"><a href="#1️⃣get-time-diff" class="headerlink" title="1️⃣get_time_diff"></a>1️⃣get_time_diff</h3><p>获取已使用的时间<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta</span><br><span class="line"></span><br><span class="line">start_time=time.time()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time_dif</span><span class="params">(start_time)</span>:</span></span><br><span class="line">    <span class="string">"""获取已使用时间"""</span></span><br><span class="line">    end_time = time.time()</span><br><span class="line">    time_dif = end_time - start_time</span><br><span class="line">    <span class="keyword">return</span> timedelta(seconds=int(round(time_dif)))</span><br></pre></td></tr></table></figure></p><hr><h3 id="2️⃣parser使用"><a href="#2️⃣parser使用" class="headerlink" title="2️⃣parser使用"></a>2️⃣parser使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--save_dir'</span>, help=<span class="string">'Location of checkpoint files'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--vocab_file'</span>, help=<span class="string">'Vocabulary file'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--train_prefix'</span>, help=<span class="string">'Prefix for train files'</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line">    </span><br><span class="line">main(args)   <span class="comment">#使用</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周碎片知识1</title>
      <link href="/2018/07/29/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%861/"/>
      <url>/2018/07/29/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%861/</url>
      
        <content type="html"><![CDATA[<p>1️⃣[Python]<br>assert用法：</p><p><code>assert expression</code><br>等价于<br><code>if not expression: raise AssertionError</code></p><hr><p>2️⃣[Pytorch]<br>Pytorch view：<br>创建一个新的tensor，但他们的<strong>data是共享的</strong>。</p><p><img src="/images/2018-07-29-15328360404485.jpg" width="50%" height="50%"></p><hr><p>3️⃣[Pytorch]<br>在Pytorch中，embedding的index是不能requires_grad=True的，否则会出错。<br><a href="https://github.com/pytorch/pytorch/issues/7021" target="_blank" rel="noopener">https://github.com/pytorch/pytorch/issues/7021</a></p><p>之前看过一份代码，设置volatile=false但没有出错，是因为在Pytorch0.4之后volatile已经被弃用了，因此volatile=false不起作用，而默认requires_grad=false</p><hr><p>4️⃣[Pytorch]<br>在Pytorch中，<code>nn.Linear(self.hidden_dim,self.vocab_size)</code>的维度是vocab_size<em>hidden_dim，之前居然没有注意到这个问题。<br>因为nn.Linear的<em>*第一个参数表示输入维度，第二个参数表示输出维度</em></em></p><hr><p>5️⃣[Pytorch]<br>Pytorch中，使用view一般来说必须要用 .contiguous()。也即：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch.view(batch_size, <span class="number">-1</span>).t().contiguous()</span><br></pre></td></tr></table></figure><p>contiguous()的官方解释：<br><a href="https://discuss.pytorch.org/t/runtimeerror-input-is-not-contiguous/930" target="_blank" rel="noopener">https://discuss.pytorch.org/t/runtimeerror-input-is-not-contiguous/930</a></p><blockquote><p>It means that your tensor is not a single block of memory, but a block with holes. view can be only used with contiguous tensors, so if you need to use it here, just call .contiguous() before.</p></blockquote><p>也就是说，contiguous会将数据存到一个连续的空间内（block）。</p><hr><p>6️⃣[Pytorch]<br>调用Cross_entropy时，Pytorch会帮助你加log和softmax。</p><p><img src="/images/2018-07-29-15328370301774.jpg" alt=""></p><hr><p>7️⃣[Paper]<br><a href="https://arxiv.org/abs/1807.02291" target="_blank" rel="noopener">Sliced_RNN</a></p><p>将RNN分块以提高并行性，甚至每层的RNN都可以不一样，达到抽取不同程度的抽象语义信息的目的。实验证明，在不同任务上都有一定的提升，但速度的提升很大。</p><p><img src="/images/2018-07-29-15328372609264.jpg" width="50%" height="50%"></p><hr><p>8️⃣[Tf-idf]<br>计算词语对于句子的重要程度</p><p><a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/Tf-idf</a></p><p>tf是词频，idf是逆向文件频率。也即如果词在该句出现的次数越多，在所有文本的出现次数越少，则词对于句子的重要程度越高。</p><hr><p>9️⃣[Numpy]<br>在Numpy中，一个列表虽然是横着表示的，但它是列向量。我之前居然没有注意到这个问题。</p><p><img src="/images/2018-07-29-15328375536418.jpg" width="50%" height="50%"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 碎片知识 </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
            <tag> Paper </tag>
            
            <tag> Tf-idf </tag>
            
            <tag> Numpy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Mac配置复旦有线网</title>
      <link href="/2018/07/29/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Mac%E9%85%8D%E7%BD%AE%E5%A4%8D%E6%97%A6%E6%9C%89%E7%BA%BF%E7%BD%91/"/>
      <url>/2018/07/29/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/Mac%E9%85%8D%E7%BD%AE%E5%A4%8D%E6%97%A6%E6%9C%89%E7%BA%BF%E7%BD%91/</url>
      
        <content type="html"><![CDATA[<h3 id="配置ip、子网掩码、DNS、路由器"><a href="#配置ip、子网掩码、DNS、路由器" class="headerlink" title="配置ip、子网掩码、DNS、路由器"></a>配置ip、子网掩码、DNS、路由器</h3><p>有线似乎不支持DHCP，因此只好自己设置。<br>首先连接上有线，将配置iPv4选为手动。问实验室的学长具体的ip地址、子网掩码、路由器、DNS服务器。其中ip地址最后三位要自己设定，只要不和其他人冲突就好。</p><p><img src="/images/2018-07-29-15328333841584.jpg" width="50%" height="50%"></p><p><img src="/images/2018-07-29-15328335236981.jpg" width="50%" height="50%"></p><h3 id="手动认证"><a href="#手动认证" class="headerlink" title="手动认证"></a>手动认证</h3><p>到认证平台，下载Mac客户端，其实就是一个.sh文件：<br><a href="http://10.108.255.249/srun_portal_pc.php?ac_id=1&amp;&amp;phone=1" target="_blank" rel="noopener">http://10.108.255.249/srun_portal_pc.php?ac_id=1&amp;&amp;phone=1</a></p><p><img src="/images/2018-07-29-15328336395029.jpg" width="30%" height="30%"></p><p>然后，打开文件配置用户名密码，注意到等号后面要有双引号：</p><p><img src="/images/2018-07-29-15328337135244.jpg" width="50%" height="50%"></p><p>保存并放入终端运行，接下来就可以使用有线网了。</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>似乎，每次重新连接都要这样配置，我没有试过不清楚；<br>有线网好像也没有比无线网快多少，但应该会稳定一些。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 网络 </tag>
            
            <tag> 配置 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ssh快速登录配置</title>
      <link href="/2018/07/29/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/ssh%E5%BF%AB%E9%80%9F%E7%99%BB%E5%BD%95%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/07/29/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/ssh%E5%BF%AB%E9%80%9F%E7%99%BB%E5%BD%95%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>分配了服务器之后，每次要ssh进入都很麻烦：<code>ssh user_name@ip_address</code> 然后还要输入密码。</p><p>特别是如果分配了多个服务器，那有时候还容易忘记ip地址。因此如果能够一条命令就进入服务器能够减少麻烦。<br>主要有三点：</p><ol><li>创建rsa key</li><li>上传public key到服务器</li><li>设置alias</li></ol><h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><h2 id="创建rsa-key"><a href="#创建rsa-key" class="headerlink" title="创建rsa key"></a>创建rsa key</h2><p>在终端输入命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p>当然如果以前有创建过的可以不用。</p><p>结果：</p><p><img src="/images/2018-07-29-Xnip2018-07-29_10-43-15.jpg" width="50%" height="50%"></p><h2 id="上传public-key到服务器"><a href="#上传public-key到服务器" class="headerlink" title="上传public key到服务器"></a>上传public key到服务器</h2><p>使用命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub user@127.0.0.1</span><br></pre></td></tr></table></figure></p><p>输入密码即可</p><p>结果：</p><p><img src="/images/2018-07-29-15328324683354.jpg" width="60%" height="60%"></p><h2 id="设置alias"><a href="#设置alias" class="headerlink" title="设置alias"></a>设置alias</h2><p>完成以上步骤就可以不输入密码登录，但还是需要输入ip地址和用户名，为了更简化操作，给命令起个别名。需要配置 .bash_profile文件。<br>输入命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure><p>在文件后面添加以下文字：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># alias </span><br><span class="line">alias ssh×××=&quot;ssh user_name@ip_address&quot;</span><br><span class="line">alias ssh×××=&quot;ssh user_name@ip_address&quot;</span><br></pre></td></tr></table></figure><p>其中 ×××是你自己起的名字，可以是服务器的名字，user_name和ip_address是自己服务器的用户名和地址。保存更改退出。</p><p>然后还要使其生效:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><p>这样，输入别名，就可以直接登录了：</p><p><img src="/images/2018-07-29-15328329815456.jpg" width="50%" height="50%"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.jianshu.com/p/66d658c7cb9e" target="_blank" rel="noopener">https://www.jianshu.com/p/66d658c7cb9e</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 配置 </tag>
            
            <tag> ssh </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于困惑度</title>
      <link href="/2018/07/29/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8E%E5%9B%B0%E6%83%91%E5%BA%A6/"/>
      <url>/2018/07/29/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8E%E5%9B%B0%E6%83%91%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<p>前几天在写新手任务<a href="https://github.com/FudanNLP/nlp-beginner" target="_blank" rel="noopener">task3</a>的时候，参考了Pytorch官方example的word language model，官方example在训练过程中计算困惑度是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">math.exp(cur_loss)</span><br></pre></td></tr></table></figure><p>其中，cur_loss表示交叉熵的loss，即 $-P(\hat{x})logP(x)$，$\hat{x}$表示ground truth。</p><p>然而，在查阅了困惑度相关资料后，我发现，困惑度的定义是这样的：</p><script type="math/tex; mode=display">\begin{aligned}PP(S)= &{P(w_{1}w_{2}...w_{N})}^{-\frac{1}{N}} \\= &\sqrt[N]{\frac{1}{p(w_1 w_2 ... w_N)}} \\= & \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} }\end{aligned}</script><p>这是另一种形式:</p><script type="math/tex; mode=display">\begin{aligned}Perplexity (W)=& 2^{H(W)} \\= & {P(w_{1}w_{2}...w_{N})}^{-\frac{1}{N}} \\= & \sqrt[N]{\frac{1}{p(w_1 w_2 ... w_N)}} \\= & \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} }\end{aligned}</script><p>可以看到，二者本质是一样的。</p><p><strong>那么，为什么在代码中以e为底去计算困惑度，而不是2呢?</strong></p><p>实际上，是因为在上述公式中，log是以2为底的，但在Pytorch中，log默认是以e为底的。因此在代码中，需要用e作为指数的底来还原成困惑度的原本形式： </p><script type="math/tex; mode=display">\begin{aligned}\sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} }\end{aligned}</script><p>最后这是perplexity的数学推导：<br><a href="https://www.zhihu.com/question/58482430" target="_blank" rel="noopener">https://www.zhihu.com/question/58482430</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 困惑度 </tag>
            
            <tag> perplexity </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词2</title>
      <link href="/2018/07/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D2/"/>
      <url>/2018/07/29/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D2/</url>
      
        <content type="html"><![CDATA[<p>本周的诗词有两篇是已经背过的，权当是复习了一遍。</p><hr><p>1️⃣</p><h3 id="下终南山过斛斯山人宿置酒"><a href="#下终南山过斛斯山人宿置酒" class="headerlink" title="下终南山过斛斯山人宿置酒"></a>下终南山过斛斯山人宿置酒</h3><p>[唐] 李白<br>暮从碧山下，山月随人归。<br>却顾所来径，苍苍横翠微。<br>相携及田家，童稚开荆扉。<br>绿竹入幽径，青萝拂行衣。<br>欢言得所憩，美酒聊共挥。<br>长歌吟松风，曲尽河星稀。<br>我醉君复乐，陶然共忘机。</p><p><a href="http://m.xichuangzhu.com/work/57b900307db2a20054269a2a" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b900307db2a20054269a2a</a></p><hr><p>2️⃣</p><h3 id="逢入京使"><a href="#逢入京使" class="headerlink" title="逢入京使"></a>逢入京使</h3><p>[唐] 岑参<br>故园东望路漫漫，双袖龙钟泪不乾。<br><strong>马上相逢无纸笔，凭君传语报平安。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b92218df0eea006335f923" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b92218df0eea006335f923</a></p><hr><p>3️⃣</p><h3 id="念奴娇·赤壁怀古"><a href="#念奴娇·赤壁怀古" class="headerlink" title="念奴娇·赤壁怀古"></a>念奴娇·赤壁怀古</h3><p>[宋] 苏轼<br>大江东去，浪淘尽、千古风流人物。故垒西边，人道是、三国周郎赤壁。乱石穿空，惊涛拍岸，卷起千堆雪。江山如画，一时多少豪杰。<br>遥想公瑾当年，小乔初嫁了，雄姿英发。羽扇纶巾，谈笑间，樯橹灰飞烟灭。故国神游，多情应笑我，早生华发。<strong>人生如梦，一尊还酹江月。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b8bda2df0eea006333ecd2" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8bda2df0eea006333ecd2</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>代码片段记录1</title>
      <link href="/2018/07/23/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB1/"/>
      <url>/2018/07/23/%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB1/</url>
      
        <content type="html"><![CDATA[<h3 id="1️⃣-get-batch"><a href="#1️⃣-get-batch" class="headerlink" title="1️⃣ get_batch"></a>1️⃣ get_batch</h3><p>注意到shuffle的标准做法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">(self,data,batch_size=<span class="number">32</span>,is_shuffle)</span>:</span></span><br><span class="line">  N=len(data)  <span class="comment">#获得数据的长度</span></span><br><span class="line">  <span class="keyword">if</span> is_shuffle <span class="keyword">is</span> <span class="keyword">True</span>:</span><br><span class="line">    r=random.Random()</span><br><span class="line">    r.seed()</span><br><span class="line">    r.shuffle(data) <span class="comment">#如果is_shuffle为真则打乱</span></span><br><span class="line">  <span class="comment">#开始获得batch，使用[ for in ]</span></span><br><span class="line">  batch=[data[k:k+batch_size] <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>,N,batch_size)]</span><br><span class="line">  <span class="keyword">if</span> N%batch_size!=<span class="number">0</span>:  <span class="comment">#处理不整除问题，如果有显式要求丢掉则不需要处理，这里默认处理</span></span><br><span class="line">    remainder=N-N%batch_size  <span class="comment">#剩下的部分</span></span><br><span class="line">    batch.append(data[temp:N])</span><br><span class="line">  <span class="keyword">return</span> batch</span><br></pre></td></tr></table></figure><hr><h3 id="2️⃣使用gensim将GloVe读入"><a href="#2️⃣使用gensim将GloVe读入" class="headerlink" title="2️⃣使用gensim将GloVe读入"></a>2️⃣使用gensim将GloVe读入</h3><p>实际上这份代码有点问题，在使用过程中，发现glove文件需要放在gensim的文件夹下才能被读到(7.20 updated,应该使用绝对地址)，并不好。</p><p>教程地址：<a href="https://radimrehurek.com/gensim/scripts/glove2word2vec.html" target="_blank" rel="noopener">gensim: scripts.glove2word2vec – Convert glove format to word2vec</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1. 使用gensim读入word2vec</span></span><br><span class="line"></span><br><span class="line">model = gensim.models.KeyedVectors.load_word2vec_format(</span><br><span class="line">        fname=<span class="string">'GoogleNews-vectors-negative300-SLIM.bin'</span>, binary=<span class="keyword">True</span>)</span><br><span class="line">words = model.vocab  <span class="comment">#获得词表</span></span><br><span class="line">vector= model[word]  <span class="comment">#word是words里面的元素</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2. 使用gensim读入glove</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line"><span class="keyword">from</span> gensim.test.utils <span class="keyword">import</span> datapath, get_tmpfile</span><br><span class="line"><span class="keyword">from</span> gensim.scripts.glove2word2vec <span class="keyword">import</span> glove2word2vec</span><br><span class="line">glove_file=datapath(<span class="string">'glove.txt'</span>)  <span class="comment">#最好使用绝对地址</span></span><br><span class="line">tmp_file=get_tmpfile(<span class="string">'word2vec.txt'</span>)</span><br><span class="line">glove2word2vec(glove_file,tmp_file)</span><br><span class="line">model=KeyedVectors.load_word2vec_format(tmp_file)</span><br><span class="line"><span class="comment">#接下来使用的方法是一样的</span></span><br></pre></td></tr></table></figure></p><hr><h3 id="3️⃣data-split方法"><a href="#3️⃣data-split方法" class="headerlink" title="3️⃣data_split方法"></a>3️⃣data_split方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_split</span><span class="params">(seed=<span class="number">1</span>, proportion=<span class="number">0.7</span>)</span>:</span> </span><br><span class="line">    data = list(iter_corpus())</span><br><span class="line">    ids = list(range(len(data)))</span><br><span class="line"></span><br><span class="line">    N = int(len(ids) * proportion)  <span class="comment"># number of training data</span></span><br><span class="line"></span><br><span class="line">    rng = random.Random(seed)</span><br><span class="line">    rng.shuffle(ids)</span><br><span class="line">    test_ids = set(ids[N:])</span><br><span class="line">    train_data = []</span><br><span class="line">    test_data = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> x[<span class="number">1</span>] <span class="keyword">in</span> test_ids:  <span class="comment"># x[1]: sentence id</span></span><br><span class="line">            test_data.append(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train_data.append(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_data, test_data</span><br></pre></td></tr></table></figure><hr><h3 id="4️⃣对string预处理"><a href="#4️⃣对string预处理" class="headerlink" title="4️⃣对string预处理"></a>4️⃣对string预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_str</span><span class="params">(string)</span>:</span></span><br><span class="line">    string = re.sub(<span class="string">r"[^A-Za-z0-9()!?\'\`]"</span>, <span class="string">""</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'s"</span>, <span class="string">" \'s"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'m"</span>, <span class="string">" \'m"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'ve"</span>, <span class="string">" \'ve"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"n\'t"</span>, <span class="string">" n\'t"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'re"</span>, <span class="string">" \'re"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'d"</span>, <span class="string">" \'d"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\'ll"</span>, <span class="string">" \'ll"</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r","</span>, <span class="string">" , "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"!"</span>, <span class="string">" ! "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\("</span>, <span class="string">" \( "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\)"</span>, <span class="string">" \) "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\?"</span>, <span class="string">" \? "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\s&#123;2,&#125;"</span>, <span class="string">" "</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"\@.*?[\s\n]"</span>, <span class="string">""</span>, string)</span><br><span class="line">    string = re.sub(<span class="string">r"https*://.+[\s]"</span>, <span class="string">""</span>, string)</span><br><span class="line">    <span class="keyword">return</span> string.strip().lower()</span><br></pre></td></tr></table></figure><hr><h3 id="5️⃣collate-fn-batch）"><a href="#5️⃣collate-fn-batch）" class="headerlink" title="5️⃣collate_fn(batch）"></a>5️⃣collate_fn(batch）</h3><p>重写collate_fn组建mini-batch，在NLP中常用，句子的不等长性<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(batch)</span>:</span>  <span class="comment"># rewrite collate_fn to form a mini-batch</span></span><br><span class="line">    lengths = np.array([len(data[<span class="string">'sentence'</span>]) <span class="keyword">for</span> data <span class="keyword">in</span> batch])</span><br><span class="line">    sorted_index = np.argsort(-lengths)</span><br><span class="line">    lengths = lengths[sorted_index]  <span class="comment"># descend order</span></span><br><span class="line"></span><br><span class="line">    max_length = lengths[<span class="number">0</span>]</span><br><span class="line">    batch_size = len(batch)</span><br><span class="line">    sentence_tensor = torch.LongTensor(batch_size, int(max_length)).zero_()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, index <span class="keyword">in</span> enumerate(sorted_index):</span><br><span class="line">        sentence_tensor[i][:lengths[i]] = torch.LongTensor(batch[index][<span class="string">'sentence'</span>][:max_length])</span><br><span class="line"></span><br><span class="line">    sentiments = torch.autograd.Variable(torch.LongTensor([batch[i][<span class="string">'sentiment'</span>] <span class="keyword">for</span> i <span class="keyword">in</span> sorted_index]))</span><br><span class="line">    <span class="keyword">if</span> config.use_cuda:</span><br><span class="line">        packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(Variable(sentence_tensor.t()).cuda(), lengths)  <span class="comment">#remember to transpose</span></span><br><span class="line">        sentiments = sentiments.cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(Variable(sentence_tensor.t()),lengths)  <span class="comment"># remember to transpose</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'sentence'</span>: packed_sequences, <span class="string">'sentiment'</span>: sentiments&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">## 重写collate_fn(batch)以用于dataloader使用，使用方法如下：</span></span><br><span class="line"></span><br><span class="line">train_dataloader=DataLoader(train_data,batch_size=<span class="number">32</span>,shuffle=<span class="keyword">True</span>,collate_fn=collate_fn)</span><br><span class="line">​</span><br><span class="line"><span class="comment">## 其中，train_dataloader可循环遍历​​。</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p><hr><h3 id="6️⃣使用yield获得数据的generator"><a href="#6️⃣使用yield获得数据的generator" class="headerlink" title="6️⃣使用yield获得数据的generator"></a>6️⃣使用yield获得数据的generator</h3><p>yield的用法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span><span class="params">(txt_file)</span>:</span>     <span class="comment"># return generator</span></span><br><span class="line">    <span class="keyword">with</span> open(txt_file,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="keyword">if</span> len(line.strip())==<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            sentence=list(line.strip())+[<span class="string">'&lt;eos&gt;'</span>]</span><br><span class="line">            <span class="keyword">yield</span> sentence</span><br><span class="line">            </span><br><span class="line"><span class="comment">#在使用的时候：</span></span><br><span class="line">dataset=get_dataset(txt_file)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> dataset:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果需要还可以改成list形式</span></span><br><span class="line">dataset=list(get_dataset(txt_file))</span><br></pre></td></tr></table></figure></p><hr><h3 id="7️⃣动态创建RNN实例"><a href="#7️⃣动态创建RNN实例" class="headerlink" title="7️⃣动态创建RNN实例"></a>7️⃣动态创建RNN实例</h3><p>根据rnn_type动态创建对象实例，使用了getattr<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rnn in ['GRU','LSTM','RNN']</span></span><br><span class="line"></span><br><span class="line">self.rnn = getattr(nn, self.rnn_type)(self.embedding_dim, self.hidden_dim, self.num_layers, dropout=self.dropout)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> code snippets </tag>
            
            <tag> 代码片段 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>每周诗词1</title>
      <link href="/2018/07/23/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D1/"/>
      <url>/2018/07/23/%E8%AF%97%E8%AF%8D&amp;%E5%8F%A5/%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D1/</url>
      
        <content type="html"><![CDATA[<p>本周背了四篇。</p><hr><p>1️⃣</p><h3 id="临江仙·夜归临皋"><a href="#临江仙·夜归临皋" class="headerlink" title="临江仙·夜归临皋"></a>临江仙·夜归临皋</h3><p>[宋] 苏轼<br>夜饮东坡醒复醉，归来彷彿三更。家童鼻息已雷鸣，敲门都不应，倚杖听江声。<br><strong>长恨此身非我有，何时忘却营营？</strong>夜阑风静縠纹平，小舟从此逝，江海寄馀生。</p><p>縠（hú）纹<br>皋（gao）<br><a href="http://m.xichuangzhu.com/work/57ae79400a2b580063150e39" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57ae79400a2b580063150e39</a></p><hr><p>2️⃣</p><h3 id="蝶恋花·阅尽天涯离别苦"><a href="#蝶恋花·阅尽天涯离别苦" class="headerlink" title="蝶恋花·阅尽天涯离别苦"></a>蝶恋花·阅尽天涯离别苦</h3><p>[清] 王国维<br>阅尽天涯离别苦。不道归来，零落花如许。花底相看无一语，绿窗春与天俱莫。<br>待把相思灯下诉。一缕新欢，旧恨千千缕。<strong>最是人间留不住，朱颜辞镜花辞树。</strong></p><p><a href="http://m.xichuangzhu.com/work/57b8ef70128fe10054c91d17" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8ef70128fe10054c91d17</a></p><hr><p>3️⃣</p><h3 id="送友人"><a href="#送友人" class="headerlink" title="送友人"></a>送友人</h3><p>[唐] 李白<br>青山横北郭，白水绕东城。<br>此地一为别，孤蓬万里征。<br><strong>浮云游子意，落日故人情。</strong><br>挥手自兹去，萧萧班马鸣。</p><p><a href="http://m.xichuangzhu.com/work/57b8facfd342d3005ac6ffb4" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8facfd342d3005ac6ffb4</a></p><hr><p>4️⃣</p><h3 id="黄鹤楼送孟浩然之广陵"><a href="#黄鹤楼送孟浩然之广陵" class="headerlink" title="黄鹤楼送孟浩然之广陵"></a>黄鹤楼送孟浩然之广陵</h3><p>[唐] 李白<br>故人西辞黄鹤楼，烟花三月下扬州。<br>孤帆远影碧空尽，唯见长江天际流。</p><p><a href="http://m.xichuangzhu.com/work/57b8f306128fe10054c92fb8" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b8f306128fe10054c92fb8</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 诗词 </tag>
            
            <tag> 诗词分享 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>安装conda错误</title>
      <link href="/2018/07/23/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/%E5%AE%89%E8%A3%85conda%E9%94%99%E8%AF%AF/"/>
      <url>/2018/07/23/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/%E5%AE%89%E8%A3%85conda%E9%94%99%E8%AF%AF/</url>
      
        <content type="html"><![CDATA[<p>在服务器上安装conda的时候，一开始使用了pip安装<br><code>pip install conda</code><br>在安装好conda之后想要使用conda命令，出现：</p><p>ERROR: The install method you used for conda—probably either <code>pip install conda</code> or <code>easy_install conda</code>—is not compatible with using conda as an application. If your intention is to install conda as a standalone application, currently supported install methods include the Anaconda installer and the miniconda installer. You can download the miniconda installer from <a href="https://conda.io/miniconda.html" target="_blank" rel="noopener">https://conda.io/miniconda.html</a>.</p><p><img src="/images/2018-07-23-15323331261104.jpg" alt=""></p><p>然后到官网下载.sh文件并bash安装，仍然没有解决该问题；接着尝试pip uninstall conda，出现<br><img src="/images/2018-07-23-15323337042406.jpg" alt=""></p><p>最后在查阅了网上之后，使用 <code>which conda</code>找到conda的地址，并删除<code>rm ×××</code><br><img src="/images/2018-07-23-15323337894186.jpg" alt=""></p><p>最后重新bash安装即可。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 杂七杂八 </tag>
            
            <tag> conda </tag>
            
            <tag> 遇到的问题 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
