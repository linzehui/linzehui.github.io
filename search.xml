<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†25]]></title>
    <url>%2F2019%2F07%2F28%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8625%2F</url>
    <content type="text"><![CDATA[[fairseq CUDA]å¸¸å¸¸é‡åˆ°fairseqè·‘ç¿»è¯‘ï¼Œè·‘ç€è·‘ç€å°±printå¾ˆå¤štensorï¼Œæ˜¾ç¤ºCUDAé—®é¢˜ã€‚ å°è¯•æ·»åŠ --ddp-backend=no_c10dä¹‹åï¼Œä¼¼ä¹å°±ä¸ä¼šå‡ºç°è¯¥é—®é¢˜äº†ã€‚ä¹Ÿå³ï¼š 12python -m torch.distributed.launch --nproc_per_node 4 \train.py --ddp-backend=no_c10d ... [Overleaf]ä¸€èˆ¬overleafå¹¶ä¸ä¼šäº§ç”Ÿä¸­é—´æ–‡ä»¶ï¼Œå‡å¦‚ç¡®å®éœ€è¦bblè€Œä¸æ˜¯bibæ–‡ä»¶ï¼ˆæ¯”å¦‚æäº¤åˆ°arxivæ—¶å°±éœ€è¦bblï¼‰ï¼Œå¯ä»¥ç‚¹å‡»compileæ—è¾¹çš„log and other filesï¼Œç¿»åˆ°æœ€ä¸‹é¢å°±å¯ä»¥é€‰æ‹©ä¸‹è½½bblã€‚ https://tex.stackexchange.com/questions/462314/overleaf-v2-how-to-get-bbl-file [Arxiv]arxivä¸Šä¼ æ—¶å‡ºç°warningçš„é—®é¢˜ï¼šâ€˜This version (v8.31) of natbib is stricter in its formatting requirements for bibitem entries than the previous version used at arXiv (v7.1)â€™ã€‚ æ£€æŸ¥ä¸€ä¸‹ï¼Œå¯èƒ½æ˜¯bblä¸ä¸»æ–‡ä»¶çš„åå­—ä¸åŒï¼Œæ”¹æˆç›¸åŒçš„å¯èƒ½å°±å¯ä»¥äº†ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡25]]></title>
    <url>%2F2019%2F07%2F26%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8725%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨è®ºæ–‡ï¼š Easy Samples First: Self-paced Reranking for Zero-Example Multimedia Search Self-Paced Learning with Diversity Self-paced dictionary learning for image classification Self-Paced Learning for Matrix Factorization Self-Paced Boost Learning for Classification [Easy Samples First: Self-paced Reranking for Zero-Example Multimedia Search]å°†self-paced learningç”¨äºrerankingä»»åŠ¡ä¸­ã€‚å¹¶ä¸”å°†self-paced learningæ›´ä¸€èˆ¬åŒ–ï¼Œä¹Ÿå³å°†åŸå…ˆçš„{0,1} weightæ‰©å±•åˆ°å®æ•°åŸŸã€‚ å…·ä½“è€Œè¨€ï¼ŒåŸå…ˆçš„self-paced learningï¼š \left(\mathbf{w}_{t+1}, \mathbf{v}_{t+1}\right)=\underset{\mathbf{w} \in \mathbb{R}^{d}, \mathbf{v} \in\{0,1\}^{n}}{\operatorname{argmin}}\left(r(\mathbf{w})+\sum_{i=1}^{n} v_{i} f\left(\mathbf{x}_{i}, \mathbf{y}_{i} ; \mathbf{w}\right)-\frac{1}{K} \sum_{i=1}^{n} v_{i}\right)å…¶ä¸­ $v\in\{0,1 \}$ã€‚åé¡¹çš„æ­£åˆ™é¡¹ä¸º $f(\mathbf{v} ; k)=-\frac{1}{k}|\mathbf{v}|_{1}=-\frac{1}{k} \sum_{i=1}^{n} v_{i}$ å¾—å‡ºçš„è§£ä¸ºï¼š v_{i}^{*}=\left\{\begin{array}{ll}{1} & {\frac{1}{m} \sum_{j=1}^{m} C \ell_{i j}]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯31]]></title>
    <url>%2F2019%2F07%2F26%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D31%2F</url>
    <content type="text"><![CDATA[æ»å·è¥¿æ¶§[å”] éŸ¦åº”ç‰©ç‹¬æ€œå¹½è‰æ¶§è¾¹ç”Ÿï¼Œä¸Šæœ‰é»„é¹‚æ·±æ ‘é¸£ã€‚æ˜¥æ½®å¸¦é›¨æ™šæ¥æ€¥ï¼Œé‡æ¸¡æ— äººèˆŸè‡ªæ¨ªã€‚]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä½³å¥åˆ†äº«3]]></title>
    <url>%2F2019%2F07%2F26%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E4%BD%B3%E5%8F%A5%E5%88%86%E4%BA%AB3%2F</url>
    <content type="text"><![CDATA[1ä¸–ç•Œä¸Šåªæœ‰ä¸€ç§çœŸæ­£çš„è‹±é›„ä¸»ä¹‰ï¼Œå°±æ˜¯è®¤æ¸…äº†ç”Ÿæ´»çš„çœŸç›¸åè¿˜ä¾ç„¶çƒ­çˆ±å®ƒã€‚ 2å® è¾±ä¸æƒŠï¼Œé—²çœ‹åº­å‰èŠ±å¼€èŠ±è½ï¼›å»ç•™æ— æ„ï¼Œæ¼«éšå¤©å¤–äº‘å·äº‘èˆ’ã€‚]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä½³å¥åˆ†äº«2]]></title>
    <url>%2F2019%2F07%2F07%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E4%BD%B3%E5%8F%A5%E5%88%86%E4%BA%AB2%2F</url>
    <content type="text"><![CDATA[1Learn to be ordinary before you wish to be extraordinary.]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡24]]></title>
    <url>%2F2019%2F07%2F07%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8724%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨è®ºæ–‡ï¼š Curriculum Learning for Multi-Task Classiï¬cation of Visual Attributes Highway Networks Deep Residual Learning for Image Recognition Gated Feedback Recurrent Neural Networks Densely Connected Convolutional Networks [Curriculum Learning for Multi-Task Classiï¬cation of Visual Attributes]å¤§è‡´æ€è·¯æ˜¯ï¼šå°†æ•°æ®åˆ†ä¸ºä¸¤ä¸ªtaskï¼Œå…ˆè®­ç»ƒå¼ºç›¸å…³çš„taskï¼Œç„¶åå†è®­ç»ƒå¼±ç›¸å…³çš„taskã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºvisual attributeï¼Œå…ˆè®­ç»ƒå¼ºç›¸å…³labelï¼Œç„¶åå†è®­ç»ƒå¼±ç›¸å…³çš„labelã€‚ å…·ä½“å½¢å¼ï¼š å¦‚ä½•è®¡ç®—ç›¸å…³æ€§ï¼š p_{i}=\sum_{j=1, j \neq i}^{T} \frac{\operatorname{cov}\left(y_{t_{i}}, y_{t_{j}}\right)}{\sigma\left(y_{t_{i}}\right) \sigma\left(y_{t_{j}}\right)}, i=1, \ldots, Ttop 50%æ˜¯å¼ºç›¸å…³çš„ï¼Œå‰©ä¸‹éƒ½æ˜¯å¼±ç›¸å…³çš„ã€‚ æˆ‘çš„ç†è§£æ˜¯ï¼Œä¼¼ä¹æ˜¯å°†labelç»™åˆ‡åˆ†äº†ï¼š [Highway Networks]åŸå…ˆçš„æ™®é€šlayerï¼š \mathbf{y}=H\left(\mathbf{x}, \mathbf{W}_{\mathbf{H}}\right)ä¸ºäº†èƒ½å¤Ÿè®­ç»ƒå¾ˆæ·±çš„ç½‘ç»œï¼Œæ”¹æˆï¼š \mathbf{y}=H\left(\mathbf{x}, \mathbf{W}_{\mathbf{H}}\right) \cdot T\left(\mathbf{x}, \mathbf{W}_{\mathbf{T}}\right)+\mathbf{x} \cdot C\left(\mathbf{x}, \mathbf{W}_{\mathbf{C}}\right)ä¹Ÿå³å¢åŠ ä¸¤ä¸ªä»¿å°„å˜æ¢ã€‚å…¶ä¸­Tæ˜¯transformer gateï¼Œè€ŒCæ˜¯carry gateã€‚è¿™æ ·èƒ½å¤Ÿæœ‰åŠ©äºæ¨¡å‹çš„ä¼˜åŒ–ã€‚ ç®€å•èµ·è§ï¼š \mathbf{y}=H\left(\mathbf{x}, \mathbf{W}_{\mathbf{H}}\right) \cdot T\left(\mathbf{x}, \mathbf{W}_{\mathbf{T}}\right)+\mathbf{x} \cdot\left(1-T\left(\mathbf{x}, \mathbf{W}_{\mathbf{T}}\right)\right)åœ¨Tä¸­ï¼Œå¯ä»¥è®¾biasä¸ºè´Ÿï¼Œä¹Ÿå³ä¸€å¼€å§‹åå‘carryçš„è¡Œä¸ºã€‚ [Deep Residual Learning for Image Recognition] æœ¬è´¨æ˜¯æ®‹å·®æ¯”ç›´æ¥å­¦ä¹ xçš„å˜æ¢æ›´å®¹æ˜“ã€‚å¯¹æ¨¡å‹çš„ä¼˜åŒ–æ›´å®¹æ˜“ã€‚ if an identity mapping were optimal, it would be easier to push the residual to zero than to ï¬t an identity mapping by a stack of nonlinear layers å¯å¦ç†è§£highway networkæ˜¯ResNetçš„è¿›é˜¶ç‰ˆï¼Ÿ [Gated Feedback Recurrent Neural Networks] å¯¹äºå¤šå±‚çš„RNNï¼Œç¬¬lå±‚çš„time stepä¸ºtçš„hï¼Œå¯ä»¥æ¥æ”¶ä¸Šä¸€ä¸ªtime stepçš„å¤§äºlå±‚çš„ä¹Ÿæ¥æ”¶å°äºlå±‚çš„hidden stateã€‚ [Densely Connected Convolutional Networks]åœ¨ä¸€ä¸ªblockå†…ï¼Œæ¯å±‚éƒ½è¿æ¥åˆ°åé¢çš„å±‚ã€‚ æ³¨æ„åˆ°ï¼Œåœ¨è¿æ¥å½¢å¼ä¸Šï¼Œresnetæ˜¯ç›¸åŠ ï¼š \mathbf{x}_{\ell}=H_{\ell}\left(\mathbf{x}_{\ell-1}\right)+\mathbf{x}_{\ell-1}è€ŒDenseNetåˆ™æ˜¯concatï¼š \mathbf{x}_{\ell}=H_{\ell}\left(\left[\mathbf{x}_{0}, \mathbf{x}_{1}, \dots, \mathbf{x}_{\ell-1}\right]\right)è¿™é‡Œçš„Hæ˜¯Bn-Relu-Convã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³äºPytorchä¸­infå¯¼æ•°çš„nané—®é¢˜]]></title>
    <url>%2F2019%2F07%2F03%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8EPyTorch%E4%B8%ADinf%E5%AF%BC%E6%95%B0%E7%9A%84nan%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[æƒ³è¦å®ç°ä¸€ä¸ªåŠŸèƒ½ã€‚å°†Transformerä¸­å¤šå±‚çš„attentionçŸ©é˜µåŠ æƒå¹³å‡ã€‚ä¹Ÿå³ï¼š 123456789# pre_attn shape: batch_size*n_head,6,target_len,source_lenself.attn_alpha = Parameter(torch.zeros(1, 6))normalized_alpha = F.softmax(self.attn_alpha, dim=-1) # 1,6normalized_alpha = normalized_alpha.reshape(1, -1, 1, 1) # 1,6,1,1weighted_attn = prev_attn * normalized_alphaweighted_attn = weighted_attn.sum(dim=1)... è·å¾—çš„weighted_attnå°±æ˜¯æ‰€æœ‰å±‚çš„attentionçŸ©é˜µçš„åŠ æƒå¹³å‡ã€‚å†å°†weighted_attnç”¨äºåé¢çš„è®¡ç®—ã€‚å…¶ä¸­attn_alphaæ˜¯å¯å­¦ä¹ çš„å‚æ•°ã€‚ åŠŸèƒ½å¾ˆç®€å•ï¼Œä½†åœ¨backwardå®Œåï¼Œattn_alphaå°±ä¼šä¸€ä¸‹å­è·³åˆ°nanã€‚æŒ‰ç†è¯´ï¼Œè™½ç„¶attnçŸ©é˜µé‡Œé¢å­˜åœ¨-infï¼Œä½†åªè¦æ˜¯åŒä¸€ä¸ªbatchï¼Œinfå­˜åœ¨çš„ç´¢å¼•ä½ç½®åº”è¯¥éƒ½æ˜¯ä¸€æ ·çš„ï¼Œå³ä½¿åŠ æƒæ±‚å’Œä¹Ÿä¸ä¼šå¯¼è‡´æŸä¸€è¡Œå…¨ä¸º-infï¼Œä½¿å¾—åœ¨softmaxåå­˜åœ¨nançš„æƒ…å†µã€‚ åœ¨èˆªæ€»æ’æŸ¥äº†ä¸€æ™šä¸Šåï¼Œä¹Ÿå°è¯•äº†å„ç§å‡è®¾ï¼Œæœ€ç»ˆå‘ç°ï¼Œæ˜¯å› ä¸ºæ¢¯åº¦å›ä¼ æ—¶çš„é—®é¢˜ã€‚ä¹Ÿå³å½“weighted_attn = prev_attn * normalized_alphaè¿™å¥ä»£ç æ¢¯åº¦å›ä¼ çš„æ—¶å€™ï¼Œç”±äºå­˜åœ¨â€™-infâ€™çš„å€¼ï¼Œalphaçš„æ¢¯åº¦å°±ä¼šæœ‰nanï¼ˆå› ä¸ºä¸Šå¥ä»£ç ä¸­alphaçš„å¯¼æ•°æ˜¯prev_attnï¼Œå½“prev_attnå­˜åœ¨infæ—¶ï¼Œåˆ™gradåˆ™ä¸ºnanã€‚ è§£å†³æ–¹æ¡ˆæ˜¯ï¼Œé¦–å…ˆè·å¾—attentionçŸ©é˜µçš„maskï¼Œæ¥ç€ä½¿ç”¨masked_fillå°†infçš„éƒ¨åˆ†ç½®ä¸º0ï¼Œå†å’Œalphaç›¸ä¹˜ï¼Œæ­¤æ—¶å°±ä¸ä¼šæœ‰nançš„æƒ…å†µå‡ºç°äº†ã€‚ä¹Ÿå³ï¼š 123456789101112# pre_attn shape: batch_size*n_head,6,target_len,source_lenself.attn_alpha = Parameter(torch.zeros(1, 6))normalized_alpha = F.softmax(self.attn_alpha, dim=-1) # 1,6normalized_alpha = normalized_alpha.reshape(1, -1, 1, 1) # 1,6,1,1# ----å¤šåŠ è¿™å‡ è¡Œ --- #_attn_mask = prev_attn == float('-inf') # -infçš„ä½ç½®ä¸º1new_pre_attn = pre_attn.data.masked_fill(_attn_mask,0) # å°†-infå¡«å……ä¸º0# --------------- # weighted_attn = new_pre_attn * normalized_alphaweighted_attn = weighted_attn.sum(dim=1) æ€»ç»“èµ·æ¥ï¼Œåˆ™æ˜¯ï¼Œå½“tensorå­˜åœ¨infæ—¶ï¼Œä¸å®ƒç›¸ä¹˜çš„tensorå¦‚æœæ˜¯å¯æ›´æ–°çš„ï¼Œåˆ™è¯¥tensorçš„gradä¸ºnanã€‚æ‰€ä»¥åœ¨å¤„ç†æœ‰infçš„tensorè¦ç‰¹åˆ«æ³¨æ„ï¼Œå¯èƒ½å‡ºç°ç›¸ä¹˜åæ¢¯åº¦å›ä¼ gradä¸ºnançš„æƒ…å†µï¼Œè¿˜æœ‰ä¸€ç§æƒ…å†µåˆ™æ˜¯è‹¥æŸä¸€è¡Œå…¨ä¸ºinfï¼Œè¿‡softmaxååˆ™ä¹Ÿä¼šå‡ºç°nanã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>é‡åˆ°çš„é—®é¢˜</tag>
        <tag>Pytorch</tag>
        <tag>nan</tag>
        <tag>inf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†24]]></title>
    <url>%2F2019%2F06%2F30%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8624%2F</url>
    <content type="text"><![CDATA[[Homebrew]Homebrewå®‰è£…é‡åˆ° Permission denied @ dir_s_mkdir No need to chown the whole /usr/local if brew only fails to create a single directory.For example, I fixed this error:Permission denied @ dir_s_mkdir - /usr/local/FrameworksWith this command:sudo install -d -o $(whoami) -g admin /usr/local/Frameworks https://gist.github.com/irazasyed/7732946 [Alfred]mac ä¸Š QQ ä¼šé˜»æ­¢ Alfred é”å±åŠŸèƒ½ï¼Œè¿™æ˜¯å› ä¸ºå¿«æ·é”®å†²çªï¼Œå–æ¶ˆæŸ¥çœ‹è”ç³»äººçš„å¿«æ·é”®å³å¯ã€‚ https://www.v2ex.com/t/477934 [Autodiff]Autodiffæœ‰ä¸¤ç§æ¨¡å¼ï¼Œforwardå’Œreverseã€‚å½“å‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶éƒ½ç”¨çš„reverseã€‚ https://blog.csdn.net/aws3217150/article/details/70214422]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Homebrew</tag>
        <tag>Alfred</tag>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL Lecture 8:Imitation Learning]]></title>
    <url>%2F2019%2F06%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FDRL%20Lecture%208%3A%20Imitation%20Learning%2F</url>
    <content type="text"><![CDATA[è®¨è®ºäº†åœ¨æ²¡æœ‰rewardçš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•åˆ©ç”¨expertæ¥è¿›è¡ŒRLã€‚ é—®é¢˜å®šä¹‰ï¼šåœ¨ä¸€äº›é—®é¢˜ä¸Šæ˜¯æ²¡æœ‰rewardçš„ï¼Œç»™å®šä¸€äº›expertçš„demonstration exampleï¼Œå¦‚ä½•åˆ©ç”¨è¿™äº›exampleä½¿å¾—æœºå™¨èƒ½å¤Ÿå­¦ä¹ ï¼Ÿ Behavior Cloningæœ¬è´¨å°±æ˜¯ç›‘ç£å­¦ä¹ ï¼Œç»™å®šè®­ç»ƒæ•°æ®ï¼Œè¦æ¨¡å‹è¾“å…¥sèƒ½å¤Ÿè·å¾—å°½é‡å’Œexpertç›¸ä¼¼çš„actionã€‚ ç”±äºexpert exampleæ˜¯è¾ƒå°‘çš„ï¼Œæœºå™¨å¯èƒ½é‡åˆ°æ²¡é‡åˆ°çš„æƒ…å†µã€‚åŒæ—¶ç”±äºæœºå™¨çš„capacityæ˜¯æœ‰é™çš„ï¼Œå¯èƒ½é€‰æ‹©æ— å…³çš„è¡Œä¸ºå»å­¦ä¹ ã€‚è¿˜æœ‰å¯èƒ½å¸¦æ¥ç”±äºè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®çš„åˆ†å¸ƒä¸åŒå¯¼è‡´çš„é—®é¢˜ã€‚å› ä¸ºRLæœ‰åºåˆ—æ€§ï¼Œå¦‚æœä½¿ç”¨Behavior Cloningï¼Œåœ¨æŸä¸ªstateä¸‹é‡‡ç”¨äº†ä¸åŒçš„actionï¼Œåˆ™ä¹‹åçš„stateéƒ½ä¼šå®Œå…¨ä¸åŒï¼ˆå¤±ä¹‹æ¯«å˜è°¬ä»¥åƒé‡Œï¼‰ Inverse Reinforcement Learning (IRL)é€šè¿‡expert exampleæ¥å­¦ä¹ reward functionï¼Œåœ¨å­¦ä¹ å®Œreward functionåè®©agentä¸ç¯å¢ƒäº¤äº’è·å¾—agent exampleã€‚æ¥ç€è°ƒæ•´reward functionä½¿å¾—expert exampleä¸€å®šå¤§äºagentçš„exampleã€‚ä¸æ–­å¾ªç¯ã€‚è¿™å’ŒGANçš„æ€æƒ³æœ‰ç‚¹åƒï¼š]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>å¼ºåŒ–å­¦ä¹ </tag>
        <tag>RL</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Deep Reinforcement Learning</tag>
        <tag>æå®æ¯…å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Imitation Learning</tag>
        <tag>IRL</tag>
        <tag>Inverse Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯30]]></title>
    <url>%2F2019%2F06%2F30%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D30%2F</url>
    <content type="text"><![CDATA[æ°´é¾™åŸ Â· ç™»å»ºåº·èµå¿ƒäº­[å®‹] è¾›å¼ƒç–¾æ¥šå¤©åƒé‡Œæ¸…ç§‹ï¼Œæ°´éšå¤©å»ç§‹æ— é™…ã€‚é¥å²‘è¿œç›®ï¼ŒçŒ®æ„ä¾›æ¨ï¼Œç‰ç°ªèºé«»ã€‚è½æ—¥æ¥¼å¤´ï¼Œæ–­é¸¿å£°é‡Œï¼Œæ±Ÿå—æ¸¸å­ã€‚æŠŠå´é’©çœ‹äº†ï¼Œæ å¹²æ‹éï¼Œæ— äººä¼šã€ç™»ä¸´æ„ã€‚ä¼‘è¯´é²ˆé±¼å ªè„ï¼Œå°½è¥¿é£ã€å­£é¹°å½’æœªï¼Ÿæ±‚ç”°é—®èˆï¼Œæ€•åº”ç¾è§ï¼Œåˆ˜éƒæ‰æ°”ã€‚å¯æƒœæµå¹´ï¼Œå¿§æ„é£é›¨ï¼Œæ ‘çŠ¹å¦‚æ­¤ã€‚å€©ä½•äººå”¤å–ï¼Œçº¢å·¾ç¿ è¢–ï¼Œæ¾è‹±é›„æ³ªã€‚ å€©ï¼ˆqÃ¬ngï¼‰ï¼šè¯·æ‰˜ã€‚æ¾ï¼ˆwÃ¨nï¼‰ï¼šæ“¦æ‹­ã€‚ æ»¡æ±Ÿçº¢[å®‹] å²³é£æ€’å‘å†²å† ï¼Œå‡­æ å¤„ã€æ½‡æ½‡é›¨æ­‡ã€‚æŠ¬æœ›çœ¼ï¼Œä»°å¤©é•¿å•¸ï¼Œå£®æ€€æ¿€çƒˆã€‚ä¸‰ååŠŸåå°˜ä¸åœŸï¼Œå…«åƒé‡Œè·¯äº‘å’Œæœˆã€‚è«ç­‰é—²ï¼Œç™½äº†å°‘å¹´å¤´ï¼Œç©ºæ‚²åˆ‡ï¼é–åº·è€»ï¼ŒçŠ¹æœªé›ªã€‚è‡£å­æ¨ï¼Œä½•æ—¶ç­ï¼Ÿé©¾é•¿è½¦ã€è¸ç ´è´ºå…°å±±ç¼ºï¼å£®å¿—é¥¥é¤èƒ¡è™è‚‰ï¼Œç¬‘è°ˆæ¸´é¥®åŒˆå¥´è¡€ã€‚å¾…ä»å¤´ã€æ”¶æ‹¾æ—§å±±æ²³ï¼Œæœå¤©é˜™ï¼]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡23]]></title>
    <url>%2F2019%2F06%2F29%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8723%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨è®ºæ–‡ï¼š Reinforcement Learning based Curriculum Optimization for Neural Machine Translation Curriculum Dropout Self-Paced Curriculum Learning Learning the Easy Things First: Self-Paced Visual Category Discovery Curriculum Learning and Minibatch Bucketing in Neural Machine Translation Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks LEARNING TO TEACH Learning to learn by gradient descent by gradient descent Curriculum Learning of Multiple Tasks [Reinforcement Learning based Curriculum Optimization for Neural Machine Translation]ä½¿ç”¨RLæ¥è¿›è¡Œå­¦ä¹ curriculum learningï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å„å¼å„æ ·çš„æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼ˆä¹Ÿå³é«˜æ•ˆåˆ©ç”¨noiseå¾ˆå¤§çš„æ•°æ®é›†ï¼Œå¦‚Paracrawl)ã€‚ ä½¿ç”¨ä¸€ä¸ªæŒ‡æ ‡CDSæ¥å°†æ•°æ®åˆ‡åˆ†ï¼š s(e, f)=\log p_{\theta_{c}}(f | e)-\log p_{\theta_{n}}(f | e)$e$å’Œ$f$æ˜¯ç¿»è¯‘å¯¹ã€‚å…¶ä¸­$\theta_c$æ˜¯åœ¨å¯ä¿¡ä»»çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼›$\theta_n$åœ¨noisy corpusä¸Šè®­ç»ƒçš„ï¼ˆæ¯”å¦‚Paracrawl)ã€‚ RLçš„å‡ ä¸ªåŸºæœ¬å› ç´ ï¼š Observation Engineeringï¼šthe observation is the vector containing sentence-level log-likelihoods produced by the NMT system for this prototype batch å‚è€ƒï¼ˆReinforced co-training.ï¼‰ Reward Engineeringï¼šThe reward is a function of the log-likelihood of the development set of interest. Actionï¼šå°†æ•°æ®é›†åˆ†ä¸ºå¤šä¸ªbinï¼Œactionå°±æ˜¯é€‰æ‹©åœ¨å“ªä¸ªbiné‡Œé¢é€‰æ‹©æ•°æ®é›†ã€‚ [Curriculum Dropout]é¡¾åæ€ä¹‰ï¼Œå°±æ˜¯é€æ¸å¢åŠ dropout rateã€‚ [Self-Paced Curriculum Learning]å°†self-paced learningä¸curriculum learningç»“åˆã€‚ Curriculum Learningé€šè¿‡å…ˆéªŒçŸ¥è¯†å¯¹exampleçš„éš¾åº¦è¿›è¡Œé¢„å®šä¹‰ï¼Œç„¶åæŒ‰ç…§å…ˆåé¡ºåºè®­ç»ƒæ¨¡å‹ï¼› è¿™æ˜¯ä¸€ç§Instructor-drivençš„è®­ç»ƒæ–¹æ³•ï¼›å¹¶ä¸è€ƒè™‘learnerçš„feedback Self-paced Learningæ˜¯ç›´æ¥å°†ç›®æ ‡å‡½æ•°å’Œcurriculumä¸€èµ·ç»“åˆèµ·æ¥ï¼Œä¹Ÿå³åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­æ ¹æ®æ¨¡å‹çš„å­¦ä¹ æƒ…å†µï¼ˆlossï¼‰è°ƒæ•´curriculumã€‚çµæ´»ï¼Œä½†ä¸è€ƒè™‘å…ˆéªŒçŸ¥è¯†ã€‚ \min _{\mathbf{w}, \mathbf{v} \in[0,1]^{n}} \mathbb{E}(\mathbf{w}, \mathbf{v} ; \lambda)=\sum_{i=1}^{n} v_{i} L\left(y_{i}, f\left(\mathbf{x}_{i}, \mathbf{w}\right)\right)-\lambda \sum_{i=1}^{n} v_{i}å½“losså°äº$\lambda$æ—¶ï¼Œå°±è¢«è®¤ä¸ºæ˜¯easy sampleï¼Œ$v=1$ï¼›è‹¥å¤§äº$\lambda$åˆ™$v=0$ã€‚ Î» controls the pace at which the model learns new samples, and physically Î» corresponds to the â€œageâ€ of the model ç”±äºå­¦ä¹ è¿‡ç¨‹å®Œå…¨è¢«lossä¸»å¯¼ï¼Œå› æ­¤å¯èƒ½ä¼šoverfittingã€‚ Self-paced Curriculum Learningåœ¨äºŒè€…åŸºç¡€ä¸Šç»“åˆï¼Œåœ¨SPLçš„æ¡†æ¶ä¸‹å¼•å…¥CLçš„å…ˆéªŒçŸ¥è¯†ã€‚ä¹Ÿå³æ—¢è€ƒè™‘äº†å…ˆéªŒçŸ¥è¯†ï¼Œåˆè€ƒè™‘äº†æ¨¡å‹å­¦ä¹ çš„åé¦ˆï¼ˆlossï¼‰ã€‚ \min _{\mathbf{w}, \mathbf{v} \in[0,1]^{n}} \mathbb{E}(\mathbf{w}, \mathbf{v} ; \lambda, \Psi)=\sum_{i=1}^{n} v_{i} L\left(y_{i}, g\left(\mathbf{x}_{i}, \mathbf{w}\right)\right)+f(\mathbf{v} ; \lambda),\text { s.t. } \mathbf{v} \in \Psiå…¶ä¸­$\Psi$ä»£è¡¨äº†é¢„å®šä¹‰çš„curriculumçš„é›†åˆã€‚ ç›¸å½“äºCLæä¾›äº†ä¸€ä¸ªå¼±sampleçš„é¡ºåºï¼Œå»ºè®®æ¨¡å‹è¦å…ˆå­¦å“ªäº›ï¼Œä½†ä»–æœ‰è‡ªç”±å»è°ƒæ•´å­¦ä¹ ç›®æ ‡ã€‚æˆ‘çš„ç†è§£æ˜¯ï¼Œ$\Psi$æ˜¯å¯ä»¥åŠ¨æ€å¢å¤§çš„ã€‚ä½†æ˜¯æ¨¡å‹æ˜¯å¦è¦å°†é›†åˆé‡Œçš„exampleç”¨äºè®­ç»ƒè¿˜æ˜¯è¦çœ‹ä¸Šè¿°çš„ç›®æ ‡å‡½æ•°çš„ï¼Œlosså°çš„æ—¶å€™ä»£è¡¨çš„easy exampleï¼Œä¼šç”¨äºå­¦ä¹ ã€‚ è¿™æ˜¯CL/SPL/SPCLçš„åŒºåˆ«ï¼š æ€è€ƒï¼š $\lambda$ä»£è¡¨äº†æ¨¡å‹çš„æˆç†Ÿåº¦ï¼Œæ§åˆ¶çš„æ˜¯æ¨¡å‹è‡ªèº«çš„competenceï¼›è€Œ$\Psi$é›†åˆå¤§å°ä»£è¡¨äº†instructorè®¤ä¸ºæ¨¡å‹çš„æˆç†Ÿåº¦ã€‚è¿™äºŒè€…çš„ç»“åˆèƒ½å¤Ÿè®©æ¨¡å‹æ›´çµæ´»ã€‚ä½†æ˜¯competenceè¿˜æ˜¯éœ€è¦ä¸€ä¸ªæ‰‹åŠ¨çš„scheduleã€‚ [Learning the Easy Things First: Self-Paced Visual Category Discovery]å°†self-pacedçš„æ€è·¯åº”ç”¨äºvisual category discoveryã€‚ä¸ä¼ ç»Ÿself-paced learningä¸åŒçš„æ˜¯ï¼Œå¹¶æ²¡æœ‰å°†self-pacedç»‘å®šåœ¨loss functionä¸Šã€‚ æ¯ä¸€æ¬¡è®¡ç®—ä¸¤ä¸ªæŒ‡æ ‡ objectnesså’Œcontext-awarenessã€‚å°†æœ€ç®€å•çš„é€‰å‡ºæ¥è®­ç»ƒï¼Œç„¶åå†è®¡ç®—æŒ‡æ ‡ï¼Œå†é€‰å‡ºç®€å•çš„è®­ç»ƒã€‚ä¸æ–­å¾ªç¯ã€‚ self-pacedä¸curriculum learningä¸åŒï¼Œæ²¡æœ‰ä¸€ä¸ªå›ºå®šçš„teacheræ¥åˆ¤æ–­éš¾æ˜“ç¨‹åº¦ï¼Œæ ¹æ®æ¯æ¬¡è‡ªå·±å­¦ä¹ çš„è¿›ç¨‹æ¥åˆ¤æ–­éš¾åº¦ã€‚æœ¬æ–‡ç›¸è¾ƒä¼ ç»Ÿself-pacedä¸åŒï¼Œå› ä¸ºå°†losså’Œå¯¹éš¾æ˜“ç¨‹åº¦çš„åˆ¤æ–­ä¸¤ä¸ªæ­¥éª¤åˆ†ç¦»å¼€æ¥ã€‚ [Curriculum Learning and Minibatch Bucketing in Neural Machine Translation]åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šåšäº†ä¸€äº›è®­ç»ƒæ–¹æ³•ä¸Šçš„ç»„åˆå°è¯•ï¼Œç»™å‡ºäº†ä¸€äº›ç»“è®ºã€‚ Minibatch Bucketingé¦–å…ˆæ˜¯å°è¯•äº†åœ¨åŒä¸€ä¸ªbatché‡Œä¸ä»…å¥å­é•¿åº¦ç›¸åŒï¼ˆåŠ é€Ÿè®­ç»ƒï¼‰ï¼Œè¿˜å¸Œæœ›åŒä¸€ä¸ªbatchå†…éƒ¨æœ‰æŸç§linguisticçš„ä¿¡æ¯(sentence length, number of coordinating conjunctions, number of nouns, number of proper nouns and the number of verbs in the training data pairs)ã€‚å¯èƒ½ç”±äºä»–é€‰çš„è¿™äº›linguisticå¹¶ä¸å¥½ï¼Œæœ€ç»ˆå¹¶æ²¡æœ‰å‘ç°ç»“æœçš„æå‡ã€‚ Curriculum Learningå¯¹curriculum learningè¿›è¡Œäº†æ”¹è¿›ï¼Œå®é™…ä¸Šæ™®é€šçš„CLé—´æ¥åœ°å¼ºè°ƒäº†easier exampleï¼Œå› ä¸ºä»–ä»¬è¢«sampleäº†å¤šæ¬¡ï¼Œæ‰€ä»¥è¿™é‡Œé‡‡ç”¨äº†ä¸€ç§æ–°çš„æ–¹æ³•èƒ½å¤Ÿè®©æ¯ä¸ªexampleåœ¨ä¸€ä¸ªepochéƒ½åªè¢«sampleä¸€æ¬¡ã€‚ æŒ‰ç…§éš¾åº¦å°†æ ·ä¾‹åˆ†ä¸ºå‡ ä¸ªbinï¼Œé¦–å…ˆä»æœ€ç®€å•çš„binå¼€å§‹å–æ ·ä¾‹ï¼Œç›´åˆ°è¯¥binçš„å‰©ä½™æ ·ä¾‹ä¸ªæ•°å’Œç¬¬äºŒä¸ªbinçš„æ ·ä¾‹ä¸€æ ·ï¼Œç„¶åä»è¿™ä¸¤ä¸ªbinå‰©ä¸‹çš„æ ·ä¾‹ä¸­å–æ ·ä¾‹ï¼Œç›´åˆ°å‰©ä¸‹å’Œç¬¬ä¸‰ä¸ªbinæ ·ä¾‹ä¸ªæ•°ä¸€æ ·ã€‚è¿™æ ·èƒ½ä¿è¯åœ¨ä¸€ä¸ªepochå†…æ¯ä¸ªexampleçš„æ¦‚ç‡æ˜¯ä¸€è‡´çš„ã€‚ å¦‚ä½•åˆ¤æ–­éš¾æ˜“ç¨‹åº¦ï¼Ÿé•¿åº¦ï¼Œè¯çš„é¢‘ç‡ç­‰ å®éªŒç»“æœâ‘ é¦–å…ˆï¼Œåœ¨è®­ç»ƒå®Œä¸€ä¸ªepochåï¼Œä½¿ç”¨CLç›¸æ¯”æ²¡ä½¿ç”¨CLæœ‰æå‡ï¼š â‘¡åœ¨ä¸€ä¸ªepochå†…çš„è®­ç»ƒæ›²çº¿ï¼š å¯ä»¥çœ‹åˆ°ç”¨CLçš„åœ¨æ¯æ¬¡æ–°åŠ exampleåéƒ½ä¼šæœ‰ä¸€ä¸ªé™¡å³­çš„è·³è·ƒã€‚ç‰¹åˆ«è¦æ³¨æ„æŒ‰ç…§sourceé•¿åº¦å’ŒæŒ‰ç…§targeté•¿åº¦çš„CLæœ‰å¾ˆå¤§çš„ä¸åŒï¼Œå¯èƒ½æ˜¯æŒ‰ç…§targetçš„CLç»™äº†è®­ç»ƒè¿›ç¨‹ä¸€ä¸ªè¾ƒå¤§çš„penalizationã€‚ â‘¢æ¨¡å‹å¾ˆå®¹æ˜“è¿‡æ‹Ÿåˆrecent exampleï¼Œå› æ­¤å¦‚æœåªæä¾›éš¾ä¸€ç‚¹çš„exampleï¼Œé‚£ä¹ˆåœ¨easyçš„exampleå°±å®¹æ˜“ä¸‹é™ã€‚æ‰€ä»¥éœ€è¦mixing strategyã€‚ ä»CL by target lengthå¯ä»¥çœ‹å‡ºï¼Œé™¡å³­çš„è·³è·ƒè¯´æ˜æ¨¡å‹åœ¨æ–°åŠ å…¥æ•°æ®ä¹‹å‰å¾ˆå¿«åœ°adaptåœ¨çŸ­çš„å¥å­ï¼Œè€Œé•¿å¥ä¸€è¿›æ¥åˆå¾ˆå¿«adaptåˆ°é•¿å¥ã€‚è¿™ç§å¿«é€Ÿè½¬æ¢ä¼¼ä¹è¯´æ˜äº†æ¨¡å‹çš„å¿«é€Ÿé€‚åº”æ€§ã€‚å¦‚æœä¸å›é¡¾ç®€å•çš„å¥å­ï¼Œè§sorted by lengthçš„æ›²çº¿ï¼Œå¯ä»¥çœ‹åˆ°performanceå¾ˆå·®ã€‚åŒæ—¶å¦‚æœreverse CLï¼Œä¹Ÿå³ä¸€å¼€å§‹evenly coveræ‰€æœ‰å¥å­ï¼Œç„¶ååªä½¿ç”¨çŸ­çš„å¥å­ï¼Œé‚£ä¹ˆå¯ä»¥çœ‹åˆ°ä¸€å¼€å§‹æ•ˆæœä¸é”™ï¼Œåˆ°åé¢å°±é™ä½äº†ï¼Œè¿™æ˜¯å› ä¸ºæ¨¡å‹å¿«é€Ÿé€‚åº”äº†ç”ŸæˆçŸ­çš„å¥å­ï¼Œå°±æ²¡æ³•ç”Ÿæˆtesté›†çš„æ­£å¸¸é•¿åº¦çš„å¥å­ã€‚ â‘£æ³¨æ„åˆ°ä»¥ä¸Šéƒ½æ˜¯åœ¨ä¸€ä¸ªepochå†…ï¼ˆä¹Ÿå³è¿‡å®Œäº†ä¸€éè®­ç»ƒæ•°æ®ï¼‰çš„ç»“è®ºã€‚åœ¨è¿™ä¸ªepochåç»§ç»­å‡ ç§è®­ç»ƒæ–¹å¼ï¼ˆåŸºäºâ€˜CL by target lengthâ€™ï¼‰ã€‚ é‡æ–°ä»æœ€ç®€å•çš„å¼€å§‹ï¼ˆsecond epoch of CL by target length)ï¼Œä¼šä¼¤å®³performanceï¼Œä½†åˆ°åé¢è¿˜æ˜¯æœ‰æå‡çš„ã€‚å¦‚æœåœ¨ç¬¬äºŒä¸ªepochç”¨shuffleçš„æ•°æ®è®­ç»ƒï¼Œé‚£ä¹ˆå¯ä»¥çœ‹åˆ°æ˜¯å‡ ä¹æ²¡æœ‰æå‡çš„ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ¨¡å‹å·²ç»é™·å…¥äº†å½“å‰çš„optimumäº†ã€‚ æ€è€ƒä¸ç»“è®ºè¿™é‡Œçš„å›é¡¾å¼çš„CLï¼Œæ— æ³•å‡å°‘è®­ç»ƒæ—¶é—´ï¼Œå› ä¸ºè¦åˆ°æœ€åæ‰èƒ½è·å¾—è¶…è¶Šbaselineçš„performanceã€‚åŒæ—¶å®éªŒè¯æ˜äº†ï¼Œæ¨¡å‹çš„å¿«é€Ÿé€‚åº”æ€§ï¼Œå¾ˆå®¹æ˜“overfittingåˆ°æœ€è¿‘çš„è®­ç»ƒæ ·ä¾‹ä¸Šï¼Œå› æ­¤è¦è®¾è®¡mixing strategyã€‚ [Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks]å¯¹CLåœ¨LSTMçš„è®­ç»ƒçš„å½±å“è¿›è¡Œåˆ†æã€‚å¾—åˆ°ä¸€äº›ç»“è®ºã€‚ å¿«é€Ÿå›é¡¾äº†ä¸¤ç§CLï¼šOne-Pass Curriculumå’ŒBaby Steps Curriculumã€‚ One-Pass Curriculumï¼šå°†è®­ç»ƒæ•°æ®åˆ†ä¸ºå‡ ä¸ªbucketï¼Œç„¶åä»ç®€å•çš„bucketå¼€å§‹ï¼Œè®­ç»ƒå®Œç®€å•çš„bucketä¹‹åè·³åˆ°éš¾çš„bucketã€‚ Baby Steps Curriculumï¼šä»ç®€å•çš„å¼€å§‹ï¼Œä½†åœ¨å¢åŠ éš¾çš„æ•°æ®åï¼Œä¸ä¼šdiscardç®€å•çš„æ•°æ®ã€‚ å®éªŒ&amp;ç»“è®ºâ‘ Baby Stepåœ¨å¤šä¸ªä»»åŠ¡ä¸Šéƒ½æ˜æ˜¾æ›´å¥½ï¼Œå¦‚ï¼Œdigit sum ä¸ä»…ä»…æ˜¯ç»“æœå¥½ï¼ŒåŒæ—¶å…¶varianceä¹Ÿæ›´å°ï¼š â‘¡åœ¨ä¸åŒå¤æ‚åº¦çš„æ¨¡å‹ä¸Šï¼ŒCLæ•ˆæœéƒ½å¥½ï¼Œå½“æ¨¡å‹è¶Šå¤§ï¼Œæ•ˆæœçš„å·®è·ä¼šè¶Šå°ã€‚æ³¨æ„åˆ°ï¼ŒCLåœ¨å‚æ•°æ•°é‡æ›´å°‘çš„æƒ…å†µä¸‹æ•ˆæœæ›´å¥½ã€‚ â‘¢åœ¨æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸Šçš„ç»“æœ é¦–å…ˆæ˜¯ä½¿ç”¨CLæ•ˆæœå¥½ï¼Œå…¶æ¬¡æ˜¯ä½¿ç”¨conjunctionï¼ˆæ˜¯æŒ‡è¿æ¥ä¸¤ä¸ªæƒ…æ„Ÿææ€§ç›¸åå¥å­çš„è¯ï¼ˆbutç­‰ï¼‰ï¼ˆconjunctions where a span of text contradicts or supports overall sentiment polarityï¼‰ï¼‰æ•ˆæœå·®è·æ›´å¤§ã€‚è¯´æ˜ä½¿ç”¨CLä½¿å¾—æ¨¡å‹çš„é²æ£’æ€§æ›´å¼ºã€‚ åŒæ—¶CLåœ¨ä½¿ç”¨ä¸åŒè¯æ¥é¢„æµ‹ï¼Œå…¶è¡¨ç°è¾ƒä¸ºä¸€è‡´ï¼š åŒæ—¶ï¼Œåœ¨æ•°æ®é‡æ›´å°‘çš„æƒ…å†µä¸‹ï¼ŒCLçš„æ•ˆæœè¶Šæ˜æ˜¾ï¼š ç»“è®ºï¼šCLåœ¨æ•°æ®å°‘ï¼Œæ¨¡å‹å°çš„æƒ…å†µä¸‹å¾ˆé‡è¦ã€‚ [LEARNING TO TEACH]é‡‡ç”¨RLçš„æ–¹æ³•æ¥è¿›è¡Œschedule learningã€‚ ä¸»è¦ç»“æ„æ˜¯ï¼Œä¸€ä¸ªteacherå†³å®šç»™å­¦ç”Ÿçš„æ•°æ®ï¼Œä¸€ä¸ªå­¦ç”Ÿé€šè¿‡ç»™å®šçš„æ•°æ®è®­ç»ƒï¼Œå¹¶è·å¾—rewardå’Œstateä½œä¸ºfeedbackè¿”å›ç»™teacherã€‚ teacherçš„ç›®æ ‡æ˜¯æä¾›æ•°æ®ï¼Œloss functionå’Œhypothesis spaceã€‚å®é™…ä¸Šè®ºæ–‡åªè®¨è®ºäº†æ•°æ®çš„æä¾›ã€‚ç›®æ ‡ï¼š \min _{D, L, \Omega} \mathcal{M}\left(\mu(D, L, \Omega), D_{t e s t}\right) RLçš„å‡ ä¸ªè¦ç´ ï¼š actionï¼šéšæœºsampleæ•°æ®ï¼Œç„¶åä»è¿™äº›sampleçš„æ•°æ®é‡Œå†ç­›é€‰å‡ºæ•°æ®ã€‚ä¹Ÿå³å¯¹æ‰€æœ‰sampleçš„æ•°æ®æ‰“æ ‡ç­¾ï¼Œ1ä»£è¡¨ç»™å­¦ç”Ÿmodelè®­ç»ƒï¼Œ0åˆ™è¢«æŠ›å¼ƒæ‰ã€‚ stateï¼šå®é™…ä¸Šå°±æ˜¯ä¸€äº›äººå·¥å®šå¥½çš„featureã€‚æ•°æ®çš„featureï¼Œæ¯”å¦‚label categoryï¼Œå¥å­é•¿åº¦ï¼Œlinguistic featureç­‰ï¼›student modelçš„featureï¼Œä¹Ÿå³ä»£è¡¨äº†å½“å‰NNè¢«è®­ç»ƒå¾—å¤šå¥½çš„featureï¼Œå†å²training losså’Œå†å²çš„validation accuracyç­‰ï¼›è¿˜æœ‰å°±æ˜¯äºŒè€…çš„ç»“åˆï¼Œæ¯”å¦‚predicted probability ï¼›dataçš„lossç­‰ rewardï¼šå’Œstudent modelæ”¶æ•›é€Ÿåº¦ç›¸å…³ï¼Œä¹Ÿå³è®°å½•ç¬¬ä¸€ä¸ªåœ¨æµ‹è¯•é›†ä¸Šå‡†ç¡®ç‡è¶…è¿‡æŸä¸ªé˜ˆå€¼çš„mini-batchçš„ç´¢å¼•ï¼Œç„¶åè®¡ç®—ï¼š r_{T}=-\log \left(i_{\tau} / T^{\prime}\right)è¿™æ˜¯ä¸ºäº†é¼“åŠ±æ—©ç‚¹æ”¶æ•›ã€‚ æœ¬ç¯‡æ–‡ç« æ¡†æ¶è®¾å®šå¾—å¾ˆå¥½ï¼Œä½†å¹¶æ²¡æœ‰è®¨è®ºå¦å¤–ä¸¤ä¸ªã€‚ [Learning to learn by gradient descent by gradient descent]meta-learningçš„ä¸€ç§æ–¹æ³•ã€‚è¢«é¢˜ç›®å¸å¼•ï¼Œå¤§æ¦‚çœ‹äº†çœ‹ã€‚ åˆ©ç”¨LSTMæ¥å­¦ä¹ optimizerçš„æ¢¯åº¦ï¼Œä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½çš„è®­ç»ƒã€‚ å…¶ä¸­è™šçº¿ä¸å›ä¼ ï¼Œå®çº¿å›ä¼ ã€‚ åŒæ—¶ï¼Œæ³¨æ„åˆ°ä¸ºäº†è®©å‚æ•°çš„é¡ºåºå¯¹è¾“å‡ºæ²¡æœ‰å½±å“ï¼Œå› ä¸ºå‡è®¾æ¯ä¸ªå‚æ•°åæ ‡éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œå› æ­¤ä½¿ç”¨separate hidden stateï¼Œä½†LSTMçš„å‚æ•°æ˜¯å…±äº«çš„ï¼Œä¹Ÿå³æ¯ä¸ªè¾“å…¥æ¢¯åº¦éƒ½å•ç‹¬å¤„ç†ã€‚ [Teacher-Student Curriculum Learning]æ²¡ä»”ç»†çœ‹ï¼Œå¤§æ¦‚æ€æƒ³æ˜¯ï¼Œä¸€ä¸ªteacherå¸®å¿™é€‰æ‹©sub-taskè®©studentå­¦ã€‚ stateä»£è¡¨çš„æ˜¯studentçš„æ•´ä¸ªçŠ¶æ€ï¼Œneural network parameters and optimizer state) and is not observable to the Teacher. actionæ˜¯teacheræ‰€é‡‡å–çš„åŠ¨ä½œï¼Œä¹Ÿå³é€‰æ‹©æŸä¸ªtaskï¼› observation æ˜¯åœ¨é€‰æ‹©äº†taskåæ‰€è·å¾—çš„scoreï¼› rewardä¹Ÿå³åœ¨è¯¥timestepçš„scoreçš„æ”¹å˜ $r_{t}=x_{t}^{(i)}-x_{t_{i}^{\prime}}^{(i)}$ CLçš„åœ°æ–¹åœ¨äºä»ç®€å•çš„å­¦èµ·ï¼ˆä¹Ÿå³å¸¦æ¥çš„æ”¹å˜æœ€å¤§çš„taskï¼‰ï¼Œç„¶åå½“å…¶æå‡çš„é€Ÿç‡é™ä½äº†ï¼Œåˆ™é™ä½å…¶sampleçš„æ¦‚ç‡ã€‚ æ€»ç»“èµ·æ¥ï¼ŒCLçš„å‡ ä¸ªåŸåˆ™ï¼š ç†æƒ³åŒ–çš„CLï¼š å½“æŸä¸ªtaskçš„scoreä¸‹é™äº†ï¼Œè¯´æ˜ä»–å¿˜äº†è¿™éƒ¨åˆ†çš„çŸ¥è¯†ï¼Œåˆè¦æå‡è¯¥sampleçš„æ¦‚ç‡ã€‚ [Curriculum Learning of Multiple Tasks]å­¦ä¹ å¤šä¸ªtaskï¼ŒæŒ‰ç…§å…ˆåé¡ºåºæ¥ï¼Œè€Œä¸æ˜¯è”åˆè®­ç»ƒã€‚ä¸Šä¸€ä¸ªtaskå­¦åˆ°çš„weightç”¨äºä¸‹ä¸€ä¸ªtaskçš„åˆå§‹åŒ–ã€‚ è‡ªåŠ¨é€‰æ‹©taské¡ºåºã€‚æˆ‘çš„ç†è§£æ˜¯ï¼Œæ¯å½“è®­ç»ƒå®Œä¸€ä¸ªsubtaskï¼Œæµ‹è¯•æ‰€æœ‰å…¶ä»–subtaskï¼Œé€‰æ‹©è¡¨ç°æœ€å¥½çš„é‚£ä¸ªï¼ˆæŸä¸ªæŒ‡æ ‡ï¼Œå¹³å‡æœŸæœ›è¯¯å·®ï¼‰ï¼Œç„¶åé€‰æ‹©è¯¥subtaskç»§ç»­è®­ç»ƒã€‚]]></content>
      <tags>
        <tag>RL</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>Curriculum Learning</tag>
        <tag>NMT</tag>
        <tag>LSTM</tag>
        <tag>Dropout</tag>
        <tag>meta-learning</tag>
        <tag>multi-task</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯29]]></title>
    <url>%2F2019%2F06%2F23%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D29%2F</url>
    <content type="text"><![CDATA[æ°¸é‡ä¹ Â· äº¬å£åŒ—å›ºäº­æ€€å¤[å®‹] è¾›å¼ƒç–¾åƒå¤æ±Ÿå±±ï¼Œè‹±é›„æ— è§…ï¼Œå­™ä»²è°‹å¤„ã€‚èˆæ¦­æ­Œå°ï¼Œé£æµæ€»è¢«ï¼Œé›¨æ‰“é£å¹å»ã€‚æ–œé˜³è‰æ ‘ï¼Œå¯»å¸¸å··é™Œï¼Œäººé“å¯„å¥´æ›¾ä½ã€‚æƒ³å½“å¹´ã€é‡‘æˆˆé“é©¬ï¼Œæ°”åä¸‡é‡Œå¦‚è™ã€‚å…ƒå˜‰è‰è‰ï¼Œå°ç‹¼å±…èƒ¥ï¼Œèµ¢å¾—ä»“çš‡åŒ—é¡¾ã€‚å››åä¸‰å¹´ï¼Œæœ›ä¸­çŠ¹è®°ï¼Œçƒ½ç«æ‰¬å·è·¯ã€‚å¯å ªå›é¦–ï¼Œä½›è²ç¥ ä¸‹ï¼Œä¸€ç‰‡ç¥é¸¦ç¤¾é¼“ã€‚å‡­è°é—®ï¼Œå»‰é¢‡è€çŸ£ï¼Œå°šèƒ½é¥­å¦ï¼Ÿ]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä½³å¥åˆ†äº«1]]></title>
    <url>%2F2019%2F06%2F23%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E4%BD%B3%E5%8F%A5%E5%88%86%E4%BA%AB1%2F</url>
    <content type="text"><![CDATA[1ä»“ä¿ƒæœ¬èº«å°±æ˜¯æœ€è¦ä¸å¾—çš„æ€åº¦ã€‚å½“ä½ åšæŸä»¶äº‹çš„æ—¶å€™ï¼Œä¸€æ—¦æƒ³è¦æ±‚å¿« ï¼Œå°±è¡¨ç¤ºä½ å†ä¹Ÿä¸å…³å¿ƒå®ƒï¼Œè€Œæƒ³å»åšåˆ«çš„äº‹ â€”ã€Šç¦…ä¸æ‘©æ‰˜è½¦ç»´ä¿®è‰ºæœ¯ã€‹ 2è‡ªå¾‹ä½¿æˆ‘ä»¬ä¸ä¼—ä¸åŒï¼Œè‡ªå¾‹ä»¤æˆ‘ä»¬æ´»å¾—æ›´é«˜çº§ã€‚ä¹Ÿæ­£æ˜¯è‡ªå¾‹ï¼Œä½¿æˆ‘ä»¬è·å¾—æ›´è‡ªç”±çš„äººç”Ÿã€‚å‡å¦‚æˆ‘ä»¬åƒåŠ¨ç‰©ä¸€æ ·ï¼Œå¬ä»æ¬²æœ›ï¼Œé€ƒé¿ç—›è‹¦ï¼Œæˆ‘ä»¬å¹¶ä¸æ˜¯çœŸçš„è‡ªç”±è¡ŒåŠ¨ã€‚æˆ‘ä»¬åªæ˜¯æˆäº†æ¬²æœ›å’Œå†²åŠ¨çš„å¥´éš¶ã€‚æˆ‘ä»¬ä¸æ˜¯åœ¨é€‰æ‹©ï¼Œè€Œæ˜¯åœ¨æœä»ã€‚ä½†äººä¹‹æ‰€ä»¥ä¸ºäººï¼Œ å°±åœ¨äºï¼Œäººä¸æ˜¯è¢«æ¬²æœ›ä¸»å®°ï¼Œè€Œæ˜¯è‡ªæˆ‘ä¸»å®°ã€‚ â€”åº·å¾·]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†23]]></title>
    <url>%2F2019%2F06%2F23%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8623%2F</url>
    <content type="text"><![CDATA[[NAS]å…³äºNAS(Neural Architecture Search)çš„ç§‘æ™®æ–‡ã€‚https://medium.com/@ashiqbuet14/neural-architecture-search-nas-the-future-of-deep-learning-c99356351136 Search spaceï¼šå°±æ˜¯ä¸€ä¸ªæ¥ä¸€ä¸ªçš„layerï¼Œä¹Ÿå¯ä»¥åŒ…æ‹¬skip connectionï¼› å¦‚æœå¸Œæœ›å¤§çš„æ¡†æ¶å®šå¥½ï¼Œåªæ˜¯é‡Œé¢çš„layeræœç´¢ï¼Œè¿™ç§°ä¸ºmicro-searchã€‚ RLï¼šå¯ä»¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥åšNASï¼Œä»¥RNNä¸ºåŸºæœ¬æ¨¡å‹ã€‚ å…¶å®å°±æ˜¯æ¯ä¸ªè¾“å‡ºæŒ‡ç¤ºä¸€ä¸ªlayeré€‰é¡¹ï¼Œç„¶åé€šè¿‡RLè·å¾—çš„rewardæ¥æ›´æ–°ã€‚ Progressive Neural Architecture Search(PNAS)ï¼šè¿™å°±æ˜¯å‰é¢æåˆ°çš„å›ºå®šæ•´ä¸ªå¤§çš„æ¡†æ¶ï¼ˆblockï¼‰ï¼Œç„¶åæœç´¢é‡Œé¢çš„layerã€‚ å¯ä»¥é€šè¿‡æ¯å±‚é€‰å®Œå»æ‰ä¸€äº›é€‰é¡¹æ¥å‡å°‘æ’åˆ—ç»„åˆå·¨å¤§çš„æ€»æ•°ã€‚ Differentiable Architecture Search(DARTS)ï¼šå°†é€‰æ‹©layerçš„discreteçš„åŠ¨ä½œå˜æˆè¿ç»­çš„ï¼Œä½¿å¾—èƒ½å¤Ÿé€šè¿‡æ±‚å¯¼çš„æ–¹å¼æ›´æ–°ã€‚ å…¶æœ¬è´¨å°±æ˜¯ä¸¤ä¸ªnodeä¹‹é—´è¿å¤šä¸ªoperationï¼Œç„¶åè®­ç»ƒè·å¾—æ¯ä¸ªoperationçš„æ¯”ä¾‹ï¼Œåªä¿ç•™æœ€å¤§çš„ã€‚ è¿™ä¸ªç”¨è¿ç»­æ¥è¾¾åˆ°ç¦»æ•£çš„åšæ³•è¿˜æŒºæœ‰åˆ›æ–°çš„ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL Lecture 7:Sparse Reward]]></title>
    <url>%2F2019%2F06%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FDRL%20Lecture%207%3A%20Sparse%20Reward%2F</url>
    <content type="text"><![CDATA[è®¨è®ºäº†å½“RLé‡åˆ°sparse rewardæ—¶çš„å‡ ä¸ªè§£å†³æ–¹æ¡ˆã€‚ Reward Shapinghand-craftedä¹Ÿå³è™šæ„å‡ºrewardå¼•å¯¼agentèµ°å‘è‡ªå·±æœŸæœ›çš„ç»“æœã€‚ å¦‚ä¸Šå›¾ï¼Œä»”ç»†å®šä¹‰äº†æ¸¸æˆä¸­æ¯ä¸ªæ“ä½œçš„rewardã€‚ Curiosityå¾€agenté‡Œæ·»åŠ å¥½å¥‡å¿ƒã€‚ è¾“å…¥æ˜¯$a_t$å’Œ$s_t$å°è¯•é¢„æµ‹å‡º$s_{t+1}$ï¼Œå¦‚æœé¢„æµ‹çš„å’ŒçœŸå®çš„å·®è·è¾ƒå¤§æ—¶ï¼Œåˆ™è¯¥actionçš„rewardå¤§ï¼Œè¿™æ ·èƒ½å¤Ÿé¼“åŠ±agentæ¢ç´¢æ›´å¤šçš„æ“ä½œã€‚ ä½†æœ‰æ—¶å€™éš¾ä»¥é¢„æµ‹çš„stateå¹¶ä¸ä»£è¡¨å…¶é‡è¦ã€‚åº”å½“è¿‡æ»¤æ‰è¿™æ ·çš„stateï¼Œæ¯”å¦‚æ¸¸æˆä¸­æ ‘å¶é£˜åŠ¨ï¼Œä½†è¿™ä¸ªstateå®Œå…¨ä¸é‡è¦ã€‚å› æ­¤å¯¹ä¸Šè¿°æ¨¡å‹è¿›è¡Œæ”¹è¿›ï¼š æ·»åŠ feature extractorï¼ŒåŒæ—¶æ·»åŠ å¦ä¸€ä¸ªç½‘ç»œï¼Œæ¥é€šè¿‡$s_t$å’Œ$s_{t+1}$é¢„æµ‹actionï¼Œè¿™æ ·å°±èƒ½å¤Ÿè¿‡æ»¤æ‰stateä¸­æ²¡æ„ä¹‰çš„éƒ¨åˆ†ã€‚ Curriculum Learningä»ç®€å•çš„å¼€å§‹å­¦èµ·ï¼Œæ¯”å¦‚ç©æ¸¸æˆçš„ä¾‹å­ï¼š è¿™ä¸ªéœ€è¦äººå·¥è¾ƒä¸ºç²¾ç»†çš„è°ƒæ•´ã€‚ Reverse Curriculum Generationé¦–å…ˆç»™å®šä¸€ä¸ªgold stateï¼Œä¹Ÿå³ç›®æ ‡ï¼Œç„¶åå¯»æ‰¾ä¸gold stateæœ€æ¥è¿‘çš„stateè·å¾—ç›¸åº”çš„rewardã€‚ ç„¶åå»æ‰rewardå¤ªå¤§æˆ–å¤ªå°çš„ã€‚åœ¨ç•™ä¸‹æ¥çš„stateä¸­å†è·å–ä¸ä»–ä»¬æ¥è¿‘çš„stateï¼Œç»§ç»­ä»¥ä¸Šæµç¨‹ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>å¼ºåŒ–å­¦ä¹ </tag>
        <tag>RL</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Deep Reinforcement Learning</tag>
        <tag>æå®æ¯…å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Sparse Reward</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL Lecture 6:Actor-Critic]]></title>
    <url>%2F2019%2F06%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FDRL%20Lecture%206%3A%20Actor-Critic%2F</url>
    <content type="text"><![CDATA[ä»‹ç»äº†actor-criticçš„ç®—æ³•ï¼Œç»“åˆäº†policy gradientå’ŒQ-learningã€‚ Actor-CriticåŸå…ˆpolicy gradientçš„ç®—æ³•æ˜¯ç›´æ¥å­¦ä¹ ä¸€ä¸ªpolicyï¼š \nabla \overline{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(\sum_{t^{\prime}=t}^{T_{n}} \gamma^{t^{\prime}-t} r_{t^{\prime}}^{n}-b\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)ä½†æ˜¾ç„¶rewardæ˜¯ä¸ç¨³å®šçš„ï¼Œå®ƒä»£è¡¨äº†é‡‡å–actionä¹‹åçš„rewardçš„æœŸæœ›å€¼ï¼Œå½“sampleæ¬¡æ•°ä¸å¤Ÿå¤šï¼Œå…¶é¢„ä¼°çš„ä¹Ÿä¸å‡†ã€‚ å› æ­¤åœ¨è¿™é‡Œå°†Q-learningå¼•å…¥åˆ°é¢„ä¼°rewardä¸­ï¼Œä¹Ÿå³policy gradientå’Œq-learningçš„ç»“åˆã€‚ ä¹Ÿå³æˆ‘ä»¬å°†rewardæ›¿æ¢æˆ$E\left[G_{t}^{n}\right]=Q^{\pi_{\theta}}\left(s_{t}^{n}, a_{t}^{n}\right)$ã€‚åŒæ—¶æ ¹æ®baselineçš„å®šä¹‰ï¼Œæˆ‘ä»¬å°†å…¶æ›¿æ¢æˆ$V^{\pi_{\theta}}\left(s_{t}^{n}\right)$ã€‚ æ‰€ä»¥æ‹¬å·å†…çš„$\sum_{t^{\prime}=t}^{T_{n}} \gamma^{t^{\prime}-t} r_{t^{\prime}}^{n}-b$å°±å˜æˆ$Q^{\pi \theta}\left(s_{t}^{n}, a_{t}^{n}\right)-V^{\pi_{\theta}}\left(s_{t}^{n}\right)$ã€‚ å®é™…ä¸Šæˆ‘ä»¬ä¸éœ€è¦åˆ†åˆ«è®­ç»ƒä¸¤ä¸ªç½‘ç»œï¼Œç›´æ¥æ•´åˆæˆä¸€ä¸ªç½‘ç»œå³å¯ã€‚ä¹Ÿå³å°†$Q^{\pi \theta}\left(s_{t}^{n}, a_{t}^{n}\right)-V^{\pi_{\theta}}\left(s_{t}^{n}\right)$æ”¹æˆ$r_{t}^{n}+V^{\pi}\left(s_{t+1}^{n}\right)-V^{\pi}\left(s_{t}^{n}\right)$ã€‚ å› æ­¤æ•´ä¸ªæµç¨‹ï¼š å½¢å¼åŒ–ä¹Ÿå³ï¼š \nabla \overline{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(r_{t}^{n}+V^{\pi}\left(s_{t+1}^{n}\right)-V^{\pi}\left(s_{t}^{n}\right)\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)ç”±äº$\pi$å’Œ$V$çš„è¾“å…¥éƒ½æ˜¯$s$ï¼Œåœ¨å®é™…æ“ä½œä¸­å¯ä»¥å°†è¿™ä¸¤ä¸ªç½‘ç»œçš„å‰å‡ å±‚å‚æ•°å…±äº«ï¼š åŒæ—¶å¯¹$\pi$çš„è¾“å‡ºåŠ ä»¥é™åˆ¶ï¼Œå¸Œæœ›æœ‰æ›´å¤§çš„entropyï¼Œè¿™æ ·èƒ½å¤Ÿæ¢ç´¢æ›´å¤šæƒ…å†µã€‚ Pathwise Derivative Policy Gradientæ¥ä¸‹æ¥ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç›´æ¥å­¦ä¹ ä¸€ä¸ª$\pi$ï¼Œè¾“å…¥$s$å¯ä»¥è·å¾—èƒ½å¤Ÿæœ€å¤§åŒ–Qçš„actionã€‚è¿™å’ŒGANçš„æ€æƒ³å¾ˆç›¸ä¼¼ã€‚ è¿™æ ·$\pi$å¤©ç„¶åœ°èƒ½å¤Ÿå¤„ç†continuousçš„æƒ…å†µã€‚ æ‰€ä»¥æ•´ä¸ªæµç¨‹ï¼š å…ˆäº¤äº’ï¼Œå­¦ä¹ ä¸€ä¸ªå¥½çš„$Q$ï¼Œç„¶åå°†è¿™ä¸ª$Q$ä½œä¸ºæ ‡å‡†ï¼Œå­¦ä¹ $\pi$ä½¿å¾—è¾“å‡ºçš„$Q$æœ€å¤§ã€‚å’ŒGANå¾ˆåƒã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>å¼ºåŒ–å­¦ä¹ </tag>
        <tag>RL</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Deep Reinforcement Learning</tag>
        <tag>æå®æ¯…å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Actor-Critic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL Lecture 5:Q-learning (Continuous Action)]]></title>
    <url>%2F2019%2F06%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FDRL%20Lecture%205%3A%20Q-learning%20(Continuous%20Action)%2F</url>
    <content type="text"><![CDATA[è®¨è®ºäº†å¦‚ä½•å°†Q-learningç”¨äºè¿ç»­çš„actionä¸­ã€‚ å‰é¢æåˆ° Q-learningå°±æ˜¯ï¼š a=\arg \max _{a} Q(s, a)è‹¥aæ˜¯è¿ç»­çš„ï¼Œå‡ ç§è§£å†³æ–¹æ¡ˆï¼š â‘ sampleä¸€å †action$\left\{a_{1}, a_{2}, \cdots, a_{N}\right\}$ï¼Œç„¶åæŒ‰ç…§discreteçš„æƒ…å†µæ¥å¤„ç†ã€‚ä½†ç²¾åº¦ä¸é«˜ï¼Œå› ä¸ºæ²¡æ³•sampleå¤ªå¤šæƒ…å†µã€‚ â‘¡ä½¿ç”¨gradient ascentæ¥è®¡ç®—å¤„ç†ä¸Šå¼ã€‚è¯¥æ–¹æ³•æ˜¾ç„¶å¤ªè€—æ—¶ï¼Œå› ä¸ºæ¯ä¸ªsampleéƒ½ç­‰äºè¦è®­ç»ƒä¸€éæ¨¡å‹ã€‚ â‘¢è®¾è®¡ä¸“é—¨çš„ç½‘ç»œä½¿å¾—è¯¥ä¼˜åŒ–å¯è¡Œã€‚é¦–å…ˆè¾“å…¥stateï¼š è·å¾—ä¸€ä¸ª$\mu$ï¼Œ$\Sigma$å’Œ$V$ã€‚æ¥ç€å’Œactionäº¤äº’ï¼š Q(s, a)=-(a-\mu(s))^{T} \Sigma(s)(a-\mu(s))+V(s)æ˜¾ç„¶,ç¬¬ä¸€é¡¹è‹¥$\Sigma$åŠæ­£å®šï¼Œå¿…å®šå°äºç­‰äº0ï¼Œæ‰€ä»¥å½“$a=\mu(s)$æ—¶$Q$æœ€å¤§ã€‚å®é™…ä¸Š$\Sigma$æ˜¯é€šè¿‡å…ˆè·å¾—ä¸€ä¸ªçŸ©é˜µ$A$ï¼Œç„¶å$A\times A^{T}$ä¿è¯å…¶æ­£å®šæ€§ã€‚ å› æ­¤ï¼š \mu(s)=\arg \max _{a} Q(s, a)â‘£åˆ«ç”¨Q-learningå¤„ç†è¿ç»­çš„æƒ…å†µï¼Œå› ä¸ºå¤„ç†è¿˜æ˜¯æ¯”è¾ƒéº»çƒ¦çš„ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>å¼ºåŒ–å­¦ä¹ </tag>
        <tag>RL</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Deep Reinforcement Learning</tag>
        <tag>æå®æ¯…å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Q-learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL Lecture 4:Q-learning (Advanced Tips)]]></title>
    <url>%2F2019%2F06%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FDRL%20Lecture%204%3A%20Q-learning%20(Advanced%20Tips)%2F</url>
    <content type="text"><![CDATA[ä»‹ç»ä¸€äº›è¿›é˜¶çš„Q-learning tipsï¼Œèƒ½å¤Ÿå¸®åŠ©Q-learningæå‡è¡¨ç°ã€‚ Double DQNå‘ç°Q-valueæ€»æ˜¯å®¹æ˜“è¢«é«˜ä¼°ï¼ŒåŸå› æ˜¯ç®—æ³•ä¸­æœ‰$Q\left(s_{t}, a_{t}\right)=r_{t}+\max _{a} Q\left(s_{t+1}, a\right)$ã€‚è¯¥å…¬å¼çš„maxä½¿å¾—$Q$æ€»æ˜¯é€‰æ‹©æœ€å¤§çš„actionï¼Œä½¿å¾—$Q$çš„æ‹Ÿåˆæ€»æ˜¯åå¤§ã€‚ é‚£ä¹ˆåœ¨è¿™é‡Œå¤šåŠ ä¸€ä¸ª$Q^{\prime}$ä»¥è§„é¿ä¸Šè¿°æƒ…å†µï¼Œä¹Ÿå³ï¼š Q\left(s_{t}, a_{t}\right)=r_{t}+Q^{\prime}\left(s_{t+1}, \arg \max _{a} Q\left(s_{t+1}, a\right)\right)è‹¥$Q$é«˜ä¼°äº†aï¼Œ$Q^{\prime}$ä¸é«˜ä¼°é‚£ä¹ˆ$Q^{\prime}$çš„å€¼ä¹Ÿä¸ä¼šé‚£ä¹ˆå¤§åˆ™å·¦å¼çš„å€¼å°±ä¸ä¼šè¢«é«˜ä¼°ï¼›è‹¥$Q^{\prime}$å¯¹æŸä¸ªactioné«˜ä¼°äº†ï¼Œåªè¦$Q$ä¸é«˜ä¼°è¯¥actionï¼Œé‚£ä¹ˆä¹Ÿä¸ä¼šé€‰æ‹©è¯¥actionã€‚ Dueling DQNå°†æ¨¡å‹ç»“æ„åšäº†æ”¹å˜ï¼š ä¹Ÿå³å°†$Q$åˆ†ç¦»å¼€æ¥ã€‚ä¸€ç§è§£é‡Šæ˜¯è¿™æ ·çš„åˆ†ç¦»å¯ä»¥ä½¿å¾—æ•°æ®çš„ä½¿ç”¨æ›´æœ‰æ•ˆç‡ï¼Œä½¿å¾—æ¨¡å‹æ›´ä¸ºçµæ´»ã€‚è¯¾ä»¶ä¸Šè¿˜ä¸¾äº†ä¸€ä¸ªä¾‹å­ã€‚ åŒæ—¶è¿˜å¯ä»¥å¯¹$A$åŠ ä¸€äº›é™åˆ¶ï¼Œæ¯”å¦‚å‘é‡å’Œä¸º0ã€‚ Prioritized Replyå¯¹replay bufferè¿›è¡Œæ”¹è¿›ã€‚å¯¹TD errorè¾ƒå¤§çš„ä¼˜å…ˆsampleï¼Œä¹Ÿå³å¯¹é‚£äº›å­¦å¾—ä¸å¥½çš„exampleä¼˜å…ˆå­¦ä¹ ã€‚ Multi-stepå°†MCå’ŒTDç»¼åˆèµ·æ¥ã€‚ç»™å®š$\left(s_{t}, a_{t}, r_{t}, \cdots, s_{t+N}, a_{t+N}, r_{t+N}, s_{t+N+1}\right)$ï¼Œæœ‰ï¼š ä¹Ÿå³ä»‹äºMCçš„æ•´ä¸ªepisodeå®Œæˆåå†è®¡ç®—å’ŒTDçš„æ¯ä¸ªstepéƒ½è®¡ç®—ä¸€æ¬¡ã€‚ Noisy Netepsilon greedyä¹Ÿå¯ä»¥çœ‹åšæ˜¯åŠ å™ªå£°ï¼Œä½†æ˜¯æ˜¯åŠ åœ¨actionä¸Šï¼š a=\left\{\begin{aligned} \arg \max _{a} Q(s, a), & \text {with probability } 1-\varepsilon \\ \text {random}, & \text { otherwise } \end{aligned}\right.è¿™æ ·ä½¿å¾—æ¨¡å‹çš„è¡Œä¸ºä¸ä¸€è‡´ï¼Œå¯èƒ½ä¸å¤§å¥½ã€‚ è€ŒNoisy Netæ˜¯åœ¨parameterä¸ŠåŠ å™ªå£°ã€‚ä¹Ÿå³åœ¨episodeå¼€å§‹ä¹‹å‰å¯¹$Q$åŠ å™ªå£°ï¼Œå˜æˆ$\tilde{Q}$ï¼š a=\arg \max _{a} \tilde{Q}(s, a)è€Œåœ¨episodeæœŸé—´ä¸ä¼šæ”¹å˜noiseã€‚è¿™æ ·æ›´æœ‰ç³»ç»Ÿæ€§çš„æ¢ç´¢å¯èƒ½ä¼šæ›´å¥½ï¼Œå› ä¸ºæ¨¡å‹è¡Œä¸ºä¸€è‡´ã€‚ Distributional Q-functionåŸºæœ¬æ€æƒ³æ˜¯ä»¤$Q$é¢„æµ‹æ¯ä¸ªè¡Œä¸ºçš„rewardçš„åˆ†å¸ƒè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªæœŸæœ›å€¼ã€‚å› ä¸ºæœŸæœ›å€¼æŸå¤±äº†å¤ªå¤šä¿¡æ¯äº†ï¼Œä¸åŒçš„distributionå¯èƒ½æœ‰åŒæ ·å¤§å°çš„æœŸæœ›ã€‚ å®é™…ä¸Šæ“ä½œä¹Ÿå³ï¼š Distributional Q-functionå¾€å¾€ä¸ä¼šé«˜ä¼°expectationè€Œæ˜¯ä½ä¼°ã€‚å› ä¸ºåœ¨é¢„æµ‹distributionæ—¶å·²ç»é™å®šäº†æœ€é«˜å’Œæœ€ä½çš„èŒƒå›´äº†ï¼Œå¯¹äºé‚£äº›å¤§äºæˆ–å°äºçš„å€¼éƒ½å¿½ç•¥æ‰ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>å¼ºåŒ–å­¦ä¹ </tag>
        <tag>RL</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Deep Reinforcement Learning</tag>
        <tag>æå®æ¯…å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Q-learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL Lecture 3:Q-learning (Basic Idea)]]></title>
    <url>%2F2019%2F06%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FDRL%20Lecture%203%3A%20Q-learning%20(Basic%20Idea)%2F</url>
    <content type="text"><![CDATA[ç®€å•ä»‹ç»Q-learningçš„æ€æƒ³ä»¥åŠç›¸å…³çš„è®­ç»ƒtipsã€‚ æ˜¯ä»€ä¹ˆä¸Policy Gradientä¸åŒçš„æ˜¯ï¼ŒQ-Learningæ˜¯å±äºvalue basedï¼Œä¹Ÿå³å­¦ä¹ ä¸€ä¸ªcriticå»ä¼°è®¡ç‰¹å®šactor Ï€åœ¨æŸä¸ªstate sä¸‹çš„ç´¯ç§¯rewardã€‚ æ³¨æ„åˆ°Q-learningè™½ç„¶åªå­¦ä¹ äº†criticï¼Œä½†ä»ç„¶å¯ä»¥ç”¨äºåšå†³ç­–ã€‚ é¦–å…ˆæ˜¯å¦‚ä½•é¢„ä¼°criticï¼Ÿcriticçš„æœ¬è´¨å°±æ˜¯å‡½æ•°æ˜ å°„$V^{\pi}(s)$ï¼Œè¾“å…¥$s$ï¼Œè¾“å‡ºä¸€ä¸ªscalarä½œä¸ºä½¿ç”¨äº†actor $\pi$çš„ç´¯ç§¯rewardã€‚æœ‰ä¸¤ç§æ–¹æ³•ï¼šMonte-Carlo (MC) based approachå’ŒTemporal-difference (TD) approachã€‚ Monte-Carlo (MC) based approachcriticçœ‹å®Œæ•´ä¸ªepisodeï¼Œç„¶åå¯¹$s$åšå‡ºé¢„ä¼°ï¼š å…¶å®è´¨ä¸Šå°±æ˜¯åœ¨åšregressionã€‚ Temporal-difference (TD) approachç”±äºæœ‰äº›episodeéå¸¸é•¿ï¼Œç­‰è·‘å®Œå†é¢„ä¼°æ•ˆç‡å¤ªä½ï¼Œå› æ­¤ç›´æ¥å¯¹æ¯ä¸ªstepè¿›è¡Œé¢„ä¼°ã€‚å¯¹äºä¸€ä¸ªtime step $\cdots s_{t}, a_{t}, r_{t}, s_{t+1} \cdots$ï¼Œç›´æ¥é¢„ä¼°ï¼š ä»‹ç»å®Œ$V^{\pi}(s)$ï¼Œè¿˜æœ‰ä¸€ç§criticï¼Œè¾“å…¥$s$å’Œ$a$ä»¥è·å¾—ä¸€ä¸ªç´¯ç§¯rewardã€‚è¿™é‡Œå’Œ$V^{\pi}(s)$ä¸åŒçš„æ˜¯ï¼Œå¯¹äºåŒä¸€ä¸ªÏ€ï¼Œèƒ½å¤Ÿè¯„ä¼°å¼ºåˆ¶é‡‡ç”¨$a$æ‰€è·å¾—çš„rewardã€‚ å¦‚ä½•ä½¿ç”¨criticå†³ç­–åœ¨è®­ç»ƒå®Œäº†ä¸€ä¸ªcriticï¼Œå¦‚ä½•ç”¨äºè¿›è¡Œå†³ç­–ï¼Ÿ å…¶åŸºæœ¬æ€æƒ³æ˜¯ï¼šç»™å®š$Q$ï¼Œæ€»èƒ½æ‰¾åˆ°ä¸€ä¸ª$\pi^{\prime}$ æ¯”$\pi$å¥½ï¼Œä¹Ÿå³$V^{\pi^{\prime}}(s) \geq V^{\pi}(s)$ã€‚å½¢å¼åŒ–ï¼š \pi^{\prime}(s)=\arg \max _{a} Q^{\pi}(s, a)ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨å­¦ä¹ å®Œä¸€ä¸ªç‰¹å®š$\pi$å¯¹åº”çš„$Q$åï¼Œåªéœ€è¦éµç…§æ¯é‡åˆ°ä¸€ä¸ªstateï¼Œé€‰æ‹©èƒ½ä½¿$Q$æœ€å¤§åŒ–çš„actionï¼Œè¯¥æ–°çš„$\pi$å°±ä¼šæ¯”åŸæ¥çš„$\pi$æ›´ä¼˜ã€‚ ä¸ºä»€ä¹ˆä¸€å®šä¼šæ›´ä¼˜ã€‚æä¾›è¯æ˜ï¼š æ¯ä¸€æ­¥é€‰æ‹©æœ€ä¼˜éƒ½ä¼šæ¯”åŸæ¥å¥½ä¸€äº›ã€‚ å¦‚ä½•è®­ç»ƒQ functionæˆ‘ä»¬é€šè¿‡ç±»ä¼¼TDçš„æ–¹æ³•æ¥è®­ç»ƒã€‚ç»™å®š$\cdots s_{t}, a_{t}, r_{t}, s_{t+1} \cdots$,æˆ‘ä»¬æœ‰ï¼š \mathrm{Q}^{\pi}\left(s_{t}, a_{t}\right)=r_{t}+\mathrm{Q}^{\pi}\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)å…¶ä¸­ç”±äºå…¬å¼æœ‰ä¸¤ä¸ªQï¼Œè‹¥è®©ä¸¤ä¸ªQåŒæ—¶å˜ï¼Œä¸å¥½åšå›å½’ã€‚å› æ­¤æˆ‘ä»¬è®©å³è¾¹çš„Qå›ºå®šä½ï¼Œè®­ç»ƒå·¦è¾¹çš„Qï¼Œåœ¨æ›´æ–°å®Œå‡ æ¬¡åï¼Œç›´æ¥å°†å·¦è¾¹çš„Qè¦†ç›–å³è¾¹ã€‚é‡å¤å¤šæ¬¡ï¼š å¦ä¸€ä¸ªé—®é¢˜æ˜¯å¦‚ä½•æ”¶é›†æ•°æ®ï¼Ÿ ç”±äºactionæ˜¯åŸºäºQå‡½æ•°çš„ï¼Œä¹Ÿå³æ¯æ¬¡éƒ½é‡‡ç”¨è´ªå¿ƒçš„ç­–ç•¥ï¼Œä¼šä½¿å¾—åœ¨åˆå§‹æƒ…å†µä¸‹å›ºå®šçš„actionä¼šä¸€ç›´å‡ºç°ï¼Œæ— æ³•æ¢ç´¢åˆ°å…¶ä»–æƒ…å†µã€‚å› æ­¤é‡‡ç”¨ä¸¤ç§ç­–ç•¥ï¼š â‘ epsilon greedyï¼š a=\left\{\begin{aligned} \arg \max _{a} Q(s, a), & \text { with probability } 1-\varepsilon \\ \text {random}, & \text { otherwise } \end{aligned}\right. å…¶ä¸­$\varepsilon$éšç€æ—¶é—´è€Œé€æ¸å˜å°ã€‚ â‘¡boltzmann explorationï¼šæŒ‰æ¦‚ç‡é‡‡æ · P(a | s)=\frac{\exp (Q(s, a))}{\sum_{a} \exp (Q(s, a))}è¿˜æœ‰ä¸€ä¸ªå°æŠ€å·§ç”¨äºæ›´å¥½åˆ©ç”¨sampleï¼Œä¹Ÿå³replay bufferï¼šå°†æ¯æ¬¡Ï€ä¸ç¯å¢ƒäº¤äº’çš„episodeéƒ½æ”¾åœ¨ä¸€ä¸ªbufferé‡Œé¢ï¼Œå¯ä»¥éƒ½ç”¨æ¥å­¦ä¹ Qå‡½æ•°ï¼Œå³ä½¿æ•°æ®æ¥è‡ªä¸åŒçš„policyä¹Ÿæ²¡å…³ç³»ã€‚è¿™å’Œoff-policyæœ‰ç‚¹åƒã€‚ä¸ºä»€ä¹ˆå¯ä»¥è¿™ä¹ˆç”¨ä¸€ç§è§£é‡Šæ˜¯è¿™æ ·å¯ä»¥ä½¿å¾—æ•°æ®æ›´diverseï¼ŒåŒæ—¶å‡å°‘sampleæ¬¡æ•°åŠ å¿«è®­ç»ƒã€‚ å› æ­¤ä¸€ä¸ªå…¸å‹çš„q-learningåˆ™æ˜¯ï¼š]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>å¼ºåŒ–å­¦ä¹ </tag>
        <tag>RL</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Deep Reinforcement Learning</tag>
        <tag>æå®æ¯…å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Q-learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡22]]></title>
    <url>%2F2019%2F06%2F22%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8722%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨è®ºæ–‡ï¼š Boosting Neural Machine Translation Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks On The Power of Curriculum Learning in Training Deep Networks XLNet: Generalized Autoregressive Pretraining for Language Understanding An Empirical Exploration of Curriculum Learning for Neural Machine Translation [Boosting Neural Machine Translation]é€šè¿‡å°†æœºå™¨å­¦ä¹ ä¸­çš„boostingå¼•å…¥NMTï¼Œå¯¹ç¿»è¯‘æ•ˆæœæœ‰ä¸€å®šæå‡ã€‚åŒæ—¶è¿˜æå‡ºäº†å¦å¤–å‡ ç§æ–¹æ³•ï¼Œå¯¹è¾“å…¥æ•°æ®pipelineè¿›è¡Œäº†ä¿®æ”¹ï¼Œå‘ç°éƒ½æœ‰ä¸€å®šçš„æå‡ã€‚æœ¬æ–‡çš„ä¸­å¿ƒæ€æƒ³å°±æ˜¯focus on difficult examplesï¼Œä½œè€…è®¤ä¸ºæ›´å¤šå…³æ³¨äºdifficult exampleï¼Œèƒ½å¤Ÿå¯¹æ¨¡å‹æœ‰æå‡çš„ä½œç”¨ã€‚ å‡ ç§policy originalï¼šå°±æ˜¯å°†æ‰€æœ‰çš„æ•°æ®éƒ½è¿‡ä¸€éboostï¼šå°†æœ€éš¾çš„10%é‡å¤ä¸€éreduceï¼šå°†æœ€ç®€å•çš„20%å»æ‰ã€‚å…·ä½“æ“ä½œæ˜¯ï¼Œæ¯ä¸ªepoché‡æ–°è¡¡é‡ä¸€æ¬¡ï¼Œæ¯ä¸‰ä¸ªepochä½œä¸ºä¸€ä¸ªè®­ç»ƒï¼Œä¹Ÿå³ä¸‰ä¸ªepochå†…éƒ¨åˆ†åˆ«ä½¿ç”¨100% 80% 64%çš„æ•°æ®bootstrapï¼šæ¯ä¸ªepochéƒ½re-sampleä¸€éï¼Œä¹Ÿå³å…è®¸é‡å¤ä»¥åŠéƒ¨åˆ†å¥å­æ¶ˆå¤±ã€‚ éš¾åº¦æ˜¯é€šè¿‡perplexityæ¥è¡¡é‡çš„ï¼Œå› ä¸ºæ¯ä¸ªepochåœ¨è®­ç»ƒæ—¶å°±å·²ç»è®¡ç®—è¿‡perplexityäº†ï¼Œå› æ­¤æ²¡æœ‰å¼•å…¥é¢å¤–çš„è®¡ç®—å¤æ‚åº¦ã€‚ å®éªŒç»“æœè®ºæ–‡çš„å®éªŒé‡‡ç”¨çš„æ˜¯åŒå‘LSTMä½œä¸ºç¿»è¯‘çš„æ¨¡å—ã€‚ å‡ ä¸ªå®éªŒç»“è®ºï¼šboostingèƒ½å¤ŸåŠ é€Ÿæ‹Ÿåˆï¼Œå¹¶ä¸”ç»“æœæ›´å¥½ï¼›reduceç”¨æ›´å°‘çš„æ•°æ®ï¼Œè¾¾åˆ°æœ€å¥½çš„æ•ˆæœã€‚è¿™ç‚¹ä»¤äººå°è±¡æ·±åˆ»boostrapï¼Œç¨å¾®å¥½ä¸€äº›ï¼Œä¸”è®­ç»ƒæ›´ç¨³å®šã€‚ ä¸ºä»€ä¹ˆè¦focus on difficult example? We emulate a human spending additional energy on learning complex conceptsTo force the system to pay much attention on them can adjust it towards â€œmasteringâ€ more information for these sentences. å‡ ä¸ªæƒ³æ³•è¿™ç¯‡æ–‡ç« æ˜¯ä¸€ç¯‡çŸ­æ–‡ï¼Œå¾ˆæ˜æ˜¾å¾ˆå¤šå®éªŒæ²¡åšï¼Œä¼°è®¡æ˜¯åˆ°deadlineäº†å°±æäº¤äº†ï¼Œæ¯”å¦‚åˆ†æä¸åŒæ¯”ä¾‹çš„ç»“æœï¼Œä»¥åŠä¸€äº›æ¶ˆèå®éªŒä¹Ÿæ²¡åšã€‚ ä¸ºä»€ä¹ˆboostrapèƒ½å¤Ÿæœ‰æå‡å¹¶ä¸”æœ‰æ›´ç¨³å®šçš„è®­ç»ƒï¼Ÿè¿™ç§resampleçš„æ–¹å¼èƒ½å¤Ÿå¸¦æ¥ä¸€å®šçš„uncertaintyï¼Œå¯èƒ½ä¼šæœ‰ä¸€å®šçš„å¸®åŠ©ï¼Œè™½ç„¶å¸®åŠ©ä¸å¤§ï¼Œè®ºæ–‡é‡Œé¢ä¹Ÿä»…ä»…æåˆ°äº†uncertaintyï¼Œæ˜¾ç„¶åº”è¯¥åšè¿›ä¸€æ­¥çš„åˆ†æã€‚ è¿™ç¯‡è®ºæ–‡æä¾›çš„insightæˆ‘è®¤ä¸ºè¿˜æ˜¯æœ‰ä¸€å®šå¯å‘çš„ï¼Œé¦–å…ˆè¿™å¹¶ä¸æ˜¯curriculum-learningï¼Œä¹Ÿå³æ²¡æœ‰ä»ç®€å•åˆ°éš¾ï¼Œè€Œæ˜¯æ­£å¸¸çš„è®­ç»ƒï¼Œåªä¸è¿‡é€šè¿‡å¢åŠ æ›´å¤šçš„difficult exampleï¼ŒåŒæ—¶å»æ‰äº†éƒ¨åˆ†å¤ªç®€å•çš„sampleï¼Œè¯´æ˜ä»…ä»…æ˜¯ä¿®æ”¹æ•°æ®åˆ†å¸ƒè€Œä¸æ˜¯ä¿®æ”¹æ•°æ®çš„è¾“å…¥é¡ºåºï¼ˆæœ¬è´¨ä¸Šä¹Ÿæ˜¯ä¿®æ”¹æ•°æ®åˆ†å¸ƒï¼‰ï¼Œä¹Ÿèƒ½å¤Ÿå¸¦æ¥æå‡æ•ˆæœï¼›ç¬¬äºŒï¼Œé€šè¿‡å‡å°‘ç®€å•æ•°æ®ï¼Œæ˜¯å¦æ„å‘³ç€ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½æœ‰ä¸€ä¸ªä¸‹é™ï¼ˆç‰¹åˆ«æ˜¯å¯¹äºç¥ç»ç½‘ç»œè¿™ç§èƒ½åŠ›å¾ˆå¼ºçš„æ¨¡å‹ï¼‰ï¼Œä½è¿‡è¿™ä¸ªä¸‹é™çš„æ•°æ®å¯¹æ¨¡å‹çš„è®­ç»ƒæ˜¯æ²¡æœ‰å¸®åŠ©çš„ï¼Œåè€Œå¯èƒ½ä¼šä½¿æ¨¡å‹overfitåˆ°æŸä¸ªç®€å•çš„patternï¼ˆè¿™å’Œlearning to executeçš„ç»“è®ºä¼¼ä¹æœ‰äº›ç±»ä¼¼ï¼‰ï¼›åŒæ—¶ï¼Œå¢åŠ æ›´å¤šçš„difficult sampleï¼Œä½¿å¾—æ¨¡å‹çš„ä¸Šé™è¢«æé«˜äº†ï¼›ä»¥åŠï¼Œæ˜¯å¦å¯ä»¥å°†curriculum learningä¸è¯¥æ€è·¯ç»“åˆèµ·æ¥ï¼Œè¾¾åˆ°æ›´å¥½çš„ç»“æœï¼Œä¸€æ–¹é¢ç”±æ˜“åˆ°éš¾ï¼Œå¦ä¸€æ–¹é¢ä¿®æ”¹æ•°æ®åˆ†å¸ƒï¼Œä½¿å¾—æ¨¡å‹æ›´å¤šå…³æ³¨éš¾çš„æ•°æ®ã€‚ [Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks]ICML18çš„æ–‡ç« ï¼Œæå…¶ç¡¬æ ¸ï¼Œæ ¹æœ¬çœ‹ä¸æ‡‚ç†è®ºçš„éƒ¨åˆ†ã€‚ å…ˆè´´å‡ºICML oralçš„ä¸‰å¼ PPTï¼š ä»ç†è®ºä¸Šè¯æ˜äº†ï¼š â‘ the rate of convergence of an ideal curriculum learning method is monotonically increasing with the difï¬culty of the examples â‘¡convergence is faster when using points which incur higher loss with respect to the current hypothesis.å½“difficulty scoreæ˜¯å›ºå®šçš„ï¼Œå¯¹äºcurrent hypothesisè€Œè¨€ï¼Œé«˜çš„lossèƒ½å¤Ÿæ¯”ä½çš„lossæ‹Ÿåˆé€Ÿåº¦æ›´å¿«ã€‚è¯¥ç»“è®ºéå¸¸ç›´è§‚ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œéš¾çš„exampleæ›´æœ‰ç›Šæ˜¯é’ˆå¯¹å½“å‰è€Œè¨€ï¼ˆcurrent hypothesisï¼‰è€Œç®€å•çš„exampleæ›´æœ‰ç›Šæ˜¯é’ˆå¯¹final hypothesisè€Œè¨€çš„ã€‚ â‘¢ä½¿ç”¨pretrainå¥½çš„æ¨¡å‹æ¥ä½œä¸ºdifficultyçš„ä¼°è®¡ï¼Œa signiï¬cant boost in convergence speed at the beginning of training â‘£ä½¿ç”¨curriculum learningèƒ½å¤Ÿæ˜¾è‘—åŠ é€Ÿæ‹Ÿåˆï¼›å½“ä»»åŠ¡éš¾åº¦ï¼ˆè¿™å’Œæ¨¡å‹æœ¬èº«çš„å®¹é‡ä¹Ÿæœ‰å…³ï¼Œæ¨¡å‹è¶Šå¼±ç›¸å¯¹çš„ä»»åŠ¡éš¾åº¦ä¹Ÿå°±è¶Šå¤§ï¼ŒåŒæ—¶ä¹Ÿå’Œregularizationç›¸å…³ï¼Œè¶Šå¼ºä»£è¡¨æ¨¡å‹çš„è‡ªç”±åº¦è¶Šå¼±ï¼‰è¶Šå¤§æ—¶ï¼Œä½¿ç”¨CLçš„æ•ˆæœå°±è¶Šæ˜æ˜¾ã€‚ å‡ ä¸ªç»“æœï¼š æ€è€ƒï¼šæ–‡ä¸­çš„ç»“è®ºè¿˜æ˜¯å¯ä»¥å‚è€ƒå‚è€ƒçš„ï¼Œä½†theoreticalçš„ç»“è®ºæ¯•ç«Ÿæ˜¯åœ¨å‡¸å‡½æ•°ä¸Šå¾—åˆ°çš„ï¼Œä¼¼ä¹è¯´æœåŠ›ä¸å¤§ã€‚æ–‡ä¸­çš„æ€è·¯æ˜¯ä»ç†è®ºä¸Šè¯æ˜å‡¸å‡½æ•°çš„ç»“è®ºï¼›ç„¶åé€šè¿‡å®éªŒåœ¨éå‡¸å‡½æ•°ä¸Šä»å®è·µè¯æ˜ç›¸ä¼¼ç»“è®ºã€‚å…¶ä¸»è¦çš„è´¡çŒ®åœ¨äºä¸€äº›CLç›¸å…³çš„ç»“è®ºå’Œå¼•å…¥transfer learningä½œä¸ºdifficulty scoreã€‚ [On The Power of Curriculum Learning in Training Deep Networks]ICML19ä¸€ç¯‡å¾ˆç¡¬æ ¸çš„æ–‡ç« ï¼Œè¯´å®è¯é‡Œé¢çš„è¯æ˜ä»¥åŠéƒ¨åˆ†å®éªŒè®¾è®¡æˆ‘è¿˜æ˜¯æ²¡æ€ä¹ˆææ‡‚ã€‚ä½†æ˜¯ä¸€äº›ç»“è®ºå€¼å¾—æ³¨æ„ã€‚è¿™å±äºæœ‰å¯å‘çš„ä¸€ç±»è®ºæ–‡ã€‚ é€šè¿‡transfer-learningå’Œself-taughtçš„æ–¹æ³•è·å¾—æ–°çš„curriculum-learningç®—æ³•ï¼ŒåŒæ—¶é€šè¿‡ç†è®ºè¯æ˜è·å¾—äº†ä¸€äº›æœ‰å¯å‘æ€§çš„ç»“è®ºã€‚ curriculum learningæœ‰ä¸¤ä¸ªæŒ‘æˆ˜ï¼š å¦‚ä½•å®šä¹‰æ•°æ®çš„éš¾åº¦ï¼›ä»¥åŠæ•°æ®å–‚ç»™æ¨¡å‹çš„é€Ÿåº¦ï¼Œå¤ªå¿«ä¼šè®©æ¨¡å‹æ›´confusedï¼Œå¤ªæ…¢å¯¼è‡´å­¦ä¹ å¤ªæ…¢ æœ¬æ–‡å¯¹è¿™ä¸¤ä¸ªæŒ‘æˆ˜éƒ½æœ‰ä¸€å®šçš„è§£å†³æ–¹æ¡ˆï¼šåˆ†åˆ«å®šä¹‰äº†scoring function å’Œ pacing function scoring functionæœ‰ä¸¤ç§ï¼štransfer learningå’Œself-tutoringï¼Œä¸€ä¸ªå°±æ˜¯pretrained modelï¼Œå¦ä¸€ä¸ªæ˜¯ä½¿ç”¨è®­ç»ƒå¥½çš„æœªé‡‡ç”¨curriculum learningçš„æ¨¡å‹ã€‚ pacing functionï¼šâ‘ Fixed exponential pacingæ²¡å›ºå®šæ¬¡æ•°çš„stepå°±æå‡ä¸€ä¸‹ â‘¡Varied exponential pacing æå‡çš„stepå¯ä»¥æ˜¯å˜åŒ–çš„ â‘¢Single-step pacing ç®€åŒ–ç‰ˆçš„â‘  å…³äºcurrent hypothesisä¸target hypothesisï¼š æœ‰äº›æ–¹æ³•ä¸­ï¼ˆself-paced learning hard example mining æˆ– active learningï¼‰æ›´å€¾å‘äºhard exampleã€‚å®é™…ä¸Šå’ŒCLä¸åŒï¼Œæ˜¯å› ä¸ºfocus on hard exampleæ˜¯åŸºäºå½“å‰æ¨¡å‹çš„çŠ¶æ€å»å®šä¹‰éš¾åº¦çš„ï¼ˆcurrent hypothesisï¼‰ï¼ŒCLåˆ™æ˜¯åŸºäºæœ€ç»ˆçš„çŠ¶æ€ï¼ˆtarget hypothesisï¼‰ã€‚å®é™…ä¸Šè¿™ä¸¤ç§å¹¶ä¸çŸ›ç›¾ï¼Œæœ‰ç ”ç©¶è¡¨æ˜æ¨¡å‹å¯ä»¥åŒæ—¶å—ç›Šäºè¿™ä¸¤ç§ã€‚è¿™ç¯‡æ–‡ç« ä¹Ÿä»ç†è®ºè§’åº¦å»è¯æ˜äº†è¿™ä¸€ç»“è®ºã€‚ å®éªŒå…³äºcurriculum by transferçš„ç»“è®ºï¼šCurriculum learning is clearly and signiï¬cantly beneï¬cial - learning starts faster, and converges to a better solution.the observed advantage of CL is more signiï¬cant when the task is more difï¬cultï¼ˆå¾ˆç›´è§‚ï¼Œå› ä¸ºè¶Šéš¾çš„ä»»åŠ¡è¶Šéœ€è¦CLï¼‰ å…¶ä¸­anti-curriculumæŒ‡çš„æ˜¯æŒ‰ç…§éš¾åº¦ä»é«˜åˆ°ä½æ’ï¼›randomåˆ™æ˜¯å¯¹éš¾åº¦éšæœºæ’ï¼š å…¶ä»–ç»“è®ºï¼š ä½¿ç”¨ä¸åŒçš„transfer functionéƒ½æŒ‡å‘äº†ç›¸ä¼¼çš„gradientæ–¹å‘ï¼›ä¸ä½¿ç”¨æ‰€æœ‰æ•°æ®ç›¸æ¯”ï¼Œtransfer functionåˆ™æŒ‡å‘äº†ä¸åŒçš„æ–¹å‘ï¼›åŒæ—¶ä½¿ç”¨æ‰€æœ‰æ•°æ®çš„gradientå’Œä½¿ç”¨random scoring functionçš„gradientç›¸ä¼¼ï¼Œè¯´æ˜randomèƒ½å¤Ÿè¾ƒä¸ºåˆç†çš„å»estimateçœŸæ­£çš„empirical gradientã€‚å¦‚å›¾ï¼š ç†è®ºç•¥è¿‡å¤§é‡å…¬å¼ã€‚ç›´æ¥è°ˆç»“è®ºï¼š â‘ é€šè¿‡CLä¿®æ”¹åçš„optimization landscapeæ‹¥æœ‰å’ŒåŸæ¥ä¸€æ ·çš„optimization functionï¼›å¹¶ä¸”ä¿®æ”¹åçš„global maximum æ¯”åŸå…ˆçš„æ›´æ˜æ˜¾ï¼ˆpronouncedï¼‰ â‘¡å¦‚æœæ•°æ®åˆ†å¸ƒpå’Œæœ€ä¼˜çš„utility $U_{\tilde{\vartheta}}(X)$ æ˜¯æ­£ç›¸å…³çš„ï¼Œä¸”æ¯”å…¶ä»–çš„$U_{\vartheta}(X)$æ›´æ­£ç›¸å…³ï¼Œé‚£ä¹ˆå¾€optimal parameter $\tilde{\vartheta}$ æ€»ä½“ä¸Šä¼šæ›´åŠ steeperï¼ˆé™¡å³­ï¼‰ã€‚ â‘¢the optimization landscape is modiï¬ed to amplify the difference between the optimal parameters vector and all other parameter values whose covariance with the optimal solution (the covariance is measured between the induced prior vectors) is small, and speciï¬cally smaller than the variance of the optimum. ç»“è®ºä¸æ€è€ƒå°±æˆ‘çš„ç†è§£è€Œè¨€ï¼Œæœ¬æ–‡çš„æœ€å¤§è´¡çŒ®å°±æ˜¯ï¼šç»Ÿä¸€äº†åŸå…ˆçš„ä»ç®€å•åˆ°éš¾ï¼ˆcurriculum learningæˆ–self-paced learningï¼‰å’Œfocus on difficulty examplesï¼ˆboostingæˆ–hard data miningï¼‰ï¼Œåªè¦ä¿®æ”¹åçš„æ•°æ®åˆ†å¸ƒä¸optimal utilityæ˜¯æ­£ç›¸å…³çš„ï¼Œé‚£ä¹ˆå°±å¯ä»¥æå‡è¡¨ç°ï¼Œå› æ­¤ä¸¤ç§strategyéƒ½æ˜¯æœ‰æ•ˆçš„ã€‚ It may even be possible to find a curriculum which is directly correlated with the optimal utility, and that outperforms both methods ä¸è¿‡è¿™ç¯‡æ–‡ç« æœ‰äº›å¥‡æ€ªï¼Œempiricalå’Œtheoreticalçš„éƒ¨åˆ†å®Œå…¨å‰²è£‚çš„æ„Ÿè§‰ã€‚ [XLNet: Generalized Autoregressive Pretraining for Language Understanding]æœ€è¿‘æ¯”è¾ƒç«çš„æ–‡ç« ï¼Œå¯¹Bertçš„å…¨é¢è¶…è¶Šã€‚å°†bertçš„åŒå‘å’Œcontextä»¥åŠlanguage modelçš„long range dependencyå·§å¦™ç»“åˆï¼Œè·å¾—æ–°çš„pretrain modelã€‚ èƒŒæ™¯é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¯ä»¥åˆ†ä¸ºä¸¤ç§ autoregressive(AR)è¯­è¨€æ¨¡å‹å’Œ autoencodingï¼ˆAEï¼‰ã€‚ARå°±æ˜¯ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹ï¼Œä»å‰åˆ°åæˆ–ä»ååˆ°å‰é¢„æµ‹ï¼Œå…¸å‹çš„å°±æ˜¯GPTï¼›AEåˆ™æ˜¯é€šè¿‡å—ç ´åçš„æ•°æ®è¿˜åŸå‡ºåŸå§‹æ•°æ®ï¼ŒBertå°±æ˜¯å…¶ä¸­ä¸€å‘˜ã€‚ ä½†è¿™ä¸¤ç§æ–¹æ³•å„æœ‰ç¼ºç‚¹ï¼š bertçš„AEæ–¹æ³•å‡è®¾äº†æ‰€æœ‰è¢«é¢„æµ‹çš„tokenæ˜¯ç‹¬ç«‹çš„ï¼ˆä¹Ÿå³maskæ‰çš„è¯ç›¸äº’ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ï¼Œä¹Ÿå³ä¸Šä¸€ä¸ªmaskçš„è¯å¹¶ä¸èƒ½å¯¹é¢„æµ‹ä¸‹ä¸€ä¸ªmaskçš„è¯æœ‰å¸®åŠ©ï¼‰ï¼Œä½†è‡ªç„¶è¯­è¨€ä¸­è¿™ç§ä¾èµ–å…³ç³»åº”è¯¥æ˜¯å­˜åœ¨çš„ï¼›åŒæ—¶[Mask]åœ¨çœŸå®æ•°æ®ä¸­å¹¶ä¸å­˜åœ¨ï¼Œä¹Ÿå³å­˜åœ¨input noiseï¼Œå¯¼è‡´pretrain-ï¬netune discrepancyã€‚ è€ŒARçš„é—®é¢˜ä¸»è¦åœ¨æ²¡æœ‰å……åˆ†åˆ©ç”¨å‰åçš„ä¸Šä¸‹æ–‡ï¼Œåªä½¿ç”¨äº†éƒ¨åˆ†ã€‚ æœ¬æ–‡é€šè¿‡permutationæ¥è¾¾åˆ°è§„é¿è¿™ä¸¤ä¸ªç¼ºç‚¹çš„ç›®çš„ï¼Œä¹Ÿå³ æ—¢åˆ©ç”¨äº†å‰åä¸Šä¸‹æ–‡ï¼Œåˆæ²¡æœ‰input noiseï¼ŒåŒæ—¶è¿˜æ²¡æœ‰independence assumptionã€‚ æ¨¡å‹å®šä¹‰ä¸€ä¸ªfactorization ordersï¼ˆæ³¨æ„æ˜¯è™šæ‹Ÿçš„é¡ºåºï¼ŒåŸå§‹é¡ºåºè¿˜æ˜¯ä¼šä¿ç•™çš„ï¼‰ï¼Œå°†åŸå§‹çš„é¡ºåºæ‰“ä¹±ã€‚ä½¿å¾—ä¸€ä¸ªè¯çš„é¢„æµ‹å¯ä»¥ç”±ä¸¤ä¾§çš„è¯æ¥å¸®åŠ©ã€‚ å¦‚å›¾ï¼Œè¾“å…¥çš„åŸå§‹é¡ºåºè¿˜æ˜¯ä¸å˜ï¼Œä½†é€šè¿‡ mask attentionæ¥è¾¾åˆ°ä¸åŒçš„factorization orderçš„ç›®çš„ï¼Œåœ¨ä¸åŒorderä¸‹é¢„æµ‹åŒä¸€ä¸ª$x_3$ï¼Œç”±äºfactorization orderçš„é¡ºåºä¸åŒï¼Œåœ¨3ä¹‹å‰çš„è¯å‘æŒ¥äº†ä½œç”¨ï¼Œè€Œåœ¨ä»–ä¹‹åçš„è¯å°±æ²¡æœ‰å‚ä¸é¢„æµ‹ã€‚ å½¢å¼åŒ–åˆ™æœ‰ï¼š \max _{\theta} \mathbb{E}_{\mathbf{z} \sim \mathcal{Z}_{T}}\left[\sum_{t=1}^{T} \log p_{\theta}\left(x_{z_{t}} | \mathbf{x}_{\mathbf{z}_{]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>Curriculum Learning</tag>
        <tag>NMT</tag>
        <tag>Language Modeling</tag>
        <tag>boosting</tag>
        <tag>XLNet</tag>
        <tag>pretrain</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL Lecture 2:Proximal Policy Optimization (PPO)]]></title>
    <url>%2F2019%2F06%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FDRL%20Lecture%202%3A%20Proximal%20Policy%20Optimization%20(PPO)%2F</url>
    <content type="text"><![CDATA[Policy Gradientçš„é—®é¢˜ï¼šæ¯æ¬¡sampleçš„dataåªèƒ½ä½¿ç”¨ä¸€æ¬¡ï¼Œè¾ƒä¸ºè€—è´¹æ—¶é—´ã€‚å› æ­¤å¼•å…¥off-policyï¼Œæ¯æ¬¡sampleçš„æ•°æ®å¯ä»¥å¤šæ¬¡ä½¿ç”¨ã€‚ ç‰¹ç‚¹ï¼šon-policyä¸­å­¦ä¹ çš„agentä¸å’Œç¯å¢ƒäº¤äº’çš„agentæ˜¯ä¸€è‡´çš„ï¼›è€Œoff-policyåˆ™æ˜¯æœ‰ä¸¤ä¸ªagentï¼Œå…¶ä¸­ä¸€ä¸ªagentè´Ÿè´£ä¸ç¯å¢ƒäº¤äº’æ¥è·å¾—episodeï¼Œå¦ä¸€ä¸ªagentåˆ™é€šè¿‡è¿™äº›episodeæ›´æ–°å‚æ•°ã€‚ åœ¨è¿™é‡Œåˆ©ç”¨importance-samplingæ¥è¾¾åˆ°è¿™ä¸€ç›®çš„ã€‚ importance-samplingç»™å®šæ¦‚ç‡$p$è¦è®¡ç®—$f(x)$çš„æœŸæœ›ï¼šå¯ä»¥é‡‡ç”¨sampleçš„æ–¹å¼æ¥è¾¾åˆ°è¯¥ç›®çš„ã€‚ä¹Ÿå³ï¼š E_{x \sim p}[f(x)] \approx \frac{1}{N} \sum_{i=1}^{N} f\left(x^{i}\right)è€Œè‹¥$p$åˆ†å¸ƒæ— æ³•è·å¾—ï¼Œå¯ä»¥åˆ©ç”¨å¦ä¸€ä¸ªå¯è·å¾—çš„åˆ†å¸ƒ$q$æ¥è¿‘ä¼¼ã€‚ä¹Ÿå³ï¼š E_{x \sim p}[f(x)]=\int f(x) p(x) d x=\int f(x) \frac{p(x)}{q(x)} q(x) d x=E_{x \sim q}[f(x) \frac{p(x)}{q(x)}]ç„¶åå†ç”±$q$æ¥åšsamplingã€‚ æ³¨æ„è¿™é‡Œ$p$ä¸$q$çš„åˆ†å¸ƒä¸åº”è¯¥å·®è·å¤ªå¤§ï¼Œå¦åˆ™å…¶varianceåˆ™ä¼šç›¸å·®å¾ˆå¤§ï¼Œé€ æˆè¿‘ä¼¼ç»“æœå·®è·è¾ƒå¤§ã€‚ off-policyå°†importance samplingç”¨äºoff-policyã€‚ åŸæ¥ï¼š \nabla \overline{R}_{\theta}=E_{\tau \sim p_{\theta}(\tau)}\left[R(\tau) \nabla \log p_{\theta}(\tau)\right]å˜æˆï¼š \nabla \overline{R}_{\theta}=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\frac{p_{\theta}(\tau)}{p_{\theta^{\prime}}(\tau)} R(\tau) \log p_{\theta}(\tau)\right]å…¶ä¸­$p_{\theta^{\prime}}$æ˜¯å¦ä¸€ä¸ªagentçš„åˆ†å¸ƒï¼Œä¹Ÿå³dataæ˜¯ä»è¯¥agentçš„åˆ†å¸ƒsampleå¾—åˆ°çš„ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨è¯¥dataæ¥å¤šæ¬¡è®­ç»ƒÎ¸ã€‚ å› æ­¤æ¢¯åº¦å…¬å¼åˆ™ä¸ºï¼š \begin{array}{l}{=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta}}\left[A^{\theta}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right]} \\ {=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{P_{\theta}\left(s_{t}, a_{t}\right)}{P_{\theta^{\prime}}\left(s_{t}, a_{t}\right)} A^{\theta}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right]} \\ {=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} \frac{p_{\theta}\left(s_{t}\right)}{p_{\theta^{\prime}}\left(s_{t}\right)} A^{\theta}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right]}\end{array}å…¶ä¸­ï¼Œæˆ‘ä»¬å°†$A^{\theta}\left(s_{t}, a_{t}\right)$æ”¹æˆ$A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)$ï¼Œå› ä¸ºæ•°æ®æ˜¯ä»$\theta^{\prime}$ æ¥çš„ï¼›åŒæ—¶æˆ‘ä»¬å‡è®¾$\frac{p_{\theta}\left(s_{t}\right)}{p_{\theta^{\prime}}\left(s_{t}\right)}=1$ï¼Œä¸€æ–¹é¢æ˜¯ä¸æ–¹ä¾¿è®¡ç®—ï¼Œä½œæ­¤å‡è®¾å¯ä»¥æ–¹ä¾¿è®¡ç®—ï¼›å¦ä¸€æ–¹é¢è¯¥å‡è®¾ä¹Ÿæœ‰ä¸€å®šåˆç†æ€§ï¼Œå› ä¸ºæŸä¸ªstateå‡ºç°çš„å‡ ç‡åº”è¯¥å’Œagentçš„å…³ç³»è¾ƒå°ã€‚ é€šè¿‡æ¢¯åº¦å…¬å¼æˆ‘ä»¬å¯ä»¥è¿˜åŸå‡ºåŸå¼ï¼š J^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]å¦ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œåœ¨ä»€ä¹ˆæ—¶å€™åœæ­¢ä½¿ç”¨åŒä¸€ç»„dataæ›´æ–°å‚æ•°ï¼Ÿä¹Ÿå³åŒä¸€ç»„dataèƒ½å¤Ÿæ›´æ–°å‚æ•°å‡ æ¬¡ï¼Ÿ ä¸€ä¸ªåŸåˆ™æ˜¯ï¼šæˆ‘ä»¬å¸Œæœ›ä¸¤ä¸ªagentçš„åˆ†å¸ƒå·®å¼‚å°ä¸€äº›ï¼Œå› ä¸ºåˆ†å¸ƒå·®å¼‚ä¸€æ—¦å¤§äº†ï¼Œvarianceåˆ™ä¼šå˜å¤§ï¼Œå› æ­¤è¿™é‡Œæ·»åŠ ä¸€ä¸ªconstraintã€‚ J_{P P O}^{\theta^{\prime}}(\theta)=J^{\theta^{\prime}}(\theta)-\beta K L\left(\theta, \theta^{\prime}\right)æ³¨æ„è¿™é‡Œçš„KLæ•£åº¦æ˜¯å¯¹actionåˆ†å¸ƒçš„KLæ•£åº¦è€Œä¸æ˜¯å‚æ•°çš„ã€‚ æ‰€ä»¥æœ€ç»ˆçš„ç®—æ³•ä¸ºï¼š å…¶ä¸­$\theta_{k}$æ˜¯ä¸Šæ¬¡æ›´æ–°å®Œçš„agentçš„å‚æ•°ã€‚ä¹Ÿå³ä¸ç¯å¢ƒäº¤äº’çš„agentçš„å‚æ•°æ€»æ˜¯ä¸Šä¸ªiteration å¦ä¸€ä¸ªagentæ›´æ–°åçš„å‚æ•°ã€‚ è¿˜æœ‰å‡ ä¸ªç»†èŠ‚ï¼Œ$\beta$å¯ä»¥æ˜¯adaptiveçš„ï¼›ä»¥åŠPPO2å¯¹PPOä¸­constraintçš„æ”¹è¿›ï¼Œä¸è¿‡éƒ½æ˜¯ç»†ææœ«èŠ‚ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>å¼ºåŒ–å­¦ä¹ </tag>
        <tag>RL</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Deep Reinforcement Learning</tag>
        <tag>æå®æ¯…å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>PPO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL Lecture 1:Policy Gradient (Review)]]></title>
    <url>%2F2019%2F06%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FDRL%20Lecture%201%3A%20Policy%20Gradient%20(Review)%2F</url>
    <content type="text"><![CDATA[æå®æ¯…æ·±åº¦å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹ç¬”è®°ã€‚ æœ¬lectureä¸»è¦æ˜¯å¤ä¹ å¼ºåŒ–å­¦ä¹ çš„policy gradientã€‚åŸºæœ¬çš„ä»‹ç»éƒ½åœ¨ä¹‹å‰çš„å¼ºåŒ–å­¦ä¹ ç¬”è®°é‡Œé¢ã€‚ åŸºæœ¬å¤§è‡´æ¡†æ¶ä¸ºï¼š æ¯æ¬¡sampleå‡ ä¸ªepisodeï¼Œç„¶åæ›´æ–°æ¨¡å‹ï¼š å€¼å¾—æçš„å‡ ä¸ªæ–°çš„ç‚¹ï¼šâ‘ æˆ‘ä»¬è¦å¯¹rewardåŠ baselineï¼Œå› ä¸ºé˜²æ­¢rewardä¸€ç›´ä¸ºæ­£ï¼Œé¼“åŠ±å…¶ä»–å‡ºç°å‡ ç‡å°ä½†rewardé«˜çš„episodeè¢«sampleåˆ°ã€‚å› æ­¤åŠ ä¸€ä¸ªbaselineï¼š \nabla \overline{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(R\left(\tau^{n}\right) -b \right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)å…¶ä¸­$b$å¯ä»¥è®¾ä¸º$b \approx E[R(\tau)]$ â‘¡å¯¹æ¯ä¸ªactionåº”è¯¥åˆ†é…ä¸åŒçš„weightï¼Œé¼“åŠ±rewardé«˜çš„actionå‡ºç°ï¼šä¸€ç§æ–¹æ³•ï¼Œæ˜¯å°†å…¨å±€çš„Ræ¢æˆç´¯ç§¯çš„Rï¼š \nabla \overline{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(\sum_{t^{\prime}=t}^{T_{n}} r_{t^{\prime}}^{n} -b \right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)ä¹Ÿå³å½“å‰actionåˆ°episodeç»“æŸç´¯åŠ çš„rewardã€‚ æ›´è¿›ä¸€æ­¥ï¼Œæ·»åŠ è¡°å‡å› å­ï¼Œå…¶ä¸­$\gamma&lt;1$ï¼š \nabla \overline{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(\sum_{t^{\prime}=t}^{T_{n}} \gamma^{t^{\prime}-t} r_{t^{\prime}}^{n} -b \right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)å½“ç„¶ï¼Œä¸ºäº†ç®€ä¾¿ï¼Œå°†reward $R(\tau^{n})$ ç»Ÿä¸€å†™æˆ $A^Î¸ (s_t,a_t )$ï¼Œä»£è¡¨çš„æ˜¯åœ¨å½“å‰stateä¸‹é‡‡ç”¨actionçš„rewardã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>å¼ºåŒ–å­¦ä¹ </tag>
        <tag>RL</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Deep Reinforcement Learning</tag>
        <tag>æå®æ¯…å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Policy Gradient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡21]]></title>
    <url>%2F2019%2F06%2F16%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8721%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨è®ºæ–‡ï¼š Named Entity Recognition with Bidirectional LSTM-CNNs [Named Entity Recognition with Bidirectional LSTM-CNNs]å…³äºNERçš„ç»å…¸è®ºæ–‡ã€‚ æ€»çš„ç»“æ„å¾ˆç®€å•ï¼Œå…¶å®å°±æ˜¯åˆ©ç”¨word embedding + CNN-char features + additional word featuresä½œä¸ºæ€»çš„featuresï¼Œè¿‡ä¸€ä¸ªåŒå‘LSTMè·å¾—ä¸Šä¸‹æ–‡ç›¸å…³çš„å‘é‡è¡¨ç¤ºï¼Œæœ€ç»ˆè·å¾—åˆ†ç±»ç»“æœã€‚ å…¶ä¸­CNNçš„featureæ˜¯characterçº§åˆ«çš„ï¼š åŒæ—¶è¿˜åŠ äº†ä¸€äº›äººå·¥çš„featureï¼šæ¯”å¦‚å¤§å†™çš„featureï¼›æ¯”å¦‚å¤–éƒ¨è¯å…¸ç­‰ã€‚ æœ€ç»ˆ]]></content>
      <tags>
        <tag>CNN</tag>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>NER</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯28]]></title>
    <url>%2F2019%2F06%2F16%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D28%2F</url>
    <content type="text"><![CDATA[è¶æ‹èŠ±[å®‹] æ¬§é˜³ä¿®åº­é™¢æ·±æ·±æ·±å‡ è®¸ï¼Ÿæ¨æŸ³å †çƒŸï¼Œå¸˜å¹•æ— é‡æ•°ã€‚ç‰å‹’é›•éæ¸¸å†¶å¤„ï¼Œæ¥¼é«˜ä¸è§ç« å°è·¯ã€‚é›¨æ¨ªé£ç‹‚ä¸‰æœˆæš®ï¼Œé—¨æ©é»„æ˜ï¼Œæ— è®¡ç•™æ˜¥ä½ã€‚æ³ªçœ¼é—®èŠ±èŠ±ä¸è¯­ï¼Œä¹±çº¢é£è¿‡ç§‹åƒå»ã€‚]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†22]]></title>
    <url>%2F2019%2F06%2F10%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8622%2F</url>
    <content type="text"><![CDATA[[Sequence Labeling]ä»‹ç»sequence labelingåšå®¢ï¼šhttps://www.cnblogs.com/jiangxinyang/p/9368482.html å…³äºå‡ ä¸ªä»»åŠ¡çš„å®šä¹‰ï¼š å‘½åå®ä½“è¯†åˆ«ï¼ˆNamed Entity Recognition, NER)ï¼šä»æ–‡æœ¬ä¸­è¯†åˆ«å‡ºå‘½åå®ä½“ï¼Œå®ä½“ä¸€èˆ¬åŒ…æ‹¬äººå(PER)ã€åœ°å(LOC)ã€æœºæ„å(ORG)ã€æ—¶é—´ã€æ—¥æœŸã€è´§å¸ã€ç™¾åˆ†æ¯”ç­‰ã€‚ç»„å—åˆ†æ (Chunking)ï¼šæ ‡å‡ºå¥å­ä¸­çš„çŸ­è¯­å—ï¼Œä¾‹å¦‚åè¯çŸ­è¯­ï¼ˆNPï¼‰ï¼ŒåŠ¨è¯çŸ­è¯­ï¼ˆVPï¼‰ç­‰è¯æ€§æ ‡æ³¨ (Part-of-speech Tagging, POS)ï¼šç¡®å®šæ–‡æœ¬ä¸­æ¯ä¸ªè¯çš„è¯æ€§ã€‚è¯æ€§åŒ…æ‹¬åŠ¨è¯ï¼ˆVerbï¼‰ã€åè¯ï¼ˆNounï¼‰ã€ä»£è¯ï¼ˆpronounï¼‰ç­‰ã€‚ [Active learning]å®šä¹‰ï¼š æ ·æœ¬ä¿¡æ¯å°±æ˜¯è¯´åœ¨è®­ç»ƒæ•°æ®é›†å½“ä¸­æ¯ä¸ªæ ·æœ¬å¸¦ç»™æ¨¡å‹è®­ç»ƒçš„ä¿¡æ¯æ˜¯ä¸åŒçš„ï¼Œå³æ¯ä¸ªæ ·æœ¬ä¸ºæ¨¡å‹è®­ç»ƒçš„è´¡çŒ®æœ‰å¤§æœ‰å°ï¼Œå®ƒä»¬ä¹‹é—´æ˜¯æœ‰å·®å¼‚çš„ã€‚å› æ­¤ï¼Œä¸ºäº†å°½å¯èƒ½åœ°å‡å°è®­ç»ƒé›†åŠæ ‡æ³¨æˆæœ¬ï¼Œåœ¨æœºå™¨å­¦ä¹ é¢†åŸŸä¸­ï¼Œæå‡ºä¸»åŠ¨å­¦ä¹ ï¼ˆactive learningï¼‰æ–¹æ³•ï¼Œä¼˜åŒ–åˆ†ç±»æ¨¡å‹ã€‚ä¸»åŠ¨å­¦ä¹ (active learning)ï¼ŒæŒ‡çš„æ˜¯è¿™æ ·ä¸€ç§å­¦ä¹ æ–¹æ³•ï¼šæœ‰çš„æ—¶å€™ï¼Œæœ‰ç±»æ ‡çš„æ•°æ®æ¯”è¾ƒç¨€å°‘è€Œæ²¡æœ‰ç±»æ ‡çš„æ•°æ®æ˜¯ç›¸å½“ä¸°å¯Œçš„ï¼Œä½†æ˜¯å¯¹æ•°æ®è¿›è¡Œäººå·¥æ ‡æ³¨åˆéå¸¸æ˜‚è´µï¼Œè¿™æ—¶å€™ï¼Œå­¦ä¹ ç®—æ³•å¯ä»¥ä¸»åŠ¨åœ°æå‡ºä¸€äº›æ ‡æ³¨è¯·æ±‚ï¼Œå°†ä¸€äº›ç»è¿‡ç­›é€‰çš„æ•°æ®æäº¤ç»™ä¸“å®¶è¿›è¡Œæ ‡æ³¨ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Sequence Labeling</tag>
        <tag>Active learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡20]]></title>
    <url>%2F2019%2F06%2F09%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8720%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨è®ºæ–‡ï¼š Neural Machine Translation in Linear Time Curriculum Learning Bilingual Word Embeddings for Phrase-Based Machine Translation LEARNING TO EXECUTE Self-Paced Learning for Latent Variable Models [Neural Machine Translation in Linear Time]ä½¿ç”¨æ–°çš„ç»“æ„å°†ç¿»è¯‘æ§åˆ¶åœ¨çº¿æ€§å¤æ‚åº¦ã€‚æ„Ÿè§‰æ€è·¯è¿˜è›®æ¸…æ™°ï¼ŒæŒºæ–°é¢–çš„ã€‚ ä¸»è¦æœ‰ä¸‰ä¸ªç‚¹ï¼š ç¬¬ä¸€ï¼Œç»“æ„ä¸Šé‡‡ç”¨bytenetï¼Œä¹Ÿå³é‡‡ç”¨äº†ç©ºæ´å·ç§¯çš„ç½‘ç»œï¼Œä½¿å¾—è®¡ç®—ä»£ä»·å‡å°ã€‚ ç¬¬äºŒï¼Œç›´æ¥å°†decoderå †åœ¨encoderä¸Šï¼Œæ¯æ¬¡decoderåªå–å¯¹åº”å¯¹åº”ä½ç½®ä¸Šçš„encoderï¼Œè¿™å’Œä¸€èˆ¬çš„åŸºäºencoder-decoderçš„æ–¹æ³•ä¸åŒã€‚ ç¬¬ä¸‰ï¼Œé‡‡ç”¨dynamic unfoldingä»¥è§£å†³encoderä¸decoderé•¿åº¦ä¸åŒçš„é—®é¢˜ã€‚äººå·¥é¢„å…ˆå®šä¹‰å¥½decoderçš„ä¸Šç•Œï¼š |\hat{\mathbf{t}}|=a|\mathbf{s}|+bä¹Ÿå³encoderå°†ä¼šç”Ÿæˆè¿™ä¹ˆå¤šçš„representationã€‚ è€Œå®é™…ä¸Štargetæ˜¯å¯ä»¥è¶…å‡ºè¿™ä¸ªä¸Šç•Œçš„ï¼Œç›´åˆ°ç”ŸæˆEOSä¸ºæ­¢ï¼Œå¦‚æœè¶…å‡ºä¸Šç•Œåˆ™åœ¨è¯¥æ­¥ä¸åˆ©ç”¨encoderçš„representationï¼Œä¸Šç•Œåªæ˜¯ç”¨ä»¥æŒ‡å¯¼encoderåº”è¯¥ç”Ÿæˆå¤šå°‘representationã€‚ è¯¥æ¨¡å‹éå¸¸è½»é‡çº§ï¼›ä½†æˆ‘è¿˜æ˜¯æ²¡æœ‰æ€ä¹ˆææ‡‚encoderå¦‚ä½•ç”Ÿæˆå‡ºæ¯”sourceé•¿åº¦è¿˜é•¿çš„ä¿¡æ¯çš„ã€‚ [Curriculum Learning]è¯¾ç¨‹å­¦ä¹ çš„å¼€å±±ä¹‹ä½œã€‚å¯¹curriculum learningè¿›è¡Œäº†å½¢å¼åŒ–çš„æ€»ç»“ï¼Œå¹¶ä¸”é€šè¿‡å‡ ä¸ªå®éªŒå¯¹curriculum learningçš„æ€§è´¨è¿›è¡Œäº†æ¢ç´¢ã€‚è¯¥æ–‡ç« å±äºå¯¹å…¶ä»–äººæœ‰å¯å‘çš„æ€§è´¨ã€‚ curriculum learningçš„æ€æƒ³ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå…ˆä½¿ç”¨ç®€å•çš„æ ·ä¾‹è¿›è¡Œè®­ç»ƒï¼Œç„¶åé€æ¸å°†éš¾çš„æ ·ä¾‹åŠ å…¥åˆ°è®­ç»ƒæ ·ä¾‹ä¸­ã€‚æœ¬è´¨å°±æ˜¯ä»æ˜“åˆ°éš¾çš„è¿‡ç¨‹ï¼Œç¬¦åˆäººçš„å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶ä¸”èƒ½å¤Ÿè·å¾—æ›´å¥½çš„æ•ˆæœã€‚ å½¢å¼åŒ–ï¼šæ„Ÿè§‰ä¸é‡è¦ï¼Œå…¶å®å°±æ˜¯å¯¹data distributionè¿›è¡Œäº†é‡æ’ã€‚ æ€§è´¨ï¼š â‘ è·å¾—æ›´å¥½çš„ç»“æœ â‘¡èƒ½å¤ŸåŠ é€Ÿæ‹Ÿåˆï¼Œâ€˜Cleaner Examples May Yield Better Generalization Faster â€™ï¼Œå› ä¸ºç®€å•çš„exampleèƒ½å¤Ÿé¿å…confuse the learner â‘¢curriculum learningå±•ç°å‡ºregularizerçš„æ€§è´¨ï¼Œä¹Ÿå³åœ¨åŒæ ·training errorçš„æƒ…å†µä¸‹èƒ½å¤Ÿè·å¾—æ›´ä½çš„generalization errorã€‚ å¯¹äºlearneræ¥è¯´ï¼Œåœ¨è®­ç»ƒçš„æ¯ä¸ªé˜¶æ®µéƒ½æœ‰å¯¹ä»–æ¥è¯´å¤ªç®€å•å’Œå¤ªéš¾çš„çŸ¥è¯†ï¼Œå¤ªç®€å•çš„å­¦ä¸åˆ°ä»€ä¹ˆï¼Œå¤ªéš¾çš„å­¦ä¸ä¼šã€‚å› æ­¤åœ¨æ¯ä¸ªé˜¶æ®µé€‰æ‹©å¯¹learnerè€Œè¨€interestingçš„ä¾‹å­ï¼Œä¸å¤ªéš¾ä¹Ÿä¸å¤ªç®€å•çš„ï¼Œæ˜¯æœ€å¥½çš„ã€‚ åŒæ—¶curriculum learningä¸boosting algorithmä¸åŒï¼Œå› ä¸ºboosting algorithm æ˜¯å¼ºè°ƒéš¾çš„exampleè€Œcurriculum learningåªæ˜¯æ¯ä¸ªé˜¶æ®µæŒ‰ç…§éš¾åº¦å¯¹exampleèµ‹äºˆä¸åŒçš„æƒå€¼ã€‚ curriculum learningå¯ä»¥çœ‹åšæ˜¯transfer learningçš„ä¸€ç§ç‰¹æ®Šå½¢å¼ã€‚transfer learningæ˜¯åˆ©ç”¨åŸå§‹ä»»åŠ¡æ¥å¼•å¯¼learneråœ¨æœ€ç»ˆä»»åŠ¡è·å¾—æ›´å¥½çš„ç»“æœï¼›è€Œcurriculum learningå°±æ˜¯åˆ©ç”¨è®­ç»ƒä¸Šçš„æŠ€å·§æ¥å¼•å¯¼learnerè·å¾—æ›´å¥½çš„æœ€ç»ˆç»“æœã€‚ å…¶å®curriculum learningçš„éš¾ç‚¹å°±åœ¨äºå¦‚ä½•å®šä¹‰easy exampleï¼Œå› ä¸ºä¸åŒä»»åŠ¡å¯èƒ½æœ‰ä¸åŒçš„å®šä¹‰ã€‚ [Bilingual Word Embeddings for Phrase-Based Machine Translation]è®²å…³äºè®­ç»ƒåŒè¯­word embeddingçš„ã€‚æˆ‘åªå…³æ³¨curriculum learningéƒ¨åˆ†ï¼Œä½†å…¶å®è®²çš„ä¸å¤šã€‚ [LEARNING TO EXECUTE]è¿™ç¯‡è®ºæ–‡æ˜¯ä½¿ç”¨LSTMæ¥è¯„ä¼°çŸ­çš„è®¡ç®—æœºç¨‹åºã€‚æˆ‘æ¯”è¾ƒå…³æ³¨curriculum learningçš„éƒ¨åˆ†ï¼Œå…¶ä¸­çš„ä¸€äº›è§£é‡Šæˆ–è®¸èƒ½å¤Ÿæœ‰ä¸€äº›å¯å‘ã€‚ æœ¬æ–‡åœ¨ä½¿ç”¨ä¼ ç»Ÿcurriculum learningè®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œå‘ç°æ•ˆæœå¹¶ä¸å¥½ï¼Œå› æ­¤è½¬è€Œä½¿ç”¨ä¸€ç§æ··åˆè®­ç»ƒæ–¹æ³•ã€‚ä¹Ÿå³éƒ¨åˆ†éšæœºé‡‡æ ·ï¼Œéƒ¨åˆ†é‡‡ç”¨ä¼ ç»Ÿcurriculum learningçš„é€æ¸æå‡éš¾åº¦ã€‚ ä¸ºä»€ä¹ˆä¼ ç»Ÿçš„ä¸å¤Ÿå¥½ï¼Ÿä½œè€…çš„è§£é‡Šæ˜¯ï¼Œå¦‚æœä»ç®€å•çš„å¼€å§‹å­¦ï¼ŒLSTMä¼šå°†æ‰€æœ‰çš„memoryç”¨ä»¥è®°å¿†ç®€å•çš„æ ·æœ¬ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°±æ„å»ºå¥½äº†è¿™å¥—patternï¼Œè€Œåœ¨é€æ¸å¢åŠ éš¾åº¦æ—¶ï¼Œè¦æ±‚å¯¹memoryçš„patterné‡ç»„ï¼Œè€Œè¿™ç‚¹ä¸å®¹æ˜“åšåˆ°ã€‚è€Œå¦‚æœé‡‡æ ·éƒ¨åˆ†éšæœºæ ·æœ¬ï¼Œåˆ™èƒ½å¤ŸåŒæ—¶å­¦ä¹ åˆ°éƒ¨åˆ†éš¾çš„exampleï¼Œä»è€Œé˜²æ­¢overfitåˆ°æŸä¸ªç®€å•çš„patternä¸Šã€‚ [Self-Paced Learning for Latent Variable Models]ä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„ä¼˜åŒ–ç®—æ³•ã€‚è¯æ˜äº†curriculum learningä¹Ÿèƒ½åœ¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•ä¸­å–å¾—æ•ˆæœã€‚ 7.2æ›´æ–°ï¼šä¹‹å‰ç†è§£é”™äº†ï¼Œself-paced learningä¸Curriculum Learningè™½æœ‰å…±é€šä¹‹å¤„ä½†å¹¶ä¸æ˜¯ä¸€ä¸ªä¸œè¥¿ã€‚CLçš„difficulty scoreæ˜¯åœ¨è®­ç»ƒå‰å›ºå®šçš„ï¼Œè€ŒSPLæ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ®exampleçš„lossæ¥åŠ¨æ€å†³å®šçš„ã€‚ \left(\mathbf{w}_{t+1}, \mathbf{v}_{t+1}\right)=\underset{\mathbf{w} \in \mathbb{R}^{d}, \mathbf{v} \in\{0,1\}^{n}}{\operatorname{argmin}}\left(r(\mathbf{w})+\sum_{i=1}^{n} v_{i} f\left(\mathbf{x}_{i}, \mathbf{y}_{i} ; \mathbf{w}\right)-\frac{1}{K} \sum_{i=1}^{n} v_{i}\right)åªæœ‰é‚£äº›èƒ½å¤Ÿå¾ˆå¥½fitçš„sampleä½œæ•°ï¼Œä¹Ÿå³ä½¿ç”¨äºŒå…ƒå˜é‡væ¥æ§åˆ¶ã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>Curriculum Learning</tag>
        <tag>NMT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯27]]></title>
    <url>%2F2019%2F06%2F09%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D27%2F</url>
    <content type="text"><![CDATA[å¤œä¸Šå—é™åŸé—»ç¬›[å”] æç›Šå›ä¹å³°å‰æ²™ä¼¼é›ªï¼Œå—é™åŸå¤–æœˆå¦‚éœœã€‚ä¸çŸ¥ä½•å¤„å¹èŠ¦ç®¡ï¼Œä¸€å¤œå¾äººå°½æœ›ä¹¡ã€‚ http://lib.xcz.im/work/57b91effa633bd00665e4f15 é¥®ä¸­å…«ä»™æ­Œ[å”] æœç”«â€¦æç™½æ–—é…’è¯—ç™¾ç¯‡ï¼Œé•¿å®‰å¸‚ä¸Šé…’å®¶çœ ï¼Œå¤©å­å‘¼æ¥ä¸ä¸Šèˆ¹ï¼Œè‡ªç§°è‡£æ˜¯é…’ä¸­ä»™ã€‚â€¦ http://lib.xcz.im/work/57b8e3a7c4c97100558e7dc6 æ–°å”ä¹¦Â·æç™½ä¼ ã€‹è½½ï¼šæç™½åº”è¯è‡³é•¿å®‰ï¼Œå”ç„å®—åœ¨é‡‘éŠ®æ®¿å¬è§ä»–ï¼Œå¹¶èµé£Ÿï¼Œäº²ä¸ºè°ƒç¾¹ï¼Œè¯ä¸ºä¾›å¥‰ç¿°æ—ã€‚æœ‰ä¸€æ¬¡ï¼Œç„å®—åœ¨æ²‰é¦™äº­å¬ä»–å†™é…ä¹çš„è¯—ï¼Œè€Œä»–å´åœ¨é•¿å®‰é…’è‚†å–å¾—å¤§é†‰ã€‚èŒƒä¼ æ­£ã€Šæç™½æ–°å¢“ç¢‘ã€‹è½½ï¼šç„å®—æ³›èˆŸç™½è²åœ°ï¼Œå¬æç™½æ¥å†™æ–‡ç« ï¼Œè€Œè¿™æ—¶æç™½å·²åœ¨ç¿°æ—é™¢å–é†‰äº†ï¼Œç„å®—å°±å‘½é«˜åŠ›å£«æ‰¶ä»–ä¸Šèˆ¹æ¥è§ã€‚]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†21]]></title>
    <url>%2F2019%2F06%2F02%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8621%2F</url>
    <content type="text"><![CDATA[[Dying Relu]Dying Reluç°è±¡æŒ‡çš„æ˜¯ï¼Œåœ¨ä½¿ç”¨Reluä½œä¸ºæ¿€æ´»å‡½æ•°æ—¶ï¼Œå› ä¸ºå­¦ä¹ ç‡è¾ƒå¤§æˆ–æŸäº›åŸå› ï¼Œå¯¼è‡´æŸä¸€å±‚çš„biaså­¦åˆ°è¾ƒå¤§çš„è´Ÿå€¼ï¼Œä½¿å¾—è¯¥å±‚åœ¨è¿‡å®ŒReluæ¿€æ´»å‡½æ•°åçš„è¾“å‡ºå§‹ç»ˆæ˜¯0ã€‚ å½“è¿›å…¥åˆ°è¿™ä¸€çŠ¶æ€æ—¶ï¼ŒåŸºæœ¬ä¸Šæ²¡åŠæ³•å†å›åˆ°æ­£å¸¸çŠ¶æ€ã€‚å› ä¸ºåœ¨å›ä¼ æ—¶ï¼Œå€¼ä¸º0å¯¼è‡´æ¢¯åº¦ä¹Ÿä¸º0ã€‚ https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Dying Relu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯26]]></title>
    <url>%2F2019%2F05%2F29%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D26%2F</url>
    <content type="text"><![CDATA[å‡ä½¿æˆ‘ä»¬ä¸å»æ‰“ä»—å‡ä½¿æˆ‘ä»¬ä¸å»æ‰“ä»—ï¼Œæ•Œäººç”¨åˆºåˆ€æ€æ­»äº†æˆ‘ä»¬ï¼Œè¿˜è¦ç”¨æ‰‹æŒ‡ç€æˆ‘ä»¬éª¨å¤´è¯´ï¼šâ€œçœ‹ï¼Œè¿™æ˜¯å¥´éš¶ï¼â€ èœ€å…ˆä¸»åº™[å”] åˆ˜ç¦¹é”¡å¤©åœ°è‹±é›„æ°”ï¼Œåƒç§‹å°šå‡›ç„¶ã€‚åŠ¿åˆ†ä¸‰è¶³é¼ï¼Œä¸šå¤äº”é“¢é’±ã€‚å¾—ç›¸èƒ½å¼€å›½ï¼Œç”Ÿå„¿ä¸è±¡è´¤ã€‚å‡„å‡‰èœ€æ•…å¦“ï¼Œæ¥èˆé­å®«å‰ã€‚ http://lib.xcz.im/work/57b90d29d342d3005ac78cb5]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AIS2019è®ºæ–‡æŠ¥å‘Šä¼šä¹‹æ­å·è¡Œæµæ°´è´¦]]></title>
    <url>%2F2019%2F05%2F28%2F%E8%A7%81%E9%97%BB%26%E6%83%B3%E6%B3%95%2FAIS2019%E8%AE%BA%E6%96%87%E6%8A%A5%E5%91%8A%E4%BC%9A%E6%9D%AD%E5%B7%9E%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[è¶ç€AIS2019è®ºæ–‡æŠ¥å‘Šä¼šï¼Œç»ˆäºæœ‰æœºä¼šå…¬è´¹æ—…æ¸¸å•¦ğŸ˜„ æ­å·éƒŠåŒºè¿˜æ˜¯è›®ç¹åçš„å“‡ã€‚å¦¹æƒ³åˆ°ã€‚ ä¸ºäº†ç­‰lzyä¸‹è½¦ï¼Œåˆ°åç‚¹é’Ÿæ‰åƒæ™šé¥­ï¼Œéš¾é¡¶ğŸ™„ï¼Œåƒåˆ°12ç‚¹åŠã€‚ æ‰¾äº†åŠå°æ—¶æ‰¾ä¸åˆ°é…’åº—ï¼Œè¿˜æ˜¯æ±‚åŠ©äº†åŒè¡Œçš„å…¶ä»–å°ä¼™ä¼´æ‰æ‰¾åˆ°ğŸ¤¦â€â™‚ï¸ Day1ï¼š ç¡åˆ°11ç‚¹æ‰åŒ†åŒ†èµ¶åˆ°ä¼šåœº å¬äº†å‡ åœºå°±æ˜æ˜æ¬²ç¡äº†ï¼Œè€Œä¸”ä¼šè®®å®¤å†…ç½‘ç»œä¹Ÿå¤ªå·®äº†8ï¸âƒ£ å†³å®šä¸‹åˆå·è·‘ ä¸‹åˆé¸½äº†ä»–ä»¬ï¼Œå›åˆ°å®¿èˆä¸€è§‰åˆç¡åˆ°5ç‚¹ã€‚ç™¾æ— èŠèµ–ï¼Œè¿˜æ˜¯åˆ°è¥¿æ¹–èµ°èµ°8ï¸âƒ£ğŸ˜† å¤©ä¸‹ç€é›¨ï¼Œåˆæ˜¯å¤§æ™šä¸Šï¼Œå…¶å®å•¥ä¹Ÿçœ‹ä¸åˆ°ã€‚èµ°äº†ä¸¤ä¸ªå°æ—¶ï¼Œä»æ–­æ¡¥èµ°åˆ°é›·å³°å¡”ğŸ¤¦â€â™‚ï¸ã€‚ å›åˆ°å®¿èˆï¼Œæœ¬æ¥è¦ç¡äº†ï¼Œä½†åˆé¢‡æœ‰äº›é¥¿ã€‚æŠŠåŒè¡Œçš„å°ä¼™ä¼´éƒ½æ‹‰ä¸Šï¼Œå¼€å§‹æ‰¾å®µå¤œğŸ˜‹ ç­‰ljzä¸‹æ¥ç­‰åˆ°ä¸‰ä¸ªäººéƒ½å¼€å§‹æ‰“æ¸¸æˆäº†ğŸ™„ å¼€å†²ï¼ å…­ä¸ªäººç‚¹äº†4ç“¶é…’æœ€åé€€äº†ä¸¤ç“¶ã€‚å¤§å®¶å¯çœŸæ˜¯å¤ªèœäº†8ï¸âƒ£ğŸ¤¦â€â™‚ï¸ Day2: åˆæ˜¯ç¡åˆ°11ç‚¹ï¼Œåƒäº†ç‚¹ä¸œè¥¿ä¼‘æ¯ä¸€ä¸‹å°±å»åƒåˆé¥­äº†ã€‚ è‚¯å¾·åŸºçš„é¥­çœŸæ˜¯éš¾åƒåˆ°çˆ†ï¼Œå·®ç‚¹åäº†ğŸ¤¢ã€‚ åœ¨åº§ä½ä¸Šç™¾æ— èŠèµ–çœ‹äº†ä¸¤ä¸ªå°æ—¶è§†é¢‘ï¼Œå‡ºå‘åˆ°ä¼šåœºï¼Œç­‰å…¶ä»–äººä¸€èµ·åˆ°é™†æ€»ğŸ åƒé¥­ğŸ˜¬ï¼ˆæ­¤è¡Œæœ€å¤§çš„motivation é™†æ€»ğŸ çš„é¥­èœä¹Ÿå¤ªä¸°ç››äº†8ï¸âƒ£ï¼Œåƒåˆ°æ’‘ã€‚è¿˜è®¤è¯†äº†å‡ ä¸ªæ–°çš„å°ä¼™ä¼´(wsä¸xlw)ã€‚äº†è§£åˆ°è¶…å¤šä»¥å‰è½¯ä»¶å­¦é™¢çš„å…«å¦ã€‚æ€»çš„æ¥è¯´éå¸¸å¼€å¿ƒ æºœäº†æºœäº†ã€‚]]></content>
      <tags>
        <tag>æ´»åŠ¨</tag>
        <tag>AIS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡19]]></title>
    <url>%2F2019%2F05%2F28%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8719%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨è®ºæ–‡ï¼š Generating Long Sequences with Sparse Transformers CBAM: Convolutional Block Attention Module Pyramid Scene Parsing Network [Generating Long Sequences with Sparse Transformers]Motivationï¼štransformerå¯¹é•¿åºåˆ—ä¸å‹å¥½ï¼Œå¤æ‚åº¦è¾ƒé«˜ï¼›æœ¬æ–‡æå‡ºsparse transformerå¯¹ç”Ÿæˆé•¿åºåˆ—çš„è®¡ç®—è¿›è¡Œä¼˜åŒ–ï¼Œå¤æ‚åº¦èƒ½å¤Ÿé™åˆ°$O(n \sqrt{n})$ã€‚ å®é™…ä¸Šå°±æ˜¯å°†å…¨è¿æ¥çš„attentionåˆ†åŒ–æˆå‡ ä¸ªsparseçš„attentionã€‚ å¦‚ä¸Šå›¾ï¼Œå®é™…ä¸Šå°±æ˜¯è®©éƒ¨åˆ†head attendå‰é¢å‡ ä¸ªlocalï¼›éƒ¨åˆ†head attendåˆ°æ›´æ—©çš„ä¿¡æ¯ï¼Œå¹¶ä¸”æ˜¯è·³ç€çœ‹çš„ï¼ˆä¸¤ç§patternï¼‰ã€‚æˆ–è€…è¿˜å¯ä»¥è®©æ‰€æœ‰headéƒ½attendåˆ°è¿™äº›sparseçš„ç‚¹ã€‚ å½“ç„¶ä¹Ÿå¯ä»¥æ‰©å±•åˆ°å¤šç»´ç©ºé—´ã€‚å°†headåˆ†ä¸ºnç»„ï¼Œæ¯ä¸ªç»„è·³ç€çœ‹çš„ä¸ä¸€æ ·ã€‚ å…¶ä»–ä¼¼ä¹éƒ½æ˜¯ç»†èŠ‚ï¼Œæ²¡å•¥å¸®åŠ©ã€‚ å¹¶ä¸”è¿˜æ”¹äº†ä¸€ä¸‹transformerçš„è®¡ç®—è¿‡ç¨‹ï¼Œä»¥åŠåšäº†GPU kernelåº•å±‚çš„åŠ é€Ÿã€‚å…¶ä»–ä¼¼ä¹æ²¡æœ‰ä»€ä¹ˆinsightçš„åœ°æ–¹ã€‚ æ€è€ƒï¼šè¿™å®é™…ä¸Šå°±æ˜¯å‡è®¾äº†ä¸€ä¸ªè¾ƒå¼ºçš„å…ˆéªŒäº†ï¼Œå¹¶ä¸”åœ¨ç”Ÿæˆçš„æ—¶å€™ç¡®å®å¯èƒ½å­˜åœ¨è¿™ç§patternï¼Œå› ä¸ºç”Ÿæˆè¦è¾¾åˆ°å¥½çš„æ•ˆæœï¼Œæ¯ä¸ªç‚¹éƒ½å¿…é¡»æ€»ç»“å‰é¢æ‰€æœ‰ç‚¹çš„ä¿¡æ¯ã€‚ä½†å¦‚æœä¸æ˜¯ç”Ÿæˆï¼Œæ˜¯å¦ä¹Ÿæœ‰è¿™ç§patternï¼Ÿè®ºæ–‡å¼ºè¡Œå°†æ–‡æœ¬ä¹Ÿåˆ‡æˆäºŒç»´ï¼Œè€Œä¸”ç”¨äº†30å±‚ï¼›æ„Ÿè§‰ä¸æ˜¯é‚£ä¹ˆæœ‰é“ç†ã€‚å› ä¸ºè¿™ç§sparseæ˜¯ä»å›¾åƒä¸­è§‚å¯Ÿå¾—åˆ°çš„ï¼Œæ˜¯å¦ä¹Ÿèƒ½åº”ç”¨äºæ–‡æœ¬ï¼Ÿä¼šä¸ä¼šæœ‰å¯èƒ½æ˜¯30å±‚æ‰è¾¾åˆ°è¿™ä¹ˆå¥½çš„æ•ˆæœï¼Ÿ [CBAM: Convolutional Block Attention Module]æå‡ºåœ¨channelç»´åº¦ä¸ç©ºé—´ç»´åº¦çš„åŒé‡attentionã€‚å’ŒSE-Netç›¸æ¯”åŠ äº†ä¸€å±‚ç©ºé—´ç»´åº¦ä¸Šçš„äº¤äº’ï¼Œåšæ³•å‡ ä¹éƒ½å·®ä¸å¤šã€‚ å…·ä½“åšæ³•ï¼š â‘ åœ¨channelç»´åº¦ä¸Šåšattention \begin{aligned} \mathbf{M}_{\mathbf{c}}(\mathbf{F}) &=\sigma(M L P(A v g P o o l(\mathbf{F}))+M L P(M a x P o o l(\mathbf{F}))) \\ &=\sigma\left(\mathbf{W}_{\mathbf{1}}\left(\mathbf{W}_{\mathbf{0}}\left(\mathbf{F}_{\mathbf{a v g}}^{\mathbf{c}}\right)\right)+\mathbf{W}_{\mathbf{1}}\left(\mathbf{W}_{\mathbf{0}}\left(\mathbf{F}_{\mathbf{m a x}}^{\mathbf{c}}\right)\right)\right) \end{aligned}å¯¹è¾“å…¥æŒ‰ç©ºé—´ç»´åº¦æ‹æ‰è·å¾—Cç»´çš„poolingã€‚å®é™…ä¸Šå°±æ˜¯ç”¨average poolingå’Œmax pooling è¿‡çº¿æ€§å±‚+sigmoidã€‚å’ŒSE-Netç›¸æ¯”åªæ˜¯å¤šåˆ©ç”¨äº†max-poolingã€‚ â‘¡åœ¨ç©ºé—´ç»´åº¦ä¸Šåšattention \begin{aligned} \mathbf{M}_{\mathbf{s}}(\mathbf{F}) &=\sigma\left(f^{7 \times 7}([A v g P o o l(\mathbf{F}) ; M a x P \operatorname{ool}(\mathbf{F})])\right) \\ &=\sigma\left(f^{7 \times 7}\left(\left[\mathbf{F}_{\mathbf{a v g}}^{\mathbf{s}} ; \mathbf{F}_{\mathbf{m a x}}^{\mathbf{s}}\right]\right)\right) \end{aligned}åŒæ ·æ˜¯æŒ‰channelç»´åº¦æ‹æ‰åšmax/mean poolingã€‚è·å¾—çš„æ˜¯HW1çš„ç»´åº¦ï¼Œç„¶åæ‹¼èµ·æ¥è¿‡CNN+sigmoidã€‚ æ€è€ƒï¼šä¼¼ä¹åˆ›æ–°æ€§ä¸è¶³ï¼Œå¹¶æ²¡æœ‰æä¾›ä¸€äº›æœ‰ç”¨çš„insightã€‚ [Pyramid Scene Parsing Network] åˆ©ç”¨pyramid poolingï¼ˆä¹Ÿå³ä¸åŒkernel sizeçš„poolingçš„ç»“åˆï¼‰åšåˆ‡å‰²çš„ä»»åŠ¡ã€‚ è¿™æ˜¯pyramid poolingï¼Œä¹Ÿå³åˆ†å±‚æ¬¡çš„poolingï¼š å…¶ä»–æ²¡æ„Ÿè§‰ã€‚]]></content>
      <tags>
        <tag>CNN</tag>
        <tag>Paper</tag>
        <tag>Transformer</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>CBAM</tag>
        <tag>pyramid pooling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[åªæœ‰åœ¨ä½ å·¥ä½œå †ç§¯å¦‚å±±æ—¶ï¼Œä½ æ‰å¯èƒ½äº«å—é—²æš‡]]></title>
    <url>%2F2019%2F05%2F27%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E5%8F%AA%E6%9C%89%E5%9C%A8%E4%BD%A0%E5%B7%A5%E4%BD%9C%E5%A0%86%E7%A7%AF%E5%A6%82%E5%B1%B1%E6%97%B6%EF%BC%8C%E4%BD%A0%E6%89%8D%E5%8F%AF%E8%83%BD%E4%BA%AB%E5%8F%97%E9%97%B2%E6%9A%87%2F</url>
    <content type="text"><![CDATA[â€œåªæœ‰åœ¨ä½ å·¥ä½œå †ç§¯å¦‚å±±æ—¶ï¼Œä½ æ‰å¯èƒ½äº«å—é—²æš‡ã€‚å½“ä½ æ— äº‹å¯åšæ—¶ï¼Œç©ºé—²å°±å˜å¾—ä¸€ç‚¹ä¹Ÿä¸æœ‰è¶£ï¼Œå› ä¸ºç©ºé—²å°±æ˜¯ä½ çš„å·¥ä½œï¼Œè€Œä¸”æ˜¯æœ€è€—äººçš„å·¥ä½œã€‚é—²æ‡’å’Œå»ä¸€æ ·ï¼Œå½“å®ƒè¢«ç›—èµ°äº†ä¹‹åï¼Œå®ƒçš„å‘³é“æ‰æ˜¯ç”œçš„ã€‚â€â€”â€”â€” æ°ç½—å§†Â·KÂ·æ°ç½—å§†]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ— åè‹±é›„çºªå¿µç¢‘é“­]]></title>
    <url>%2F2019%2F05%2F17%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%97%A0%E5%90%8D%E8%8B%B1%E9%9B%84%E7%BA%AA%E5%BF%B5%E7%A2%91%E9%93%AD%2F</url>
    <content type="text"><![CDATA[æ— åè‹±é›„çºªå¿µç¢‘é“­å¤«å¤©ä¸‹æœ‰å¤§å‹‡è€…ï¼Œæ™ºä¸èƒ½æµ‹ï¼Œåˆšä¸èƒ½åˆ¶ï¼ŒçŒç„¶ä¸´ä¹‹è€Œä¸æƒŠï¼Œæ— æ•…åŠ ä¹‹è€Œä¸æ€’ï¼Œæ­¤å…¶æ™ºç”šè¿œï¼Œæ‰€æ€€ç”šå¤§ä¹Ÿã€‚æ‰€æ€€è€…ä½•ï¼Ÿå¤©ä¸‹æœ‰é¥¥è€…ï¼Œå¦‚å·±ä¹‹é¥¥ï¼Œå¤©ä¸‹æœ‰æººè€…ï¼Œå¦‚å·±ä¹‹æººè€³ã€‚æ°‘æ—å±æ€¥ï¼Œåˆ«äº²ç¦»å­è€Œèµ´æ°´ç«ï¼Œæ˜“é¢äº‹æ•Œè€Œæ±‚å¤§åŒã€‚é£è§æ°´å¯’ï¼Œæ—Œéœœå±¥è¡€ï¼Œæˆ–æˆæˆ–è´¥ï¼Œæˆ–å›šæˆ–æ®(mÃ²)ï¼Œäººä¸çŸ¥ä¹‹ï¼Œä¹ƒè‡³æ®’åæ— åã€‚ é“­æ›°ï¼šå‘œå‘¼ï¼å¤§éŸ³å¸Œå£°ï¼Œå¤§è±¡æ— å½¢ã€‚æ¥å…®ç²¾é­„ï¼Œå®‰å…®è‹±çµã€‚é•¿æ²³ä¸ºå’½ï¼Œé’å±±ä¸ºè¯ï¼›å²‚æ›°æ— å£°ï¼Ÿæ²³å±±å³åï¼ äººæœ‰æ‰€å¿˜ï¼Œå²æœ‰æ‰€è½»ã€‚ä¸€ç»Ÿå¯æœŸï¼Œæ°‘æ—å°†å…´ï¼Œè‚ƒä¹‹å˜‰çŸ³ï¼Œæ²æ‰‹å‹’é“­ã€‚å™«æˆ‘å­å­™ï¼Œä»£ä»£æ°¸æ—Œã€‚ å…¬å…ƒäºŒé›¶ä¸€ä¸‰å¹´åæœˆç«‹ã€‚]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ— é¢˜]]></title>
    <url>%2F2019%2F05%2F14%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%F0%9F%87%A8%F0%9F%87%B3%2F</url>
    <content type="text"><![CDATA[äº”åƒå¹´å‰ï¼Œæˆ‘ä»¬å’Œå¤åŸƒåŠäººä¸€æ ·é¢å¯¹æ´ªæ°´ï¼›å››åƒå¹´å‰ï¼Œæˆ‘ä»¬å’Œå·´æ¯”ä¼¦äººä¸€æ ·ç©é’é“œå™¨ï¼›ä¸‰åƒå¹´å‰ï¼Œæˆ‘ä»¬å’Œå¤å¸Œè…Šäººä¸€æ ·æ€è€ƒå“²å­¦ï¼›ä¸¤åƒå¹´å‰ï¼Œæˆ‘ä»¬å’Œç½—é©¬äººä¸€æ ·å››å¤„å¾ä¼ï¼›ä¸€åƒå¹´å‰ï¼Œæˆ‘ä»¬å’Œé˜¿æ‹‰ä¼¯äººä¸€æ ·æ— æ¯”å¯Œè¶³ï¼›äº”åƒå¹´äº†ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨ä¸–ç•Œçš„ç‰Œæ¡Œä¸Šæ‰“ç€éº»å°†ï¼Œè€Œå¦å¤–å‡ å®¶å·²ç»æ¢è¿‡å¥½å¤šè½®ã€‚]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•17]]></title>
    <url>%2F2019%2F05%2F12%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9517%2F</url>
    <content type="text"><![CDATA[[Pytorch restartå†™æ³•]123if args.restart: with open(os.path.join(args.restart_dir, 'model.pt'), 'rb') as f: model = torch.load(f) [Pytorchè·å¾—æ¨¡å‹å‚æ•°é‡]1args.n_all_param = sum([p.nelement() for p in model.parameters()]) [Pytorchå°†æ•°æ®ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–¹ä¾¿å¿«é€Ÿè¯»å…¥]123456789101112131415161718192021222324252627# transformer-xlæ ·ä¾‹def get_lm_corpus(datadir, dataset): fn = os.path.join(datadir, 'cache.pt') if os.path.exists(fn): print('Loading cached dataset...') corpus = torch.load(fn) else: print('Producing dataset &#123;&#125;...'.format(dataset)) kwargs = &#123;&#125; if dataset in ['wt103', 'wt2']: kwargs['special'] = ['&lt;eos&gt;'] kwargs['lower_case'] = False elif dataset == 'ptb': kwargs['special'] = ['&lt;eos&gt;'] kwargs['lower_case'] = True elif dataset == 'lm1b': kwargs['special'] = [] kwargs['lower_case'] = False kwargs['vocab_file'] = os.path.join(datadir, '1b_word_vocab.txt') elif dataset in ['enwik8', 'text8']: pass corpus = Corpus(datadir, dataset, **kwargs) torch.save(corpus, fn) return corpus [Pytorchè‡ªå¸¦APIå®ç°inverse sqrtçš„lr schedule]123456789101112# from transformer-xl# originally used for Transformer (in Attention is all you need)def lr_lambda(step): # return a multiplier instead of a learning rate if step == 0 and args.warmup_step == 0: return 1. else: return 1. / (step ** 0.5) if step &gt; args.warmup_step \ else step / (args.warmup_step ** 1.5) scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯25]]></title>
    <url>%2F2019%2F05%2F12%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D25%2F</url>
    <content type="text"><![CDATA[è¥¿æ±Ÿæœˆä¸–äº‹ä¸€åœºå¤§æ¢¦ï¼Œäººç”Ÿå‡ åº¦æ–°å‡‰ï¼Ÿå¤œæ¥é£å¶å·²é¸£å»Šï¼Œçœ‹å–çœ‰å¤´é¬“ä¸Šã€‚é…’è´±å¸¸æ„å®¢å°‘ï¼Œæœˆæ˜å¤šè¢«äº‘å¦¨ã€‚ä¸­ç§‹è°ä¸å…±å­¤å…‰ï¼ŒæŠŠç›å‡„ç„¶åŒ—æœ›ã€‚]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡18]]></title>
    <url>%2F2019%2F05%2F12%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8718%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨è®ºæ–‡ï¼š GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond Ordered Neurons- Integrating Tree Structures into Recurrent Neural Networks Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification Unified Language Model Pre-training for Natural Language Understanding and Generation Language Models are Unsupervised Multitask Learners MASS: Masked Sequence to Sequence Pre-training for Language Generation Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks PSANet: Point-wise Spatial Attention Network for Scene Parsing CCNet: Criss-Cross Attention for Semantic Segmentation [GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond]æå‡ºä¸€ç§æ–°çš„å¯¹é•¿è·ç¦»ä¾èµ–å»ºæ¨¡çš„æ–¹æ³•ï¼Œå¹¶ç»“åˆäº†ä¹‹å‰å…¶ä»–ç ”ç©¶è€…çš„å·¥ä½œï¼ŒæŠ½è±¡å¾—åˆ°å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼ˆglobal context modelingï¼‰çš„æ¡†æ¶ã€‚ ç›®å‰é•¿è·ç¦»ä¾èµ–å»ºæ¨¡æœ‰ä¸¤ç§ï¼šä¸€ç§æ˜¯å¼•å…¥self-attentionæœºåˆ¶ï¼Œè·å¾—query-dependentçš„å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¦‚NL-Netï¼›å¦ä¸€ç§åˆ™æ˜¯query-independentçš„å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¦‚SE-Netã€‚ Simplified NL-NetMotivationï¼šé€šè¿‡å¯¹Non-local networkçš„åˆ†æï¼Œå‘ç°ç½‘ç»œå®é™…ä¸Šå­¦åˆ°çš„æ˜¯queryæ— å…³çš„ä¸Šä¸‹æ–‡ï¼Œå› æ­¤å¯ä»¥ç›´æ¥å¯¹NL-Netè¿›è¡Œç®€åŒ–ã€‚ é¦–å…ˆæ˜¯å¯¹NL-Netçš„è§‚å¯Ÿï¼Œé€šè¿‡å¯è§†åŒ–ï¼Œä»¥åŠç»Ÿè®¡å¾—åˆ°çš„æ•°æ®ï¼Œå¯ä»¥å‘ç°ï¼ŒNL-Netå¯¹äºæ¯ä¸ªqueryæ¥è¯´ï¼Œå…¶å­¦åˆ°çš„å…¨å±€ä¿¡æ¯å·®å¼‚å¾ˆå°ã€‚ åŒæ—¶ï¼ŒNL-Netç”±äºè¿™ç§query-dependentçš„é•¿è·ç¦»ä¾èµ–å»ºæ¨¡ï¼Œæ‹¥æœ‰è¾ƒé«˜çš„å¤æ‚åº¦ï¼ˆå¹³æ–¹çº§åˆ«ï¼‰ã€‚å› æ­¤é¦–å…ˆæˆ‘ä»¬å¯ä»¥å¯¹NL-Netè¿›è¡Œç®€åŒ–ã€‚ åŸæ¥çš„NL-Netæ˜¯ï¼š \mathbf{z}_{i}=\mathbf{x}_{i}+W_{z} \sum_{j=1}^{N_{p}} \frac{f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)}{\mathcal{C}(\mathbf{x})}\left(W_{v} \cdot \mathbf{x}_{j}\right)æ¯ä¸¤ä¸ªfeatureä¹‹é—´è®¡ç®—ä¸€ä¸ªattentionåˆ†æ•°ã€‚ å°†query-dependentå»æ‰åï¼Œåˆ™æœ‰ï¼š \mathbf{z}_{i}=\mathbf{x}_{i}+\sum_{j=1}^{N_{p}} \frac{\exp \left(W_{k} \mathbf{x}_{j}\right)}{\sum_{m=1}^{N_{p}} \exp \left(W_{k} \mathbf{x}_{m}\right)}\left(W_{v} \cdot \mathbf{x}_{j}\right)è¿˜å¯ä»¥å°†$W_{v}$ç§»åˆ°å¤–é¢ï¼Œæ›´è¿›ä¸€æ­¥åœ°ç®€åŒ–ï¼š \mathbf{z}_{i}=\mathbf{x}_{i}+W_{v} \sum_{j=1}^{N_{p}} \frac{\exp \left(W_{k} \mathbf{x}_{j}\right)}{\sum_{m=1}^{N_{p}} \exp \left(W_{k} \mathbf{x}_{m}\right)} \mathbf{x}_{j}Global Context Modeling Frameworké€šè¿‡å¯¹æ¯”è¿‘æœŸç›¸å…³çš„å·¥ä½œï¼Œä½œè€…å°†å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡çš„æ–¹æ³•æŠ½è±¡å‡ºæ¥ï¼Œåˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼ša)global attention pooling å°†å…¨å±€çš„ä¿¡æ¯æ”¶é›†èµ·æ¥ï¼Œä¼´éšç€ä¸€ä¸ªå…¨è¿æ¥å’Œsoftmaxb)feature transform via a 1x1 convolution Wv å°†æ‰€è·å¾—çš„featureè¿›è¡Œçº¿æ€§è½¬æ¢ã€‚c)feature aggregation å°†global featureä¸æ¯ä¸ªpositionèåˆã€‚ å½¢å¼åŒ–åˆ™æœ‰ï¼š \mathbf{z}_{i}=F\left(\mathbf{x}_{i}, \delta\left(\sum_{j=1}^{N_{p}} \alpha_{j} \mathbf{x}_{j}\right)\right)å¦‚æœä»è¿™ä¸ªè§’åº¦å»ç†è§£ï¼Œé‚£ä¹ˆSE-Netä¹Ÿæ˜¯å±äºè¯¥æ¡†æ¶çš„ä¸€ç§å®ä¾‹ã€‚ ä½œè€…åœ¨è¯¥æ¡†æ¶çš„åŸºç¡€ä¸Šæå‡ºäº†æ–°çš„å®ä¾‹ï¼Œä¹Ÿå³GC-Netï¼ŒåŒæ—¶æœ‰NL-Netçš„é«˜æ•ˆå»ºæ¨¡çš„ä¼˜ç‚¹å’ŒSE-Netçš„è®¡ç®—æ•ˆç‡é«˜çš„ä¼˜ç‚¹ã€‚ \mathbf{z}_{i}=\mathbf{x}_{i}+W_{v 2} \operatorname{ReLU}\left(\operatorname{LN}\left(W_{v 1} \sum_{j=1}^{N_{p}} \frac{e^{W_{k} \mathbf{x}_{j}}}{\sum_{m=1}^{N_{p}} e^{W_{k} \mathbf{x}_{m}}} \mathbf{x}_{j}\right)\right)ç¬¬ä¸€ï¼ŒåŸºæœ¬é‡‡ç”¨NL-Netçš„å½¢å¼ï¼Œç®€åŒ–æˆquery-independentçš„å½¢å¼ï¼Œå¹¶ä¸”ä½¿ç”¨çš„æ˜¯åŠ çš„å½¢å¼è€Œä¸æ˜¯SE-Netçš„rescaleçš„å½¢å¼ï¼Œå°†global featureä¸æ¯ä¸ªä½ç½®èåˆã€‚ç¬¬äºŒï¼Œé‡‡ç”¨SE-Netçš„bottleneckçš„å½¢å¼å»å‡å°‘å‚æ•°å’Œè®¡ç®—é‡ï¼Œå¹¶ä¸”åœ¨æ­¤åŸºç¡€ä¸Šå¤šäº†ä¸€æ­¥layer normä½¿å¾—æ¨¡å‹æ›´æ˜“è®­ç»ƒï¼Œå®è·µè¯æ˜ï¼Œlayer normèƒ½å¤Ÿæå‡è¡¨ç°ã€‚ å¯¹æ¯”å¯¹æ¯”NL-Netï¼šä¸åŒä¹‹å¤„åœ¨äºglobal attention poolingï¼Œç”¨çš„æ˜¯query-independentå¯¹æ¯”SE-Netï¼šä¸åŒä¹‹å¤„åœ¨äºfusion moduleï¼ˆæ¡†æ¶çš„ç¬¬ä¸‰æ­¥ï¼‰ï¼Œä½¿ç”¨çš„æ˜¯additionè€Œä¸æ˜¯rescaleï¼›ä»¥åŠåœ¨bottleneckä¸Šåšäº†ä¸€ç‚¹æ”¹è¿›ï¼ŒåŠ äº†layer normã€‚ æ€è€ƒï¼šæ–‡ç« æœ‰æ„æ€çš„ç‚¹æ˜¯ç«‹è¶³äºå®éªŒè§‚å¯Ÿï¼Œè¿™ç‚¹å€¼å¾—å­¦ä¹ ï¼Œä»å®è·µä¸­å‘ç°é—®é¢˜ã€‚åŒæ—¶ï¼Œåº”å­¦ä¼šç”¨æŠ½è±¡çš„æ€æƒ³å»æ€»ç»“å‰äººçš„å·¥ä½œï¼ˆæ¯”å¦‚NL-Netå®é™…ä¸Šä¹Ÿæ˜¯å¯¹å‰é¢çš„å·¥ä½œçš„æŠ½è±¡æ€»ç»“ï¼Œå®é™…ä¸Šä¸ªäººè®¤ä¸ºå¹¶æ²¡æœ‰ä»€ä¹ˆå¤§çš„åˆ›æ–°ï¼‰ï¼Œä»ä¸€ä¸ªæ›´é«˜çš„è§’åº¦å»çœ‹é—®é¢˜èƒ½å°†é—®é¢˜çœ‹å¾—æ›´æ¸…æ™°ã€‚ æŠ½è±¡å‡ºæ¡†æ¶æ˜¯æœ‰å¿…è¦çš„å—ï¼Ÿæˆ‘çœ‹åœ¨è¿™ç¯‡è®ºæ–‡é‡Œé¢æœ‰äº›å‹‰å¼ºï¼Œå› ä¸ºå®Œå…¨å¯ä»¥è¯´inspired by SE-Net for the computation efficiencyâ€¦ ä½†éè¦æŠ½è±¡æˆæ¡†æ¶ï¼Œä¼šä¸ä¼šåªæ˜¯è¦è¡¨ç°å‡ºå¯¹æ¨¡å‹çš„ç†è§£å¤Ÿæ·±ï¼Ÿä»¥åŠå¢åŠ ç‚¹å†…å®¹ï¼Ÿ ä»¥åŠquery-dependentæ˜¯å¦çœŸçš„æ²¡å¿…è¦ï¼Ÿä¼¼ä¹åœ¨å…¶ä»–è®ºæ–‡ä¸­çš„ç»“è®ºåè€Œæ˜¯ç›¸åçš„ã€‚ [Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks]ICLR19 best paperã€‚ åœ¨LSTMä¸­å¼•å…¥äº†æ–°çš„inductive biasï¼Œéšå¼å¯¹å¥å­è¿›è¡Œæ ‘çš„å»ºæ¨¡ã€‚ Motivationï¼šè¯­è¨€éƒ½æœ‰ä¸€å®šçš„æ ‘çš„ç»“æ„è€Œä¸æ˜¯åºåˆ—ç»“æ„ã€‚å‰äººç ”ç©¶è¡¨æ˜ï¼Œåœ¨NLPä¸­å¼•å…¥æ ‘çš„ç»“æ„æœ‰åŠ©äºå¢å¼ºæ³›åŒ–ï¼Œå¸®åŠ©å‡å°‘é•¿ç¨‹ä¾èµ–é—®é¢˜ï¼Œèƒ½å¤Ÿè·å¾—æ›´å¥½çš„æŠ½è±¡è¡¨ç¤ºã€‚ä¸€äº›æ–¹æ³•åŒ…æ‹¬å¢åŠ ç›‘ç£ä¿¡å·ï¼Œä¹Ÿå³è¯­æ³•æ ‘ç­‰ï¼Œä½†è¯¥æ–¹æ³•æœ‰é™åˆ¶ï¼Œå¦‚æ ‡æ³¨æ•°æ®ä¸è¶³ï¼Œåœ¨ä¸€äº›é¢†åŸŸè¯­æ³•è§„åˆ™å®¹æ˜“è¢«æ‰“ç ´ï¼ˆå¦‚æ¨ç‰¹ï¼‰ï¼›åŒæ—¶éšç€æ—¶é—´çš„æ¨ç§»ï¼Œè¯­æ³•ä¹Ÿåœ¨å˜åŒ–ã€‚åŒæ—¶ï¼Œä¸€äº›ç ”ç©¶ä¹Ÿè¡¨æ˜ï¼Œæœ‰è¶³å¤Ÿå®¹é‡çš„LSTMåœ¨èƒ½å¤Ÿéšå¼åœ°å¯¹å¥å­è¿›è¡Œæ ‘çš„å»ºæ¨¡ã€‚ å› æ­¤åœ¨æœ¬æ–‡å¼•å…¥æ–°çš„inductive biasï¼Œå®Œå…¨æ•°æ®é©±åŠ¨ï¼ˆç›¸å¯¹äºæ˜¾å¼æ„å»ºæ ‘ï¼‰ï¼Œéšå¼åœ°å¯¹å¥å­è¿›è¡Œæ ‘çš„å»ºæ¨¡ã€‚è¿™ç§å½’çº³åç½®ä¿ƒè¿›äº†æ¯ä¸ªç¥ç»å…ƒå†…å­˜å‚¨çš„ä¿¡æ¯çš„ç”Ÿå‘½å‘¨æœŸçš„åˆ†åŒ–ã€‚high-rankingçš„ç¥ç»å…ƒä¿å­˜é•¿ç¨‹çš„ä¿¡æ¯ï¼Œåœ¨ä¸€ä¸ªè¾ƒé•¿æ—¶é—´æ­¥å†…ä¿å­˜ï¼Œè€Œlow-rankingåˆ™ä¿å­˜çŸ­ç¨‹ä¿¡æ¯ï¼Œèƒ½å¤Ÿå¿«é€Ÿè¢«é—å¿˜ã€‚å¼•å…¥cumulative softmaxï¼Œä¸€ç§æ–°çš„æ¿€æ´»å‡½æ•°æ¥ç”Ÿæˆmaster input gateå’Œforget gate ä¿è¯å½“ä¸€ä¸ªç¥ç»å…ƒè¢«æ›´æ–°/é—å¿˜ï¼Œå…¶åçš„ç¥ç»å…ƒéƒ½ä¼šè¢«æ›´æ–°/é—å¿˜ã€‚ ORDERED NEURONSç»™å®šè¯­è¨€åºåˆ—$S=\left(x_{1}, \dots, x_{T}\right)$ä»¥åŠå¯¹åº”çš„constituency treeï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨è®¡ç®—æ—¶é—´æ­¥tçš„æ—¶å€™ï¼ŒéšçŠ¶æ€$h_t$èƒ½åŒ…å«è¯¥èŠ‚ç‚¹åˆ°æ ¹èŠ‚ç‚¹è·¯å¾„ä¸Šæ‰€æœ‰èŠ‚ç‚¹çš„ä¿¡æ¯ã€‚ç›´è§‚ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›è¯¥è·¯å¾„ä¸Šçš„èŠ‚ç‚¹éƒ½èƒ½å¤Ÿè¢«$h_t$çš„ä¸€éƒ¨åˆ†ç¥ç»å…ƒè¡¨ç¤ºã€‚ç”±äº$h_t$çš„ç»´åº¦æ˜¯å›ºå®šçš„ï¼Œè€Œè·¯å¾„ä¸Šçš„èŠ‚ç‚¹åˆ™æ˜¯åŠ¨æ€çš„ï¼Œå› æ­¤ä¸€ç§æœ€å¥½çš„æƒ…å†µå°±æ˜¯èƒ½å¤ŸåŠ¨æ€åˆ†é…æ¯ä¸ªèŠ‚ç‚¹åœ¨hidden stateçš„ç»´åº¦ã€‚ åœ¨ä¸Šè¿°æ€è·¯çš„åŸºç¡€ä¸Šï¼Œä½œè€…å¼•å…¥äº†ordered neuronsï¼Œå¼ºåˆ¶è®©ç¥ç»å…ƒè¡¨ç¤ºä¸åŒæ—¶é—´å°ºåº¦ä¸Šçš„ä¿¡æ¯ã€‚æ­£å¦‚å‰é¢æåˆ°çš„ï¼Œhigh-rankingçš„ç¥ç»å…ƒä¿å­˜çš„æ—¶é—´é•¿ï¼Œä»£è¡¨çš„å°±æ˜¯æ¥è¿‘æ ‘æ ¹çš„èŠ‚ç‚¹ï¼Œè€Œlow-rankingçš„ç¥ç»å…ƒä¿å­˜æ—¶é—´çŸ­ï¼Œä»£è¡¨çš„å°±æ˜¯å°çš„æˆåˆ†ï¼ˆå¦‚phraseï¼‰ã€‚åŸåˆ™æ˜¯ï¼šè¦æ›´æ–°/é—å¿˜high-rankingçš„ç¥ç»å…ƒï¼Œåº”è¯¥å…ˆæŠŠlow-rankingçš„ç¥ç»å…ƒå…ˆæ›´æ–°/é—å¿˜æ‰ã€‚ å¦‚å›¾ï¼š ON-LSTM (â€œOrdered Neurons LSTMâ€)LSTMç»“æ„ï¼š $\begin{array}{ll}{f_{t}=\sigma\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\right)} \\ {i_{t}=\sigma\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\right)} \\ {o_{t}=\sigma\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\right)} \\ {\hat{c}_{t}=\tanh \left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\right)} \\ {h_{t}=o_{t} \circ \tanh \left(c_{t}\right)}\end{array}$ OH-LSTMä¸LSTMçš„ä¸åŒåœ¨äºä¿®æ”¹äº†cell stateçš„æ›´æ–°æ–¹å¼ã€‚ é¦–å…ˆå®šä¹‰æ–°çš„æ¿€æ´»å‡½æ•°ï¼š \hat{g}=\operatorname{cumax}(\ldots)=\operatorname{cumsum}(\operatorname{softmax}(\ldots))$\hat{g}$å¯ä»¥çœ‹åšæ˜¯äºŒå…ƒgateçš„æœŸæœ›ï¼Œè€Œgå°†cell stateåˆ†æˆä¸¤ä¸ªsegment $g=(0, \dots, 0,1, \dots, 1)$ã€‚ å› ä¸ºç¦»æ•£çš„æ–¹æ³•ä¸å¥½ä¼˜åŒ–ï¼Œä»¥åŠå¤ªè¿‡ä¸¥æ ¼ã€‚å…·ä½“çš„è§£é‡Šå’Œè¯æ˜åœ¨è®ºæ–‡é‡Œã€‚ å¼•å…¥ä¸¤ä¸ªæ–°çš„gateï¼Œmaster forget gateå’Œmaster input gateã€‚forget gateçš„å€¼å•è°ƒé€’å¢ï¼›è€Œinput gateçš„å€¼å•è°ƒé€’å‡ã€‚ $\begin{aligned} \tilde{f}_{t} &amp;=\operatorname{cumax}\left(W_{\tilde{f}} x_{t}+U_{\tilde{f}} h_{t-1}+b_{\tilde{f}}\right) \\ \tilde{i}_{t} &amp;=1-\operatorname{cumax}\left(W_{\tilde{i}} x_{t}+U_{i} h_{t-1}+b_{\tilde{i}}\right) \end{aligned}$ æ–°çš„æ›´æ–°è§„åˆ™ï¼š$\begin{aligned} \omega_{t} &amp;=\tilde{f}_{t} \circ \tilde{i}_{t} \\ \hat{f}_{t} &amp;=f_{t} \circ \omega_{t}+\left(\tilde{f}_{t}-\omega_{t}\right)=\tilde{f}_{t} \circ\left(f_{t} \circ \tilde{i}_{t}+1-\tilde{i}_{t}\right) \\ \hat{i}_{t} &amp;=i_{t} \circ \omega_{t}+\left(\tilde{i}_{t}-\omega_{t}\right)=\tilde{i}_{t} \circ\left(i_{t} \circ \tilde{f}_{t}+1-\tilde{f}_{t}\right) \\ c_{t} &amp;=\hat{f}_{t} \circ c_{t-1}+\hat{i}_{t} \circ \hat{c}_{t} \end{aligned}$ master forget gate æ§åˆ¶çš„æ˜¯erasingè¡Œä¸ºï¼›master input gateåˆ™æ˜¯æ§åˆ¶writingè¡Œä¸ºã€‚å…·ä½“çš„ä¾‹å­è§è®ºæ–‡ã€‚$\omega_{t}$åˆ™æ˜¯$\tilde{f}_{t}$ä¸$\tilde{i}_{t}$çš„é‡å éƒ¨åˆ†ï¼Œè¡¨ç¤ºçš„æ˜¯ ç›¸åº”çš„ç¥ç»å…ƒæ®µç¼–ç åŒ…å«ä¸€äº›å…ˆå‰å•è¯å’Œå½“å‰è¾“å…¥å•è¯$x_t$çš„ä¸å®Œæ•´æˆåˆ†ã€‚ åŒæ—¶ï¼Œmaster gatesä¸“æ³¨ä¸€äº›ç²—ç²’åº¦çš„æ§åˆ¶ï¼Œå› æ­¤æ²¡å¿…è¦å’Œhidden stateä¸€æ ·çš„ç»´åº¦ï¼Œåˆ†é…å°ä¸€äº›çš„ç»´åº¦$D_{m}=\frac{D}{C}$å³å¯ã€‚å› æ­¤æ¯C-sizedä¸ªchunkæœ‰åŒæ ·çš„master gatesã€‚ æ€è€ƒï¼šè®¾è®¡å¾ˆç²¾å·§ï¼Œé€šè¿‡å°†ç¥ç»å…ƒåˆ†åŒ–ï¼Œéšå¼å»ºæ¨¡ã€‚æ›´æ–°è§„åˆ™éƒ¨åˆ†æˆ‘å…¶å®è¿˜æ²¡ä»”ç»†å»çœ‹ï¼Œä½†è¯¥æ€æƒ³ç¡®å®å¾ˆæœ‰æ„æ€ã€‚ [Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification]é…åˆä¸Šä¸€ç¯‡ON-LSTMçœ‹ï¼Œå‘ç°ä»–ä»¬ä¹‹é—´å…·æœ‰ç›¸ä¼¼ä¹‹å¤„ã€‚ æœ¬æ–‡åœ¨LSTMä¸Šå¼•å…¥cacheæœºåˆ¶ï¼Œå°†memoryåˆ‡åˆ†ä¸ºå¤šä¸ªgroupå¹¶èµ‹äºˆä¸åŒçš„forget rateï¼Œä½¿æ¨¡å‹æ›´å¥½åœ°ä¿ç•™å…¨å±€çš„ä¿¡æ¯ï¼Œå¯¹documentçº§åˆ«çš„æƒ…æ„Ÿåˆ†ç±»èƒ½å¤Ÿæœ‰æ›´å¥½çš„ç»“æœã€‚ LSTMï¼š \begin{aligned} \mathbf{i}^{(t)} &=\sigma\left(\mathbf{W}_{i} \mathbf{x}^{(t)}+\mathbf{U}_{i} \mathbf{h}^{(t-1)}\right) \\ \mathbf{f}^{(t)} &=\sigma\left(\mathbf{W}_{f} \mathbf{x}^{(t)}+\mathbf{U}_{f} \mathbf{h}^{(t-1)}\right) \\ \mathbf{o}^{(t)} &=\sigma\left(\mathbf{W}_{o} \mathbf{x}^{(t)}+\mathbf{U}_{o} \mathbf{h}^{(t-1)}\right) \\ \tilde{\mathbf{c}}^{(t)} &=\tanh \left(\mathbf{W}_{c} \mathbf{x}^{(t)}+\mathbf{U}_{c} \mathbf{h}^{(t-1)}\right) \\ \mathbf{c}^{(t)} &=\mathbf{f}^{(t)} \odot \mathbf{c}^{(t-1)}+\mathbf{i}^{(t)} \odot \tilde{\mathbf{c}}^{(t)} ) \\ \mathbf{h}^{(t)} &=\mathbf{o}^{(t)} \odot \tanh \left(\mathbf{c}^{(t)}\right) \end{aligned}åœ¨æœ¬æ–‡ä¸­ï¼Œå®é™…ä¸Šæ˜¯ä½¿ç”¨äº†LSTMçš„å˜ä½“ï¼Œå°†input gateä¸forget gateåˆå¹¶ä¸ºä¸€ä¸ªï¼Œä¹Ÿå³ï¼š \mathbf{c}^{(t)}=\mathbf{f}^{(t)} \odot \mathbf{c}^{(t-1)}+\left(\mathbf{1}-\mathbf{f}^{(t)}\right) \odot \tilde{\mathbf{c}}^{(t)}Cached LSTMæ”¹è¿›LSTMï¼Œå°†memoryåˆ†ä¸ºå¤šä¸ªgroupï¼Œæ¯ä¸ªgroupä»£è¡¨ä¸åŒçš„é•¿ç¨‹ä¾èµ–ï¼Œåˆ†é…ä¸åŒçš„forget rateã€‚ç›´è§‚ä¸Šï¼Œé«˜çš„rateä»£è¡¨äº†çŸ­ç¨‹ä¾èµ–ï¼Œä½çš„rateä»£è¡¨é•¿ç¨‹ä¾èµ–ã€‚ å°†memory cellåˆ‡æˆKå—$\left\{G_{1}, \cdots, G_{K}\right\}$ã€‚å› æ­¤æœ‰å¦‚ä¸‹å…¬å¼ï¼š \begin{aligned} \mathbf{r}_{k}^{(t)} &=\psi_{k}\left(\sigma\left(\mathbf{W}_{r}^{k} \mathbf{x}^{(t)}+\sum_{j=1}^{K} \mathbf{U}_{f}^{j \rightarrow k} \mathbf{h}_{j}^{(t-1)}\right)\right) \\ \mathbf{o}_{k}^{(t)} &=\sigma\left(\mathbf{W}_{o}^{k} \mathbf{x}^{(t)}+\sum_{j=1}^{K} \mathbf{U}_{o}^{j \rightarrow k} \mathbf{h}_{j}^{(t-1)}\right) \\ \tilde{\mathbf{c}}_{k}^{(t)} &=\tanh \left(\mathbf{W}_{c}^{(t)} \mathbf{x}^{(t-1)}+\left(\mathbf{r}_{k}^{(t)}\right) \odot \tilde{\mathbf{c}}_{k}^{(t)}\right) \\ \mathbf{h}_{k}^{(t)} &=\mathbf{o}_{k}^{(t)} \odot \tanh \left(\mathbf{c}_{k}^{(t)}\right) \end{aligned}$\psi_{k}(\mathbf{z})$æ˜¯å‹ç¼©å‡½æ•°ï¼š \mathbf{r}_{k}=\psi_{k}(\mathbf{z})=\frac{1}{K} \cdot \mathbf{z}+\frac{k-1}{K}å°†forget rateå‹ç¼©åœ¨ä¸€å®šèŒƒå›´$\left(\frac{k-1}{K}, \frac{k}{K}\right)$ã€‚ ç±»ä¼¼bi-LSTMï¼ŒåŒæ ·CLSTMå¯ä»¥æœ‰åŒå‘ã€‚åœ¨åšåˆ†ç±»æ—¶ï¼Œåªå°†ä»£è¡¨é•¿ç¨‹ä¾èµ–çš„é‚£ç»„å–å‡ºæ¥è¿‡softmaxã€‚ æ€è€ƒï¼šä¸Ordered Neuronç›¸æ¯”ï¼Œè¿™é‡Œæ˜¾å¼åœ°å°†å›ºå®šç»´åº¦åˆ‡åˆ†æˆå¤šä¸ªç»„ï¼Œç›¸æ¯”è€Œè¨€Ordered Neuronæ›´åŠ çµæ´»ï¼Œä½†äºŒè€…è¿˜æ˜¯æœ‰ç›¸å½“çš„ç›¸ä¼¼ç¨‹åº¦çš„ï¼Œè™½ç„¶ä»»åŠ¡å’Œmotivationä¸åŒã€‚ [Unified Language Model Pre-training for Natural Language Understanding and Generation]å°†pretrainæ‰©å±•åˆ°ç”Ÿæˆé¢†åŸŸï¼Œä½¿ç”¨ç”Ÿæˆä»»åŠ¡æ¥å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œpretrainã€‚ åŒæ—¶ä½¿ç”¨ä¸‰ç§è¯­è¨€æ¨¡å‹æ¥è¿›è¡Œpretrainã€‚ä¸€ç§æ˜¯bidirectionalçš„ï¼Œå’Œbertä¸€æ ·ï¼›ä¸€ç§æ˜¯ä»å·¦åˆ°å³/ä»å³åˆ°å·¦å•å‘çš„ï¼Œå’ŒGPTä¸€æ ·ï¼›å¦ä¸€ç§æ˜¯åšç”Ÿæˆçš„ï¼Œä¹Ÿå³encoderç«¯ç›¸äº’éƒ½attendåˆ°ï¼Œè€Œdecoderç«¯åªèƒ½çœ‹åˆ°encoderéƒ¨åˆ†å’Œdecoderçš„å·¦è¾¹ã€‚ ç»Ÿä¸€ä½¿ç”¨[MASK]çš„æ–¹æ³•ï¼ˆbertï¼‰åŒæ—¶è®­ç»ƒè¿™ä¸‰ç§è¯­è¨€æ¨¡å‹ï¼Œè¿™æ ·å¯ä»¥ä½¿ç”¨åŒä¸€å¥—è®­ç»ƒæµç¨‹åŒæ—¶è®­ç»ƒä¸‰ç§æ¨¡å‹ã€‚ æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®å°±æ˜¯å°†ç”ŸæˆåŠ è¿›æ¥äº†å§ï¼Œå¹¶ä¸”æ•ˆæœè¿˜å¯ä»¥ã€‚å…¶ä»–å¹¶æ²¡æœ‰å¾ˆå¤§çš„åˆ›æ–°ç‚¹ã€‚ [Language Models are Unsupervised Multitask Learners]å¤§åé¼é¼çš„GPT2.0 é€šè¿‡å¢åŠ æ›´å¤šå±‚ï¼Œå¢åŠ æ›´å¤šæ•°æ®ï¼Œè®­ç»ƒä¸€ä¸ªæ›´å¥½çš„è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸”å°è¯•åœ¨ä¸fine-tuneçš„æƒ…å†µä¸‹å®Œæˆä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶å–å¾—ä¸é”™çš„æ•ˆæœã€‚ å…¨æ–‡éƒ½åœ¨è®¨è®ºæ€ä¹ˆè·å¾—æ•°æ®ä»¥åŠæ€ä¹ˆè®­ç»ƒã€‚å®é™…ä¸Šå¸®åŠ©å¹¶ä¸å¤§ï¼Œä½†ä¸ªäººè®¤ä¸ºæœ¬æ–‡æœ€å¤§çš„è´¡çŒ®å°±æ˜¯å°è¯•å»åšç”Ÿæˆï¼Œå¹¶ä¸”åœ¨zero-shotçš„æƒ…æ™¯ä¸‹å»æ¢ç´¢language modelçš„ä¸Šé™ã€‚ å…¶ä»–æˆ‘æ²¡ä»”ç»†çœ‹ã€‚ [MASS: Masked Sequence to Sequence Pre-training for Language Generation]å¼•å…¥encoder-decoderç»“æ„æ¥åšpretrainï¼Œå¯ä»¥åŒæ—¶è®­ç»ƒencoderå’Œdecoderï¼Œå¯ä»¥ç”¨äºç”Ÿæˆä»»åŠ¡ã€‚ ideaè¿˜æ˜¯æŒºæœ‰æ„æ€çš„ã€‚ Motivation:é‡‡ç”¨encoder-decoderæ¡†æ¶èƒ½è¿›ä¸€æ­¥æ›´å¥½åœ°ç”¨äºç”Ÿæˆä»»åŠ¡ä¸Šï¼Œè€Œä¸åƒbertå’ŒGPTé‚£æ ·åªæœ‰ä¸€ä¸ªencoderæˆ–decoderï¼Œæ²¡æ³•å¯¹attentioné¢„è®­ç»ƒï¼Œå¯¹ç”Ÿæˆä»»åŠ¡ä¸å‹å¥½ã€‚ åšæ³•ï¼šåœ¨encoderç«¯maskè¿ç»­çš„è¯ï¼Œç„¶åä½¿ç”¨transformerå¯¹å…¶è¿›è¡Œencodeï¼›ç„¶åå†decoderç«¯è¾“å…¥åŒæ ·çš„å¥å­ï¼Œä½†æ˜¯maskedæ‰çš„æ­£å¥½å’Œencoderç›¸åï¼Œå’Œç¿»è¯‘ä¸€æ ·ï¼Œä½¿ç”¨attentionæœºåˆ¶å»è®­ç»ƒï¼Œä½†åªé¢„æµ‹encoderç«¯è¢«maskæ‰çš„è¯ã€‚ ä½œè€…è®¤ä¸ºè¿™æ ·åšçš„å¥½å¤„ï¼šå¯¹encoderç«¯çš„maskèƒ½å¤Ÿå¼ºåˆ¶è®©encoderç«¯æ›´å¥½åœ°å­¦ä¹ æœªè¢«maskæ‰çš„è¯çš„æ„ä¹‰ï¼Œè¿™æ ·æ‰èƒ½é¢„æµ‹maskæ‰çš„è¯ï¼›å¯¹decoderç«¯çš„inputè¿›è¡Œmaskèƒ½å¤Ÿå¼ºåˆ¶æ¨¡å‹æ›´å¤šä¾èµ–äºsourceç«¯ï¼Œè€Œä¸æ˜¯å‰é¢çš„inputã€‚ ä½œè€…è¿˜å°†MASSä¸Bert/GPTåšäº†å¯¹æ¯”ï¼Œå‘ç°Bert/GPTæ˜¯MASSçš„ä¸€ä¸ªç‰¹ä¾‹ã€‚MASSæœ‰ä¸€ä¸ªè¶…å‚kï¼Œæ§åˆ¶maskæ‰çš„segmenté•¿åº¦ã€‚å½“k=1æ—¶ï¼Œåˆ™æ˜¯BERTï¼›å½“k=mï¼Œä¹Ÿå³æ•´ä¸ªå¥å­é•¿åº¦æ—¶åˆ™æ˜¯GPTã€‚ å½“k=1æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç†è§£ä¸ºdecoderç«¯æ²¡æœ‰inputä¿¡æ¯ï¼Œå…¨éƒ¨ä¿¡æ¯æ¥è‡ªencoderï¼Œå’ŒBertå¯¹æ¯”ä¸€ä¸‹ï¼Œè™½ç„¶åœ¨ç»“æ„ä¸Šä¸ä¸€æ ·ï¼Œä½†åšçš„äº‹æƒ…æ˜¯ä¸€æ ·çš„ï¼Œæ­¤æ—¶decoderçš„è§’è‰²å°±æ˜¯berté‡Œé¢çš„åˆ†ç±»å™¨ã€‚å½“k=mæ—¶ï¼Œå®é™…ä¸Šå°±æ˜¯å°†encoderç«¯çš„æ‰€æœ‰ä¿¡æ¯éƒ½maskæ‰äº†ï¼Œæ­¤æ—¶decoderè¦é¢„æµ‹åªèƒ½é decoderç«¯çš„inputï¼Œè¿™å®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªæ ‡å‡†çš„è¯­è¨€æ¨¡å‹ã€‚ åœ¨å‡ ä¸ªç”Ÿæˆä»»åŠ¡ä¸Šçš„ç»“æœç›¸å½“ä¸é”™ï¼Œæˆ‘æ²¡ä»”ç»†çœ‹ã€‚ æ€è€ƒï¼šå°†bertå’ŒGPTæŠ½è±¡å‡ºæ¥ï¼Œä½œä¸ºå…¶æ¡†æ¶çš„ä¸€ç§ç‰¹æ®Šå½¢å¼ï¼Œè¿™åˆå’ŒNL-Netæœ‰ä¸€äº›ç›¸ä¼¼ã€‚ [Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks]å¦ä¸€ç§Gather-Distributeæ€æƒ³çš„å®ä¾‹ã€‚è¯¥æ¨¡å‹åŒæ ·æ˜¯ä¸ºäº†æ•è·é•¿è·ç¦»ä¸Šä¸‹æ–‡ï¼Œä»¥æå‡è¡¨ç°ã€‚ åˆ†ä¸ºä¸¤æ­¥ï¼šgatherï¼Œå°†è¾ƒå¤§ç©ºé—´å†…çš„ä¿¡æ¯èšé›†èµ·æ¥ï¼Œexciteï¼Œå°†ä¿¡æ¯é‡æ–°åˆ†å‘ç»™local featuresã€‚ Motivationï¼šCNNçš„ä¿¡æ¯æµåŠ¨æ–¹å¼ã€‚æ¯æ¬¡æŠ½å–å‘¨å›´çš„ä¿¡æ¯èšåˆåœ¨ä¸€èµ·ï¼Œéšç€å±‚æ•°çš„å¢å¤šé€æ¸æŠ½è±¡ï¼Œå…¶æ„Ÿå—é‡ä¹Ÿé€æ¸å¢å¤§ã€‚æœ¬æ–‡æå‡ºçš„æ¨¡å‹å®é™…ä¸Šå°±æ˜¯åœ¨åŒä¸€å±‚å†…è®©æ¯ä¸ªç‚¹éƒ½æ„Ÿå—åˆ°å…¶å‘¨å›´æ›´å¤§ç©ºé—´çš„ä¿¡æ¯ã€‚ æ¨¡å‹æ¨¡å‹å®šä¹‰ï¼šè¾“å…¥ï¼š$x=\left\{x^{c} : c \in\{1, \ldots, C\}\right\}$Cä»£è¡¨Cä¸ªfeature mapsï¼Œä¹Ÿå³channelç»´ã€‚ å®šä¹‰selection operatorï¼š$\iota(u, e)=\left\{e u+\delta : \delta \in[-\lfloor(2 e-1) / 2\rfloor,\lfloor(2 e-1) / 2\rfloor]^{2}\right\}$ uæ˜¯è¾“å‡ºçš„å…ƒç´ ï¼Œeæ˜¯extent ratioï¼Œä»£è¡¨çš„å®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªçª—å£å¤§å°ã€‚ å› æ­¤gather operatorå¯ä»¥å®šä¹‰ä¸ºæ˜ å°„å‡½æ•°ï¼š$\xi_{G} : \mathbb{R}^{H \times W \times C} \rightarrow \mathbb{R}^{H^{\prime} \times W^{\prime} \times C}\left(H^{\prime}=\left\lceil\frac{H}{e}\right\rceil, W^{\prime}=\left\lceil\frac{W}{e}\right\rceil\right)$$\xi_{G}(x)_{u}^{c}=\xi_{G}\left(x \odot \mathbf{1}_{\iota_{(u, e)}}^{c}\right)$ å…¶å®å°±æ˜¯å¯¹è¯¥çª—å£å†…çš„å…ƒç´ è¿›è¡Œäº†æ˜ å°„ï¼ˆå¦‚mean-poolingï¼‰ã€‚å…¶ä¸­$u \in\left\{1, \ldots, H^{\prime}\right\} \times\left\{1, \ldots, W^{\prime}\right\}, c \in\{1, \ldots, C\}$ ä»ä¸Šå¼å¯ä»¥çœ‹å‡ºï¼Œgatheræ“ä½œå®é™…ä¸Šå°±æ˜¯å¯¹äºæ¯ä¸ªè¾“å‡ºuï¼Œå…¶æ„Ÿå—é‡ä¸ºå•ä¸ªchannelçš„ä¸€ä¸ªçª—å£ã€‚å¦‚æœè¯¥çª—å£æ°å¥½è¦†ç›–äº†æ•´ä¸ªç©ºé—´ï¼Œåˆ™ç§°è¯¥gatheræ“ä½œæœ‰global extentã€‚ è€Œexciteæ“ä½œåˆ™æ˜¯åˆ©ç”¨gatherè·å¾—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ›´æ–°æ¯ä¸ªfeatureã€‚ä¹Ÿå³ï¼š \begin{array}{l}{\xi_{E}(x, \hat{x})=x \odot f(\hat{x})} \\ {f : \mathbb{R}^{H^{\prime} \times W^{\prime} \times C} \rightarrow[0,1]^{\overline{H} \times W \times C}}\end{array}é‚£ä¹ˆGæ˜¯å¦‚ä½•è·å¾—çš„ï¼Ÿå¯ä»¥æœ‰ä¸¤ç§ï¼Œä¸€ç§æ˜¯æ— å‚æ•°ï¼Œå¦ä¸€ç§æ˜¯æœ‰å‚æ•°ã€‚ æ— å‚æ•°GEå®é™…ä¸Šå°±æ˜¯mean-poolingã€‚åˆ™æ•´ä¸ªGE-Netä¸ºï¼š y^{c}=x \odot \sigma\left(\text {interp}\left(\xi_{G}(x)^{c}\right)\right)å…¶ä¸­interp(Â·)ä»£è¡¨äº†æœ€é‚»è¿‘æ’å€¼ã€‚å®é™…ä¸Šå¯ä»¥ç†è§£æˆï¼Œå°†ä¸€ä¸ªè¾ƒå¤§çª—å£çš„ä¿¡æ¯éƒ½mean-poolingä¸€ä¸‹ï¼Œç„¶åè¯¥çª—å£çš„featureéƒ½ç”¨mean-poolingçš„å€¼ä¹˜ä¸€ä¸‹ï¼ˆå› ä¸ºæœ€é‚»è¿‘çš„ç‰¹ç‚¹ï¼Œè¯¥çª—å£çš„æ’å€¼éƒ½æ˜¯è‡ªèº«ï¼‰ã€‚ å½“è®¾è®¡ä¸åŒçš„eæ—¶ï¼Œä¹Ÿå³çª—å£å¤§å°ï¼Œå¯ä»¥çœ‹åˆ°çª—å£è¶Šå¤§ï¼Œå…¶è¡¨ç°è¶Šå¥½ã€‚ æœ‰å‚æ•°GEé‡‡ç”¨strided depth-wise convolutionã€‚ åŒæ ·è¶Šå¤§çš„eè¶Šå¥½ï¼š å¹¶ä¸”è¡¨ç°ä¼šæ¯”æ— å‚æ•°çš„æ›´å¥½ã€‚ å®éªŒè¡¨æ˜ï¼Œåœ¨æ•´ä¸ªæ¨¡å‹çš„ä¸­é—´å±‚æˆ–è€…åé¢å±‚ï¼ˆæœ‰æ›´å¤šçš„channelï¼‰åŠ GEä¼šæ›´å¥½ã€‚ ä¸SE-Netçš„å…³ç³»SE-Netå¯ä»¥çœ‹åšæ˜¯ç‰¹æ®Šçš„GE-Netã€‚SE-Netçš„gatheræ“ä½œå°±æ˜¯å…¨å±€çš„mean-poolingï¼›è€Œåœ¨exciteæ—¶å¤šäº†ä¸€å±‚å…¨è¿æ¥çš„ç½‘ç»œï¼ˆï¼Ÿè®ºæ–‡è¯´å°±æ˜¯ä¸€å±‚å…¨è¿æ¥ï¼Œä½†ä¼¼ä¹ä¸æ˜¯è¿™æ ·çš„ï¼‰ã€‚ è®ºæ–‡ä¸­è¿˜å°†SE-Netå’ŒGE-Netç»“åˆèµ·æ¥ï¼Œå‘ç°æœ‰æ›´å¤§çš„æå‡ï¼Œè¯æ˜äºŒè€…ä¸æ˜¯æ’æ–¥çš„ã€‚ åº”ç”¨GE-Netçš„å‡ ä¸ªä¾‹å­ï¼š æˆ‘çš„æ€è€ƒï¼šä¸SE-Netçš„å…³ç³»å¯†åˆ‡ï¼ˆå®é™…ä¸Šå°±æ˜¯åŒä¸€æ‹¨äººåšçš„ï¼‰ã€‚ä½†è¿™é‡Œå¼ºè°ƒçš„æ˜¯channelä¹‹é—´æ²¡æœ‰è”ç³»ï¼Œä»…ä»…æ˜¯é€šè¿‡æ‰©å¤§æ„Ÿå—é‡ï¼Œå¢å¼ºglobalçš„ä¿¡æ¯ï¼›è€ŒSE-Netåˆ™æ˜¯å¼ºè°ƒçš„channelä¹‹é—´çš„è”ç³»ï¼Œå¹¶æ²¡æœ‰è€ƒè™‘channelå†…éƒ¨çš„å…³ç³»ï¼Œç›¸å½“äºGE-Netå…·æœ‰å…¨å±€æ„Ÿå—é‡ã€‚å¦‚æœGE-Netæœ‰å…¨å±€æ„Ÿå—é‡ï¼Œé‚£ä¹ˆä»–æ¯”SE-Netå°±å·®åœ¨channelä¹‹é—´çš„è”ç³»äº†ã€‚ [PSANet: Point-wise Spatial Attention Network for Scene Parsing]æå‡ºå¦ä¸€ç§è§£å†³local constraintçš„æ–¹æ¡ˆï¼Œä¹Ÿå³ä½¿å¾—featureä¹‹é—´èƒ½å¤Ÿå»ºç«‹é•¿è·ç¦»ä¾èµ–ã€‚feature mapä¸Šçš„æ¯ä¸ªä½ç½®é€šè¿‡attention mapä¸å…¶ä»–ç‚¹è¿›è¡Œè¿æ¥ï¼ŒåŒæ—¶ä¿¡æ¯æµåŠ¨æ˜¯åŒå‘çš„ï¼Œæ¯ä¸ªç‚¹åŒæ—¶è¿›è¡Œæ”¶é›†ä¸åˆ†å‘çš„æ“ä½œã€‚ é€šå¸¸è€Œè¨€ï¼Œä¿¡æ¯çš„aggregationå¯ä»¥å½¢å¼åŒ–æˆï¼š \mathbf{z}_{i}=\frac{1}{N} \sum_{\forall j \in \Omega(i)} F\left(\mathbf{x}_{i}, \mathbf{x}_{j}, \Delta_{i j}\right) \mathbf{x}_{j}$\mathbf{z}_{i}$æ˜¯ç¬¬iä¸ªä½ç½®çš„è¾“å‡ºï¼›$\mathbf{x}_{j}$æ˜¯è¾“å…¥çš„feature map $X$ã€‚$\forall j \in \Omega(i)$ æ˜¯ä¸iç›¸å…³çš„æ‰€æœ‰ä½ç½®çš„featureé›†åˆã€‚$ F\left(\mathbf{x}_{i}, \mathbf{x}_{j}, \Delta_{i j}\right)$ ä»£è¡¨çš„æ˜¯jåˆ°içš„ä¿¡æ¯æµåŠ¨ã€‚$\Delta$ ä»£è¡¨çš„æ˜¯ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚ å¯ä»¥å°†ä¸Šè¿°å¼å­ç®€åŒ–ä¸ºï¼š \mathbf{z}_{i}=\frac{1}{N} \sum_{\forall j \in \Omega(i)} F_{\Delta_{i j}}\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \mathbf{x}_{j}å…¶ä¸­$\left\{F_{\Delta_{i j}}\right\}$æ˜¯ä½ç½®ç›¸å…³çš„å‡½æ•°æ˜ å°„ã€‚ å½“ä¸€ä¸ªfeature mapçš„ä½ç½®å¾ˆå¤šæ—¶ï¼Œ$x_i$ä¸$x_j$çš„pairå°†ä¼šå¾ˆå¤§ã€‚ å› æ­¤å°†ä¸Šå¼å‡½æ•°æ˜ å°„ç®€åŒ–ä¸ºï¼š F_{\Delta_{i j}}\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \approx F_{\Delta_{i j}}\left(\mathbf{x}_{i}\right)ä¹Ÿå³$j$åˆ°$i$çš„ä¿¡æ¯æµåŠ¨åªä¸$i$ä½ç½®çš„featureä»¥åŠ$i$ä¸$j$ä¹‹é—´çš„ç›¸å¯¹ä½ç½®æœ‰å…³ã€‚ åŒç†ï¼Œè¿˜å¯ä»¥å°†å‡½æ•°æ˜ å°„ç®€åŒ–æˆï¼š F_{\Delta_{i j}}\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \approx F_{\Delta_{i j}}\left(\mathbf{x}_{j}\right)ä¹Ÿå³ä¿¡æ¯æµåŠ¨åªä¸$i$ä¸$j$çš„ç›¸å¯¹ä½ç½®ä»¥åŠ$j$ä½ç½®ä¸Šçš„featureæœ‰å…³ã€‚ å°†ä¸Šè¿°ä¸¤ä¸ªç®€åŒ–å‡½æ•°ç»“åˆèµ·æ¥ï¼Œå¯ä»¥è·å¾—åŒå‘ä¿¡æ¯ä¼ æ’­è·¯å¾„ã€‚ä¹Ÿå³ï¼š F_{\Delta_{i j}}\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \approx F_{\Delta_{i j}}\left(\mathbf{x}_{i}\right)+F_{\Delta_{i j}}\left(\mathbf{x}_{j}\right)æ­¤æ—¶æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š \mathbf{z}_{i}=\frac{1}{N} \sum_{\forall j \in \Omega(i)} F_{\Delta_{i j}}\left(\mathbf{x}_{i}\right) \mathbf{x}_{j}+\frac{1}{N} \sum_{\forall j \in \Omega(i)} F_{\Delta_{i j}}\left(\mathbf{x}_{j}\right) \mathbf{x}_{j}ç¬¬ä¸€é¡¹$F_{\Delta_{i j}}\left(\mathbf{x}_{i}\right)$encodeäº†åœ¨å…¶ä»–ä½ç½®ä¸Šçš„ä¿¡æ¯åœ¨å¤šå¤§ç¨‹åº¦ä¸Šèƒ½å¤Ÿå¸®åŠ©ä½ç½®iï¼ˆé€šè¿‡ä½ç½®içš„featureä»¥åŠç›¸å¯¹ä½ç½®ï¼‰ã€‚ ç¬¬äºŒé¡¹$F_{\Delta_{i j}}\left(\mathbf{x}_{j}\right)$æ‰€åšçš„ä¹Ÿå³é¢„æµ‹å…¶ä»–ä½ç½®ä¸Šçš„featureçš„é‡è¦æ€§ï¼ˆé€šè¿‡ç›¸å¯¹ä½ç½®ï¼Œä»¥åŠä½ç½®jçš„featureï¼‰ã€‚ å¦‚ä¸‹å›¾ï¼š ä¸Šè¿°ä¸¤ä¸ªFå®é™…ä¸Šå¯ä»¥çœ‹åšæ˜¯åœ¨é¢„æµ‹ä¸€ä¸ªattentionçš„å€¼ï¼Œå»åšaggregationã€‚ä¹Ÿå³ï¼š \mathbf{z}_{i}=\frac{1}{N} \sum_{\forall j} \mathbf{a}_{i, j}^{c} \mathbf{x}_{j}+\frac{1}{N} \sum_{\forall j} \mathbf{a}_{i, j}^{d} \mathbf{x}_{j}é—®é¢˜åœ¨äºå¦‚ä½•è·å¾—aï¼Ÿä¸‹å›¾æ˜¯è¾ƒä¸ºæ¸…æ™°çš„ä¸€ä¸ªæ¡†æ¶å›¾ï¼š å¯ä»¥çœ‹å‡ºæ˜¯é€šè¿‡å¤šä¸ªCNNæ¥è·å¾—attentionçŸ©é˜µçš„ã€‚ ä¸Šä¸‹ä¸¤æ¡çº¿å¾ˆç±»ä¼¼ã€‚ç¬¬ä¸€æ­¥æ˜¯å…ˆå‹ç¼©channelä»¥å‡å°‘è®¡ç®—é‡ï¼ˆC2&lt;C1)ã€‚ç¬¬äºŒæ­¥æ‰©å±•channelä¸º$(2H-1)\times(2W-1)$ï¼Œä¸‹é¢è§£é‡Šä¸ºä»€ä¹ˆã€‚æ¥ä¸‹æ¥åœ¨é‡æ–°è·å¾—$H\times W$çš„channelç»´ï¼Œè¯¥channelç»´çš„æ¯ä¸€ç»´æ‰€ä»£è¡¨çš„å°±æ˜¯ä¸€ä¸ªfeatureï¼ˆå…±æœ‰$H\times W$ä¸ªfeatureï¼‰çš„attentionå€¼ã€‚æœ€åä¹˜èµ·æ¥å†concatä¸€ä¸‹ï¼Œè·å¾—æœ€åçš„outputã€‚ ä¸ºä»€ä¹ˆæ˜¯$(2H-1)\times(2W-1)$çš„channelç»´ï¼Œå› ä¸ºå¸Œæœ›å°†è¯¥featureå‰ªè£ä¸€ä¸‹å˜æˆ$H\times W$ï¼Œæ­£å¥½å¯ä»¥è¡¨ç¤ºç›¸å¯¹ä½ç½®ã€‚ å¯¹äºä¸€ä¸ª$(2H-1)\times(2W-1)$çš„featureå¯ä»¥å±•å¼€æˆäºŒç»´çš„ï¼Œå…¶ä¸­ä½ç½®iä¸ºä¸­å¿ƒï¼Œä»…æœ‰$H\times W$ä¸ªæœ‰ç”¨ã€‚å…·ä½“è€Œè¨€,åœ¨ç¬¬kè¡Œç¬¬låˆ—çš„ä½ç½®iï¼Œåˆ™æœ‰ç”¨çš„çŸ©é˜µæ˜¯ä»$H-k$è¡Œå’Œ$W-l$åˆ—å¼€å§‹çš„ã€‚è¿™ä¸ªåšæ³•å€’æŒºæœ‰æ„æ€çš„ã€‚ ä¸NL-Netçš„å…³ç³»ï¼šNL-Netæ²¡æœ‰è€ƒè™‘ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚ æ€è€ƒï¼šè¯¥æ–¹æ³•ä¼¼ä¹ç¡®å®ç›¸æ¯”NL-Netçš„è®¡ç®—é‡å°ï¼Œè™½ç„¶çœ‹èµ·æ¥ä¹Ÿå¾ˆå¤§ã€‚NL-Netçš„è®¡ç®—é‡æ˜¯(HW)^2ã€‚è€Œè¿™é‡Œçš„æ•°é‡çº§æ˜¯HWã€‚ç©¶å…¶åŸå› ï¼Œæ˜¯å› ä¸ºattentionæ˜¯é¢„æµ‹å‡ºæ¥çš„ã€‚ ä»è·å¾—attentionçŸ©é˜µçš„æ–¹å¼å¯ä»¥çœ‹å‡ºï¼Œchannelä¸channelä¹‹é—´æœ‰äº¤äº’ã€‚ attentionçŸ©é˜µæ˜¯é¢„æµ‹å‡ºæ¥çš„ï¼ˆ$1\times 1$çš„convolutionï¼‰ï¼Œè€Œä¸æ˜¯ä¸€å¯¹pairè®¡ç®—å‡ºæ¥çš„ã€‚ä¼¼ä¹å°±æ²¡é‚£ä¹ˆæœ‰é“ç†ã€‚ å¹¶ä¸”ï¼Œä¸Šä¸‹ä¸¤æ¡æ”¯çº¿çš„æ“ä½œéƒ½æ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯å°†å…¶è§£é‡Šä¸ºåŒå‘ä¿¡æ¯æµåŠ¨ï¼›é‚£è¿˜å¯ä»¥è§£é‡ŠæˆåƒTransformeré‚£æ ·ï¼Œå¤šä¸ªheadï¼Œå°†åŒä¸€ä¸ªè¡¨ç¤ºæ˜ å°„åˆ°å¤šä¸ªéšç©ºé—´ä¸­å¢å¼ºè¡¨ç¤ºã€‚ä¹‹å‰ç†è§£é”™äº†ã€ [CCNet: Criss-Cross Attention for Semantic Segmentation]å¯¹NL-Netçš„æ”¹è¿›ï¼Œé€šè¿‡å¼•å…¥åå­—äº¤å‰çš„attentionå’Œrecurrentç»“æ„ï¼Œå‡å°‘äº†è®¡ç®—é‡ï¼ŒåŒæ—¶ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•è·é•¿è·ç¦»ä¾èµ–ï¼Œä»¥åŠæå‡äº†æ¨¡å‹è¡¨ç°ã€‚ Motivation:NL-Netä¼šç”Ÿæˆä¸€ä¸ªå¾ˆå¤§çš„attention mapï¼Œå…¶å¤æ‚åº¦ä¸º${\mathcal{O}}((H \times W)\times(H \times W))$ã€‚ ä¸»è¦åšæ³•ï¼Œå°†è¯¥position-wiseçš„attentionåˆ†è§£æˆä¸¤æ­¥ï¼šç¬¬ä¸€æ­¥æ˜¯æ¯ä¸ªç‚¹åªå’Œå…¶åŒä¸€è¡Œå’ŒåŒä¸€åˆ—çš„è¿›è¡Œattentionï¼Œå°†attentionçš„æ“ä½œå¾ªç¯å¤šæ¬¡ï¼Œè¾¾åˆ°æ¯ä¸ªç‚¹é—´æ¥å’Œå…¶ä»–ç‚¹éƒ½åšäº†attentionã€‚å…¶å¤æ‚åº¦åˆ™ä¸º $\mathcal{O}((H \times W) \times(H+W-1))$ ä¸¤ç§æ–¹æ³•çš„å¯¹æ¯”ï¼š æ³¨æ„åˆ°recurrentçš„ç»“æ„ä¸­å‚æ•°æ˜¯å…±äº«çš„ã€‚ å…·ä½“çš„æ¡†æ¶ï¼š å…ˆè¿›è¡Œé™ç»´ï¼Œåšå®Œcriss-cross attentionåçš„outputä¸åŸå…ˆçš„xæ‹¼èµ·æ¥ï¼Œå†è¿‡CNNç­‰è¿›è¡Œèåˆã€‚ Criss-Cross Attention è¿‡ä¸‰ä¸ªçº¿æ€§å±‚å¾—åˆ°QKVï¼ˆå’Œtransformerç±»ä¼¼ï¼‰ï¼›æ¥ç€Qä¸Kåšçºµæ¨ªäº¤å‰çš„attentionï¼Œè·å¾—softmaxï¼Œæ¥ç€å†å’ŒVå¯¹åº”çš„ä½ç½®ç›¸ä¹˜ã€‚ å…·ä½“è€Œè¨€ï¼šè¾“å…¥ï¼š$\mathbf{H} \in \mathbb{R}^{C \times W \times H}$è¿‡çº¿æ€§å±‚ï¼š$\{\mathbf{Q}, \mathbf{K}\} \in \mathbb{R}^{C^{\prime} \times W \times H}$attentionåˆ†æ•°ï¼š$\mathbf{A} \in \mathbb{R}^{(H+W-1) \times W \times H}$ä¸uå…ƒç´ åšattentionçš„featureé›†åˆï¼Œä¹Ÿå³åŒä¸€è¡Œæˆ–åŒä¸€åˆ—çš„featureï¼š$\boldsymbol{\Omega}_{\mathbf{u}} \in \mathbb{R}^{(H+W-1) \times C^{\prime}} \cdot \mathbf{\Omega}_{\mathbf{i}, \mathbf{u}} \in \mathbb{R}^{C^{\prime}}$åšattentionï¼š$d_{i, u}=\mathbf{Q}_{\mathbf{u}} \mathbf{\Omega}_{\mathbf{i}, \mathbf{u}^{\top}}$å†åšsoftmaxï¼Œæœ€ç»ˆè·å¾—outputï¼š$\mathbf{H}_{\mathbf{u}}^{\prime}=\sum_{i \in\left|\mathbf{\Phi}_{\mathbf{u}}\right|} \mathbf{A}_{\mathbf{i}, \mathbf{u}} \mathbf{\Phi}_{\mathbf{i}, \mathbf{u}}+\mathbf{H}_{\mathbf{u}}$$\boldsymbol{\Phi}_{\mathbf{i}, \mathbf{u}}$ä¸$\boldsymbol{\Omega}_{\mathbf{u}}$æ˜¯åŒä¸€é›†åˆã€‚ Recurrent Criss-Cross Attentionå¤šåšå‡ æ¬¡ï¼Œæ¯æ¬¡éƒ½å…±äº«ï¼Œå°±æ˜¯recurrentäº†ã€‚å½“å¾ªç¯æ¬¡æ•°æ˜¯2æ—¶ï¼Œæ¯ä¸ªç‚¹éƒ½èƒ½å¤Ÿattendåˆ°å…¶ä»–ä»»ä½•ç‚¹äº†ã€‚ æ€è€ƒï¼šé€šè¿‡çºµæ¨ªæ¥é—´æ¥attendåˆ°æ‰€æœ‰ç‚¹ï¼Œè¿™ä¸ªæƒ³æ³•è¿˜è›®æœ‰è¶£çš„ã€‚å¹¶ä¸”å‡å°‘äº†è®¡ç®—é‡ã€‚å°±æ˜¯è¿™ç§çºµæ¨ªçš„æ–¹æ³•ä»£ç è¦æ€ä¹ˆå®ç°ï¼Ÿæœ‰äº›å¥½å¥‡ã€‚åŒæ—¶æœ¬æ–‡çš„å›¾ä¹Ÿå¾ˆæ¼‚äº®ï¼Œæ¯ä¸ªå›¾éƒ½æ°åˆ°å¥½å¤„ï¼Œå¯ä»¥é€šè¿‡å›¾å°±å¤§è‡´ç†è§£æœ¬æ–‡åœ¨è®²ä»€ä¹ˆã€‚]]></content>
      <tags>
        <tag>Classification</tag>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>LSTM</tag>
        <tag>Language Modeling</tag>
        <tag>pretrain</tag>
        <tag>long-term dependency</tag>
        <tag>GC-Net</tag>
        <tag>Ordered Neuron</tag>
        <tag>GPT</tag>
        <tag>GE-Net</tag>
        <tag>PSANet</tag>
        <tag>CCNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Macä¸Šå¾ˆå¥½ç”¨çš„è½¯ä»¶æ¨è]]></title>
    <url>%2F2019%2F05%2F07%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2FMac%E4%B8%8A%E5%BE%88%E5%A5%BD%E7%94%A8%E7%9A%84%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[è®°å½•ä¸ªäººè§‰å¾—å¾ˆå¥½ç”¨çš„Macè½¯ä»¶ï¼Œè®©Macä½œä¸ºï¼ˆç¨‹åºå‘˜ğŸ‘¨â€ğŸ’»â€/ç§‘ç ”äººå‘˜ğŸ‘¨â€ğŸ”¬ï¼‰ç”Ÿäº§åŠ›å·¥å…·æ›´é¡ºæ‰‹ï¼Œæå‡ç”Ÿäº§æ•ˆç‡ã€‚ [Alfred3]ç½‘ä¸Šæœ‰å¤ªå¤šæ¨èAlfredçš„äº†ï¼Œæˆ‘ä¸»è¦æ˜¯ç”¨Alfredåšä¸€äº›å¸¸è§„æ“ä½œï¼Œå¦‚æ‰“å¼€/æœç´¢æ–‡ä»¶ï¼Œæ–‡æœ¬æ‰©å±•ç­‰ï¼Œä»¥åŠä¸€äº›workflowï¼Œå¦‚æœ‰é“ç¿»è¯‘ã€‚ [SwitchResX]ç”¨äºæ˜¾ç¤ºå™¨çš„è°ƒæ•´ï¼Œå¯¹äºæˆ‘è€Œè¨€ä¸»è¦ç”¨äºå¤–æ¥å±å¹•å¼€å¯HiDPIã€‚éå¸¸å¥½ç”¨ï¼ å¦‚ä½•å¼€å¯HiDPIï¼šhttps://www.zhihu.com/question/35300978/answer/126332986 [gfxCardStatus]ç”¨äºåˆ‡æ¢å¤–æ¥æ˜¾å¡å’Œç‹¬ç«‹æ˜¾å¡ã€‚èƒ½å¤Ÿåœ¨menu barä¸Šè°ƒæ•´ï¼Œæ¯”æ¯æ¬¡è¿›å…¥system preferenceè®¾ç½®æ–¹ä¾¿ä¸€äº›ã€‚ [Mendeley]æ–‡çŒ®ç®¡ç†å·¥å…·ï¼Œæœ‰ä¸°å¯Œçš„åŠŸèƒ½å’ŒåŒæ­¥åŠŸèƒ½ã€‚å¤šå¹³å°ä¸”å…è´¹ï¼Œå¾ˆçœå¿ƒã€‚ [Bartender3]å¦‚æœå¤ªå¤šå›¾æ ‡éƒ½æ˜¾ç¤ºåœ¨menu barä¸Šï¼Œä¼šå½±å“è§‚æ„Ÿï¼Œä¸èƒ½ä¸€ä¸‹å­æ‰¾åˆ°è‡ªå·±æƒ³è¦çš„ä¸œè¥¿ã€‚ä½¿ç”¨Bartender3èƒ½å¤Ÿéšè—éƒ¨åˆ†å›¾æ ‡ï¼Œå¹¶ä¸”å¾ˆä¼˜é›…ã€‚ éšè—çŠ¶æ€ï¼š å±•å¼€çŠ¶æ€ï¼š [Todoist]å­˜æ”¾è¦å®Œæˆçš„äº‹åŠ¡ï¼Œè¿˜å¯ä»¥å­˜æ”¾ä¸€äº›å…¶ä»–çš„ä¸œè¥¿ã€‚ç•Œé¢éå¸¸å¥½çœ‹ï¼ŒåŠŸèƒ½ä¸°å¯Œï¼Œå¹¶ä¸”ä¹Ÿæ˜¯å…¨å¹³å°çš„ã€‚ä»˜è´¹ï¼Œä½†å®Œå…¨å€¼å¾—ã€‚å¯¹æˆ‘è€Œè¨€Todoistå¸®åŠ©æˆ‘å°†å·¥ä½œæ•´ç†å¾—äº•äº•æœ‰æ¡ã€‚é™¤äº†å·¥ä½œï¼Œæˆ‘è¿˜ä¼šä¿å­˜ä¸€äº›å…¶ä»–listï¼ˆå¦‚èœå•/æ„¿æœ›æ¸…å•ğŸ˜„ï¼‰ã€‚ [MWeb]Markdownå†™ä½œå·¥å…·ã€‚æˆ‘ç”¨è¿™ä¸ªè½¯ä»¶å†™æ‰€æœ‰çš„åšå®¢ï¼Œé¢œå€¼å¾ˆé«˜ï¼Œå¹¶ä¸”åŠŸèƒ½ä¹Ÿå¾ˆå¼ºå¤§ï¼Œèƒ½å¤Ÿå®æ—¶é¢„è§ˆæ•ˆæœã€‚æ’å…¥å›¾ç‰‡å’Œå…¬å¼éå¸¸éå¸¸æ–¹ä¾¿ã€‚ [IINA]Macä¸Šæœ€å¥½ç”¨çš„æ’­æ”¾å™¨ã€‚ [VMware Fusion]å¶å°”éœ€è¦ç”¨åˆ°è™šæ‹Ÿæœºï¼Œå¯ä»¥æ— ç¼åˆ‡æ¢å¤šç³»ç»Ÿã€‚ [Dash]APIæµè§ˆå™¨ï¼Œä¸€ä¸ªçª—å£æŸ¥æ‰€æœ‰è¯­è¨€/åŒ…çš„APIã€‚ [Cinch][Deprecated] Macä¸Šçš„åˆ†å±ç¡®å®ä¸å¦‚Windowsä¸Šçš„å¥½ç”¨ï¼ŒCinchè‡´åŠ›äºåœ¨Macä¸Šä¹Ÿæœ‰Windowsçš„åˆ†å±ä½“éªŒã€‚å°†çª—å£å‘ä¸Šæ‹–æˆ–å‘ä¸¤è¾¹æ‹–ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å…¨å±æˆ–è€…åˆ†å±ã€‚ å¯¹å¤šå±å¹•ä¸æ˜¯å¾ˆå‹å¥½ï¼Œå¦‚æœæ˜¯å‘å³æ‹–åˆ†å±å¯èƒ½ä¼šæ‹–åˆ°å¤–æ¥å±å¹•ä¸Šã€‚ [Moom]åŠŸèƒ½æå…¶å¼ºå¤§çš„åˆ†å±åº”ç”¨ã€‚æ—¢é€‚åˆæ™®é€šé¼ æ ‡å…šä½¿ç”¨ï¼Œä¹Ÿæ”¯æŒé«˜åº¦å®šåˆ¶åŒ–é€‚åˆé”®ç›˜å…šçš„ä½¿ç”¨ã€‚ https://zhuanlan.zhihu.com/p/20258341 [Mathpix Snipping Tool]å¼ºçƒˆæ¨èè¿™æ¬¾è½¯ä»¶ï¼å†™åšå®¢æ—¶ç»å¸¸è¦æ’å…¥è®ºæ–‡é‡Œé¢çš„å…¬å¼ï¼Œå¦‚æœè‡ªå·±æ‰“ä¸ä»…éº»çƒ¦ï¼Œæœ‰äº›è¿˜ä¸çŸ¥é“æ€ä¹ˆæ‰“ã€‚Mathpixèƒ½å¤Ÿå°†æˆªå›¾è‡ªåŠ¨è½¬æ¢æˆå…¬å¼ï¼Œå¹¶ä¸”è¯†åˆ«ç‡æŒºé«˜ï¼Œçœäº†å¾ˆå¤šåŠ›æ°”ã€‚ [Xnip]ä¸€æ¬¾å¾ˆæ–¹ä¾¿çš„æˆªå›¾è½¯ä»¶ã€‚æä¾›äº†è®¸å¤šå¦‚é©¬èµ›å…‹ç”»å›¾çš„åŠŸèƒ½ï¼Œè¿˜æ”¯æŒæˆªé•¿å›¾ã€‚ [Paste]ä¿å­˜æ‰€æœ‰ä¹‹å‰å¤åˆ¶å†…å®¹çš„å†å²ï¼ŒåŒ…æ‹¬æ–‡æœ¬ï¼Œå›¾ç‰‡ï¼Œæ–‡ä»¶ç­‰ï¼Œè¿˜å¯ä»¥ä¿å­˜ä¸€äº›å¸¸ç”¨çš„å†…å®¹ã€‚å¹¶ä¸”å…¶æœ€å¤§çš„äº®ç‚¹åœ¨äºèƒ½å¤Ÿä¸iOSåŒæ­¥ï¼Œä¹Ÿå³åœ¨ä¸€ä¸ªå¹³å°å¤åˆ¶çš„å†…å®¹å¯ä»¥ç›´æ¥åœ¨å¦ä¸€ä¸ªå¹³å°ç”¨åˆ°ã€‚ [Transmit]Macä¸ŠFTPåšå¾—å¾ˆå¥½çš„ä¸€ä¸ªè½¯ä»¶ï¼Œé¢œå€¼ä¹ŸæŒºé«˜ã€‚ [iStat Menus]ç›‘æ§ç³»ç»ŸçŠ¶æ€ï¼ˆCPU/GPU/å†…å­˜/ç½‘ç»œï¼‰çš„è½¯ä»¶ã€‚ç¨³å®šä¸”ç¾è§‚ã€‚æŒ‚åœ¨menu barä¸Šå¯ä»¥å¾ˆæ–¹ä¾¿æŸ¥çœ‹ã€‚ [Downie3]è½»é‡çº§çš„ä¸€ä¸ªä¸‹è½½è§†é¢‘çš„å·¥å…·ï¼Œèƒ½å¤Ÿè‡ªåŠ¨/æ‰‹åŠ¨æå–æµè§ˆå™¨çš„è§†é¢‘é“¾æ¥ã€‚ [The Unarchiver]è½»é‡çº§çš„è§£å‹è½¯ä»¶ï¼Œè½»åˆ°ç”šè‡³æ„Ÿè§‰ä¸åˆ°ä»–çš„å­˜åœ¨ã€‚ [OneNote]è®°ç¬”è®°çš„åˆ©å™¨ï¼Œofficeå¥—ä»¶ä¸­æˆ‘ç”¨å¾—æœ€é¢‘ç¹çš„è½¯ä»¶ã€‚è®°å½•è®ºæ–‡é˜…è¯»ç¬”è®°/æƒ³æ³•/å®éªŒã€‚ç‹¬ä¸€æ— äºŒçš„çµæ´»æ€§ï¼Œå¯ä»¥åœ¨é¡µé¢çš„ä»»ä½•åœ°æ–¹åˆ›å»ºç¬”è®°ï¼ˆæˆ‘è¯•äº†å¸‚é¢ä¸Šæ‰€æœ‰æµè¡Œçš„ç¬”è®°è½¯ä»¶ï¼Œéƒ½æ²¡æœ‰è¿™æ ·çš„çµæ´»æ€§ï¼‰ã€‚åŒæ—¶è¿˜æ˜¯å…¨å¹³å°ï¼Œè¿˜æ”¯æŒç¬”ï¼Œåœ¨iPadä¸Šçš„ç¬”è¿¹å¯ä»¥å¾ˆå¿«é€ŸåŒæ­¥åˆ°Macä¸Šã€‚å½“ç„¶ç›®å‰è€Œè¨€åŠŸèƒ½è¿˜æ²¡æœ‰Windowsä¸Šçš„OneNoteé‚£ä¹ˆå¼ºå¤§ã€‚ [Maipo]Macç«¯å¾ˆä¸é”™çš„å¾®åšå®¢æˆ·ç«¯ã€‚å¯ä»¥å¾ˆæ–¹ä¾¿åœ°åœ¨ç”µè„‘ç«¯åˆ·å¾®åšå­¦ä¹ ï¼ˆå¤§é›¾ ã€‚ [linux-command]æ–¹ä¾¿æœç´¢Linuxå‘½ä»¤ã€‚ [iTerm]å‘½ä»¤è¡Œæ˜¯ğŸ‘¨â€ğŸ’»â€å¿…å¤‡ã€‚è€ŒiTermç›¸å¯¹åŸç”Ÿterminalæœ‰æ›´ä¸°å¯Œçš„è®¾ç½®ä»¥åŠæ›´å¼ºå¤§çš„åŠŸèƒ½ã€‚ [AppCleaner]è½»æ¾åœ°åˆ é™¤Macè½¯ä»¶ï¼Œå¯ä»¥æ£€æµ‹è¯¥è½¯ä»¶æ‰€å¸¦çš„å…¶ä»–æ–‡ä»¶ï¼Œä¸€å¹¶åˆ é™¤ã€‚é…åˆAlfredçš„workflowå¾ˆæ–¹ä¾¿ã€‚ [Mos]Macçš„è§¦æ§æ¿å’Œé¼ æ ‡çš„é€»è¾‘æ˜¯åçš„ã€‚å¦‚æœå¸Œæœ›è§¦æ§æ¿å’Œé¼ æ ‡ä¸€èµ·ç”¨ï¼Œå¹¶ä¸”é€»è¾‘å„ä¸ç›¸åŒï¼Œåˆ™å¯ä»¥ä½¿ç”¨Mosï¼ŒMosèƒ½å¤Ÿå°†é¼ æ ‡ç¿»è½¬ã€‚ [PDF Expert]å¼ºçƒˆæ¨èçš„PDFé˜…è¯»ç¼–è¾‘è½¯ä»¶ã€‚ç®€å•æ˜“ç”¨ï¼Œå¹¶ä¸”åŠŸèƒ½ä¹Ÿè¶³å¤Ÿå¼ºå¤§ã€‚å¾ˆé‡è¦çš„ä¸€ç‚¹æ˜¯å¯ä»¥å¤šå¹³å°åŒæ­¥ï¼ˆMac/iOSï¼‰ï¼Œè¿˜å¯ä»¥ä½¿ç”¨â€æ¥åŠ›â€œåŠŸèƒ½ï¼Œå‡å°‘åˆ‡æ¢è®¾å¤‡çš„éº»çƒ¦ã€‚ [Contexts]å¿«é€Ÿåˆ‡æ¢çª—å£çš„æ•ˆç‡å·¥å…·ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤šæ˜¾ç¤ºå™¨çš„æƒ…å†µä¸‹ï¼Œå¾€å¾€ä¼šæ‰¾ä¸åˆ°æƒ³è¦çš„çª—å£ã€‚å¯¹æˆ‘è€Œè¨€ï¼ŒContextsæå¤§å¢åŠ äº†æ•ˆç‡ã€‚è°ƒå‡ºçª—å£åˆ—è¡¨ï¼Œæ¥ç€æœç´¢å³å¯ï¼Œè¿˜å¯ä»¥é€šè¿‡é¼ æ ‡ç‚¹å‡»å±å¹•è¾¹ä¸Šçš„æµ®åŠ¨çª—å£åˆ‡æ¢ã€‚ [SnippetsLab]æ”¶è—ä¸€äº›æœ‰ç”¨çš„ä»£ç ç‰‡æ®µã€‚ä¿æŒè®°å½•çš„ä¹ æƒ¯èƒ½å¤Ÿæé«˜ä»£ç æ•ˆç‡ã€‚ [Sublime Text]ä¸éœ€è¦æ¨èï¼Œå¤§åé¼é¼çš„æ–‡æœ¬ç¼–è¾‘è½¯ä»¶ã€‚ğŸ‘¨â€ğŸ’»â€å¿…å¤‡ã€‚ [AdGuard for Safari]å› ä¸ºSafariçš„è½»é‡ä»¥åŠé¢œå€¼æ”¾å¼ƒäº†ä½¿ç”¨å¾ˆä¹…çš„Chromeï¼Œä½†ä¹Ÿæ„å‘³ç€æ”¾å¼ƒäº†ä¸°å¯Œçš„æµè§ˆå™¨æ’ä»¶ã€‚åœ¨Chromeå¯ä»¥ä½¿ç”¨AdBlockï¼Œåœ¨Safariåˆ™å¯ä»¥ä½¿ç”¨AdGuardï¼Œéå¸¸è½»é‡ï¼Œç”šè‡³æ„Ÿè§‰ä¸åˆ°å®ƒçš„å­˜åœ¨ã€‚ [CatchMouse]é’ˆå¯¹å¤šå±å¹•è€Œè®¾è®¡ã€‚æœ‰æ—¶å€™ä¼šæ‰¾ä¸åˆ°é¼ æ ‡åœ¨å“ªä¸ªå±å¹•ã€‚é€šè¿‡è®¾ç½®å¿«æ·é”®ï¼Œèƒ½å¤Ÿå¿«é€Ÿå°†é¼ æ ‡ç§»åŠ¨åˆ°æŒ‡å®šå±å¹•ï¼Œåœ¨åˆ‡æ¢çš„æ—¶å€™è¿˜ä¼šç¼©æ”¾é¼ æ ‡çš„å›¾æ ‡ä½œä¸ºæé†’ã€‚ï¼ˆæ‰¾äº†å¥½ä¹…æ‰æ‰¾åˆ°è¿™ä¸ªç¬¦åˆæˆ‘éœ€æ±‚çš„è½¯ä»¶ï¼‰ [Zoom]ä¼šè®®ç”µè¯çš„è½¯ä»¶ã€‚å¼€è¿œç¨‹PaperReadingå¯ä»¥ç”¨ğŸŒšã€‚ [Mate Translate]é›†æˆåœ¨å³é”®çš„ç¿»è¯‘å·¥å…·ã€‚åœ¨çœ‹è®ºæ–‡æˆ–è€…æµè§ˆç½‘é¡µæ—¶å¯ä»¥éšæ—¶ç¿»è¯‘å¥å­ã€‚ [ToothFairy]ä¸€é”®è¿æ¥è“ç‰™è®¾å¤‡çš„åº”ç”¨ã€‚é©»æ‰åœ¨MenuBarä¸Šå¯ä»¥ä¸€é”®è¿æ¥è‡ªå·±çš„è“ç‰™è®¾å¤‡ã€‚æˆ‘ä¸€èˆ¬ç”¨äºè¿æ¥æ— çº¿è“ç‰™è€³æœºï¼ˆXM3ï¼‰å’ŒAirpodsã€‚ [Quicklookæ’ä»¶]Macä¸Šä¸€ä¸ªå¾ˆäººæ€§åŒ–çš„æ“ä½œå°±æ˜¯å¯ä»¥ç©ºæ ¼é¢„è§ˆã€‚ä½†å¯¹äºä¸€äº›æ ¼å¼æ”¯æŒè¿˜ä¸å¤Ÿå¥½ï¼Œæ¯”å¦‚æ— æ‰©å±•åçš„æ— æ³•æ”¯æŒé¢„è§ˆï¼ŒMarkdownåªèƒ½é¢„è§ˆæºä»£ç ï¼Œé¢„è§ˆæºä»£ç åªæ”¯æŒé¢„è§ˆçº¯æ–‡æœ¬ï¼Œå¹¶æ²¡æœ‰é«˜äº®æ˜¾ç¤ºã€‚å› æ­¤Quicklookæ’ä»¶å¯ä»¥è§£å†³è¿™äº›ç»†èŠ‚é—®é¢˜ã€‚ https://github.com/sindresorhus/quick-look-plugins é¢„è§ˆMarkdownï¼š é¢„è§ˆæ— æ‰©å±•åçš„çº¯æ–‡æœ¬ï¼š æœ€åæ™’æ™’è‡ªå·±çš„å·¥ä½œå°â˜ºï¸ï¼ˆæœ¬ç§‘ä¸€ç›´å¿ƒå¿ƒå¿µå¿µçš„å·¥ä½œå°ï¼Œä¼¼ä¹ç»ˆäºè¾¾æˆäº†ï¼Œè™½ç„¶æ¡Œé¢å¤§å°è¿˜æ˜¯å°äº†ä¸€äº›ï¼‰ã€‚æœ‰å¥½çš„ç”Ÿäº§å·¥å…·çœŸæ˜¯å¯ä»¥è®©å·¥ä½œå˜æˆä¸€ç§äº«å—ğŸ˜„ï¼Œå¼€å¿ƒã€‚]]></content>
      <tags>
        <tag>æ‚ä¸ƒæ‚å…«</tag>
        <tag>Mac</tag>
        <tag>app</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³äºtransformer-xlä¸­rel-shiftå®ç°çš„è§£è¯»]]></title>
    <url>%2F2019%2F05%2F07%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E5%85%B3%E4%BA%8Etransformer-xl%E4%B8%ADrel-shift%E5%AE%9E%E7%8E%B0%E7%9A%84%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[èƒŒæ™¯transformer-xlä¸­æœ‰ä¸€æ­¥ä½¿ç”¨ç›¸å¯¹ä½ç½®è®¡ç®—attention weightï¼š $\mathbf{A}_{i, j}^{\mathrm{rel}}=\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}}_{(b)}+\underbrace{u^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{v^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}}_{(d)}$ ç”±äºç›¸å¯¹ä½ç½®è¦è®¡ç®—æ‰€æœ‰çš„queryä¸keyå¯¹ï¼Œå› æ­¤æ˜¯å¹³æ–¹çš„å¤æ‚åº¦ã€‚è€Œåœ¨è®ºæ–‡çš„é™„å½•ä¸­æåˆ°å¯ä»¥é€šè¿‡ç®€å•çš„æ¨å¯¼å°†å¤æ‚åº¦é™ä¸ºçº¿æ€§ã€‚ç®€å•åœ°è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›è·å¾—ï¼š$\mathbf{B} = \left[ \begin{array}{cccccc}{q_{0}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{M}} &amp; {\cdots} &amp; {q_{0}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{0}} &amp; {0} &amp; {\cdots} &amp; {0} \\ {q_{1}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{M+1}} &amp; {\cdots} &amp; {q_{1}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{1}} &amp; {q_{1}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{0}} &amp; {\cdots} &amp; {0} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {q_{L-1}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{M+L-1}} &amp; {\cdots} &amp; {q_{L-1}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{M+L-1}} &amp; {q_{L-1}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{L-1}} &amp; {\cdots} &amp; {q_{L-1}^{\top} \mathbf{W}_{k, R} \mathbf{R}_{0}}\end{array}\right] \\ = \left[ \begin{array}{cccccc}{q_{0}^{\top} \mathbf{Q}_{L-1}} &amp; {\cdots} &amp; {q_{0}^{\top} \mathbf{Q}_{M+L-1}} &amp; {0} &amp; {\cdots} &amp; {0} \\ {q_{1}^{\top} \mathbf{Q}_{L-2}} &amp; {\cdots} &amp; {q_{1}^{\top} \mathbf{Q}_{M+L-2}} &amp; {q_{1}^{\top} \mathbf{Q}_{M+L-1}} &amp; {\cdots} &amp; {0} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {q_{L-1}^{\top} \mathbf{Q}_{0}} &amp; {\cdots} &amp; {q_{L-1}^{\top} \mathbf{Q}_{M}} &amp; {q_{L-1}^{\top} \mathbf{Q}_{M+1}} &amp; {\cdots} &amp; {q_{L-1}^{\top} \mathbf{Q}_{M+L-1}}\end{array}\right]$ å…¶ä¸­ï¼š$\mathbf{Q} :=\left[ \begin{array}{c}{\mathbf{R}_{M+L-1}^{\top}} \\ {\mathbf{R}_{M+L-2}^{\top}} \\ {\vdots} \\ {\mathbf{R}_{1}^{\top}} \\ {\mathbf{R}_{0}^{\top}}\end{array}\right] \mathbf{W}_{k, R}^{\top}=\left[ \begin{array}{c}{\left[\mathbf{W}_{k, R} \mathbf{R}_{M+L-1}\right]^{\top}} \\ {\vdots} \\ {\vdots} \\ {\left[\mathbf{W}_{k, R} \mathbf{R}_{1}\right]^{\top}} \\ {\left[\mathbf{W}_{k, R} \mathbf{R}_{0}\right]^{\top}}\end{array}\right] \in \mathbb{R}^{(M+L) \times d}$ è€Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è·å¾—çš„æ˜¯ï¼š$\tilde{\mathbf{B}}=\mathbf{q} \mathbf{Q}^{\top}=\left[ \begin{array}{cccccc}{q_{0}^{\top} \mathbf{Q}_{0}} &amp; {\cdots} &amp; {q_{0}^{\top} \mathbf{Q}_{M}} &amp; {q_{0}^{\top} \mathbf{Q}_{M+1}} &amp; {\cdots} &amp; {q_{0}^{\top} \mathbf{Q}_{M+L-1}} \\ {q_{1}^{\top} \mathbf{Q}_{0}} &amp; {\cdots} &amp; {q_{1}^{\top} \mathbf{Q}_{M}} &amp; {q_{1}^{\top} \mathbf{Q}_{M+1}} &amp; {\cdots} &amp; {q_{1}^{\top} \mathbf{Q}_{M+L-1}} \\ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {q_{L-1}^{\top} \mathbf{Q}_{0}} &amp; {\cdots} &amp; {q_{L-1}^{\top} \mathbf{Q}_{M}} &amp; {q_{L-1}^{\top} \mathbf{Q}_{M+1}} &amp; {\cdots} &amp; {q_{L-1}^{\top} \mathbf{Q}_{M+L-1}}\end{array}\right]$ $\tilde{\mathbf{B}}$ä¸$\mathbf{B}$çš„åŒºåˆ«åœ¨äº$\tilde{\mathbf{B}}$æ˜¯$\mathbf{B}$çš„left-shiftedç‰ˆæœ¬ï¼Œå…¶ä¸­ç¬¬ä¸€è¡Œå·¦ç§»äº†L-1ï¼Œåé¢æ¯è¡Œä¾æ¬¡é€’å‡å·¦ç§»ä¸ªæ•°ï¼Œæœ€åä¸€è¡Œåˆ™ä¸å·¦ç§»ã€‚ æ–¹æ³•æŠ½è±¡åœ°çœ‹ï¼Œæˆ‘ä»¬è¦åšçš„äº‹æƒ…å°±æ˜¯ï¼Œç»™å®šä¸€ä¸ªçŸ©é˜µï¼Œæ¯è¡Œéƒ½è¿›è¡Œå·¦ç§»ï¼Œè€Œç§»åŠ¨çš„ä¸ªæ•°éšè¡Œæ•°é€’å¢è€Œé€’å‡ã€‚ æˆ‘ç›®å‰æƒ³åˆ°çš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨gatherï¼Œå°†æƒ³è¦çš„indexæå‰å®šå¥½ï¼Œç„¶åä½¿ç”¨Pytorchçš„gatherå°±èƒ½å¤Ÿå®ç°ã€‚ è€Œtransformer-xlå®ç°äº†å¦ä¸€ç§æ›´å¥½çš„æ–¹æ³•ï¼š_rel_shiftã€‚ 1234567891011def _rel_shift(self, x, zero_triu=False): # x: q,k,bs,n_head zero_pad = torch.zeros((x.size(0), 1, *x.size()[2:]), device=x.device, dtype=x.dtype) x_padded = torch.cat([zero_pad, x], dim=1) x_padded = x_padded.view(x.size(1) + 1, x.size(0), *x.size()[2:]) x = x_padded[1:].view_as(x) return x ç¬¬ä¸€æ­¥æ˜¯ï¼Œå°†xçš„ç¬¬ä¸€åˆ—å¡«ä¸Špaddingï¼Œæ­¤æ—¶x.size()=q,k+1,bs,n_headï¼Œæ¥ä¸‹æ¥å°†å…¶é‡æ–°reshapeï¼Œåˆ™å˜æˆäº†x.size()=k+1,q,bs,n_headï¼Œæœ€åå°†ç¬¬ä¸€è¡Œå»æ‰ï¼Œå˜æˆx.size()=k,q,bs,n_headï¼Œå†å°†å…¶reshapeå›xåŸæ¥çš„æ ·å­ã€‚ ä¸ºä»€ä¹ˆè¿™ä¹ˆåšå®ç°äº†æˆ‘ä»¬æƒ³è¦çš„å·¦ç§»çš„åŠŸèƒ½ï¼Ÿæˆ‘ä»¬åº”è¯¥ä»ä¸€ç»´çš„è§’åº¦å»ç†è§£ã€‚å› ä¸ºå®é™…ä¸Šåœ¨å†…å­˜ä¸­æ‰€æœ‰å…ƒç´ éƒ½æ˜¯æŒ‰ç…§ä¸€ç»´å»æ’åˆ—çš„ã€‚ åŸæ¥çš„çŸ©é˜µï¼š å®é™…ä¸Šå°±æ˜¯æœ‰qä¸ªkeyæŒ‰ç…§ä¸€è¡Œå»æ’åˆ—ã€‚ åœ¨åšå®Œpaddingä¹‹åï¼Œåˆ™ï¼š å®é™…ä¸Šå°±æ˜¯åœ¨æ¯ä¸ªkeyå‰é¢æ’å…¥äº†0ã€‚ æ¥ä¸‹æ¥viewï¼Œå®é™…ä¸Šæ•°æ®çš„å…ˆåé¡ºåºè¿˜æ˜¯æ²¡æœ‰å˜ï¼ˆå› ä¸ºä¸æ˜¯transposeï¼‰ï¼š å®é™…ä¸Šåªæ˜¯å¼ºè¡Œå°†è¯¥è¡Œåˆ‡æˆä¸€ä¸ªä¸€ä¸ªqè€Œå·²ã€‚ é‚£ä¹ˆæœ€åä¸€ä¸ªæ“ä½œï¼Œå°†ç¬¬ä¸€è¡Œä¸¢æ‰ï¼Œå®é™…ä¸Šå°±æ˜¯è¦æŠŠåŸæ¥çš„xçš„ç¬¬ä¸€è¡Œå¼ºè¡Œå·¦ç§»q-1ä¸ªï¼ˆå› ä¸ºæœ‰paddingï¼‰ã€‚é‚£ä¹ˆä¸ºä»€ä¹ˆåé¢çš„è¡Œèƒ½å¤Ÿå·¦ç§»çš„ä¸ªæ•°ä¾æ¬¡å‡å°‘ï¼Ÿåˆ«å¿˜äº†paddingï¼Œç¬¬ä¸€è¡Œå·¦ç§»äº†q-1ä¸ªï¼Œä½†ç¬¬äºŒä¸ªkeyå‰é¢ä¹Ÿæœ‰ä¸€ä¸ªpaddingï¼Œæ‰€ä»¥ç›¸å½“äºå°†å…¶å‘å³æ¨äº†ä¸€æ ¼ï¼›ç¬¬ä¸‰ä¸ªåˆæœ‰ä¸€ä¸ªpaddingï¼Œå°±åœ¨åŸæ¥çš„åŸºç¡€ä¸Šåˆæ¨äº†ä¸€æ ¼ï¼Œä¹Ÿå³æ¨äº†ä¸¤æ ¼ã€‚å› æ­¤æœ€åè¾¾åˆ°äº†æˆ‘ä»¬æƒ³è¦çš„ç›®çš„ã€‚ å®é™…ä¸Šè¦ç†è§£è¯¥æ–¹æ³•ï¼Œéœ€è¦ç‰¢ç‰¢æŠŠæ¡æ•°æ®å­˜å‚¨çš„æœ¬è´¨æ˜¯ä¸€æ•´è¡Œã€‚ è¯¥æ–¹æ³•æ²¡æœ‰æ•°æ®çš„æ‹·è´ï¼Œå…¨éƒ¨éƒ½æ˜¯viewæ“ä½œï¼Œå› æ­¤æ›´é«˜æ•ˆã€‚ ä¸å¾—ä¸ä½©æœæƒ³åˆ°è¯¥æ–¹æ³•çš„äººçš„å·¥ç¨‹èƒ½åŠ›ï¼ŒåŒæ—¶ä¹Ÿæ„Ÿè°¢æˆ´å®å¸¦æˆ‘ç†è§£è¯¥æ–¹æ³•çš„æœ¬è´¨ï¼Œä¸€å¼€å§‹æˆ‘æ˜¯æ­»æ´»ä¸ç†è§£çš„ã€‚ä»¥åæˆ–è®¸å¯ä»¥å°†è¯¥æ€æƒ³çµæ´»åº”ç”¨åˆ°å…¶ä»–æ–¹é¢ã€‚]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
        <tag>transformer-xl</tag>
        <tag>rel-shift</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorchä¸­é‡åˆ°çš„é—®é¢˜ï¼ˆåˆé›†ï¼‰]]></title>
    <url>%2F2019%2F05%2F07%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPyTorch%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%88%E5%90%88%E9%9B%86%EF%BC%89%2F</url>
    <content type="text"><![CDATA[æ–­æ–­ç»­ç»­è®°å½•ä¸€ä¸‹åœ¨å†™ä»£ç è¿‡ç¨‹ä¸­é‡åˆ°çš„é—®é¢˜ä»¥åŠè§£å†³æ–¹æ¡ˆã€‚ [Parameter nan]http://www.linzehui.me/2019/05/07/ç¢ç‰‡çŸ¥è¯†/å…³äºPytorchä¸­Parameterçš„nan/ é—®é¢˜æè¿°åœ¨å®šä¹‰Parameteræ—¶é‡åˆ°å…¶å…ƒç´ å­˜åœ¨nançš„é—®é¢˜ã€‚ åŸå› torch.Tensor(m,n)åªåˆ†é…ç©ºé—´è€Œæ²¡æœ‰å°†å…¶ä¸­çš„å€¼æ›´æ–°ã€‚ è§£å†³æ–¹æ¡ˆtorch.Tensor(m,n)æ˜¯ä¸å»ºè®®ä½¿ç”¨çš„åˆå§‹åŒ–æ–¹æ³•ã€‚æ”¹æˆtorch.nn.Parameter(torch.rand(10,10))æˆ–torch.nn.Parameter(torch.zeros(10,10))ã€‚ [+= inplace operation]http://www.linzehui.me/2018/12/09/ç¢ç‰‡çŸ¥è¯†/Pythonä¸­çš„+=æ“ä½œ/ é—®é¢˜æè¿°1output+=pos # posæ˜¯ä¸å¯æ›´æ–°çš„tensorï¼Œoutputæ˜¯å¯æ›´æ–°çš„tensor ç¨‹åºæŠ¥é”™ï¼šâ€œone of the variables needed for gradient computation has been modified by an inplace operationâ€ã€‚ åŸå› åœ¨Pythonä¸­ï¼Œi=i+1å’Œi+=1æ˜¯ä¸åŒçš„ï¼Œå¦‚æœè¢«æ“ä½œæ•°æ²¡æœ‰éƒ¨ç½² â€™iaddâ€˜æ–¹æ³•ï¼Œåˆ™i=i+1å’Œi+=1æ˜¯ç­‰ä»·çš„ï¼Œâ€™+=â€˜å¹¶ä¸ä¼šäº§ç”Ÿin-placeæ“ä½œï¼›å½“è¢«æ“ä½œæ•°æœ‰éƒ¨ç½²è¯¥æ–¹æ³•ä¸”æ­£ç¡®éƒ¨ç½²ï¼Œåˆ™æ˜¯ä¼šäº§ç”Ÿin-placeæ“ä½œçš„ã€‚å½“æ²¡æœ‰in-placeæ“ä½œæ—¶ï¼Œi=i+1è¡¨ç¤ºå¯¹ié‡åˆ†é…ï¼Œä¹Ÿå³iæŒ‡å‘äº†å¦ä¸€ä¸ªç©ºé—´è€Œä¸æ˜¯åŸæ¥çš„ç©ºé—´ã€‚åœ¨Pytorchä¸­ï¼Œä¹Ÿæœ‰éƒ¨ç½²â€™iadd()â€˜æ“ä½œï¼Œæ‰€ä»¥å¯¹äºoutput+=posï¼Œoutputå†…éƒ¨çš„å€¼è¢«æ”¹å˜äº†ï¼Œä¹Ÿå³åœ¨è®¡ç®—å›¾ä¸­å¼•å…¥äº†ç¯ï¼Œåœ¨åå‘æ±‚å¯¼æ—¶åˆ™ä¼šå‡ºé”™ã€‚ è§£å†³æ–¹æ¡ˆå°½é‡ä¸ä½¿ç”¨inplaceæ“ä½œï¼Œå³ä½¿æ˜¯å®˜æ–¹çš„APIï¼Œå¦‚unsqueeze_()ã€‚å·²ç»å¥½å‡ æ¬¡è¢«inplaceæ“ä½œå‘äº†ã€‚ [squeeze dim]http://www.linzehui.me/2019/05/06/ç¢ç‰‡çŸ¥è¯†/æ¯å‘¨ç¢ç‰‡çŸ¥è¯†20/ é—®é¢˜æè¿°å½“éœ€è¦squeezeæ—¶ï¼ŒæœªæŒ‡å®šsqueezeçš„ç»´åº¦ï¼Œå¯¼è‡´åé¢çš„ç»´æ•°ä¸ä¸€è‡´ï¼ŒæŠ¥é”™ã€‚ åŸå› ç”±äºåœ¨æŸäº›æç«¯æƒ…å†µä¸‹ï¼Œå¯èƒ½ä¼šå‡ºç°batch sizeä¸º1çš„æƒ…å†µï¼Œå½“é‡åˆ°è¿™ä¸ªæƒ…å†µæ—¶ï¼Œsqueezeä¼šå°†å…¶ä¸€å¹¶å‹ç¼©æ‰ï¼Œä½¿å¾—åé¢ä¼šå‡ºé”™ã€‚ è§£å†³æ–¹æ¡ˆå°½å¯èƒ½æ˜¾å¼æŒ‡å®šè¦å‹ç¼©çš„ç»´åº¦ï¼Œé™¤éå¾ˆæ˜ç¡®å°±è¦å°†æ‰€æœ‰çš„å‹ç¼©æ‰ã€‚ [infâ€”&gt;nan]http://www.linzehui.me/2019/07/03/ç¢ç‰‡çŸ¥è¯†/å…³äºPyTorchä¸­infå¯¼æ•°çš„nané—®é¢˜/ é—®é¢˜æè¿°ä¸¤ä¸ªtensorç›¸ä¹˜ï¼Œè‹¥å…¶ä¸­ä¸€ä¸ªtensorå¸¦æœ‰infï¼Œåˆ™å¦ä¸€ä¸ªtensorï¼ˆè¯¥tensorå¯æ›´æ–°ï¼‰çš„gradåˆ™ä¸ºnanã€‚ åŸå› ä¹˜æ³•çš„å¯¼æ•°çš„å®šä¹‰ã€‚ä¸¤ä¸ªtensorç›¸ä¹˜ï¼Œå¯¼æ•°ä¸ºå¯¹æ–¹ã€‚åœ¨PyTorchä¸­ï¼Œinfçš„gradåˆ™ä¸ºnanã€‚ è§£å†³æ–¹æ¡ˆåœ¨ç›¸ä¹˜å‰ï¼Œå°†å¸¦æœ‰infçš„tensoråšmasked_fillï¼Œinfè¢«ç½®ä¸º0åå†ç›¸ä¹˜ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>é‡åˆ°çš„é—®é¢˜</tag>
        <tag>Pytorch</tag>
        <tag>nan</tag>
        <tag>inf</tag>
        <tag>Parameter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³äºPytorchä¸­Parameterçš„nan]]></title>
    <url>%2F2019%2F05%2F07%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADParameter%E7%9A%84nan%2F</url>
    <content type="text"><![CDATA[å‰å‡ å¤©é‡åˆ°ä¸€ä¸ªå¾ˆç¥å¥‡çš„bugï¼Œåœ¨Modelé‡Œé¢å®šä¹‰ä¸€ä¸ªParameterï¼ŒParameterå‡ºç°äº†nanã€‚å¦‚ï¼š æ‰¾äº†ä¸€åœˆç½‘ä¸Šæ²¡æœ‰æ‰¾åˆ°å…¶åŸå› ï¼Œå·²ç»åœ¨è®ºå›æé—®äº†ã€‚æˆ‘çš„è§£å†³æ–¹æ¡ˆæ˜¯æ˜¾å¼å¯¹å…¶è¿›è¡Œåˆå§‹åŒ–ï¼š 1234567if args.init == 'uniform': nn.init.uniform_(self.u, -args.init_range, args.init_range) nn.init.uniform_(self.v, -args.init_range, args.init_range)elif args.init == 'normal': nn.init.normal_(self.u, 0.0, args.init_std) nn.init.normal_(self.v, 0.0, args.init_std) åŸæ¥æ˜¯torch.Tensorçš„é”…ï¼Œtorch.Tensorä¼šåˆ†é…å†…å­˜ç©ºé—´ï¼Œä½†ä¸ä¼šæ¸…ç©ºè¯¥ç©ºé—´çš„å€¼ï¼Œå› æ­¤é‡Œé¢å¯èƒ½ä¼šæœ‰å¥‡æ€ªçš„å€¼ã€‚æ­£ç¡®çš„åšæ³•åº”è¯¥æ˜¯ï¼š 123torch.nn.Parameter(torch.rand(10,10))# æˆ–è€…torch.nn.Parameter(torch.zeros(10,10)) å‚è€ƒèµ„æ–™ï¼šhttps://discuss.pytorch.org/t/nan-in-torch-tensor/8987]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>é‡åˆ°çš„é—®é¢˜</tag>
        <tag>Pytorch</tag>
        <tag>nan</tag>
        <tag>Parameter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•16]]></title>
    <url>%2F2019%2F05%2F06%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9516%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Pytorchæ£€æŸ¥tensor nan]12345678910# æ³•ä¸€ï¼ŒåŸºäºnan!=nan &gt;&gt;&gt; x = torch.tensor([1, 2, np.nan])tensor([ 1., 2., nan.])&gt;&gt;&gt; x != xtensor([ 0, 0, 1], dtype=torch.uint8)# æ³•äºŒï¼Œtorch.isnan(x)&gt;&gt;&gt; torch.isnan(x)tensor([ 0, 0, 1], dtype=torch.uint8)]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†20]]></title>
    <url>%2F2019%2F05%2F06%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8620%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Pytorch]â‘ 1torch.gather(input, dim, index, out=None) â†’ Tensor èƒ½å¤Ÿæ ¹æ®indexçš„å€¼åœ¨æŒ‡å®šç»´åº¦æ”¶é›†æ•°å€¼ã€‚å¯ä»¥ç”¨äºåˆ‡sliceã€‚ â‘¡expandå’Œrepeatä¸åŒï¼Œä¸ä¼šåˆ†é…æ–°çš„å†…å­˜ã€‚å¦‚æœä¸€ä¸ªtensorä½¿ç”¨expandå†catåˆ°å…¶ä»–tensorä¸Šï¼Œè¿™ä¸ªexpandè¿˜ä¼šçœå†…å­˜å—ï¼Ÿä¸ä¼šã€‚åœ¨catçš„æ—¶å€™ä¼šé‡æ–°åˆ†é…æ•´ä¸ªtensorçš„å†…å­˜ï¼Œå¹¶ä¸”å°†å…ƒç´ ä¸€ä¸ªä¸€ä¸ªcopyè¿‡å»ã€‚ https://discuss.pytorch.org/t/efficiency-of-torch-cat/8830 it pre-allocates the full tensor and then copy into it each element â‘¢ 1torch.einsum(equation, *operands) â†’ Tensor This function provides a way of computing multilinear expressions (i.e. sums of products) using the Einstein summation convention. å‘ç°ä¸€ä¸ªç¥å¥‡çš„apiï¼ŒPytorchæ”¯æŒçˆ±å› æ–¯å¦æ±‚å’Œçº¦å®š(Einstein summation convention)ã€‚ä¹Ÿå³åœ¨ç»™å®šä¸¤ä¸ªtensoræ—¶ï¼Œå¯ä»¥æŒ‡å®šç»´åº¦è¿›è¡Œæ±‚å’Œï¼Œç›¸å½“çµæ´»ï¼Œå¯ä»¥ç†è§£æˆbmmæˆ–è€…mmçš„æ‰©å±•ç‰ˆï¼Œè¿™æ ·åœ¨åšä¸€äº›tensorä¹‹é—´çš„æ“ä½œå°±ä¸éœ€è¦view/permuteè°ƒæ•´æˆbmmæ”¯æŒçš„æ ¼å¼äº†ã€‚ å®˜æ–¹ä¾‹å­ï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142&gt;&gt;&gt; x = torch.randn(5)&gt;&gt;&gt; y = torch.randn(4)&gt;&gt;&gt; torch.einsum('i,j-&gt;ij', x, y) # outer producttensor([[-0.0570, -0.0286, -0.0231, 0.0197], [ 1.2616, 0.6335, 0.5113, -0.4351], [ 1.4452, 0.7257, 0.5857, -0.4984], [-0.4647, -0.2333, -0.1883, 0.1603], [-1.1130, -0.5588, -0.4510, 0.3838]])&gt;&gt;&gt; A = torch.randn(3,5,4)&gt;&gt;&gt; l = torch.randn(2,5)&gt;&gt;&gt; r = torch.randn(2,4)&gt;&gt;&gt; torch.einsum('bn,anm,bm-&gt;ba', l, A, r) # compare torch.nn.functional.bilineartensor([[-0.3430, -5.2405, 0.4494], [ 0.3311, 5.5201, -3.0356]])&gt;&gt;&gt; As = torch.randn(3,2,5)&gt;&gt;&gt; Bs = torch.randn(3,5,4)&gt;&gt;&gt; torch.einsum('bij,bjk-&gt;bik', As, Bs) # batch matrix multiplicationtensor([[[-1.0564, -1.5904, 3.2023, 3.1271], [-1.6706, -0.8097, -0.8025, -2.1183]], [[ 4.2239, 0.3107, -0.5756, -0.2354], [-1.4558, -0.3460, 1.5087, -0.8530]], [[ 2.8153, 1.8787, -4.3839, -1.2112], [ 0.3728, -2.1131, 0.0921, 0.8305]]])&gt;&gt;&gt; A = torch.randn(3, 3)&gt;&gt;&gt; torch.einsum('ii-&gt;i', A) # diagonaltensor([-0.7825, 0.8291, -0.1936])&gt;&gt;&gt; A = torch.randn(4, 3, 3)&gt;&gt;&gt; torch.einsum('...ii-&gt;...i', A) # batch diagonaltensor([[-1.0864, 0.7292, 0.0569], [-0.9725, -1.0270, 0.6493], [ 0.5832, -1.1716, -1.5084], [ 0.4041, -1.1690, 0.8570]])&gt;&gt;&gt; A = torch.randn(2, 3, 4, 5)&gt;&gt;&gt; torch.einsum('...ij-&gt;...ji', A).shape # batch permutetorch.Size([2, 3, 5, 4]) æˆ‘å¯¹è¿™ä¸ªapiä¸€èˆ¬çš„ç”¨æ³•å°±æ˜¯ï¼Œå°†ä¸¤ä¸ªtensorçš„æ¯ä¸€ç»´ç”¨ä¸åŒçš„è®°å·æ ‡å·ï¼Œç„¶åæƒ³ä¸€ä¸‹æˆ‘æƒ³è¦çš„tensorçš„æ ¼å¼ï¼ŒæŒ‰ç…§è®°å·å†™ä¸‹å°±å¯ä»¥ç›´æ¥å¾—åˆ°äº†ã€‚ â‘£squeezeåœ¨ä½¿ç”¨çš„æ—¶å€™å°½é‡æŒ‡å®šç»´åº¦ï¼Œå¦åˆ™å¯èƒ½ä¼šå‡ºç°åœ¨è®­ç»ƒæœ€åä¸€ä¸ªbatchæ—¶ï¼Œbatch_sizeæ­£å¥½æ˜¯1ï¼Œå°±æŠŠbatch_sizeç»™squeezeæ‰äº†ã€‚ï¼ˆå·²ç»ä¸¤æ¬¡é‡åˆ°è¿™æ ·çš„bugäº†ï¼‰ â‘¤ 1apply(fn) Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch-nn-init). å¯ä»¥ç”¨äºæ‰€æœ‰çš„å­æ¨¡å—çš„åˆå§‹åŒ–ï¼Œå¥½åƒå¾ˆæ–¹ä¾¿çš„æ ·å­ã€‚ä½†æˆ‘çªç„¶æƒ³åˆ°è¿™ç§æ–¹æ³•å¯èƒ½ä¼šä¸å°å¿ƒæŠŠembeddingåˆå§‹åŒ–ç»™è¦†ç›–äº†ï¼Œå¦‚æœembeddingæœ‰ç”¨pretrainåˆå§‹åŒ–çš„è¯ã€‚ å®˜æ–¹ä¾‹å­ï¼š123456789101112131415161718192021222324&gt;&gt;&gt; def init_weights(m): print(m) if type(m) == nn.Linear: m.weight.data.fill_(1.0) print(m.weight)&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))&gt;&gt;&gt; net.apply(init_weights)Linear(in_features=2, out_features=2, bias=True)Parameter containing:tensor([[ 1., 1.], [ 1., 1.]])Linear(in_features=2, out_features=2, bias=True)Parameter containing:tensor([[ 1., 1.], [ 1., 1.]])Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True))Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True)) 2ï¸âƒ£[bpc]bits per character(bpc)æ˜¯language modelä¸€ä¸ªè¯„ä»·æŒ‡æ ‡ï¼Œå¦ä¸€ä¸ªå¸¸ç”¨æŒ‡æ ‡æ˜¯pplï¼ˆå›°æƒ‘åº¦ï¼‰ã€‚å®é™…ä¸Šbpcå’Œppléƒ½æ˜¯å’Œäº¤å‰ç†µæŒ‚é’©çš„ï¼Œå…¶è®¡ç®—å…¬å¼ä¸ºï¼š \begin{aligned} b p c(s t r i n g)=\frac{1}{T} \sum_{t=1}^{T} H\left(P_{t}, \hat{P}_{t}\right) &=-\frac{1}{T} \sum_{t=1}^{T} \sum_{c=1}^{n} P_{t}(c) \log _{2} \hat{P}_{t}(c) \\ &=-\frac{1}{T} \sum_{t=1}^{T} \log _{2} \hat{P}_{t}\left(x_{t}\right) \end{aligned}åœ¨ä»£ç ä¸­è®¡ç®—äº¤å‰ç†µçš„lossæ˜¯ä»¥eä¸ºåº•çš„ï¼Œå› æ­¤éœ€è¦å°†lossé™¤ä»¥$\log _{e}2$å³å¯ï¼ˆlogçš„æ¢åº•å…¬å¼ï¼‰ã€‚ 1cur_loss / math.log(2) https://stats.stackexchange.com/questions/211858/how-to-compute-bits-per-character-bpchttps://arxiv.org/pdf/1308.0850.pdf]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>é‡åˆ°çš„é—®é¢˜</tag>
        <tag>Pytorch</tag>
        <tag>bpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯24]]></title>
    <url>%2F2019%2F05%2F05%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D24%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£ è§‚ä¹¦[æ˜] äºè°¦ä¹¦å·å¤šæƒ…ä¼¼æ•…äººï¼Œæ™¨æ˜å¿§ä¹æ¯ç›¸äº²ã€‚çœ¼å‰ç›´ä¸‹ä¸‰åƒå­—ï¼Œèƒ¸æ¬¡å…¨æ— ä¸€ç‚¹å°˜ã€‚æ´»æ°´æºæµéšå¤„æ»¡ï¼Œä¸œé£èŠ±æŸ³é€æ—¶æ–°ã€‚é‡‘éç‰å‹’å¯»èŠ³å®¢ï¼Œæœªä¿¡å¾åºåˆ«æœ‰æ˜¥ã€‚ http://lib.xcz.im/work/582ee1a2da2f600063ec45ea]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡17]]></title>
    <url>%2F2019%2F04%2F28%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8717%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨è®ºæ–‡ï¼š PHRASE-BASED ATTENTIONS Regularizing and Optimizing LSTM Language Models 1ï¸âƒ£[PHRASE-BASED ATTENTIONS]è¿™ç¯‡æŠ•äº†ICLRä½†æ²¡ä¸­ã€‚æå‡ºå¯¹Transformerçš„attentionæœºåˆ¶è¿›è¡Œæ”¹è¿›ï¼Œä»¥è¯ç»„ä¸ºå•ä½è¿›è¡Œattentionï¼Œå¼•å…¥è¯ç»„çš„å¯¹é½æ¥æå‡ç¿»è¯‘è¡¨ç°ã€‚æå‡ºçš„æƒ³æ³•ä¹Ÿæ˜¯æ¯”è¾ƒç®€å•ç›´è§‚çš„ã€‚ å›é¡¾ï¼štransformerçš„åšæ³•ï¼š$\begin{aligned} \text { Attention }\left(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}, \boldsymbol{W}_{q}, \boldsymbol{W}_{k}, \boldsymbol{W}_{v}\right) &amp;=\mathcal{S}\left(\frac{\left(\boldsymbol{Q} \boldsymbol{W}_{q}\right)\left(\boldsymbol{K} \boldsymbol{W}_{k}\right)^{T}}{\sqrt{d_{k}}}\right)\left(\boldsymbol{V} \boldsymbol{W}_{v}\right) \\ \text { Head }^{i} &amp;=\text { Attention }\left(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}, \boldsymbol{W}_{q}^{i}, \boldsymbol{W}_{k}^{i}, \boldsymbol{W}_{v}^{i}\right) \text { for } i=1 \ldots h \\ \text { AttentionOutput }(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}, \boldsymbol{W}) &amp;=\text { concat (Head}^{1}, \text { Head}^{2}, \ldots, \text { Head}^{h} ) \boldsymbol{W} \end{aligned}$ æ¨¡å‹ä»‹ç»PHRASE-BASED ATTENTION METHODSå…¶æœ¬è´¨æ˜¯ä½¿ç”¨CNNæ“ä½œä½¿å¾—è¯æœ‰phraseçš„ä¿¡æ¯ã€‚ä¹Ÿå³ï¼š O_{t}=\mathbf{w} \oplus_{k=0}^{n} \mathbf{x}_{t \pm k}ä¸‹é¢ä½¿ç”¨$\operatorname{Conv}_{n}(\boldsymbol{X}, \boldsymbol{W})$ä»£è¡¨$\boldsymbol{W}$å¯¹$\boldsymbol{X}$è¿›è¡Œå·ç§¯æ“ä½œã€‚å…¶ä¸­$\boldsymbol{W} \in \mathbb{R}^{n \times d_{1} \times d_{2}}$ æ¥ä¸‹æ¥æå‡ºä¸¤ç§æ–¹æ³•ã€‚ KEY-VALUE CONVOLUTION\operatorname{Conv} \mathrm{KV}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V})=\mathcal{S}\left(\frac{\left(\boldsymbol{Q} \boldsymbol{W}_{q}\right) \operatorname{Conv}_{n}\left(\boldsymbol{K}, \boldsymbol{W}_{k}\right)^{T}}{\sqrt{d_{k}}}\right) \operatorname{Conv}_{n}\left(\boldsymbol{V}, \boldsymbol{W}_{v}\right)Qä¸å˜ï¼Œåªå¯¹Kå’ŒVè¿›è¡Œå·ç§¯ã€‚ QUERY AS-KERNEL CONVOLUTION\operatorname{QUERYK}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V})=\mathcal{S}\left(\frac{\operatorname{Conv}_{n}\left(\boldsymbol{K} \boldsymbol{W}_{k}, \boldsymbol{Q} \boldsymbol{W}_{q}\right)}{\sqrt{d_{k} * n}}\right) \operatorname{Conv}_{n}\left(\boldsymbol{V}, \boldsymbol{W}_{v}\right)å°†Qä½œä¸ºconvolutionçš„kernelå‚æ•°è¿›è¡Œå·ç§¯ã€‚$\boldsymbol{W}_{q} \in \mathbb{R}^{n \times d_{q} \times d_{k}}, \boldsymbol{W}_{k} \in \mathbb{R}^{d_{k} \times d_{k}}, \boldsymbol{W}_{v} \in \mathbb{R}^{n \times d_{v} \times d_{v}}$ ä»¥ä¸Šæ˜¯åŸºæœ¬å½¢å¼ï¼Œæ‰©å±•åˆ°å¤šä¸ªheadå¯ä»¥æœ‰å¤šç§æ–¹æ³•ã€‚ MULTI-HEADED PHRASAL ATTENTIONHOMOGENEOUS N-GRAM ATTENTION æ¯ä¸ªheadä¸“æ³¨æŸç§gramã€‚ä½†è¿™æ ·ä¼¼ä¹ä¸æ˜¯å¾ˆå¥½ï¼Œå› ä¸ºå¼ºè¡Œå¯¹æŸäº›headå¼•å…¥è¿™ç§ç‰¹æ€§ï¼Œæœ‰æ—¶å€™è¯ä¸è¯ä¹‹é—´æ²¡æœ‰è¿™ç§å…³ç³»ï¼Œè¿™æ ·ä¼šå¸¦æ¥å™ªå£°ã€‚ HETEROGENEOUS N-GRAM ATTENTION å°†æ‰€æœ‰çš„graméƒ½åŒæ—¶attendã€‚ ä¹Ÿå³ï¼š \mathcal{S}\left(\frac{\left(\boldsymbol{Q} \boldsymbol{W}_{q}\right)\left[\left(\boldsymbol{K} \boldsymbol{W}_{k, 1}\right)^{T} ; \operatorname{Conv}_{2}\left(\boldsymbol{K}, \boldsymbol{W}_{k, 2}\right)^{T} ; \ldots\right]}{\sqrt{d_{k}}}\right)\left[\left(\boldsymbol{V} \boldsymbol{W}_{v, 1}\right) ; \operatorname{Conv}_{2}\left(\boldsymbol{V}, \boldsymbol{W}_{v, 2}\right) ; \ldots\right]æˆ–ï¼š \mathcal{S}\left(\left[\frac{\left(\boldsymbol{Q} \boldsymbol{W}_{q, 1}\right)\left(\boldsymbol{K} \boldsymbol{W}_{k, 1}\right)^{T}}{\sqrt{d}} ; \frac{\operatorname{Conv}_{2}\left(\boldsymbol{K} \boldsymbol{W}_{k, 2}, \boldsymbol{Q} \boldsymbol{W}_{q, 2}\right)}{\sqrt{d * n_{2}}} ; \ldots\right]\right)\left[\left(\boldsymbol{V} \boldsymbol{W}_{v, 1}\right) ; \operatorname{Conv}_{2}\left(\boldsymbol{V}, \boldsymbol{W}_{v, 2}\right) ; \ldots\right]INTERLEAVED PHRASES TO PHRASE HETEROGENEOUS ATTENTIONä¸Šé¢ä»‹ç»çš„éƒ½æ˜¯sourceç«¯çš„phraseåˆ°targetçš„tokenï¼Œæœ‰æ—¶å€™éœ€è¦åè¿‡æ¥ï¼Œå› æ­¤å¯ä»¥äº¤å‰åœ°äº¤äº’ã€‚ æˆ‘ä»¬å…ˆå¯¹Qè¿›è¡Œä¸¤ç§å·ç§¯ï¼Œè·å¾—unigramå’Œbigramã€‚ç„¶åä¸KVçš„unigramä¸æ¯”bigramè¿›è¡Œäº¤å‰ã€‚$\boldsymbol{A}_{1, \mathrm{ConvKV}}=\mathcal{S}\left(\frac{\left(\boldsymbol{Q} \boldsymbol{W}_{q_{1}}\right)\left[\left(\boldsymbol{K} \boldsymbol{W}_{\boldsymbol{k}, 1}\right)^{T} ; \operatorname{Conv}_{2}\left(\boldsymbol{K}, \boldsymbol{W}_{k, 2}\right)^{T}\right]}{\sqrt{d_{k}}}\right)\left[\left(\boldsymbol{V} \boldsymbol{W}_{v, 1}\right) ; \operatorname{Conv}_{2}\left(\boldsymbol{V}, \boldsymbol{W}_{v, 2}\right)\right]$ $\boldsymbol{A}_{2, \text {ConvKV }}=\mathcal{S}\left(\frac{\operatorname{Conv}_{2}\left(\boldsymbol{Q}, \boldsymbol{W}_{q_{2}}\right)\left[\left(\boldsymbol{K} \boldsymbol{W}_{\boldsymbol{k}, 1}\right)^{T} ; \operatorname{Conv}_{2}\left(\boldsymbol{K}, \boldsymbol{W}_{\boldsymbol{k}, 2}\right)^{T}\right]}{\sqrt{d_{k}}}\right)\left[\left(\boldsymbol{V} \boldsymbol{W}_{v, 1}\right) ; \operatorname{Conv}_{2}\left(\boldsymbol{V}, \boldsymbol{W}_{v, 2}\right)\right]$ $\boldsymbol{A}_{1, \text {QueryK }}=\mathcal{S}\left(\left[\frac{\left(\boldsymbol{Q} \boldsymbol{W}_{q_{1}, 1}\right)\left(\boldsymbol{K} \boldsymbol{W}_{\boldsymbol{k}, 1}\right)^{T}}{\sqrt{d}} ; \frac{\operatorname{Conv}_{2}\left(\boldsymbol{K} \boldsymbol{W}_{\boldsymbol{k}, 2}, \boldsymbol{Q} \boldsymbol{W}_{q_{1}, 2}\right)}{\sqrt{d * n_{2}}}\right]\right)\left[\left(\boldsymbol{V} \boldsymbol{W}_{v, 1}\right) ; \operatorname{Conv}_{2}\left(\boldsymbol{V}, \boldsymbol{W}_{v, 2}\right)\right]$ $\boldsymbol{A}_{2, \text { QueryK }}=\mathcal{S}\left(\left[\frac{\operatorname{Conv}_{2}\left(\boldsymbol{Q}, \boldsymbol{W}_{q_{2}, 1}\right)\left(\boldsymbol{K} \boldsymbol{W}_{\boldsymbol{k}, 1}\right)^{T}}{\sqrt{d}} ; \frac{\operatorname{Conv}_{2}\left(\boldsymbol{K} \boldsymbol{W}_{\boldsymbol{k}, 2}, \operatorname{Conv}_{2}\left(\boldsymbol{Q}, \boldsymbol{W}_{q_{2}, 2}\right)\right)}{\sqrt{d * n_{2}}}\right]\right)\left[\left(\boldsymbol{V} \boldsymbol{W}_{v, 1}\right) ; \operatorname{Conv}_{2}\left(\boldsymbol{V}, \boldsymbol{W}_{v, 2}\right)\right]$ æ€è€ƒï¼šè¿™æ ·ä¼¼ä¹å‚æ•°é‡ä¼šæš´å¢ï¼Œå…¶å®åº”è¯¥å¯¹æ¯”çš„å°±ä¸æ˜¯transformer baseäº†ï¼Œåº”è¯¥æ˜¯å‚æ•°é‡å¤§è‡´ç›¸ç­‰çš„transformerï¼Œè¿™ä¹Ÿåœ¨reviewé‡Œé¢æåˆ°è¿‡ã€‚åŒæ—¶æˆ‘è§‰å¾—è¿™ä¸ªæ–¹æ³•æ˜¯å¦æœ‰äº›å¤ªå¤æ‚ï¼Œä¸å¤Ÿç®€å•æ˜äº†ã€‚ä»¥åŠç»“æœä¼¼ä¹ä¸å¤§ä»¤äººä¿¡æœï¼Œå› ä¸ºä»–çš„baselineæ²¡æœ‰å¤ç°å‡ºtransformer baseçš„ç»“æœï¼ˆdue to the limited GPU)ã€‚ 2ï¸âƒ£[Regularizing and Optimizing LSTM Language Models]æå‡ºä¸€äº›ä¼˜åŒ–æå‡LSTM-basedè¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚æ­¤å³å¤§åé¼é¼çš„AWD-LSTMã€‚ Weight-dropped LSTMLSTMå…¬å¼å›é¡¾ï¼š \begin{aligned} i_{t} &=\sigma\left(W^{i} x_{t}+U^{i} h_{t-1}\right) \\ f_{t} &=\sigma\left(W^{f} x_{t}+U^{f} h_{t-1}\right) \\ o_{t} &=\sigma\left(W^{o} x_{t}+U^{o} h_{t-1}\right) \\ \tilde{c}_{t} &=\tanh \left(W^{c} x_{t}+U^{c} h_{t-1}\right) \\ c_{t} &=i_{t} \odot \tilde{c}_{t}+f_{t} \odot+\tilde{c}_{t-1} \\ h_{t} &=o_{t} \odot \tanh \left(c_{t}\right) \end{aligned}å¯¹hidden-to-hiddençš„weightåº”ç”¨DropConnectã€‚ä¹Ÿå³å¯¹å…¶ä¸­çš„$\left[U^{i}, U^{f}, U^{o}, U^{c}\right]$è¿›è¡Œdropconnectã€‚æ³¨æ„åˆ°maskçŸ©é˜µåœ¨åŒä¸€ä¸ªbatchçš„æ¯ä¸ªæ—¶é—´æ­¥téƒ½æ˜¯ä¸€æ ·çš„ã€‚ Optimizationä¹‹å‰çš„å·¥ä½œè¡¨æ˜ï¼Œåœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œä½¿ç”¨æ™®é€šçš„SGDï¼Œä¸å¸¦momentumï¼Œèƒ½è¶…è¿‡å…¶ä»–çš„ä¼˜åŒ–æ–¹æ³•ã€‚æ™®é€šSGDï¼š$w_{k+1}=w_{k}-\gamma_{k} \hat{\nabla} f\left(w_{k}\right)$ æœ¬æ–‡æå‡ºåœ¨averaged SGD(ASGDï¼‰çš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ã€‚ ASGDå’Œä¸Šå¼ä¸€è‡´ï¼Œåªä¸è¿‡æœ€åæ›´æ–°å®Œæ˜¯å°†æœ€åå‡ æ¬¡æ›´æ–°çš„weightåšäº†å¹³å‡å¹¶è¿”å›ã€‚ä¹Ÿå³ï¼š \frac{1}{(K-T+1)} \sum_{i=T}^{K} w_{i}å…¶ä¸­Kæ˜¯totalçš„å¾ªç¯æ¬¡æ•°ï¼›Tæ˜¯äººå·¥å®šä¹‰çš„é˜ˆå€¼ã€‚ä½†Tçš„é˜ˆå€¼éœ€è¦äººå·¥è°ƒï¼Œå› æ­¤è¯¥æ–¹æ³•ä¸æ˜¯å¾ˆå¥½ã€‚æœ€ç†æƒ³çš„å°±æ˜¯åœ¨SGDæ‹Ÿåˆåˆ°ä¸€ä¸ªç¨³å®šçŠ¶æ€æ—¶å†å¹³å‡ã€‚ å› æ­¤æå‡ºä¸€ç§æ–°çš„æ–¹æ³•ä»¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡validation lossè§¦å‘æœºåˆ¶ã€‚ Extended regularization techniquesVariable length backpropagation sequencesè‹¥æ¯æ¬¡éƒ½å›ºå®šçª—å£åˆ‡åˆ†å¥å­ï¼Œåˆ™æ€»ä¼šæœ‰ä¸€äº›è¯æ²¡æ³•æ›´æ–°è‡ªå·±ï¼Œå¦‚æœ€åä¸€ä¸ªè¯ï¼ŒåŒæ—¶é™¤äº†ç¬¬ä¸€ä¸ªè¯ï¼Œå…¶ä»–çš„è¯éƒ½åªèƒ½æ¥æ”¶åˆ°éƒ¨åˆ†bpã€‚è¿™å®é™…ä¸Šæ˜¯ä¸€ç§data inefficientã€‚ å¯ä»¥ä»åˆ‡åˆ†å¥å­çš„æ–¹æ³•ä¸Šè¿›è¡Œæ”¹è¿›ã€‚ä½¿ç”¨éšæœºé‡‡æ ·å¥å­é•¿åº¦çš„æ–¹å¼å»ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚ä»¥è¾ƒé«˜çš„pé€‰æ‹©seqé•¿åº¦ï¼Œ1-pé€‰æ‹©seq/2ã€‚æ¥ç€ä»¥æ­¤ä¸ºé«˜æ–¯å‡å€¼ï¼Œä»¥æ­£æ€åˆ†å¸ƒ$\mathcal{N}(\operatorname{seq}, s)$é‡‡æ ·å¥å­é•¿åº¦ã€‚ Variational dropoutåœ¨LSTMä¸­ï¼Œé™¤äº†hidden-to-hiddençš„ï¼Œå…¶ä»–åœ°æ–¹éƒ½é‡‡ç”¨variational dropoutã€‚ Embedding dropoutå­—çº§åˆ«ï¼Œä¹Ÿå³å°†æ•´ä¸ªå­—çš„embeddingå»æ‰ã€‚åŒæ—¶ç”±äºæ˜¯åœ¨embedding matrixä¸Šåšçš„ï¼Œåœ¨ä¸€ä¸ªå®Œæ•´çš„forward passä¸backward passéƒ½ç”¨äº†ï¼Œå› æ­¤å°±ç›¸å½“äºä½¿ç”¨variational dropoutç”¨åœ¨one-hot embeddingä¸embedding lookupä¹‹é—´ã€‚ Weight tying embeddingä¸softmaxçš„æƒé‡ç»‘å®šã€‚ Independent embedding size and hidden sizeLSTMçš„ç¬¬ä¸€å±‚ä¸æœ€åä¸€å±‚ä¸embedding sizeä¸€è‡´ï¼Œå…¶å®ƒå±‚çš„å°±æœ‰è‡ªå·±çš„hidden sizeã€‚ Activation Regularization (AR) and Temporal Activation Regularization (TAR)ARï¼šL2æ­£åˆ™åŒ–ï¼š$\alpha L_{2}\left(m \odot h_{t}\right)$å…¶ä¸­mæ˜¯maskï¼Œhæ˜¯hidden state TARï¼š$\beta L_{2}\left(h_{t}-h_{t+1}\right)$å‡å°‘ä¸¤ä¸ªhä¹‹é—´çš„å·®è·ã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>attention</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯23]]></title>
    <url>%2F2019%2F04%2F28%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D23%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£è¡Œé¦™å­ Â· è¿°æ€€[å®‹] è‹è½¼æ¸…å¤œæ— å°˜ï¼Œæœˆè‰²å¦‚é“¶ã€‚é…’æ–Ÿæ—¶ã€é¡»æ»¡ååˆ†ã€‚æµ®åæµ®åˆ©ï¼Œè™šè‹¦åŠ³ç¥ã€‚å¹éš™ä¸­é©¹ï¼ŒçŸ³ä¸­ç«ï¼Œæ¢¦ä¸­èº«ã€‚è™½æŠ±æ–‡ç« ï¼Œå¼€å£è°äº²ã€‚ä¸”é™¶é™¶ã€ä¹å°½å¤©çœŸã€‚å‡ æ—¶å½’å»ï¼Œä½œä¸ªé—²äººã€‚å¯¹ä¸€å¼ ç´ï¼Œä¸€å£¶é…’ï¼Œä¸€æºªäº‘ã€‚ éš™ä¸­é©¹ï¼šè¯­å‡ºã€Šåº„å­Â·çŸ¥åŒ—æ¸¸ã€‹ï¼šâ€œäººç”Ÿå¤©åœ°ä¹‹é—´ï¼Œè‹¥ç™½é©¹ä¹‹è¿‡éš™ï¼Œå¿½ç„¶è€Œå·²ã€‚â€œçŸ³ä¸­ç«ï¼Œæ¢¦ä¸­èº«ï¼šæ¯”å–»ç”Ÿå‘½çŸ­ä¿ƒï¼Œåƒå‡»çŸ³è¿¸å‡ºä¸€é—ªå³ç­çš„ç«èŠ±ï¼Œåƒåœ¨æ¢¦å¢ƒä¸­çŸ­æš‚çš„ç»å†ã€‚çŸ³ä¸­ç«ï¼Œè¯­å‡ºåŒ—é½åˆ˜æ˜¼ã€Šæ–°è®ºÂ·æƒœæ—¶ã€‹ï¼šâ€œäººä¹‹çŸ­ç”Ÿï¼ŒçŠ¹å¦‚çŸ³ç«ï¼Œç‚¯ç„¶è€Œè¿‡ã€‚â€æ¢¦ä¸­èº«ï¼Œè¯­å‡ºã€Šå…³å°¹å­Â·å››ç¬¦ã€‹ï¼šâ€œçŸ¥æ­¤èº«å¦‚æ¢¦ä¸­èº«ã€‚â€ http://lib.xcz.im/work/57b2c8fa7db2a20054377ecd 2ï¸âƒ£æ—·æ€¡äº­å£å [ç°ä»£] é©¬ä¸€æµ®æµè½¬çŸ¥ä½•ä¸–ï¼Œæ±Ÿå±±å°šæ­¤äº­ã€‚ç™»ä¸´çš†æ—·å£«ï¼Œä¸§ä¹±æœ‰é—ç»ã€‚å·²è¯†ä¹¾å¤å¤§ï¼ŒçŠ¹æ€œè‰æœ¨é’ã€‚é•¿ç©ºé€é¸Ÿå°ï¼Œç•™å¹»ä¸äººçµã€‚ http://lib.xcz.im/work/5992e274570c35006b8394b3]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†19]]></title>
    <url>%2F2019%2F04%2F22%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8619%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Pytorch]expand&amp;repeat expandåªèƒ½å¯¹ç»´æ•°ä¸º1çš„ç»´åº¦è¿›è¡Œæ‰©å±•ï¼Œä¸”æ‰©å±•è¿‡ç¨‹ä¸­ä¸åˆ†é…æ–°å†…å­˜ï¼›repeatèƒ½å¯¹ä»»æ„ç»´åº¦è¿›è¡Œæ‰©å±•ï¼Œä½†éœ€è¦åˆ†é…æ–°å†…å­˜ã€‚ å¦‚æœæ»¡è¶³expandçš„éœ€è¦ï¼Œåº”å°½é‡ä½¿ç”¨expandã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯22]]></title>
    <url>%2F2019%2F04%2F21%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D22%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£ä¹æ—¥é½å®‰ç™»é«˜[å”] æœç‰§æ±Ÿæ¶µç§‹å½±é›åˆé£ï¼Œä¸å®¢æºå£¶ä¸Šç¿ å¾®ã€‚å°˜ä¸–éš¾é€¢å¼€å£ç¬‘ï¼ŒèŠèŠ±é¡»æ’æ»¡å¤´å½’ã€‚ä½†å°†é…©é…Šé…¬ä½³èŠ‚ï¼Œä¸ç”¨ç™»ä¸´æ¨è½æ™–ã€‚å¤å¾€ä»Šæ¥åªå¦‚æ­¤ï¼Œç‰›å±±ä½•å¿…ç‹¬æ²¾è¡£ï¼Ÿ http://lib.xcz.im/work/57ba4972efa631005a799815]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡16]]></title>
    <url>%2F2019%2F04%2F21%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8716%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨è®ºæ–‡ï¼š MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES Fine-Grained Attention Mechanism for Neural Machine Translation Competence-based Curriculum Learning for Neural Machine Translation 1ï¸âƒ£[MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES]æœ¬æ–‡æ¢ç©¶æ·±åº¦å­¦ä¹ ä¸­çš„è¿‡é‡å‚æ•°é—®é¢˜ï¼Œé€šè¿‡å®šä¹‰intrinsic dimensionï¼Œå»è¡¡é‡ç‰¹å®šæ¨¡å‹åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šæ‰€éœ€ç»´åº¦ã€‚ åœ¨ç»™å®šæ¨¡å‹ç»“æ„å’Œloss functionæ—¶ï¼Œæ•´ä¸ªä¼˜åŒ–ç©ºé—´ä¹Ÿéšä¹‹ç¡®å®šï¼Œè®­ç»ƒè¿‡ç¨‹ç±»ä¼¼äºåœ¨ä¸€ä¸ªç©ºé—´å†…ç§»åŠ¨ä½¿å¾—losså°½é‡å°ã€‚ ç»™å®šä¸€ä¸ªæœ‰Dä¸ªå‚æ•°çš„æ¨¡å‹ï¼Œé€šè¿‡é™åˆ¶è®­ç»ƒéšæœºsliceçš„å‚æ•°ï¼Œä¹Ÿå³é€‰å–ä¸€ä¸ªéšæœºæœ‰dä¸ªå‚æ•°çš„å­ç©ºé—´è®­ç»ƒï¼Œä¸æ–­å¢åŠ dï¼Œä½¿å¾—é¢„å®šä¹‰çš„solutionç¬¬ä¸€æ¬¡å‡ºç°ï¼Œåˆ™ç§°dä¸ºintrinsic dimensionï¼Œå¯ä»¥ç†è§£ä¸ºè¯¥dæ˜¯è§£å†³æŸç‰¹å®šé—®é¢˜æ‰€éœ€çš„å‚æ•°é‡ã€‚ å¦‚ä½•åšï¼Ÿ$\theta^{(D)}=\theta_{0}^{(D)}+P \theta^{(d)}$å…¶ä¸­Pæ˜¯éšæœºç”Ÿæˆçš„$D\times d$çš„æŠ•å½±çŸ©é˜µï¼Œè€Œ$\theta (d)$ æ˜¯å­ç©ºé—´çš„å‚æ•°ï¼›$P$æ˜¯å›ºå®šçš„è€Œä¸æ˜¯å¯è®­ç»ƒçš„ï¼Œä¸”$P$å¯ä»¥æ˜¯å½’ä¸€åŒ–ä¸ºå•ä½é•¿åº¦ä¸”æ­£äº¤çš„ã€‚ ï¼ˆè¿™é‡Œçš„æŠ•å½±ç°åœ¨è¿˜æ˜¯ä¸èƒ½ç†è§£ï¼Ÿç­‰ä¹‹åçœ‹è¿™æ–¹é¢çš„è®ºæ–‡å†è¯´å§ï¼‰ å› ä¸ºä¸€äº›éšæœºæ€§ä»¥åŠå®é™…æ•ˆæœé—®é¢˜ï¼Œæ¯”å¦‚æ­£åˆ™åŒ–æ•ˆæœåœ¨å­ç©ºé—´æ— æ³•è¾¾åˆ°åœ¨å…¨ç©ºé—´çš„æ•ˆæœï¼Œå› æ­¤åœ¨è¿™é‡Œå®šä¹‰$d_{\mathrm{int} 90}$ï¼Œä¹Ÿå³è¾¾åˆ°baselineçš„90%æ‰€éœ€è¦çš„å‚æ•°é‡ã€‚ ä¸€äº›ç»“æœï¼š MNISTçš„æ¨¡å‹å¯ä»¥çœ‹åˆ°æ‰€éœ€å‚æ•°éå¸¸å°‘ï¼›æ¨ªå‘å¯¹æ¯”ï¼ŒCNNä¼šæ¯”å…¨è¿æ¥æ‰€éœ€çš„å°‘å¤šäº†ï¼Œè¿™ä¹Ÿç¬¦åˆæˆ‘ä»¬çš„ç›´è§‰ï¼Œä¹Ÿå³CNNæ¯”å…¨è¿æ¥æ›´é«˜æ•ˆã€‚ åœ¨å…¨è¿æ¥ä¸­ï¼Œå¯¹äºæ¨¡å‹ä¸åŒçš„å®½åº¦ä»¥åŠlayeræ•°ï¼Œå‘ç°ä»–ä»¬çš„dint90ç›¸å·®ä¸å¤§ï¼Œè¯´æ˜å¯¹äºç‰¹å®šä»»åŠ¡ï¼ŒåŒä¸€ä¸ªæ¨¡å‹å®¶æ—æ‰€éœ€è¦çš„å‚æ•°é‡æ˜¯ç±»ä¼¼çš„ã€‚ 2ï¸âƒ£[Fine-Grained Attention Mechanism for Neural Machine Translation]æœ¬æ–‡æå‡ºå¯¹attentionè¿›è¡Œç»†åŒ–ï¼Œå°†åŸæ¥çš„æ¯ä¸ªè¯åˆ†é…ä¸€ä¸ªscoreæ‰©å±•ä¸ºæ¯ä¸ªè¯åˆ†é…dç»´ä¸ªscoreï¼Œå¹¶åœ¨æœºå™¨ç¿»è¯‘ä¸Šæœ‰ä¸€å®šæå‡ã€‚ ç®€å•åœ°è¯´ï¼ŒåŸæ¥çš„attentionæœºåˆ¶æ˜¯ï¼š$e_{t^{\prime}, t}=f_{\mathrm{Att}}\left(\boldsymbol{z}_{t^{\prime}-1}, \boldsymbol{h}_{t}\right)$ å…¶ä¸­$t^{\prime}$æ˜¯decoderç«¯çš„æ—¶é—´æ­¥ï¼Œ$t$åˆ™æ˜¯encoderç«¯çš„ç¬¬$t$ä¸ªè¯ã€‚ è€Œæœ¬æ–‡çš„ç»†ç²’åº¦attentionæœºåˆ¶ï¼š$e_{t^{\prime}, t}^{d}=f_{\mathrm{Att} \mathrm{Y} 2 \mathrm{D}}^{d}\left(\boldsymbol{z}_{t^{\prime}-1}, \boldsymbol{h}_{t}, \boldsymbol{y}_{t^{\prime}-1}\right)$ ä¹Ÿå³åœ¨åŸæ¥çš„åŸºç¡€ä¸Šåšäº†dæ¬¡æ“ä½œï¼Œä¹Ÿå³å®é™…ä¸Šåœ¨è·å¾—æ¯ä¸€ç»´çš„åˆ†æ•°æ—¶ï¼Œæ˜¯èƒ½çœ‹åˆ°å…¶ä»–ç»´çš„ä¿¡æ¯çš„ã€‚ï¼ˆå¦‚æœæ˜¯æˆ‘è‡ªå·±åšï¼Œæˆ‘å¯èƒ½ä¼šå°†ä»–ä»¬éš”ç»å¼€æ¥ã€‚ï¼‰ æ€è€ƒï¼šå°†RNNä½œä¸ºbaselineï¼Œä¸ºä»€ä¹ˆä¸ä½¿ç”¨transformerï¼Ÿå½“æ—¶transformerå·²ç»å‡ºäº†ï¼Œå¯èƒ½æ˜¯transformerä¸Šæ²¡æ•ˆæœï¼Ÿå› ä¸ºtransformerè‡ªå¸¦å¤šheadï¼Œå¯èƒ½è¡¨ç¤ºèƒ½åŠ›å°±å·²ç»è¶³å¤Ÿäº†ã€‚ 3ï¸âƒ£[Competence-based Curriculum Learning for Neural Machine Translation]ä»‹ç»æå‡ºä¸€ç§æ–°çš„è®­ç»ƒç¿»è¯‘æ¨¡å‹çš„ç®—æ³•ï¼ŒåŸºæœ¬æ€æƒ³æ˜¯è®©æ¨¡å‹ä»ç®€å•çš„æ ·ä¾‹å¼€å§‹å­¦èµ·ï¼Œéšç€è®­ç»ƒè¿‡ç¨‹çš„è¿›è¡Œé€æ¸å¢åŠ éš¾åº¦è¾ƒå¤§çš„æ ·ä¾‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¢å¼ºæ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œä¸”åœ¨æ•ˆæœä¸Šä¹Ÿæœ‰æå‡ï¼ŒåŒæ—¶è¿˜èƒ½å‡å°‘æ”¶æ•›æ‰€éœ€çš„è®­ç»ƒæ—¶é—´ã€‚ è®ºæ–‡çš„Motivationï¼šå¦‚æœè®­ç»ƒæ•°æ®ä»¥ç‰¹å®šçš„é¡ºåºè¾“å…¥ï¼Œä¹Ÿå³ä»ç®€å•çš„æ•°æ®å¼€å§‹å­¦ï¼Œç­‰åˆ°æ¨¡å‹æœ‰ä¸€å®šçš„èƒ½åŠ›åå†å»å­¦éš¾çš„æ•°æ®ï¼Œè¿™æ ·ä¹Ÿæ›´ç¬¦åˆäººç±»çš„ç›´è§‰ï¼›åŒæ—¶ï¼Œä»æœºå™¨å­¦ä¹ çš„è§’åº¦å»çœ‹ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥é¿å…è¿‡æ—©é™·å…¥ä¸å¥½çš„å±€éƒ¨æœ€ä¼˜è§£ã€‚ è®ºæ–‡è¿˜æåˆ°äº†å¯¹äºç¿»è¯‘è€Œè¨€ï¼Œæ¨¡å‹å¾ˆéš¾è®­ç»ƒï¼Œéœ€è¦å¤æ‚çš„è°ƒå‚ï¼Œè´¹æ—¶è´¹åŠ›ã€‚ç‰¹åˆ«æ˜¯å¯¹äºTransformerè€Œè¨€ï¼Œéœ€è¦ç²¾ç»†çš„learning rate scheduleã€‚ æœ¬æ–‡æå‡ºçš„æ–¹æ³•ï¼Œåªæœ‰ä¸€ä¸ªå‚æ•°ï¼Œå› æ­¤ä¸éœ€è¦ç²¾ç»†çš„è°ƒå‚ï¼ŒåŒæ—¶å› ä¸ºåªæ”¹å˜è¾“å…¥çš„pipelineï¼Œå› æ­¤å¾ˆæ–¹ä¾¿åœ°ä½¿ç”¨åˆ°å·²æœ‰çš„æ¨¡å‹ã€‚ æ–¹æ³•å¼•å…¥ä¸¤ä¸ªæ¦‚å¿µï¼šDifficultyï¼šä»£è¡¨ä¸€ä¸ªè®­ç»ƒæ ·ä¾‹çš„éš¾åº¦ï¼Œå¯èƒ½å’Œæ¨¡å‹å½“å‰çš„çŠ¶æ€ç›¸å…³ã€‚æ¯”å¦‚å¥å­é•¿åº¦å°±æ˜¯è¡¡é‡æ ·ä¾‹éš¾åº¦çš„ä¸€ä¸ªæŒ‡æ ‡ã€‚ Competenceï¼šèŒƒå›´0-1çš„æ•°å€¼ï¼Œä»£è¡¨æ¨¡å‹è®­ç»ƒçš„è¿›åº¦ï¼Œå®šä¹‰ä¸ºæ¨¡å‹çŠ¶æ€çš„ä¸€ä¸ªå‡½æ•°ã€‚æ›´è¿›ä¸€æ­¥ï¼Œå®šä¹‰$c(t)$ä¸ºæ¨¡å‹åœ¨æ—¶é—´æ­¥tæ‰€å…è®¸ä½¿ç”¨çš„è®­ç»ƒæ ·ä¾‹çš„æ¯”ä¾‹ã€‚ä¹Ÿå³è®­ç»ƒæ ·ä¾‹æ ¹æ®difficultyæ’åˆ—ï¼Œåœ¨æ—¶é—´æ­¥$t$åªå…è®¸top $c(t)$çš„æ•°æ®ä½¿ç”¨ã€‚ æ ¹æ®ä¸Šè¿°ä¸¤ä¸ªå®šä¹‰ï¼Œå¼•å…¥ç®—æ³•ï¼š é‚£ä¹ˆæœ‰ä¸¤ä¸ªé—®é¢˜ï¼Œå¦‚ä½•è¡¡é‡difficultyä»¥åŠcompetenceï¼Ÿ Difficulty Metricsâ‘ å¥å­é•¿åº¦é•¿å¥å­æ›´éš¾ç¿»è¯‘ï¼Œå› ä¸ºé•¿å¥å­å¾€å¾€åŒ…å«äº†çŸ­å¥å­ï¼ŒåŒæ—¶åœ¨ç”Ÿæˆç›®æ ‡è¯­è¨€æ—¶ï¼Œå®¹æ˜“å‡ºç°é”™è¯¯ä¼ æ’­ã€‚ d_{\text { length }}\left(s_{i}\right) \triangleq N_{i}â‘¡Word Rarityè‹¥ä¸€ä¸ªå¥å­å­˜åœ¨ç½•è§è¯ï¼Œæ›´éš¾ç¿»è¯‘è¯¥å¥å­ï¼Œå› ä¸ºæ¨¡å‹éœ€è¦å¤šæ¬¡çœ‹è§è¯¥è¯æ‰èƒ½å­¦åˆ°é²æ£’çš„è¡¨ç¤ºï¼›åŒæ—¶ç½•è§è¯çš„æ¢¯åº¦å®¹æ˜“æœ‰è¾ƒå¤§çš„æ–¹å·®ã€‚ å› æ­¤æˆ‘ä»¬å®šä¹‰ç›¸å¯¹è¯é¢‘ï¼š \hat{p}\left(w_{j}\right) \triangleq \frac{1}{N_{\text {total }}} \sum_{i=1}^{M} \sum_{k=1}^{N_{i}} \mathbb{1}_{w_{k}^{i}=w_{j}}å…¶ä¸­ï¼Œ$j=1, \ldots, \{\text {unique words in corpus}\}$ï¼Œ$\mathbb{1}$ ä¸ºæŒ‡ç¤ºå‡½æ•°ã€‚ å› æ­¤æœ€ç»ˆåº¦é‡æ–¹æ³•ä¸ºï¼š$d_{\text {rarity}}\left(s_{i}\right) \triangleq-\sum_{k=1}^{N_{i}} \log \hat{p}\left(w_{k}^{i}\right)$ è¿™æ ·å³è€ƒè™‘åˆ°äº†é•¿åº¦ä¹Ÿè€ƒè™‘åˆ°äº†è¯é¢‘ï¼ŒåŒæ—¶è¯¥æ–¹æ³•æœ‰ç‚¹ç±»ä¼¼language modelï¼Œå¯ä»¥ç†è§£ä¸ºlanguage modelçš„è¿‘ä¼¼ã€‚ Competence Functionsæˆ‘ä»¬å®šä¹‰competence functionåªä¸æ—¶é—´æ­¥$t$æœ‰å…³ï¼Œå› æ­¤åªéœ€è¦è€ƒè™‘å…·ä½“çš„å½¢å¼ã€‚â‘ linearï¼š c(t) \triangleq \min \left(1, t r+c_{0}\right)$c_{0}$æ˜¯åˆå§‹å€¼ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥å®šä¹‰Tä¸ºæ—¶é—´æ­¥é˜ˆå€¼ï¼Œå½“è¶…è¿‡è¯¥é˜ˆå€¼ï¼Œæˆ‘ä»¬è®¤ä¸ºæ¨¡å‹å·²ç»å®Œå…¨æœ‰èƒ½åŠ›äº†ï¼Œåˆ™ä¸Šå¼è¿˜å¯ä»¥å†™æˆï¼š c_{\text { linear }}(t) \triangleq \min \left(1, t \frac{1-c_{0}}{T}+c_{0}\right)â‘¡Rootï¼šçº¿æ€§çš„ä¸€ä¸ªä¸å¥½çš„åœ°æ–¹ï¼Œå½“æ ·ä¾‹å¢åŠ æ—¶ï¼Œæ¯ä¸ªæ ·ä¾‹è¢«sampleçš„å‡ ç‡å‡å°ï¼Œå› æ­¤æ–°åŠ è¿›å»çš„æ ·ä¾‹è¢«sampleåˆ°çš„å‡ ç‡ä¹Ÿå‡å°ï¼Œå› æ­¤åº”æ¯æ¬¡å‡å°‘æ–°åŠ å…¥çš„æ ·ä¾‹ï¼Œä½¿å¾—æ¨¡å‹æœ‰è¶³å¤Ÿçš„æ—¶é—´å»å­¦ä¹ çŸ¥è¯†ã€‚ä¹Ÿå³ï¼š \frac{d c(t)}{d t}=\frac{P}{c(t)}ç§¯åˆ†åå¯å¾—ï¼š c_{\mathrm{sqrt}}(t) \triangleq \min \left(1, \sqrt{t \frac{1-c_{0}^{2}}{T}+c_{0}^{2}}\right)å½“ç„¶è¿˜å¯ä»¥å°†å¼€næ¬¡æ–¹æ ¹ c_{\mathrm{root}-p}(t) \triangleq \min \left(1, \sqrt[p]{t \frac{1-c_{0}^{p}}{T}+c_{0}^{p}}\right)ä½¿å¾—æ›²çº¿æ›´ä¸ºé™¡å³­ï¼Œä¹Ÿå³ç»™æ¯ä¸ªæ ·ä¾‹çš„æ—¶é—´æ›´å¤šã€‚ æ›²çº¿å¯¹æ¯”ï¼š å®éªŒè¯æ˜æ˜¯p=2æ—¶æœ€å¥½ã€‚ å®éªŒç»“æœ å®éªŒæœ‰ç›¸å½“ä¸é”™çš„ç»“æœï¼Œåœ¨RNNä»¥åŠåœ¨Transformerä¸Šéƒ½æœ‰æå‡ï¼Œå¹¶ä¸”æ˜¯åœ¨ä¸ç”¨learning rate scheduleçš„æƒ…å†µä¸‹ï¼Œå¹¶ä¸”æ—¶é—´æ›´çŸ­ã€‚ å‡ ä¸ªå®éªŒç°è±¡ï¼šâ‘ RNNçš„æå‡è¾ƒå°‘ï¼Œè€ŒTransformerå¾ˆå¤šï¼Œè¯´æ˜RNNæ¯”Transformeræ›´é²æ£’ã€‚RNNæ¯”Transformerè®­ç»ƒæ›´ä¸ºç¨³å®šã€‚â‘¡å¯¹äºTransformerè€Œè¨€ï¼Œè‹¥åŒæ ·ä½¿ç”¨learning rate scheduleï¼Œä»ç„¶æœ‰å¸®åŠ©ï¼Œè¯´æ˜è¯¥æ–¹æ³•æ˜¯è¾ƒä¸ºé€šç”¨çš„ã€‚â‘¢ä¸ä½¿ç”¨lr scheduleè€Œåªä½¿ç”¨æœ¬æ–‡æ–¹æ³•ï¼Œä¹Ÿèƒ½è¾¾åˆ°ä¸ä½¿ç”¨æœ¬æ–‡æ–¹æ³•è€Œä½¿ç”¨lr scheduleçš„ç»“æœï¼Œä½†éœ€è¦æ›´å¤šçš„stepã€‚ æ€è€ƒä¸ºä»€ä¹ˆè¯¥æ–¹æ³•èƒ½workï¼Ÿç¬¦åˆç›´è§‰ï¼Œæ¨¡å‹ä»ç®€å•åˆ°éš¾ï¼Œæ›´å¥½è®­ã€‚åŒæ—¶ä»æœºå™¨å­¦ä¹ è§’åº¦ï¼Œå¦‚æœå®Œå…¨æ­£å¸¸çš„sampleï¼Œåˆ™å®¹æ˜“é™·å…¥å±€éƒ¨æœ€å°æˆ–è€…saddle pointï¼Œå› æ­¤éœ€è¦æ›´é•¿æ—¶é—´æˆ–è€…ä¸å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚ åŒæ—¶è®ºæ–‡è¿˜æåˆ°äº†ï¼Œä¸ºä»€ä¹ˆTransformeråœ¨å¢åŠ batchèƒ½å¤Ÿæœ‰æ›´å¥½çš„æ”¶æ•›ï¼Œè¿™æ˜¯å› ä¸ºä¸€å¼€å§‹è®­ç»ƒçš„noisy gradientå¤ªå¤§ï¼Œè‹¥å¢åŠ batchèƒ½å¤Ÿä¿¡å™ªæ¯”ï¼Œè€Œæœ¬æ–‡æ–¹æ³•åœ¨æŸç§ç¨‹åº¦ä¸Šä¹Ÿè§£å†³äº†è¯¥é—®é¢˜ã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>Curriculum Learning</tag>
        <tag>NMT</tag>
        <tag>intrinsic dimension</tag>
        <tag>attention mechanism</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transformerç›¸å…³è¿‘æœŸç›˜ç‚¹]]></title>
    <url>%2F2019%2F04%2F12%2F%E8%AE%BA%E6%96%87%2FTransformer%E7%9B%B8%E5%85%B3%E8%BF%91%E6%9C%9F%E7%9B%98%E7%82%B9%2F</url>
    <content type="text"><![CDATA[è¿‘å¹´æ¥è‡ªç„¶è¯­è¨€å¤„ç†æœ‰ç›¸å½“å¤§çš„è¿›å±•ï¼Œå›¿äºä¸ªäººæµ…è–„çš„èƒ½åŠ›ï¼Œå› æ­¤ä»…è°ˆè°ˆè‡ªå·±è¾ƒä¸ºäº†è§£çš„ä¸Transformerç›¸å…³ä¸€è·¯ä¸‹æ¥çš„ä¸€äº›å·¥ä½œã€‚è¿™ç¯‡çš„ä¸»è¦ç›®çš„æ˜¯å®Œæˆé‚±åšç»™æˆ‘çš„ä»»åŠ¡ï¼Œä¹Ÿé¡ºä¾¿æ¢³ç†ä¸€ä¸‹æ€ç»ªã€‚ è‡ª2017å¹´çš„é—®ä¸–ï¼ŒTransformerå°±å¸å¼•äº†å¤§æ‰¹å­¦è€…çš„æ³¨æ„ï¼Œ2018å¹´Bertçš„å‡ºç°ï¼Œæ›´æ˜¯å°†Transformeræ¨ä¸Šäº†NLPèˆå°çš„ä¸­å¤®ã€‚Transformerä»¥å…¶é«˜æ•ˆç‡ï¼ˆé«˜å¹¶è¡Œæ€§ï¼‰ä»¥åŠæå¼ºçš„æ¨¡å‹èƒ½åŠ›ï¼Œä¿¨ç„¶æœ‰æ›¿ä»£ä¼ ç»ŸRNN/CNNçš„æ€åŠ¿ã€‚å› æ­¤æœ¬æ¬¡å°±è®¨è®ºè®¨è®ºTransformeråŠå…¶ç³»åˆ—ï¼ŒåŒæ—¶æœ€ååŠ ä¸Šæˆ‘ä¸ªäººå…³äºRNN/CNN/Transformerçš„ä¸€ç‚¹æ€è€ƒã€‚ è¦ç‚¹ï¼š TransformeråŠå…¶å˜ä½“ Transformeråœ¨å…¶ä»–ä»»åŠ¡ é¢„è®­ç»ƒæ¨¡å‹ Transformer/CNN/RNNå¯¹æ¯”åŠæ€è€ƒ TransformeråŠå…¶å˜ä½“Transformerç®€å•å›é¡¾Transformer[1]é‡‡ç”¨å®Œå…¨çš„attentionæœºåˆ¶ç”¨ä»¥åºåˆ—å»ºæ¨¡ï¼Œåºåˆ—ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½èƒ½å¤Ÿç›´æ¥ä¸å…¶ä»–èŠ‚ç‚¹äº¤äº’ï¼Œè€Œè¿™æ˜¯é€šè¿‡attentionæœºåˆ¶æ¥å®ç°çš„ã€‚ Transformeræ¨¡å‹æ¶æ„Transformeræ¶æ„ï¼š ç”±äºTransformeræœ€æ—©ç”±äºç¿»è¯‘æ¨¡å‹ä¸­ï¼Œå› æ­¤æ¶æ„æ˜¯ç”±ä¸€ä¸ªencoderå’Œä¸€ä¸ªdecoderç»„æˆï¼Œè€Œencoderå’Œdecoderéƒ½æ˜¯ç”±å¤šä¸ªåŸºæœ¬çš„blockå †å è€Œæˆã€‚ä¸€ä¸ªblockç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼Œä¹Ÿå³multi-head attentionå±‚å’ŒPosition-wise Feed-Forward Networkså±‚ã€‚ Multi-head attentionå¯¹äºåºåˆ—ä¸­ä¸€ä¸ªç‰¹å®šèŠ‚ç‚¹$x_i$ï¼Œ$x_i$ä½œä¸ºqueryï¼Œå…¶ä»–èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰ä½œä¸ºkeyå’Œvalueï¼Œé€šè¿‡å‘é‡ç‚¹ç§¯è®¡ç®—å‡ºattentionåˆ†æ•°ï¼Œè¿›è¡Œå½’ä¸€åŒ–åï¼ˆsoftmaxï¼‰å°†valueåŠ æƒå¹³å‡è·å¾—è¯¥èŠ‚ç‚¹$x_i$æ–°çš„è¡¨ç¤ºã€‚åŒæ—¶ï¼Œå¯¹äºæ¯ä¸ªèŠ‚ç‚¹ï¼Œä¸ºäº†å¢å¼ºè¡¨ç¤ºèƒ½åŠ›ï¼Œå¯ä»¥å°†å…¶æ˜ å°„åˆ°å¤šä¸ªä¸åŒéšç©ºé—´ä¸­ï¼Œåˆ†åˆ«å®Œæˆä¸Šè¿°åŸºæœ¬æ“ä½œã€‚ å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š å·¦å›¾ä¸ºåŸºæœ¬æ“ä½œä¹Ÿå³scale-dot productï¼Œå³å›¾ä¸ºå¤šä¸ªscale-dot productåœ¨ä¸åŒéšç©ºé—´åŒæ—¶è¿›è¡Œï¼Œå¹¶ä¸”å°†å¤šä¸ªheadçš„ç»“æœæ‹¼æ¥èµ·æ¥ä½œä¸ºæœ€ç»ˆç»“æœã€‚ Position-wise Feed-Forward Networksä¸ºäº†å¢å¼ºæ¨¡å‹è¡¨ç¤ºèƒ½åŠ›ï¼Œåœ¨Multi-head attentionä¹‹åï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½è¿‡ä¸¤å±‚MLPä»¥è·å¾—æ–°çš„å‘é‡è¡¨ç¤ºã€‚ä¹Ÿå³: \mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}ç›¸æ¯”ä¼ ç»Ÿçš„RNN/CNNè€Œè¨€ï¼Œå…¶æœ€å¤§çš„ä¼˜åŠ¿æ˜¯å…¨å±€çš„æ„Ÿå—é‡ï¼ˆGlobal Receptive Fieldï¼‰ä»¥åŠé«˜åº¦å¹¶è¡Œæ€§ï¼ˆparallelizationï¼‰ã€‚ å˜ä½“Universal Transformersæå‡ºä¸€ç§æ–°å‹é€šç”¨çš„Transformerï¼Œåœ¨Transformerçš„åŸºç¡€ä¸Šå¼•å…¥RNNçš„å½’çº³åç½®(inductive bias)ï¼Œä¹Ÿå³è¿­ä»£å­¦ä¹ (learning iterative)çš„ç‰¹å¾ã€‚Universal Transformer[2]çš„ä¸»è¦ç‰¹ç‚¹æœ‰ï¼š åœ¨Transformerä¸­æ¯å±‚çš„æƒé‡æ˜¯ç‹¬ç«‹çš„ï¼Œè€Œåœ¨Universal Transformerä¸­ï¼Œæ¯å±‚çš„æƒé‡æ˜¯å…±äº«çš„ï¼Œä¹Ÿå³multi-head Attentionä¸Feed-Forwardåœ¨æ¯å±‚çš„æƒé‡æ˜¯ä¸€è‡´çš„ã€‚ åœ¨Transformerä¸­å¼•å…¥è‡ªé€‚åº”è®¡ç®—æ—¶é—´(Adaptive Computation Time, ACT[3])ï¼Œä¹Ÿå³å¯¹äºä¸åŒçš„è¯å…è®¸è¿­ä»£ä¸åŒæ¬¡æ•°ã€‚è¿™æ˜¯åŸºäºæœ‰äº›è¯ç›¸æ¯”å…¶ä»–è¯è¯æ„æ›´ä¸°å¯Œï¼Œæ›´éš¾è¢«æ¨¡å‹å­¦ä¼šï¼Œå› æ­¤éœ€è¦æ›´å¤šçš„è¿­ä»£æ¬¡æ•°ã€‚ä¸å›ºå®šå±‚æ•°çš„Transformerç›¸æ¯”æœ‰æ›´å¥½çš„é€šç”¨æ€§ã€‚ å› æ­¤å…¶æ€»ä½“ç»“æ„ä¸ºï¼š åœ¨è¿™é‡Œæœ‰ä¸¤ä¸ªç»†èŠ‚ï¼š åŠ äº†Timestep embeddingå»æŒ‡ç¤ºå½“å‰è¿­ä»£çš„æ¬¡æ•° å°†Feedforward Functionç”¨æ›´ä¸ºé€šç”¨çš„Transition Functionï¼Œå¯ä»¥æ˜¯æ™®é€šçš„å…¨è¿æ¥ï¼Œä¹Ÿå¯ä»¥æ˜¯å‚æ•°æ›´å°‘çš„Depth-wise Convolutionã€‚ Star TransformerStar Transformer[20]æ˜¯ä¸€ç§è½»é‡çº§çš„Transformerï¼Œé€šè¿‡å°†å…¨è¿æ¥çš„ç»“æ„æ›¿æ¢ä¸ºæ˜Ÿå‹æ‹“æ‰‘ç»“æ„ï¼Œæ˜¾è‘—å‡å°Transformerçš„å¤æ‚åº¦ï¼Œä»$O(n^2)$å‡ä¸º$O(n)$ã€‚ å…¶ä¸»è¦æ€æƒ³æ˜¯â€™Gather-Distributeâ€™ï¼Œä¹Ÿå³æ¯ä¸ªèŠ‚ç‚¹ä¸ç›´æ¥ä¸å…¶ä»–èŠ‚ç‚¹äº¤äº’ï¼Œè€Œæ˜¯ä¸å…¨å±€èŠ‚ç‚¹è¿›è¡Œäº¤äº’ã€‚ å®éªŒè¡¨æ˜ï¼ŒStar Transformerä¸ä»…åœ¨å¤šä¸ªæ•°æ®é›†è¡¨ç°æ›´ä¼˜ï¼Œä¸”é€Ÿåº¦æ›´å¿«ã€‚ å…¶ä»–å°æ”¹è¿›æ¥ä¸‹æ¥ä»‹ç»åŸºäºTransformerçš„å‡ ä¸ªå°æ”¹è¿›å·¥ä½œã€‚ åœ¨Convolutional Self-Attention Network[5]ä¸­ï¼Œé€šè¿‡åœ¨self-attentionå±‚å¼•å…¥CNNçš„å½’çº³åç½®ï¼Œåœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šæœ‰ä¸€å®šçš„æå‡ã€‚å…·ä½“åšæ³•ï¼šæ™®é€šself-attentionå±‚ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½èƒ½å¤Ÿç›´æ¥ä¸å…¶ä»–èŠ‚ç‚¹äº¤äº’ï¼Œè€Œåœ¨1D-Convolutionalçš„self-attentionå±‚ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹åªèƒ½ä¸ä»¥è¯¥èŠ‚ç‚¹ä¸ºä¸­å¿ƒçš„çª—å£å†…çš„èŠ‚ç‚¹äº¤äº’ã€‚è€Œåœ¨2D-Convolutionä¸­ï¼Œå¯¹headè¿™ä¸€ç»´è¿›è¡Œæ‰©å±•ï¼Œä¹Ÿå³å¯¹äºä»»æ„ä¸€ä¸ªèŠ‚ç‚¹ï¼Œä¸ä»…èƒ½å’Œå‘¨å›´çš„èŠ‚ç‚¹äº¤äº’ï¼Œè¿˜å¯ä»¥ä¸å…¶ä»–headçš„èŠ‚ç‚¹äº¤äº’ã€‚å®éªŒç»“æœï¼š åœ¨Multi-Head Attention with Disagreement Regularization[6]ä¸­ï¼Œæ˜¾å¼å¯¹multi-head attentionæ·»åŠ æ­£åˆ™åŒ–ï¼Œä½¿å¾—ä¸åŒheadå°½é‡åŒºåˆ†å¼€æ¥ï¼Œä»¥ä½¿å¾—ä¸åŒheadæ•è·åˆ°ä¸åŒçš„ç‰¹å¾ã€‚è®ºæ–‡æå‡ºäº†ä¸‰ç§ä¸åŒä½ç½®çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼šâ‘ å¯¹Valueï¼š D_{\text {subpace}}=-\frac{1}{H^{2}} \sum_{i=1}^{H} \sum_{j=1}^{H} \frac{V^{i} \cdot V^{j}}{\left\|V^{i}\right\|\left\|V^{j}\right\|}ä¹Ÿå³å¯¹ä¸åŒheadä¹‹é—´çš„valueï¼Œè®¡ç®—ä»–ä»¬ä¹‹é—´çš„coså€¼ï¼Œä½œä¸ºä¼˜åŒ–ç›®æ ‡ä¹‹ä¸€ã€‚ â‘¡å¯¹Attentionæƒé‡ï¼š D_{\text {position}}=-\frac{1}{H^{2}} \sum_{i=1}^{H} \sum_{j=1}^{H}\left|A^{i} \odot A^{j}\right|ä¹Ÿå³å°†æ¯ä¸ªheadæ‰€è®¡ç®—å¾—åˆ°çš„attentionçŸ©é˜µï¼Œè®¡ç®—ä»–ä»¬ä¹‹é—´çš„element-wiseä¹˜æ³•ï¼Œä½œä¸ºä¼˜åŒ–ç›®æ ‡ä¹‹ä¸€ã€‚ â‘¢å¯¹è¾“å‡ºï¼š D_{\text {output}}=-\frac{1}{H^{2}} \sum_{i=1}^{H} \sum_{j=1}^{H} \frac{O^{i} \cdot O^{j}}{\left\|O^{i}\right\|\left\|O^{j}\right\|}ä¹Ÿå³å¯¹æ¯ä¸ªheadçš„è¾“å‡ºé€šè¿‡coså€¼è¿›è¡Œæ­£åˆ™åŒ–ã€‚ Modeling Localness for Self-Attention Networks[7]åˆ™æ˜¯é€šè¿‡åŠ å¼ºå¯¹å±€éƒ¨ä¿¡æ¯çš„å…³æ³¨ï¼Œåœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šæå‡è¡¨ç°ã€‚å…¶ä¸»è¦çš„åŠ¨æœºæ˜¯ï¼šâ‘ åœ¨Transformerä¸­æ¯ä¸ªè¯éƒ½ç›´æ¥ä¸æ‰€æœ‰è¯äº¤äº’ï¼Œå¯¹æ‰€æœ‰è¯è¿›è¡Œçº¿æ€§åŠ æƒå¯¼è‡´å¯¹é‚»è¿‘è¯çš„å…³æ³¨ä¸å¤Ÿï¼ˆå› ä¸ºæƒé‡çš„åˆ†æ•£ï¼‰ï¼›â‘¡ä»ç›´æ¥ä¸Šçœ‹ï¼Œå½“è¯$i$ä¸è¯$j$æœ‰å¯¹é½å…³ç³»æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸è¯$j$å‘¨å›´çš„è¯ä¹Ÿå¯¹é½ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•è·æ•´ä¸ªè¯­ä¹‰å•å…ƒçš„ä¿¡æ¯ã€‚å…¶å…·ä½“åšæ³•æ˜¯åœ¨softmaxå‡½æ•°å†…å¢åŠ ä¸€ä¸ªé«˜æ–¯åç½®ï¼ˆGaussian biasï¼‰å»ä¿®æ­£attentionæƒé‡åˆ†å¸ƒï¼š \operatorname{ATT}(Q, K)=\operatorname{softmax}(\text {energy}+G)$G_{ij}$æ˜¯è¡¡é‡è¯jä¸è¯iæ‰€é¢„æµ‹çš„ä¸­å¿ƒè¯ä¹‹é—´çš„è”ç³»ç´§å¯†ç¨‹åº¦ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼š G_{i, j}=-\frac{\left(j-P_{i}\right)^{2}}{2 \sigma_{i}^{2}}å…¶ä¸­$\sigma_{i}=\frac{D_{i}}{2}$ï¼Œ$D_{i}$æ˜¯çª—å£å¤§å°ã€‚è€Œ$P_{i}$æ˜¯é€šè¿‡è®¡ç®—å¾—å‡ºçš„ï¼Œ$P_{i}$ä¸å¯¹åº”çš„queryæœ‰å…³ï¼Œå› æ­¤å¯ä»¥é€šè¿‡$p_{i}=U_{p}^{T} \tanh \left(W_{p} Q_{i}\right)$è®¡ç®—å¾—åˆ°ï¼›è€Œçª—å£å¤§å°$D_{i}$å¯ä»¥æœ‰å¤šç§é€‰æ‹©ï¼Œâ‘ å›ºå®šçª—å£å¤§å°ï¼›â‘¡æ¯å±‚ç‰¹å®šçš„å¤§å°ï¼Œä¹Ÿå³å°†è¯¥å±‚çš„keyå¹³å‡èµ·æ¥ï¼Œé€šè¿‡$z=U_{d}^{T} \tanh \left(W_{d} \overline{\mathbf{K}}\right)$è®¡ç®—ï¼›â‘¢æ¯ä¸ªqueryéƒ½æœ‰è‡ªå·±çš„çª—å£å¤§å°ï¼š$z_{i}=U_{d}^{T} \tanh \left(W_{p} Q_{i}\right)$ã€‚ Self-attention with relative position representations[8]åˆ™æ˜¯å°†Transformerä¸­çš„ç»å¯¹ä½ç½®embeddingæ”¹ä¸ºç›¸å¯¹ä½ç½®embeddingä»¥æå‡ç¿»è¯‘æ•ˆæœã€‚ Transformeråœ¨å…¶ä»–ä»»åŠ¡Transformer XLæœ¬æ–‡æ¢ç´¢å°†Transformerç”¨äºè¯­è¨€æ¨¡å‹(language model)ï¼Œå¹¶åœ¨Transformerå¼•å…¥RNNçš„å½’çº³åç½®ï¼Œä¹Ÿå³RNNçš„å†å²ä¿¡æ¯ï¼Œä½¿å¾—Transformerèƒ½å¤Ÿå¤„ç†é•¿å¥å­ã€‚ ç”±äºTransformerçš„å¤æ‚åº¦æ˜¯$O(n^2)$ï¼Œè™½ç„¶åœ¨GPUä¸Šèƒ½å¤Ÿå¹¶è¡Œæ“ä½œï¼Œä½†å ç”¨æ˜¾å­˜è¾ƒå¤§ï¼Œå› æ­¤åœ¨å®ç°æ—¶ï¼Œé€šå¸¸æ˜¯å°†å¥å­åˆ‡åˆ†ä¸ºä¸€ä¸ªä¸€ä¸ªsegmentï¼Œsegmentä¹‹é—´æ²¡æœ‰è”ç³»ï¼š è€Œåœ¨æµ‹è¯•é˜¶æ®µï¼Œåˆ™æ¯ç”Ÿæˆä¸€ä¸ªè¯æ—¶æ»‘åŠ¨ä¸€ä¸ªçª—å£ï¼š è¿™æ ·çš„æ–¹æ³•æ˜¾ç„¶æ•ˆç‡å¾ˆä½ã€‚ è€Œåœ¨Transformer-XL[9]ä¸­ï¼Œæ¯ä¸ªsegmenté˜¶æ®µéƒ½æ¥å—å‰ä¸€ä¸ª(ç”šè‡³å‰Lä¸ª)çš„å†å²ä¿¡æ¯ï¼šå› æ­¤è¿‡ç¨‹å¦‚ä¸‹ï¼š è€Œåœ¨æµ‹è¯•é˜¶æ®µï¼Œç”±äºæœ‰å†å²ä¿¡æ¯ï¼Œåˆ™ä¸éœ€è¦æ»‘åŠ¨çª—å£ï¼Œå› æ­¤æ•ˆç‡æ›´é«˜ã€‚ å…·ä½“è€Œè¨€ï¼š$\begin{aligned} \widetilde{\mathbf{h}}_{\tau+1}^{n-1} &amp;=\left[\mathrm{SG}\left(\mathbf{h}_{\tau}^{n-1}\right) \circ \mathbf{h}_{\tau+1}^{n-1}\right] \\ \mathbf{q}_{\tau+1}^{n}, \mathbf{k}_{\tau+1}^{n}, \mathbf{v}_{\tau+1}^{n} &amp;=\mathbf{h}_{\tau+1}^{n-1} \mathbf{W}_{q}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{k}^{\top}, \widetilde{\mathbf{h}}_{\tau+1}^{n-1} \mathbf{W}_{v}^{\top} \\ \mathbf{h}_{\tau+1}^{n} &amp;=\text { Transformer-Layer }\left(\mathbf{q}_{\tau+1}^{n}, \mathbf{k}_{\tau+1}^{n}, \mathbf{v}_{\tau+1}^{n}\right) \end{aligned}$SGä»£è¡¨stop gradientï¼Œè€Œå†å²ä¿¡æ¯ä¸å½“å‰é˜¶æ®µçš„éšçŠ¶æ€æ‹¼æ¥åœ¨ä¸€èµ·ã€‚ åŒæ—¶æœ¬æ–‡å¦ä¸€å¤§ä¸¤ç‚¹æ˜¯å¼•å…¥ç›¸å¯¹ä½ç½®çš„encodingã€‚å¦‚æœä½¿ç”¨ç»å¯¹ä½ç½®encodingï¼Œé‚£ä¹ˆåˆ™ä¼šå‡ºç°ä¸‹è¿°æƒ…å†µï¼š$\mathbf{h}_{\tau+1}=f\left(\mathbf{h}_{\tau}, \mathbf{E}_{\mathbf{s}_{\tau+1}}+\mathbf{U}_{1 : L}\right) \quad \text { and } \quad \mathbf{h}_{\tau}=f\left(\mathbf{h}_{\tau-1}, \mathbf{E}_{\mathbf{s}_{\tau}}+\mathbf{U}_{1 : L}\right)$ä¹Ÿå³æ¯ä¸ªsegmentéƒ½ä¼šæœ‰ç›¸åŒçš„ä½ç½®ä¿¡æ¯ã€‚å› æ­¤åœ¨è¿™é‡Œå¼•å…¥$\mathbf{R} \in \mathbb{R}^{L_{\max } \times d}$ï¼Œç¬¬$i$è¡Œä»£è¡¨ç›¸å¯¹è·ç¦»$i$çš„encodingã€‚ å…·ä½“è€Œè¨€ï¼šåœ¨æ ‡å‡†Transformerä¸­ï¼Œquery $q_i$ä¸key $k_j$æ‰€è·å¾—çš„attentionåˆ†æ•°å¯ä»¥æ‹†è§£ä¸ºï¼š$\mathbf{A}_{i, j}^{\mathrm{abs}}=q_{i}^{\top} k_{j}=\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(b)}+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k} \mathbf{U}_{j}}_{(d)}$ å¯¹å…¶è¿›è¡Œæ”¹è¿›ï¼Œè½¬åŒ–ä¸ºç›¸å¯¹ä½ç½®encodingï¼Œæœ‰ï¼š$\mathbf{A}_{i, j}^{\mathrm{rel}}=\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, E} \mathbf{E}_{x_{j}}}_{(a)}+\underbrace{\mathbf{E}_{x_{i}}^{\top} \mathbf{W}_{q}^{\top} \mathbf{W}_{k, E} \mathbf{R}_{i-j}}_{(b)}+\underbrace{u^{\top} \mathbf{W}_{k, R} \mathbf{E}_{x_{j}}}_{(c)}+\underbrace{v^{\top} \mathbf{W}_{k, R} \mathbf{R}_{i-j}}_{(d)}$ é¦–å…ˆæ˜¯å°†æ‰€æœ‰å‡ºç°ç»å¯¹ä½ç½®çš„åœ°æ–¹éƒ½æ”¹ä¸ºç›¸å¯¹ä½ç½®ï¼Œç¬¬äºŒæ˜¯å°†å¼•å…¥ä¸€ä¸ªå¯å­¦ä¹ çš„$u \in \mathbb{R}^{d}$å’Œ$v \in \mathbb{R}^{d}$å»æ›¿ä»£$\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top}$å’Œ$\mathbf{U}_{i}^{\top} \mathbf{W}_{q}^{\top}$ã€‚ç¬¬ä¸‰ï¼Œæ˜¯å°†åŒæ ·çš„$\mathbf{W}_{k}$ç»†åŒ–æˆä¸¤ä¸ªä¸åŒçš„$\mathbf{W}_{k, E}$ä¸$\mathbf{W}_{k, R}$ã€‚ Transformer-XLåœ¨ä¸åŒæ•°æ®é›†ä¸Šæœ‰ç›¸å½“å¥½çš„æ•ˆæœï¼š Character-Level Language Modeling with Deeper Self-AttentionåŒæ ·æ˜¯åœ¨è¯­è¨€æ¨¡å‹ä¸Šä½¿ç”¨Transformerï¼Œä½†æ˜¯characterçº§åˆ«çš„è¯­è¨€æ¨¡å‹ã€‚å…¶ä¸»è¦æ€è·¯æ˜¯æ·»åŠ å¤šä¸ªlossæ¥æå‡å…¶è¡¨ç°ä»¥åŠåŠ å¿«æ‹Ÿåˆé€Ÿåº¦ã€‚ å¯¹äºä¼ ç»Ÿçš„RNN character-levelè¯­è¨€æ¨¡å‹ï¼Œä¸€èˆ¬åšæ³•æ˜¯â€œtruncated backpropagation through timeâ€ (TBTT)ï¼šä¹Ÿå³æ¯ä¸ªbatché¢„æµ‹æœ€åä¸€ä¸ªå­—ç¬¦ï¼Œç„¶åå°†è¯¥batchçš„éšçŠ¶æ€ä¼ å…¥ä¸‹ä¸€ä¸ªbatchã€‚ è€Œåœ¨Transformerä¸­ä¹Ÿå¯ä»¥é‡‡ç”¨è¯¥æ–¹æ³•ã€‚ä½†åœ¨è¯¥åŸºç¡€ä¸Šï¼Œå¼•å…¥ä¸‰ç§lossã€‚â‘ Multiple Positionsåœ¨ä¸€ä¸ªbatchå†…ï¼Œæ¯ä¸ªæ—¶é—´æ­¥téƒ½é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œè€Œä¸æ˜¯åƒä¼ ç»Ÿæ–¹æ³•ï¼Œåªé¢„æµ‹batchæœ€åä¸€ä¸ªå­—ç¬¦ï¼š â‘¡Intermediate Layer Lossesä¸ä»…ä»…æœ€åä¸€å±‚è¦è¿›è¡Œé¢„æµ‹ï¼Œä¸­é—´å±‚ä¹Ÿéœ€è¦é¢„æµ‹ã€‚ è¶Šåº•å±‚çš„lossæ‰€åˆ†é…çš„æƒé‡è¶Šå°ã€‚ â‘¢Multiple Targetsæ¯ä¸ªä½ç½®ä¸ä»…ä»…è¦é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œè¿˜éœ€è¦é¢„æµ‹åå‡ ä¸ªçš„å­—ç¬¦ï¼š å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨å¤šä¸ªlossèƒ½å¤ŸåŠ é€Ÿæ‹Ÿåˆï¼Œä¸”èƒ½å¤Ÿè·å¾—æ›´å¥½çš„ç»“æœã€‚ é¢„è®­ç»ƒæ¨¡å‹è‡ªELMoå¼€å§‹ï¼Œé¢„è®­ç»ƒæ¨¡å‹å°±å¼€å§‹å—åˆ°å¹¿æ³›çš„å…³æ³¨ï¼Œè€ŒBertéšåçš„é—®ä¸–åˆ™æ›´æ˜¯å°†é¢„è®­ç»ƒæ¨¡å‹æ¨ä¸Šäº†æ–°çš„é˜¶æ®µã€‚å› æ­¤åœ¨è¿™é‡Œç®€è¦ä»‹ç»é¢„è®­ç»ƒæ¨¡å‹çš„å†å²ã€‚ Non-Transformer-basedWord Embeddingè¯å‘é‡æ˜¯æœ€æ—©çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ŒBengio, et al.[10] æœ€æ—©æå‡ºç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ï¼Œè¯å‘é‡ä½œä¸ºè®­ç»ƒè¯­è¨€æ¨¡å‹çš„å‰¯äº§å“ï¼Œå¯ä»¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚è€Œéšååˆ™å‡ºç°äº†word2vec[11],GloVe[12]ï¼Œæ˜¯ç›®å‰æœ€ä¸ºå¹¿æ³›ä½¿ç”¨çš„è¯å‘é‡ã€‚ CoVe/ELMoword2vecä¸GloVeä½œä¸ºé™æ€è¯å‘é‡ï¼Œä¸€å¤§é—®é¢˜å°±æ˜¯éš¾ä»¥è§£å†³å¤šä¹‰è¯ï¼Œè€Œå¤šä¹‰è¯çš„è¡¨ç¤ºå¯ä»¥é€šè¿‡ä¸Šä¸‹æ–‡æ¥æ¨æµ‹ã€‚CoVe[13]å°†è¯å‘é‡ä»é™æ€æ‰©å±•ä¸ºåŠ¨æ€ã€‚é€šè¿‡ä¸Šä¸‹æ–‡æ¥è·å¾—ç‰¹å®šè¯çš„åŠ¨æ€è¡¨ç¤ºï¼Œå…·ä½“æ˜¯é€šè¿‡ç¿»è¯‘æ¨¡å‹æ¥è¾¾åˆ°è¯¥ç›®çš„çš„ã€‚è€ŒELMo[14]ç»§æ‰¿äº†åŠ¨æ€è¯å‘é‡çš„æ€æƒ³ï¼Œä¸è¿‡æ˜¯é€šè¿‡åŒå‘è¯­è¨€æ¨¡å‹æ¥è¾¾åˆ°è¿™ä¸€ç›®çš„çš„ã€‚é€šè¿‡åŒå‘LSTMçš„è¯­è¨€æ¨¡å‹ï¼Œå°†å‰å‘ä¸åå‘éšçŠ¶æ€æ‹¼æ¥èµ·æ¥ä½œä¸ºè¯¥è¯çš„è¡¨ç¤ºã€‚ä»…ä»…æ˜¯å°†ä¼ ç»Ÿé™æ€è¯å‘é‡æ›¿æ¢æˆELMoï¼Œå°±èƒ½æœ‰å¾ˆå¤§çš„æå‡ã€‚è‡ªæ­¤å¼€å§‹ï¼Œé¢„è®­ç»ƒæ¨¡å‹å¼€å§‹å—åˆ°å¹¿æ³›çš„å…³æ³¨ã€‚ ULMFitåœ¨ä¸Šè¿°é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œç ”ç©¶è€…çš„æ€è·¯ä¸»è¦é›†ä¸­åœ¨é¢„è®­ç»ƒè¯å‘é‡ç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚ULMFit[16]åˆ™å°è¯•ç›´æ¥å¯¹åˆ†ç±»æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œæ¥ç€å†é€šè¿‡å¾®è°ƒ(fine-tuning)ä»¥æé«˜åˆ†ç±»çš„æ•ˆæœã€‚ ULMFitçš„æˆåŠŸè¯´æ˜ç›´æ¥å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒè€Œä¸æ˜¯åªé¢„è®­ç»ƒè¯å‘é‡ç”¨äºä¸‹æ¸¸ä»»åŠ¡æ˜¯å¯è¡Œçš„ã€‚ Transformer-basedGPT[15]å°è¯•é€šè¿‡æ¢ç´¢æ„å»ºä¸€ç§é€šç”¨æ¨¡å‹å¹¶åœ¨å…¶ä¸Šè®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥åœ¨å¤šç§ä»»åŠ¡ä¸Šæœ‰æ›´å¥½çš„è¡¨ç°ã€‚å…¶ä¸»è¦äº®ç‚¹åœ¨äºâ‘ æ„å»ºä¸€ç§é€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†ä¸åŒä»»åŠ¡ï¼Œç¬¬äºŒä½¿ç”¨Transformerè€Œä¸æ˜¯LSTMä½œä¸ºå…¶åŸºæœ¬æ¨¡å‹ã€‚å…¶åŸºæœ¬æ¨¡å‹ï¼š å…·ä½“è€Œè¨€ï¼Œä¸»è¦æ˜¯æ— ç›‘ç£çš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒåŠ ä¸Šæœ‰ç›‘ç£çš„å¾®è°ƒã€‚ è€ŒBert[17]åœ¨GPTçš„åŸºç¡€ä¸Šï¼Œå¼•å…¥maskedè¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡éšæœºmaskæ‰éƒ¨åˆ†è¯ï¼Œå¼ºè¿«æ¨¡å‹é€šè¿‡ä¸Šä¸‹æ–‡å»é¢„æµ‹maskæ‰çš„è¯ï¼ŒåŠ å¼ºäº†æ¨¡å‹çš„èƒ½åŠ›ã€‚ åŒæ—¶å¼•å…¥æœ‰ç›‘ç£å­¦ä¹ ï¼Œä¹Ÿå³é¢„æµ‹ä¸‹ä¸€å¥ï¼ˆNext Sentence Predictionï¼‰ï¼Œéšæœºåœ¨å¥å¯¹ä¸­å–ä¸¤ä¸ªå¥å­ï¼Œä½¿å¾—æœ‰50%å¯èƒ½å¥å­å¯¹æœ‰ä¸Šä¸‹æ–‡å…³ç³»ï¼Œ50%å¥å¯¹æ²¡æœ‰å…³ç³»ï¼Œä½¿æ¨¡å‹å»é¢„æµ‹å¥å¯¹ä¹‹é—´çš„å…³ç³»ã€‚å…·ä½“è€Œè¨€åˆ™æ˜¯é€šè¿‡åœ¨å¥å­å¼€å¤´åŠ [CLS]ç¬¦å·ï¼Œåœ¨æœ€é«˜å±‚å°†è¯¥ç¬¦å·çš„è¡¨ç¤ºé€šè¿‡å…¨è¿æ¥å±‚ã€‚ Bertåœ¨11é¡¹æ•°æ®é›†ä¸Šåˆ·æ–°æœ€é«˜è®°å½•ã€‚ åœ¨æ­¤ä¹‹åï¼ŒMT-DNN[19]ã€GPT2.0[18]ç›¸ç»§é—®ä¸–ï¼Œé€šè¿‡æ·»åŠ æ›´å¤šçš„ä»»åŠ¡æˆ–è€…æ›´å¤šçš„æ•°æ®ä½¿å¾—æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚ç›¸ä¿¡åœ¨æ¥ä¸‹æ¥ä¸€æ®µæ—¶é—´å†…ï¼Œç›¸å…³ä¸»é¢˜çš„è®ºæ–‡ä¹Ÿä¼šæœ‰å¾ˆå¤šã€‚ Transformer/CNN/RNNå¯¹æ¯”åŠæ€è€ƒä¸Šè¿°çš„ä»‹ç»ï¼Œå¤§æ¦‚å¯¹Transformerä¸€æ”¯æœ‰ä¸€ä¸ªç®€å•çš„æ¢³ç†ã€‚Transformerä½œä¸ºä¸RNN/CNNå¹¶ç«‹çš„æ¨¡å‹ï¼Œç¡®å®å€¼å¾—é‡è§†ã€‚ ä¸ºä»€ä¹ˆTransformerè¿™ä¹ˆå¥½ï¼Œæ˜¯å¦èƒ½å¤Ÿæ›¿ä»£RNN/CNNï¼Ÿè¿™ä¹Ÿæ˜¯å€¼å¾—æ‰€æœ‰äººæ€è€ƒçš„ã€‚ æ­£å¦‚å‰é¢ä»‹ç»çš„é‚£æ ·ï¼ŒTransformerçš„ä¸€å¤§ä¼˜åŠ¿æ˜¯å…¨å±€æ„Ÿå—é‡ï¼Œä¹Ÿå³RNN/CNNæ¯æ¬¡åªèƒ½â€˜çœ‹â€™åˆ°éƒ¨åˆ†ä¸Šä¸‹æ–‡ï¼Œè€ŒTransformeråˆ™æ²¡æœ‰è¿™ä¸ªé™åˆ¶ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½èƒ½å¤Ÿç›´æ¥ä¸å…¶ä»–èŠ‚ç‚¹è¿›è¡Œäº¤äº’ã€‚ä¹Ÿå¯ä»¥è¿™ä¹ˆè¯´ï¼ŒRNN/CNNå…·æœ‰æ›´å¼ºçš„å…ˆéªŒ(prior)ã€‚ä½†è¿™ç§ä¼˜åŠ¿æœ‰æ—¶ä¹Ÿä¼šæˆä¸ºåŠ£åŠ¿ï¼šå®è·µè¯æ˜ï¼ŒTransformeråœ¨å°æ•°æ®é›†ä¸Šçš„æ•ˆæœæ˜¯ä¸å¦‚RNN/CNNçš„ã€‚æˆ–è®¸å¯ä»¥è¿™ä¹ˆç†è§£è¿™ç§ç°è±¡ï¼šTransformerç”±äºä¸å¼•å…¥å¼ºçš„å…ˆéªŒï¼Œå› æ­¤éœ€è¦å¤§é‡çš„æ•°æ®å»ä»å¤´å­¦ä¹ æ•°æ®æ‰€å­˜åœ¨çš„æŸç§patternï¼ˆå¦‚å±€éƒ¨æ€§ï¼‰ï¼Œè€Œå¼•å…¥å¼ºçš„å…ˆéªŒçš„RNN/CNNåˆ™å¯¹å°æ•°æ®é›†æ›´åŠ å‹å¥½ä¸€äº›ã€‚ä½†å½“æœ‰å¤§é‡è®­ç»ƒæ•°æ®æ—¶ï¼ˆå¦‚ç¿»è¯‘ã€è¯­è¨€æ¨¡å‹ï¼‰ï¼ŒTransformeråˆ™ä¼šæœ‰æ›´é«˜çš„ä¸Šé™ï¼ŒBert/GPTä¹Ÿå°è¯äº†è¿™ç‚¹ã€‚è€Œè¿™ç§Transformerçš„åŠ£åŠ¿æˆ–è®¸ä¹Ÿæ˜¯ä¸Šè¿°å‡ ä¸ªå·¥ä½œï¼ˆå¦‚universal transformerï¼‰çš„å…¶ä¸­ä¸€ä¸ªå‡ºå‘ç‚¹ï¼Œä¹Ÿå³åœ¨Transformerå†…å¼•å…¥RNN/CNNçš„å½’çº³åç½®ï¼ŒåŠ å¼ºå¯¹Transformerçš„å…ˆéªŒçŸ¥è¯†çš„çº¦æŸã€‚ å‚è€ƒæ–‡çŒ®[1] Vaswani, Ashish, et al. â€œAttention is all you need.â€ Advances in neural information processing systems. 2017.[2]Dehghani, Mostafa, et al. â€œUniversal transformers.â€ arXiv preprint arXiv:1807.03819 (2018).[3]Graves, Alex. â€œAdaptive computation time for recurrent neural networks.â€ arXiv preprint arXiv:1603.08983 (2016).[4]Ahmed, Karim, Nitish Shirish Keskar, and Richard Socher. â€œWeighted transformer network for machine translation.â€ arXiv preprint arXiv:1711.02132 (2017).[5]Yang, Baosong, et al. â€œConvolutional Self-Attention Networks.â€ arXiv preprint arXiv:1904.03107 (2019).[6]Li, Jian, et al. â€œMulti-head attention with disagreement regularization.â€ arXiv preprint arXiv:1810.10183 (2018).[7]Yang, Baosong, et al. â€œModeling localness for self-attention networks.â€ arXiv preprint arXiv:1810.10182 (2018).[8]Shaw, Peter, Jakob Uszkoreit, and Ashish Vaswani. â€œSelf-attention with relative position representations.â€ arXiv preprint arXiv:1803.02155 (2018).[9]Dai, Zihang, et al. â€œTransformer-xl: Attentive language models beyond a fixed-length context.â€ arXiv preprint arXiv:1901.02860 (2019).[10]Bengio, Yoshua, et al. â€œA neural probabilistic language model.â€ Journal of machine learning research 3.Feb (2003): 1137-1155.[11]Mikolov, Tomas, et al. â€œEfficient estimation of word representations in vector space.â€ arXiv preprint arXiv:1301.3781 (2013).[12]Pennington, Jeffrey, Richard Socher, and Christopher Manning. â€œGlove: Global vectors for word representation.â€ Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014.[13]McCann, Bryan, et al. â€œLearned in translation: Contextualized word vectors.â€ Advances in Neural Information Processing Systems. 2017.[14]Peters, Matthew E., et al. â€œDeep contextualized word representations.â€ arXiv preprint arXiv:1802.05365 (2018).[15]Radford, Alec, et al. â€œImproving language understanding by generative pre-training.â€ URL https://s3-us-west-2. amazonaws. com/openai-assets/research-covers/languageunsupervised/language understanding paper. pdf (2018).[16]Universal Language Model Fine-tuning for Text Classification[17]Devlin, Jacob, et al. â€œBert: Pre-training of deep bidirectional transformers for language understanding.â€ arXiv preprint arXiv:1810.04805 (2018).[18]Radford, Alec, et al. â€œLanguage models are unsupervised multitask learners.â€ OpenAI Blog 1 (2019): 8.[19]Multi-Task Deep Neural Networks for Natural Language Understanding[20]Guo, Qipeng, et al. â€œStar-Transformer.â€ arXiv preprint arXiv:1902.09113 (2019).]]></content>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ— é¢˜]]></title>
    <url>%2F2019%2F04%2F11%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E4%BA%BA%E7%B1%BB%E6%B0%B8%E6%81%92%E7%9A%84%E6%84%9A%E8%A0%A2%2F</url>
    <content type="text"><![CDATA[äººç±»æ°¸æ’çš„æ„šè ¢ï¼Œæ˜¯æŠŠè«åå…¶å¦™çš„æ‹…å¿§ï¼Œç­‰åŒäºæ™ºåŠ›è¶…ç¾¤ã€‚ â€”â€”ç¾å›½åŠ å°”å¸ƒé›·æ–¯]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡15]]></title>
    <url>%2F2019%2F03%2F31%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8715%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨è®ºæ–‡: Selective Kernel Networks Attentional pooling for action recognition 1ï¸âƒ£[Selective Kernel Networks]é€šè¿‡å¯¹ä¸åŒkernel sizeçš„feature mapä¹‹é—´è¿›è¡Œä¿¡æ¯ç­›é€‰è·å¾—æ›´ä¸ºé²æ£’çš„è¡¨ç¤ºï¼Œèƒ½å¤Ÿå¯¹ä¸åŒçš„æ„Ÿå—é‡è¿›è¡Œæ•´åˆï¼Œå®ç°åŠ¨æ€è°ƒæ•´æ„Ÿå—é‡ã€‚å…¶æ€è·¯è¿˜æŒºæœ‰æ„æ€çš„ã€‚ Introductionå°†è¯¥æ¨¡å‹ä¸è§†è§‰ç¥ç»çš„ç†è®ºç»“åˆåœ¨ä¸€èµ·ï¼Œä¹Ÿå³ï¼Œå¯¹äºäººç±»è€Œè¨€ï¼Œåœ¨çœ‹ä¸åŒå°ºå¯¸ä¸åŒè¿œè¿‘çš„ç‰©ä½“æ—¶ï¼Œè§†è§‰çš®å±‚ç¥ç»å…ƒæ„Ÿå—é‡å¤§å°æ˜¯ä¼šæ ¹æ®åˆºæ¿€æ¥è¿›è¡Œè°ƒèŠ‚çš„ï¼Œä½†ä¸€èˆ¬è€Œè¨€åœ¨CNNä¸­å·ç§¯æ ¸çš„å¤§å°æ˜¯å›ºå®šçš„ã€‚è¯¥æ¨¡å‹æ­£æ˜¯ä»è¿™ä¸€ç°è±¡ä¸­è·å¾—çµæ„Ÿã€‚ æ•´ä¸ªæ¨¡å‹ä¸€å…±åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šsplitï¼Œfuseï¼Œselect splitç”Ÿæˆå¤šä¸ªä¸åŒkernel sizeçš„feature mapï¼Œä¹Ÿå³å¯¹åº”ä¸åŒçš„æ„Ÿå—é‡å¤§å°ï¼›fuseå°†ä¸åŒfeature mapç»“åˆèµ·æ¥ï¼Œè·å¾—ä¸€ä¸ªå…¨å±€çš„ç»¼åˆçš„å‘é‡è¡¨ç¤ºï¼›selectæ ¹æ®ä¸åŒçš„weighté€‰æ‹©ä¸åŒæ„Ÿå—é‡çš„feature mapã€‚ ä»¥ä¸Šå›¾ä¸ºä¾‹ã€‚ SK-Netç¬¬ä¸€æ­¥splitç»™å®šè¾“å…¥$\mathbf{X} \in \mathbb{R}^{H^{\prime} \times W^{\prime} \times C^{\prime}}$ï¼Œé€šè¿‡ä¸åŒçš„kernel sizeçš„CNNçš„å·ç§¯è·å¾—ä¸åŒçš„feature mapï¼Œä¸Šå›¾æ˜¯$3\times 3$ä¸$5\times 5$çš„å·ç§¯æ ¸ã€‚å·ç§¯å¯ä»¥æ˜¯ä¼ ç»Ÿçš„convolutionå·ç§¯ï¼Œä¹Ÿå¯ä»¥æ˜¯ç©ºæ´å·ç§¯ï¼ˆdilated convolutionï¼‰ï¼Œæˆ–è€…æ·±åº¦å·ç§¯ï¼ˆdepthwise convolutionï¼‰ã€‚åˆ™æœ‰ï¼š$\widetilde{\mathcal{F}} : \mathbf{X} \rightarrow \widetilde{\mathbf{U}} \in \mathbb{R}^{H \times W \times C}$ ä¸ $\widehat{\mathcal{F}} : \mathbf{X} \rightarrow \widehat{\mathbf{U}} \in \mathbb{R}^{H \times W \times C}$ï¼Œå…¶ä¸­$\widetilde{\mathcal{F}},\widehat{\mathcal{F}}$æ˜¯å·ç§¯å˜æ¢ã€‚ ç¬¬äºŒæ­¥fuseç›´æ¥å°†ä¸åŒçš„feature mapç»“åˆèµ·æ¥ä»¥è·å¾—å…¨å±€ä¿¡æ¯ï¼Œç”¨ä»¥ä¹‹åçš„åŠ¨æ€è°ƒæ•´ã€‚è¿™é‡Œé‡‡ç”¨ç®€å•çš„æ±‚å’Œä»¥åŠglobal average poolingä»¥è·å¾—channel-wiseçš„ä¿¡æ¯$\mathbf{s} \in \mathbb{R}^{C}$ï¼š \mathbf{U}=\widetilde{\mathbf{U}}+\widehat{\mathbf{U}} \\ s_{c}=\mathcal{F}_{g p}\left(\mathbf{U}_{c}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} \mathbf{U}_{c}(i, j)åœ¨è·å¾—$\mathbf{s}$åå†é€šè¿‡MLPè·å¾—$\mathbf{z}$ï¼š \mathbf{z}=\mathcal{F}_{f c}(\mathbf{s})=\delta(\mathcal{B}(\mathbf{W} \mathbf{s}))å…¶ä¸­$\mathcal{B}$æ˜¯batch normalizationï¼›$\delta$æ˜¯Reluã€‚ ç¬¬ä¸‰æ­¥selectä½¿ç”¨soft attentionå»é€‰æ‹©ä¸åŒkernel sizeçš„feature mapå¹¶ç»“åˆåœ¨ä¸€èµ·ã€‚ä¹Ÿå³ï¼š a_{c}=\frac{e^{\mathbf{A}_{c} \mathbf{z}}}{e^{\mathbf{A}_{c} \mathbf{z}}+e^{\mathbf{B}_{c} \mathbf{z}}}, b_{c}=\frac{e^{\mathbf{B}_{c} \mathbf{z}}}{e^{\mathbf{A}_{c} \mathbf{z}}+e^{\mathbf{B}_{c} \mathbf{z}}}å…¶ä¸­$\mathbf{A}_{c}$æ˜¯å¯¹åº”$\widetilde{\mathbf{U}}$ç¬¬$c$ä¸ªchannelçš„å‚æ•°ï¼Œ$\mathbf{B}_{c}$æ˜¯å¯¹åº”$\widehat{\mathbf{U}}$ç¬¬$c$ä¸ªchannelçš„å‚æ•°ã€‚$\mathbf{A}, \mathbf{B} \in \mathbb{R}^{C \times d}$ï¼Œé‚£ä¹ˆ$a_{c},b_{c}$å°±å¯¹åº”ä¸åŒfeature mapçš„weightã€‚ å› æ­¤ï¼Œæœ€ç»ˆçš„feature map $\mathbf{V}$ï¼š \mathbf{V}_{c}=a_{c} \cdot \tilde{\mathbf{U}}_{c}+b_{c} \cdot \widehat{\mathbf{U}}_{c}, \quad a_{c}+b_{c}=1 \\ \mathbf{V}=\left[\mathbf{V}_{1}, \mathbf{V}_{2}, \dots, \mathbf{V}_{C}\right], \mathbf{V}_{c} \in \mathbb{R}^{H \times W}å¯¹æ¯”&amp;æ€è€ƒä¸SE-NetSE-Netæ˜¯é€šè¿‡ä¸åŒchannelä¹‹é—´çš„äº¤äº’ï¼Œä½¿å¾—channelè·å¾—å…¨å±€çš„æ„Ÿå—é‡ï¼Œä½¿ç”¨çš„æ˜¯å¯¹channelçš„æ”¾ç¼©ï¼ˆè¯¦è§ä¸Šä¸€ç¯‡è®ºæ–‡ç¬”è®°ï¼‰ï¼›è€ŒSK-Netæ˜¯ä¸åŒçš„æ„Ÿå—é‡ä¹‹é—´çš„åŒä¸€channelåœ¨é€šè¿‡å…¨å±€ä¿¡æ¯çš„æŒ‡å¯¼ä¸‹ä»¥soft-attentionçš„å½¢å¼åŠ æƒå¹³å‡ï¼Œè¿™å°±å’Œè®ºæ–‡ä¸­æåˆ°çš„äººç±»è§†è§‰å¯¹ä¸åŒç‰©ä½“è¿›è¡ŒåŠ¨æ€è°ƒæ•´æ„Ÿå—é‡çš„æ€è·¯ä¸€è‡´ã€‚ ä¸dynamic convolutionåœ¨è®ºæ–‡[PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS]ä¸­ï¼Œç ”ç©¶äººå‘˜æå‡ºåŠ¨æ€æ„Ÿå—é‡çš„convolutionï¼Œé€šè¿‡åˆ©ç”¨å½“å‰è¯é¢„æµ‹ä¸€ä¸ªå·ç§¯çª—å£ï¼Œå¢åŠ äº†æ¨¡å‹çš„çµæ´»æ€§ï¼Œå¹¶åœ¨æœºå™¨ç¿»è¯‘ä¸Šå–å¾—äº†å¾ˆå¥½çš„ç»“æœã€‚ è™½ç„¶ç›®çš„ä¸æœ¬ç¯‡è®ºæ–‡ä¸€è‡´ï¼Œä½†æ€è·¯æ˜¯å®Œå…¨ä¸åŒçš„ã€‚ä¸€ä¸ªæ˜¯é€šè¿‡é¢„æµ‹ï¼›å¦ä¸€ä¸ªæ˜¯åœ¨å…¨å±€ä¿¡æ¯çš„æŒ‡å¯¼ä¸‹è¿›è¡ŒåŠ æƒã€‚åœ¨æˆ‘çš„ç†è§£çœ‹æ¥ï¼Œæˆ–è®¸æœ¬ç¯‡è®ºæ–‡çš„æ€è·¯æ›´åŠ åˆç†ä¸€äº›ï¼Œç¬¬ä¸€ï¼Œåœ¨æœ‰äº†å…¨å±€ä¿¡æ¯çš„æŒ‡å¯¼ä¸‹èƒ½å¤Ÿæ›´å¥½çš„è¿›è¡ŒåŠ æƒï¼Œè€Œé€šè¿‡é¢„æµ‹ï¼Œä¼¼ä¹æœ‰äº›ç›²ç›®ï¼Œå¯èƒ½éœ€è¦æ›´å¤šçš„æ•°æ®æ‰èƒ½å­¦å¾—æ›´å¥½ï¼›ç¬¬äºŒï¼Œdynamic convolutionè®ºæ–‡ä¸­ä¹Ÿæåˆ°äº†ï¼Œå¦‚æœä¸ä½¿ç”¨å¦‚æ·±åº¦å¯åˆ†ç¦»å·ç§¯ç­‰è½»é‡çº§å·ç§¯æ–¹æ³•ï¼Œdynamic convolutionä¸å¤§ç°å®ï¼ˆA dynamic version of standard convolutions would be impractical for current GPUs due to their large memory requirementsï¼‰ï¼Œè€ŒSK-Netåˆ™ä¸ä¼šæœ‰è¿™ä¸ªé—®é¢˜ã€‚ å…¶ä»–æ€è€ƒä»å¦ä¸€ä¸ªè§’åº¦å»æ€è€ƒï¼ŒSK-Neté€šè¿‡äººå·¥å®šä¹‰å¥½çš„å‡ ç§ä¸åŒå¤§å°çš„å·ç§¯ï¼Œç›¸å½“äºåœ¨æ¨¡å‹ä¸­å¼•å…¥æ›´å¼ºçš„å…ˆéªŒï¼ˆinductive biasï¼‰ï¼Œä¹Ÿå³å‡è®¾äº†æ•°æ®ä¸ä¼šè¶…è¿‡è¿™å‡ ç§å¤§å°çš„å·ç§¯çš„å¤„ç†èŒƒå›´ï¼Œè¿™æˆ–è®¸æ¯”ä¸å¼•å…¥å…ˆéªŒï¼Œå®Œå…¨é æ•°æ®å»å­¦æŸç§ç‰¹å®špatternçš„dynamic convolutionå¯¹å°æ•°æ®é›†æ›´å‹å¥½ï¼Œå› æ­¤å¯ä»¥ä¸éœ€è¦æ›´å¤šçš„æ•°æ®æ¥ä½¿å¾—æ¨¡å‹è¡¨ç°è‰¯å¥½ã€‚ç±»ä¼¼çš„ç†è§£å¯ä»¥åœ¨CNN/RNNä¸Transformerçš„å¯¹æ¯”ä¸­çœ‹è§ï¼Œå› ä¸ºCNN/RNNå¼•å…¥äº†è¾ƒå¼ºçš„local biasï¼Œå› æ­¤å¯¹äºå°æ•°æ®é›†æ›´å‹å¥½ï¼Œä½†åŒæ—¶å…¶ä¸Šé™æˆ–è®¸ä¸å¦‚Transformeré«˜ï¼›è€ŒTransformerä¸€å¼€å§‹å°±æ˜¯å…¨å±€æ„Ÿå—é‡ï¼Œä½¿å¾—éœ€è¦æ›´å¤šæ•°æ®æ¥å¸®åŠ©æ¨¡å‹å­¦åˆ°æŸç§ç‰¹å®špatternï¼ˆå¦‚æŸç§local biasï¼‰ï¼Œä½†å½“æ•°æ®å……è¶³æ—¶ï¼ŒTransformerçš„ä¸Šé™æ›´é«˜ï¼Œè¿‘æœŸéå¸¸ç«çš„pretrained model GPT/GPT-2.0/Bertä¼¼ä¹ä¹Ÿå°è¯äº†è¿™ç‚¹ã€‚ 2ï¸âƒ£[Attentional pooling for action recognition]æå‡ºä¸€ç§åŸºäºattentionçš„poolingç­–ç•¥ï¼Œé‡‡ç”¨ä½ç§©è¿‘ä¼¼çš„æ–¹æ³•ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨è®¡ç®—é‡ä¸å¢åŠ å¾ˆå¤šçš„æƒ…å†µä¸‹è¾¾åˆ°æ›´å¥½çš„æ•ˆæœã€‚å¯ä»¥å°†è¯¥æ–¹æ³•ç†è§£æˆå¯¹äºŒé˜¶poolingçš„ä½ç§©è¿‘ä¼¼ã€‚ æ–¹æ³•ä¸€é˜¶poolingè®°$X \in R^{n \times f}$ä¸ºè¢«poolingçš„å±‚ï¼Œå…¶ä¸­nä¸ºç©ºé—´ä½ç½®çš„ä¸ªæ•°ï¼Œå¦‚$16\times 16$ï¼Œ$f$ä¸ºchannelä¸ªæ•°ã€‚æ ‡å‡†çš„sum/max poolingå°†è¯¥çŸ©é˜µç¼©å‡ä¸º$R^{f \times 1}$ï¼Œç„¶åä½¿ç”¨å…¨è¿æ¥çš„æƒé‡$\mathbf{w} \in R^{f \times 1}$è·å¾—ä¸€ä¸ªåˆ†ç±»çš„åˆ†æ•°ã€‚è¿™é‡Œå‡è®¾çš„æ˜¯äºŒåˆ†ç±»ï¼Œä½†å¯ä»¥å¾ˆå®¹æ˜“æ¨å¹¿ä¸ºå¤šåˆ†ç±»ã€‚ ä¸Šè¿°æ“ä½œå½¢å¼åŒ–å¯ä»¥å†™æˆï¼š \operatorname{score}_{p o o l}(X)=\mathbf{1}^{T} X \mathbf{w}, \quad \text { where } \quad X \in R^{n \times f}, \mathbf{1} \in R^{n \times 1}, \mathbf{w} \in R^{f \times 1}å…¶ä¸­$\mathbf{1}$ä¸ºå…¨1å‘é‡ï¼Œ$\mathbf{x}=\mathbf{1}^{T} X \in R^{1 \times f}$å°±æ˜¯é€šè¿‡sum poolingåçš„featureã€‚ äºŒé˜¶poolingæ„å»ºäºŒé˜¶feature $X^{T} X \in R^{f \times f}$ï¼Œåœ¨è·å¾—äºŒé˜¶featureåï¼Œé€šå¸¸æˆ–å‘é‡åŒ–è¯¥çŸ©é˜µï¼Œå†é€å…¥å…¨è¿æ¥ä»¥åšåˆ†ç±»ã€‚ä¹Ÿå³æˆ‘ä»¬ä¼šå­¦ä¹ ä¸€ä¸ª$f\times f$çš„å…¨è¿æ¥æƒé‡å‘é‡ã€‚è‹¥ä¿æŒäºŒé˜¶featureä¸å¯¹åº”çš„å…¨è¿æ¥æƒé‡å‘é‡çš„å½¢å¼ä¸ºçŸ©é˜µï¼ŒçŸ©é˜µç›¸ä¹˜ï¼Œå…¶ä¸­çš„è¿¹å®é™…ä¸Šå°±æ˜¯è¿™ä¸¤ä¸ªå‘é‡åŒ–åçš„çŸ©é˜µæ‰€åšå†…ç§¯è·å¾—çš„åˆ†æ•°ã€‚å½¢å¼åŒ–å¯ä»¥å†™æˆï¼š \text {score}_{order2}(X)=\operatorname{Tr}\left(X^{T} X W^{T}\right), \quad \text { where } \quad X \in R^{n \times f}, W \in R^{f \times f}è¿™å¯ä»¥ç”¨è¿¹çš„å®šä¹‰å»è¯æ˜ï¼šç¤ºæ„å›¾ ä½ç§©äºŒé˜¶poolingç°å°è¯•ä½¿ç”¨ä½ç§©å»è¿‘ä¼¼è¯¥äºŒé˜¶poolingï¼Œä¹Ÿå³å¯¹$W$è¿‘ä¼¼ï¼Œå°†$W$å†™æˆä¸¤ä¸ªå‘é‡çš„ä¹˜ç§¯ï¼Œä¹Ÿå³ï¼š W=\mathbf{a b}^{T} \text { where } \mathbf{a}, \mathbf{b} \in R^{f \times 1}å°†ä¸Šå¼ä»£å…¥äºŒé˜¶poolingï¼Œå¯è·å¾—ï¼š \begin{aligned} \text {score}_{\text {attention}}(X) &=\operatorname{Tr}\left(X^{T} X \mathbf{b a}^{T}\right), \quad \text { where } \quad X \in R^{n \times f}, \mathbf{a}, \mathbf{b} \in R^{f \times 1} \\ &=\operatorname{Tr}\left(\mathbf{a}^{T} X^{T} X \mathbf{b}\right) \\ &=\mathbf{a}^{T} X^{T} X \mathbf{b} \\ &=\mathbf{a}^{T}\left(X^{T}(X \mathbf{b})\right) \end{aligned}ç¬¬äºŒè¡Œä½¿ç”¨çš„æ˜¯è¿¹çš„å®šç†ï¼š$\operatorname{Tr}(A B C)=\operatorname{Tr}(C A B)$ç¬¬ä¸‰è¡Œä½¿ç”¨çš„æ˜¯æ ‡é‡çš„è¿¹ç­‰äºæ ‡é‡æœ¬èº«ã€‚æœ€åä¸€è¡Œè¡¨æ˜æ•´ä¸ªæµç¨‹ï¼šç»™å®šä¸€ä¸ªfeature map $X$ï¼Œé¦–å…ˆè®¡ç®—ä¸€ä¸ªå¯¹æ‰€æœ‰ç©ºé—´ä½ç½®çš„attentional mapï¼š$\mathbf{h}= {X \mathbf{b} \in R^{n \times 1}}$ï¼›ç„¶åæ ¹æ®è¯¥attentional mapè®¡ç®—åŠ æƒå¹³å‡çš„featureï¼š$\mathbf{x}=X^{T} \mathbf{h} \in R^{f \times 1}$ã€‚è¯¥featureå†é€šè¿‡çº¿æ€§å±‚è·å¾—æœ€ç»ˆçš„åˆ†æ•°ã€‚ å®é™…ä¸Šä¸Šå¼è¿˜æœ‰å…¶ä»–ç†è§£çš„è§’åº¦ï¼š \begin{aligned} \text {score}_{\text {attention}}(X) &=\left((X \mathbf{a})^{T} X\right) \mathbf{b} \\ &=(X \mathbf{a})^{T}(X \mathbf{b}) \end{aligned}ç¬¬ä¸€è¡Œè¡¨æ˜attentional mapä¹Ÿå¯ä»¥é€šè¿‡$X \mathbf{a} \in R^{n \times 1}$æ¥è®¡ç®—ï¼Œ$\mathbf{b}$æ¥åšclassifierã€‚ç¬¬äºŒè¡Œè¡¨æ˜ï¼Œè¯¥å¼å­æœ¬è´¨ä¸Šæ˜¯å¯¹ç§°çš„ï¼Œå¯ä»¥çœ‹æˆä¸¤ä¸ªattentional heapmapçš„å†…ç§¯ã€‚ ä¸‹å›¾æ˜¯æ•´ä¸ªæµç¨‹ï¼š Top-down attentionç°å°†äºŒåˆ†ç±»æ¨å¹¿ä¸ºå¤šåˆ†ç±»ï¼š \text {score}_{order2}(X, k)=\operatorname{Tr}\left(X^{T} X W_{k}^{T}\right), \quad \text { where } \quad X \in R^{n \times f}, W_{k} \in R^{f \times f}ä¹Ÿå³å°†$W$æ›¿æ¢æˆç±»ç›¸å…³çš„å‚æ•°ï¼Œä»¿ç…§ä¸Šé¢çš„æ¨å¯¼ï¼Œå¯ä»¥æ¨å‡ºæ¯ä¸ªç±»éƒ½æœ‰ç‰¹å®šçš„$\boldsymbol{a}_{k}$ä¸$\boldsymbol{b}_{k}$ã€‚ ä½†åœ¨è¿™é‡Œé€šè¿‡å›ºå®šå…¶ä¸­ä¸€ä¸ªå‚æ•°ä¸ºä¸ç±»æ— å…³çš„å‚æ•°ï¼Œä¹Ÿå³$\boldsymbol{b}_{k}=\boldsymbol{b}$ã€‚å®é™…ä¸Šå°±ç­‰ä»·äºä¸€ä¸ªæ˜¯ç±»ç›¸å…³çš„top-down attentionï¼›å¦ä¸€ä¸ªæ˜¯ç±»æ— å…³çš„bottom-up attentionã€‚ä¸€ä¸ªè·å¾—ç±»ç‰¹å®šçš„ç‰¹å¾ï¼›å¦ä¸€ä¸ªè·å¾—å…¨å±€é€šç”¨çš„ç‰¹å¾ã€‚ å› æ­¤æœ€ç»ˆä½ç§©attention modelä¸ºï¼š \text {score}_{attention}(X, k)=\mathbf{t}_{k}^{T} \mathbf{h}, \quad \text { where } \quad \mathbf{t}_{k}=X \mathbf{a}_{k}, \mathbf{h}=X \mathbf{b}Average-pooling Revisitedå½“ç„¶åœ¨ç»™å®šäº†ä¸Šè¿°ä¸€ç³»åˆ—çš„æ¨å¯¼ï¼Œæˆ‘ä»¬å¯¹average-poolingé‡æ–°è¿›è¡Œå½¢å¼åŒ–ï¼š \text {score}_{top-down}(X, k)=\mathbf{1}^{T} X \mathbf{w}_{k}=\mathbf{1}^{T} \mathbf{t}_{k} \quad \text { where } \quad \mathbf{t}_{k}=X \mathbf{w}_{k}å°†$\mathbf{w}$æ›¿æ¢æˆç±»ç›¸å…³çš„$\mathbf{w}_{k}$ï¼Œå®é™…ä¸Šå°±æ˜¯å°†äºŒåˆ†ç±»æ¨å¹¿ä¸ºå¤šåˆ†ç±»ã€‚ä½†è¯¥å½¢å¼èµ‹äºˆäº†average-poolingæ–°çš„ç†è§£ã€‚ å½“ç„¶ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å°†rank-1æ¨å¹¿ä¸ºrank-kï¼Œå®éªŒè¯æ˜å¯¹äºå¤§æ•°æ®é›†ä½¿ç”¨å¤§çš„ç§©ä¼šæ›´å¥½ã€‚ å¯¹æ¯”ä¸Self-attentiveçš„è”ç³»è®ºæ–‡[A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING]å°±æå‡ºäº†åˆ©ç”¨å¯å­¦ä¹ çš„headå¯¹featureè¿›è¡ŒattentionåŠ æƒå¹³å‡çš„æ–¹æ³•ï¼Œå¹¶ä¸”å°†ä¸€ä¸ªheadæ¨å¹¿åˆ°å¤šä¸ªheadã€‚å®é™…ä¸Šåœ¨$\mathbf{h}= {X \mathbf{b} \in R^{n \times 1}}$æˆ‘ä»¬å°±å¯ä»¥çœ‹å‡ºï¼Œ$\mathbf{b}$åœ¨è¿™é‡Œæ‰®æ¼”çš„è§’è‰²å°±æ˜¯self-attentiveçš„headçš„è§’è‰²ã€‚å¯¹äºç§©ä¸º1çš„è¿‘ä¼¼ï¼Œå°±æ˜¯headä¸º1çš„æƒ…å†µï¼Œè‹¥å°†ç§©ä¸º1æ¨å¹¿ä¸ºç§©ä¸ºkï¼Œä¹Ÿå³ç­‰ä»·äºåœ¨Self-attentiveä¸­å¤šä¸ªheadçš„æƒ…å†µã€‚ æœ¬æ–‡å·§å¦™çš„åœ°æ–¹åœ¨äºheadæœ‰ä¸¤ä¸ªä½œç”¨ï¼Œä¸€ç§æ˜¯top-downçš„headï¼Œè·å¾—çš„æ˜¯ç±»ç›¸å…³çš„featureï¼›å¦ä¸€ä¸ªæ˜¯bottom-upçš„featureï¼Œè·å¾—çš„æ˜¯é€šç”¨çš„featureã€‚å¹¶ä¸”æœ¬æ–‡é€šè¿‡å·§å¦™çš„æ•°å­¦æ¨å¯¼æ¥è·å¾—æ–°çš„è§£é‡Šï¼Œæœ¬æ¥ä»…ä»…æ˜¯äºŒé˜¶featureè¿‡ä¸€ä¸ªå…¨è¿æ¥ï¼Œä½†é€šè¿‡å…¬å¼æ¨å¯¼èµ‹äºˆäº†attentionçš„è§£é‡Šï¼Œè¿™ç‚¹è®©äººçœ¼å‰ä¸€äº®ã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>second-order</tag>
        <tag>pooling</tag>
        <tag>SK-Net</tag>
        <tag>attentional pooling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯21]]></title>
    <url>%2F2019%2F03%2F31%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D21%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£é†‰è½é­„ Â· å¸­ä¸Šå‘ˆå…ƒç´ [å®‹] è‹è½¼åˆ†æºå¦‚æ˜¨ï¼Œäººç”Ÿåˆ°å¤„èé£˜æ³Šã€‚å¶ç„¶ç›¸èšè¿˜ç¦»ç´¢ã€‚å¤šç—…å¤šæ„ï¼Œé¡»ä¿¡ä»æ¥é”™ã€‚å°Šå‰ä¸€ç¬‘ä¼‘è¾å´ï¼Œå¤©æ¶¯åŒæ˜¯ä¼¤æ²¦è½ã€‚æ•…å±±çŠ¹è´Ÿå¹³ç”Ÿçº¦ã€‚è¥¿æœ›å³¨åµ‹ï¼Œé•¿ç¾¡å½’é£é¹¤ã€‚ http://lib.xcz.im/work/57c467a86be3ff0058452840 2ï¸âƒ£æˆä¸ºå…­ç»å¥[å”] æœç”«ã€å…¶ä¸€ã€‘åº¾ä¿¡æ–‡ç« è€æ›´æˆï¼Œå‡Œäº‘å¥ç¬”æ„çºµæ¨ªã€‚ä»Šäººå—¤ç‚¹æµä¼ èµ‹ï¼Œä¸è§‰å‰è´¤ç•åç”Ÿã€‚ ã€å…¶ä¸‰ã€‘çºµä½¿å¢ç‹æ“ç¿°å¢¨ï¼ŒåŠ£äºæ±‰é­è¿‘é£éªšã€‚é¾™æ–‡è™è„Šçš†å›é©­ï¼Œå†å—è¿‡éƒ½è§å°”æ›¹ã€‚ è¿‡éƒ½å†å— (guÃ² dÅu lÃ¬ kuÃ i)è§£é‡Šï¼šè¶Šè¿‡éƒ½å¸‚ï¼Œç»è¿‡å±±é˜œã€‚æ„æŒ‡çºµæ¨ªé©°éª‹ï¼Œæ–½å±•æ‰èƒ½ã€‚]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡14]]></title>
    <url>%2F2019%2F03%2F24%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8714%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨è®ºæ–‡: Is Second-order Information Helpful for Large-scale Visual Recognition? The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classification 1ï¸âƒ£[Is Second-order Information Helpful for Large-scale Visual Recognition?]é€šè¿‡åæ–¹å·®çš„æ–¹æ³•è·å¾—å›¾åƒçš„äºŒé˜¶ä¿¡æ¯ã€‚å‚è€ƒäº†https://zhuanlan.zhihu.com/p/46864160 æ·±åº¦åˆ†ç±»ç½‘ç»œä¸»è¦åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šç‰¹å¾æå–å’Œåˆ†ç±»å™¨ã€‚æ— è®ºæ˜¯VGGè¿˜æ˜¯GoogleNetï¼Œåæ¥çš„Resnetã€Densenetï¼Œåœ¨è¿æ¥åˆ†ç±»å™¨ä¹‹å‰ï¼Œä¸€èˆ¬éƒ½è¿æ¥äº†ä¸€ä¸ªPoolingå±‚ã€‚ä½†poolingåªè·å¾—äº†featureçš„ä¸€é˜¶ä¿¡æ¯ï¼Œå¯¹äºç»†åˆ†ç±»é—®é¢˜ä¸­ç±»é—´å·®å¼‚ä¸æ˜¾è‘—ï¼Œä¸€é˜¶ä¿¡æ¯å¯èƒ½æœ‰ä¸€äº›ä¸é€‚ç”¨ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€é˜¶ä¿¡æ¯è·å¾—äºŒé˜¶ä¿¡æ¯ï¼Œä»è€Œè·å–æ›´æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚ æœ¬æ–‡é€šè¿‡è·å–ç‰¹å¾åæ–¹å·®çš„æ–¹æ³•ï¼Œä»¥è¾¾åˆ°è¯¥ç›®çš„ã€‚ è¾“å…¥:$\mathbf{X} \in \mathbb{R}^{d \times N}$ åˆ™åæ–¹å·®çŸ©é˜µä¸º$\mathbf{X} \mapsto \mathbf{P}, \quad \mathbf{P}=\mathbf{X} \overline{\mathbf{I}} \mathbf{X}^{T}$ï¼Œå…¶ä¸­$\overline{\mathbf{I}}=\frac{1}{N}\left(\mathbf{I}-\frac{1}{N} \mathbf{1} \mathbf{1}^{T}\right)$, $\mathbf{I}$æ˜¯å•ä½é˜µï¼Œ$\mathbf{1}$æ˜¯å…¨1çš„å‘é‡ã€‚ åæ–¹å·®çŸ©é˜µæ˜¯åŠæ­£å®šçŸ©é˜µï¼Œå› æ­¤å¯å†™æˆ$\mathbf{P} \mapsto(\mathbf{U}, \mathbf{\Lambda}), \quad \mathbf{P}=\mathbf{U} \mathbf{\Lambda} \mathbf{U}^{T}$ï¼Œå…¶ä¸­$\boldsymbol{\Lambda}=\operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{d}\right)$ï¼Œ$\mathbf{U}=\left[\mathbf{u}_{1}, \dots, \mathbf{u}_{d}\right]$ï¼Œ$\mathbf{U}$æ˜¯å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ æœ€ç»ˆå¯è·å¾—$\mathbf{Q}$çŸ©é˜µï¼š$(\mathbf{U}, \boldsymbol{\Lambda}) \mapsto \mathbf{Q}, \mathbf{Q} \triangleq \mathbf{P}^{\alpha}=\mathbf{U F}(\mathbf{\Lambda}) \mathbf{U}^{T}$ï¼Œå…¶ä¸­$\alpha$æ˜¯ä¸€ä¸ªæ­£å®æ•°ï¼Œ$\mathbf{F}(\boldsymbol{\Lambda})=\operatorname{diag}\left(f\left(\lambda_{1}\right), \ldots, f\left(\lambda_{d}\right)\right)$ï¼Œå…¶ä¸­$f\left(\lambda_{i}\right)=\lambda_{i}^{\alpha}$ï¼Œæ˜¯ç‰¹å¾å€¼çš„å¹‚ï¼Œå¦‚æœè¦åšå½’ä¸€åŒ–ï¼Œé‚£ä¹ˆå¯ä»¥æœ‰ï¼š f\left(\lambda_{i}\right)=\left\{\begin{array}{cc}{\lambda_{i}^{\alpha} / \lambda_{1}^{\alpha}} & {\text { for MPN+M }-\ell_{2}} \\ {\lambda_{i}^{\alpha} /\left(\sum_{k} \lambda_{k}^{2 \alpha}\right)^{\frac{1}{2}}} & {\text { for MPN+M-Fro }}\end{array}\right.ä¹‹æ‰€ä»¥å–å¹‚ï¼Œæ˜¯ä¸ºäº†è§£å†³åœ¨åæ–¹å·®ä¼°è®¡ä¸­å°æ ·æœ¬é«˜ç»´åº¦çš„é—®é¢˜ï¼Œä»¥resnetä¸ºä¾‹ï¼Œæœ€åå¾—åˆ°çš„featureä¸º7X7X512ï¼Œä¹Ÿå°±æ˜¯49ä¸ª512ç»´çš„featureï¼Œè¿™æ ·ä¼°è®¡å‡ºæ¥çš„åæ–¹å·®çŸ©é˜µæ˜¯ä¸é è°±çš„ï¼Œè€Œé€šè¿‡å¹‚è¿™ä¸ªæ“ä½œï¼Œå¯ä»¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚é€šè¿‡å®éªŒå¯ä»¥å‘ç°ï¼Œå½“å¹‚æ¬¡ä¸º0.5ä¹Ÿå°±æ˜¯å¹³æ–¹æ ¹æ“ä½œæ—¶ï¼Œæ•ˆæœæœ€ä¼˜ã€‚ï¼ˆä¼¼ä¹ç±»ä¼¼çš„æœ‰word2vecçš„å¹³æ»‘ï¼‰ ï¼ˆè™½ç„¶è¿™ç¯‡æœ‰äº›çœ‹ä¸å¤§æ‡‚ï¼Œä½†ä¸€ä¸ªå¯å‘å°±æ˜¯ï¼Œå¯ä»¥é€šè¿‡åæ–¹å·®çš„æ–¹å¼è¿›è¡Œç‰¹å¾ä¹‹é—´çš„äº¤äº’ï¼‰ 2ï¸âƒ£[The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classification]æå‡ºä½¿ç”¨å·ç§¯å‡ºæ¥åçš„featureç»è¿‡poolingä½œä¸ºæœ€åçš„å›¾åƒç‰¹å¾è¡¨ç¤ºè€Œä¸æ˜¯å…¨è¿æ¥åçš„ç‰¹å¾è¡¨ç¤ºã€‚ Motivationï¼šåªä½¿ç”¨æœ€åä¸€å±‚fcçš„ç‰¹å¾æœ‰ä¸€ä¸ªç¼ºç‚¹ï¼Œå°±æ˜¯ä¸¢å¤±ä½ç½®ä¿¡æ¯ï¼Œè€Œconvolution layeråŒ…å«äº†ä¸°å¯Œçš„ç©ºé—´ä¿¡æ¯ã€‚åœ¨poolingå®Œåæ¯ä¸ªlocalåŒºåŸŸéƒ½èƒ½è·å¾—ä¸€ä¸ªç‰¹å¾ï¼Œå¹¶æ‹¼æ¥èµ·æ¥ä½œä¸ºæœ€åçš„è¡¨ç¤ºã€‚ prerequisite:â‘ é¦–å…ˆæœ‰ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„æ¨¡å‹â‘¡æœ‰ä¸¤å±‚ä¸€æ ·$H\times W$çš„convolutionã€‚è®ºæ–‡ä»¥AlexNetä½œä¸ºä¾‹å­ å‡è®¾å·ç§¯åçš„feature mapæ˜¯$H Ã— W Ã— D$ï¼Œé‚£ä¹ˆå¯ä»¥ç†è§£æˆï¼Œæˆ‘ä»¬å°†å›¾ç‰‡åˆ†ä¸º$H Ã— W$çš„åŒºåŸŸï¼Œæ¯ä¸ªåŒºåŸŸçš„ç‰¹å¾ç”¨$D$ç»´è¡¨ç¤ºã€‚æˆ‘ä»¬ç§°æ¯ä¸ª$D$ç»´ç‰¹å¾ä¸€ä¸ªspatial unitã€‚å½“ä½¿ç”¨å…¨è¿æ¥æ—¶ï¼Œè¿™éƒ¨åˆ†çš„ç©ºé—´ä¿¡æ¯å°±ä¸¢å¤±äº†ï¼Œå¹¶ä¸”æ— æ³•è¿˜åŸã€‚ æœ¬æ–‡æå‡ºï¼Œå°†æ¯ä¸ªåŒºåŸŸæå–å‡ºä¸€ä¸ªç‰¹å¾ï¼Œç„¶åæ‹¼èµ·æ¥ç»„æˆä¸€æ•´å¼ å›¾çš„ç‰¹å¾ï¼Œå¦‚ä¸‹å›¾ï¼Œæ¯ä¸ªé•¿æ¡ï¼ˆä¹Ÿå³$1\times 1\times channel$ï¼‰ä½œä¸ºä¸€ä¸ªç‰¹å¾ï¼š å¦‚ä½•åˆ¤æ–­åŒºåŸŸï¼Ÿä¸€ç§æ–¹æ³•æ˜¯é¦–å…ˆæ£€æµ‹å‡ºå¤šä¸ªåŒºåŸŸï¼Œæ¯ä¸ªåŒºåŸŸå¯¹åº”ä¸€ç§object partï¼Œç„¶åå¯¹äºè½å…¥è¯¥åŒºåŸŸçš„ç‰¹å¾è¿›è¡Œpoolingï¼Œç»™å®šDç§human-specified object partsï¼Œé‚£ä¹ˆå¯ä»¥è·å¾—Dä¸ªfeatureä¸”æ‹¼åœ¨ä¸€èµ·ã€‚ \mathbf{P}_{k}^{t}=\sum_{i=1} \mathbf{x}_{i} I_{i, k}å…·ä½“è€Œè¨€ï¼Œ$\mathbf{x}_{i}$æ˜¯ç‰¹å¾ï¼Œ$I_{i, k}$æ˜¯äºŒå…ƒçš„indicatorï¼Œè¡¨æ˜$\mathbf{x}_{i}$æ˜¯å¦è½å…¥è¯¥åŒºåŸŸï¼Œæ¯ä¸ª$I$å®é™…ä¸Šå®šä¹‰äº†ä¸€ä¸ªæ± åŒ–é€šé“ã€‚å½“ç„¶ï¼Œè¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥å°†indicatorä»äºŒå…ƒæ‰©å±•ä¸ºæƒé‡ã€‚ ä½†åœ¨å®ç°çš„è¿‡ç¨‹ä¸­ï¼Œå¹¶æ²¡æœ‰human-specifiedçš„åŒºåŸŸã€‚è¿™é‡Œæˆ‘ä»¬å°±å€ŸåŠ©ä¸‹ä¸€å±‚çš„å·ç§¯ä½œä¸ºindicatorã€‚ By doing so, D t+1 pooling channels are created for the local features extracted from the tth convolutional layer è¿™ä¹Ÿå°±è¢«ç§°ä¸ºcross-convolutional-layer poolingã€‚ å¦‚ä½•åšï¼Ÿ the filter of a convolutional layer works as a part detector and its feature map serves a similar role as the part region indicator map. å…·ä½“è€Œè¨€ï¼Œæœ‰ï¼š \begin{array}{l}{\mathbf{P}^{t}=\left[\mathbf{P}_{1}^{t}, \mathbf{P}_{2}^{t}, \cdots, \mathbf{P}_{k}^{t}, \cdots, \mathbf{P}_{D_{t+1}}^{t}\right]} \\ {\text { where, } \mathbf{P}_{k}^{t}=\sum_{i=1}^{N_{t}} \mathbf{x}_{i}^{t} a_{i, k}^{t+1}}\end{array}$\mathbf{P}^{t}$è¡¨ç¤ºç¬¬tå±‚convolutionåœ¨å·ç§¯è¿‡ååšcross-poolingåçš„ç‰¹å¾é›†åˆï¼Œä¹Ÿå³æˆ‘ä»¬è¦è·å¾—çš„è¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºé€šè¿‡$D_{t+1}$æ¬¡poolingåçš„ç»“æœæ‹¼æ¥è€Œæˆã€‚$D_{t+1}$å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯ç¬¬t+1å±‚çš„å·ç§¯çš„channelç»´æ•°ã€‚å‡è®¾$\mathbf{a}_{i}^{t+1} \in \mathbb{R}^{D_{t+1}}$æ˜¯ç¬¬t+1å±‚convolutionçš„ç¬¬iä¸ªç©ºé—´å•ä½ï¼ˆspatial unitï¼‰çš„feature vectorï¼Œå…¶ä¸­$a_{i, k}^{t+1}$æ˜¯è¯¥å‘é‡çš„ä¸€ä¸ªå€¼ï¼Œè¯¥å€¼å°±ä½œä¸ºpoolingçš„æƒé‡ã€‚ ä¸Šè¿°æœ‰äº›ç»•å£ä¸”éš¾æ‡‚ï¼Œç›´æ¥çœ‹ä¾‹å­ï¼š å³ï¼Œç¬¬t+1å±‚convolutionçš„channelç»´åº¦ä¸ºå¤šå°‘ï¼Œåˆ™poolingåçš„ç‰¹å¾ä¸ªæ•°å³ä¸ºå¤šå°‘ã€‚å› ä¸ºç¬¬tå±‚ä¸ç¬¬t+1å±‚çš„$H\times W$æ˜¯ä¸€è‡´çš„ï¼Œé‚£ä¹ˆå¯ä»¥ç”¨t+1å±‚çš„æ¯ä¸ªsliceå»å¯¹ç¬¬tå±‚çš„convolutionè¿›è¡ŒåŠ æƒã€‚ ä¸ºä»€ä¹ˆè¿™æ ·æ˜¯åˆç†çš„ï¼Ÿå› ä¸ºç¬¬t+1å±‚çš„convolutionæå–äº†$D_{t+1}$ä¸ªç‰¹å¾ï¼Œä½¿ç”¨çš„æ˜¯$m\times n$çš„kernel sizeï¼Œå¦‚æœ$x$æ˜¯è¢«$m\times n$çš„æŸä¸ªkernelæå–äº†ï¼Œé‚£ä¹ˆå¾ˆè‡ªç„¶çš„ï¼Œ$x$å°±æ˜¯å¯¹åº”è¯¥kernelæå–å‡ºæ¥çš„featureçš„ä¸€ä¸ªspatial unitã€‚è¯´ç™½äº†å°±æ˜¯ç¬¬tå±‚ä¸ç¬¬t+1å±‚çš„ç©ºé—´å¯¹åº”ã€‚]]></content>
      <tags>
        <tag>CNN</tag>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>second-order</tag>
        <tag>pooling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯20]]></title>
    <url>%2F2019%2F03%2F23%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D20%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£æ‚è¯—ä¸ƒé¦–ï¼ˆå…¶å››ï¼‰[ä¸‰å›½] æ›¹æ¤å—å›½æœ‰ä½³äººï¼Œå®¹åè‹¥æ¡ƒæã€‚æœæ¸¸æ±ŸåŒ—å²¸ï¼Œå¤•å®¿æ½‡æ¹˜æ²šã€‚æ—¶ä¿—è–„æœ±é¢œï¼Œè°ä¸ºå‘çš“é½¿ï¼Ÿä¿¯ä»°å²å°†æš®ï¼Œè£è€€éš¾ä¹…æƒã€‚ http://lib.xcz.im/work/58ad4c78ac502e007e9f6f9f 2ï¸âƒ£æ¢¦æ±Ÿå—[å”] æ¸©åº­ç­ åƒä¸‡æ¨ï¼Œæ¨æåœ¨å¤©æ¶¯ã€‚å±±æœˆä¸çŸ¥å¿ƒé‡Œäº‹ï¼Œæ°´é£ç©ºè½çœ¼å‰èŠ±ï¼Œæ‘‡æ›³ç¢§äº‘æ–œã€‚ http://lib.xcz.im/work/57b8d0c77db2a2005425c856]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡13]]></title>
    <url>%2F2019%2F03%2F17%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8713%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Depthwise Separable Convolutions for Neural Machine Translation]å°†depthwise separable convolution æ·±åº¦å¯åˆ†ç¦»å·ç§¯ ç”¨äºç¿»è¯‘ä»»åŠ¡ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¯¹depthwise separableè¿›è¡Œæ›´è¿›ä¸€æ­¥çš„å‚æ•°é‡ä¼˜åŒ–ï¼Œä¹Ÿå³super-separableã€‚ï¼ˆå…¶å®æˆ‘è§‰å¾—å¹¶æ²¡æœ‰å•¥åˆ›æ–°æ€§çš„æ„Ÿè§‰ï¼‰ é¦–å…ˆä»‹ç»ä»€ä¹ˆæ˜¯depthwise separable convolutionï¼Œå®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªdepthwise+pointwiseã€‚ \operatorname{Conv}(W, y)_{(i, j)}=\sum_{k, l, m}^{K, L, M} W_{(k, l, m)} \cdot y_{(i+k, j+l, m)}\operatorname{PointwiseConv}(W, y)_{(i, j)}=\sum_{m}^{M} W_{m} \cdot y_{(i, j, m)}\text {DepthwiseConv}(W, y)_{(i, j)}=\sum_{k, l}^{K, L} W_{(k, l)} \odot y_{(i+k, j+l)}\operatorname{SepConv}\left(W_{p}, W_{d}, y\right)_{(i, j)}=\text {PointwiseConv}_{(i, j)}\left(W_{p}, \text { DepthwiseConv }_{(i, j)}\left(W_{d}, y\right)\right)å‡ ç§convolutionçš„å‚æ•°é‡å¯¹æ¯”ï¼šå…¶ä¸­kæ˜¯kernel sizeï¼Œcæ˜¯channelï¼Œgæ˜¯groupã€‚ g-Sub-separableæ˜¯æŒ‡å°†channelåˆ†ä¸ºå‡ ä¸ªgroupï¼Œæ¯ä¸ªgroupè¿›è¡Œå¸¸è§„çš„convolutionæ“ä½œï¼›g-Super-separableï¼Œä¹Ÿå³æœ¬æ–‡ä¸­æå‡ºçš„convolutionï¼ŒåŒæ ·æ˜¯å°†channelåˆ†ä¸ºå‡ ä¸ªgroupï¼Œç„¶åå¯¹æ¯ä¸ªgroupè¿›è¡Œdepthwise-separableçš„å·ç§¯ã€‚ 2ï¸âƒ£[Squeeze-and-Excitation Networks]æå‡ºä¸€ç§æ–°å‹çš„ç½‘ç»œï¼Œèƒ½å¤Ÿé€šè¿‡å»ºæ¨¡channelä¹‹é—´çš„å…³ç³»ï¼Œä½¿å¾—æ¯ä¸ªchannelèƒ½å¤Ÿè·å¾—å…¨å±€çš„ä¿¡æ¯ï¼Œè¿›è€Œæé«˜æ¨¡å‹çš„èƒ½åŠ›ã€‚ åˆ†ä¸ºä¸¤æ­¥ï¼šç¬¬ä¸€æ­¥æ˜¯è·å¾—ä¸€ä¸ªå…¨å±€çš„è¡¨ç¤ºï¼Œç¬¬äºŒæ­¥æ˜¯æ ¹æ®å…¨å±€ä¿¡æ¯æ›´æ–°æ¯ä¸ªchannelçš„ä¿¡æ¯ã€‚ ç¬¦å·è¾“å…¥ï¼š$ \mathbf{X} \in \mathbb{R}^{H^{\prime} \times W^{\prime} \times C^{\prime}} $ç»è¿‡ç‰¹å¾æå–åï¼ˆå¦‚Convolution)ï¼š$\mathbf{U} \in \mathbb{R}^{H \times W \times C}$ï¼Œä¹Ÿå³ï¼š$\mathbf{U}=\mathbf{F}_{t r}(\mathbf{X})$å°†$\mathbf{U}$å†™æˆï¼š$\mathbf{U}=\left[\mathbf{u}_{1}, \mathbf{u}_{2}, \dots, \mathbf{u}_{C}\right]$$\mathbf{V}$ æ˜¯å¯å­¦ä¹ çš„å·ç§¯æ ¸å‚æ•°ï¼š $\mathbf{V}=\left[\mathbf{v}_{1}, \mathbf{v}_{2}, \ldots, \mathbf{v}_{C}\right]$ åˆ™ä¸Šè¿°å·ç§¯å˜æ¢å¯å†™æˆï¼š$\mathbf{u}_{c}=\mathbf{v}_{c} \ast \mathbf{X}=\sum_{s=1}^{C^{\prime}} \mathbf{v}_{c}^{s} \ast \mathbf{x}^{s}$ Squeeze: Global Information Embeddingç¬¬ä¸€æ­¥ï¼Œå°†æ‰€æœ‰çš„ç‰¹å¾è¿›è¡Œæ•´åˆå¾—åˆ°å…¨å±€çš„ç‰¹å¾ï¼š z_{c}=\mathbf{F}_{s q}\left(\mathbf{u}_{c}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} u_{c}(i, j)è®ºæ–‡æå–å…¨å±€ç‰¹å¾çš„æ–¹æ³•ç›´æ¥ç”¨ç®€å•çš„global average poolingã€‚é‚£ä¹ˆ$\mathbf{z} \in \mathbb{R}^{C}$çš„æ¯ä¸€ç»´å°±ä»£è¡¨æ¯ä¸€ç»´çš„channelã€‚ Excitation: Adaptive Recalibrationä¸attentionä¸åŒçš„æ˜¯ï¼Œè®ºæ–‡å¸Œæœ›èƒ½å¤ŸåŒæ—¶å¼ºè°ƒä¸åŒå¤šä¸ªchannelçš„é‡è¦ï¼ˆè€Œä¸æ˜¯one-hotçš„å½¢å¼ï¼‰ï¼Œå› æ­¤ä½¿ç”¨ä¸€ä¸ªç®€å•çš„é—¨æ§åˆ¶æœºåˆ¶ï¼Œé‡‡ç”¨sigmoidæ¿€æ´»å‡½æ•°ï¼šï¼ˆè¿™é‡Œçš„æƒ³æ³•æŒºæœ‰æ„æ€ï¼Œç›¸å¯¹attentionçš„softmaxä¼¼ä¹ç¡®å®ä¼šæ›´å¥½çš„æ ·å­ï¼‰ \mathbf{s}=\mathbf{F}_{ex}(\mathbf{z}, \mathbf{W})=\sigma(g(\mathbf{z}, \mathbf{W}))=\sigma\left(\mathbf{W}_{2} \delta\left(\mathbf{W}_{1} \mathbf{z}\right)\right)ä¸ºäº†å‡å°‘å‚æ•°è¿™é‡Œçš„MLPé‡‡ç”¨äº†bottleneckçš„å½¢å¼ã€‚äº¦å³ï¼š${\mathbf{W}_{1} \in \mathbb{R}^{\frac{C}{r} \times C}}$ $ {\mathbf{W}_{2} \in \mathbb{R}^{C \times \frac{C}{r}}}$$r$æ˜¯reduction ratioã€‚ è´´ä¸Šä½œè€…çš„æ€è·¯ï¼š To make use of the information aggregated in the squeeze operation, we follow it with a second operation which aims to fully capture channel-wise dependencies. To fulfill this objective, the function must meet two criteria: first, it must be ï¬‚exible (in particular, it must be capable of learning a nonlinear interaction between channels) and second, it must learn a non-mutually-exclusive relationship since we would like to ensure that multiple channels are allowed to be emphasised (rather than enforcing a one-hot activation). To meet these criteria, we opt to employ a simple gating mechanism with a sigmoid activation. æœ€åå¯¹æ¯ä¸ªchannelè¿›è¡Œæ”¾ç¼©ï¼Œè·å¾—æ–°çš„è¡¨ç¤ºï¼š \widetilde{\mathbf{x}}_{c}=\mathbf{F}_{\text {scale}}\left(\mathbf{u}_{c}, s_{c}\right)=s_{c} \cdot \mathbf{u}_{c} 3ï¸âƒ£[Non-local Neural Networks]æå‡ºä¸€ç§æ–°çš„ç»“æ„ï¼Œä¸ä¸Šä¸€ç¯‡ç±»ä¼¼ï¼Œå¸Œæœ›æ¨¡å‹çš„æ¯ä¸ªä½ç½®éƒ½èƒ½æ„ŸçŸ¥åˆ°å…¶ä»–ä½ç½®ï¼Œä»è€Œæ•è·é•¿ç¨‹ä¾èµ–ï¼Œæ‹¥æœ‰å…¨å±€ä¿¡æ¯ã€‚ Non-local Networkå®šä¹‰non-localç½‘ç»œï¼š \mathbf{y}_{i}=\frac{1}{\mathcal{C}(\mathbf{x})} \sum_{\forall j} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) g\left(\mathbf{x}_{j}\right)å…¶ä¸­$\mathcal{C}$æ˜¯å½’ä¸€åŒ–å‡½æ•°ï¼›$f$æ˜¯ç¬¬$i$ä¸ªä½ç½®ä¸ç¬¬$j$ä¸ªä½ç½®çš„äº¤äº’å‡½æ•°ï¼›$g$è®¡ç®—ç¬¬$j$ä¸ªä½ç½®çš„è¡¨ç¤ºã€‚ $g$çš„å…·ä½“å½¢å¼ä¸€ä¸ªçº¿æ€§å‡½æ•°ï¼š$g\left(\mathbf{x}_{j}\right)=W_{g} \mathbf{x}_{j}$åœ¨å®ç°çš„æ—¶å€™æ˜¯ä¸€ä¸ª$1\times1$æˆ– $1\times1\times1$çš„convolutionã€‚ $f$çš„å…·ä½“å½¢å¼â‘ Gaussian$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=e^{\mathbf{x}_{i}^{T} \mathbf{x}_{j}}$åˆ™å½’ä¸€åŒ–å®šä¹‰ä¸º$\mathcal{C}(\mathbf{x})=\sum_{\forall j} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)$ ã€‚ â‘¡Embedded Gaussian$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=e^{\theta\left(\mathbf{x}_{i}\right)^{T} \phi\left(\mathbf{x}_{j}\right)}$å…¶ä¸­ï¼š$\theta\left(\mathbf{x}_{i}\right)=W_{\theta} \mathbf{x}_{i} $, $ \phi\left(\mathbf{x}_{j}\right)=W_{\phi} \mathbf{x}_{j}$å½’ä¸€åŒ–ï¼š$\mathcal{C}(\mathbf{x})=\sum_{\forall j} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)$ å¯ä»¥çœ‹åˆ°self-attentionæ˜¯Embedded Gaussiançš„ä¸€ç§å½¢å¼ã€‚è™½ç„¶æœ‰è¿™æ ·çš„å…³ç³»ï¼Œä½†ä½œè€…åœ¨å®éªŒä¸­å‘ç°softmaxå¹¶ä¸æ˜¯å¿…è¦çš„ã€‚ â‘¢Dot product$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\theta\left(\mathbf{x}_{i}\right)^{T} \phi\left(\mathbf{x}_{j}\right)$å½’ä¸€åŒ–ï¼š$\mathcal{C}(\mathbf{x})=N$ â‘£Concatenation$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\operatorname{ReLU}\left(\mathbf{w}_{f}^{T}\left[\theta\left(\mathbf{x}_{i}\right), \phi\left(\mathbf{x}_{j}\right)\right]\right)$$\mathcal{C}(\mathbf{x})=N$ æœ‰äº†ä¸Šé¢çš„non-localçš„ä»‹ç»ï¼Œå¯ä»¥ç›´æ¥å°†å…¶ç”¨äºresidual networkã€‚$\mathbf{z}_{i}=W_{z} \mathbf{y}_{i}+\mathbf{x}_{i}$$y$åˆ™æ˜¯non-local networkçš„è¾“å‡ºã€‚ Non-local blockçš„ç­–ç•¥/tricksâ‘ è®¾ç½®$W_g$,$W_Î¸$,$W_Ï•$çš„channelçš„æ•°ç›®ä¸ºxçš„channelæ•°ç›®çš„ä¸€åŠï¼Œè¿™æ ·å°±å½¢æˆäº†ä¸€ä¸ªbottleneckï¼Œèƒ½å¤Ÿå‡å°‘ä¸€åŠçš„è®¡ç®—é‡ã€‚Wzå†é‡æ–°æ”¾å¤§åˆ°xçš„channelæ•°ç›®ï¼Œä¿è¯è¾“å…¥è¾“å‡ºç»´åº¦ä¸€è‡´ã€‚ â‘¡åœ¨$\frac{1}{\mathcal{C}(\hat{\mathbf{x}})} \sum_{\forall j} f\left(\mathbf{x}_{i}, \hat{\mathbf{x}}_{j}\right) g\left(\hat{\mathbf{x}}_{j}\right)$ä½¿ç”¨ä¸‹é‡‡æ ·ï¼Œå¦‚max-poolingï¼Œå‡å°‘è®¡ç®—é‡ã€‚ 4ï¸âƒ£[Bilinear CNN Models for Fine-grained Visual Recognition]æå‡ºä¸€ç§åŒçº¿æ€§æ¨¡å‹ï¼Œç”±ä¸¤ä¸ªç‰¹å¾æå–å™¨ç»„æˆï¼Œä»–ä»¬çš„è¾“å‡ºåšå¤–ç§¯ï¼Œæœ€ç»ˆè·å¾—å›¾åƒæè¿°ç‰¹å¾ã€‚ Motivation(?ä¸ç¡®å®šæ˜¯ä¸æ˜¯è¿™æ ·)ï¼šå¯¹äºç»†ç²’åº¦ç‰©ä½“çš„åˆ†ç±»ï¼Œå…ˆå¯¹å±€éƒ¨å®šä½ï¼Œå†æå–ç‰¹å¾ã€‚ä¸¤ä¸ªç‰¹å¾æå–å™¨ä¸€ä¸ªæ˜¯æå–locationï¼Œå¦ä¸€ä¸ªæå–ç‰¹å¾ã€‚ ä¸ºä»€ä¹ˆç”¨å¤–ç§¯ï¼Ÿ outer product captures pairwise correlations between the feature channels æœ‰æ„æ€çš„æ˜¯ä½œè€…å°†è¯¥æ¨¡å‹å’Œäººè„‘è§†è§‰å¤„ç†çš„ä¸¤ä¸ªå‡è®¾è”ç³»åœ¨ä¸€èµ·(stream hypothesis)ï¼šhere are two main pathways, or â€œstreamsâ€. The ventral stream (or, â€œwhat pathwayâ€) is involved with object identiï¬cation and recognition. The dorsal stream (or, â€œwhere pathwayâ€) is involved with processing the objectâ€™s spatial location relative to the viewer.ä¸è¿‡çœ‹çœ‹å°±å¥½ï¼Œå¹¶æ²¡æœ‰ä»€ä¹ˆé“ç†ã€‚ å¯¹äºä¸€ä¸ªåˆ†ç±»çš„åŒçº¿æ€§æ¨¡å‹è€Œè¨€ï¼Œå…¶ä¸€èˆ¬å½¢å¼æ˜¯ä¸€ä¸ªå››å…ƒç»„ï¼š$\mathcal{B}=\left(f_{A}, f_{B}, \mathcal{P}, \mathcal{C}\right)$ã€‚å…¶ä¸­$f$æ˜¯ç‰¹å¾å‡½æ•°ï¼Œ$\mathcal{P}$æ˜¯poolingå‡½æ•°ï¼Œ$\mathcal{C}$æ˜¯åˆ†ç±»å‡½æ•°ã€‚å…·ä½“è€Œè¨€ï¼Œ$f$æ˜¯ä¸€ä¸ªæ˜ å°„ï¼Œ${f : \mathcal{L} \times \mathcal{I} \rightarrow} {R^ {c\times D}} $ã€‚ä¹Ÿå³å°†ä¸€ä¸ªimageå’Œä¸€ä¸ªlocation L æ˜ å°„æˆfeatureã€‚ï¼ˆWe refer to locations generally which can include position and scale å…¶å®è¿™é‡Œä¸æ˜¯å¾ˆæ‡‚locationçš„æ„æ€ï¼‰ å°†feature aå’Œfeature bç»“åˆåœ¨ä¸€èµ·ï¼š$\text { bilinear }\left(l, \mathcal{I}, f_{A}, f_{B}\right)=f_{A}(l, \mathcal{I})^{T} f_{B}(l, \mathcal{I})$ poolingæœ‰å¥½å‡ ç§ï¼Œå¯ä»¥ç›´æ¥åŠ èµ·æ¥ï¼Œæˆ–è€…ä½¿ç”¨max-poolingã€‚è¿™é‡Œä½¿ç”¨ç›´æ¥åŠ èµ·æ¥çš„æ–¹å¼ï¼Œå¯ä»¥ç†è§£ä¸ºï¼Œè¿™äº›ç‰¹å¾æ˜¯æ— åº(orderless)çš„å åŠ ã€‚ åœ¨è·å¾—è¾“å‡ºåå†åšä¸€äº›æ“ä½œ/trickèƒ½å¤Ÿæå‡è¡¨ç°ï¼š$\begin{array}{l}{\mathbf{y} \leftarrow \operatorname{sign}(\mathbf{x}) \sqrt{|\mathbf{x}|}} \\ {\mathbf{z} \leftarrow \mathbf{y} /|\mathbf{y}|_{2}}\end{array}$ è®¨è®ºï¼šâ‘ But do the networks specialize into roles of localization (â€œwhereâ€) and appearance modeling (â€œwhatâ€) when initialized asymmetrically and ï¬ne-tuned?é€šè¿‡å¯è§†åŒ–å‘ç°ï¼Œå¹¶æ²¡æœ‰æ˜ç¡®çš„åŠŸèƒ½åˆ†å¼€ã€‚Both these networks tend to activate strongly on highly speciï¬c semantic parts â‘¡bilinearçš„å¥½å¤„è¿˜å¯ä»¥æ‰©å±•æˆtrilinearï¼Œæ·»åŠ æ›´å¤šçš„ä¿¡æ¯ã€‚]]></content>
      <tags>
        <tag>CNN</tag>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>NMT</tag>
        <tag>SE-Net</tag>
        <tag>Non-local</tag>
        <tag>Bilinear</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯19]]></title>
    <url>%2F2019%2F03%2F17%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D19%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£é€çµæ¾ˆä¸Šäºº[å”] åˆ˜é•¿å¿è‹è‹ç«¹æ—å¯ºï¼Œæ³æ³é’Ÿå£°æ™šã€‚è·ç¬ å¸¦æ–œé˜³ï¼Œé’å±±ç‹¬å½’è¿œã€‚ è·ï¼ˆhÃ¨ï¼‰ç¬ ï¼šèƒŒç€æ–—ç¬ ã€‚ http://lib.xcz.im/work/57b90887128fe10054c9c750 2ï¸âƒ£è‹å¹•é® Â· æ€€æ—§[å®‹] èŒƒä»²æ·¹ç¢§äº‘å¤©ï¼Œé»„å¶åœ°ï¼Œç§‹è‰²è¿æ³¢ï¼Œæ³¢ä¸Šå¯’çƒŸç¿ ã€‚å±±æ˜ æ–œé˜³å¤©æ¥æ°´ï¼ŒèŠ³è‰æ— æƒ…ï¼Œæ›´åœ¨æ–œé˜³å¤–ã€‚é»¯ä¹¡é­‚ï¼Œè¿½æ—…æ€ã€‚å¤œå¤œé™¤éï¼Œå¥½æ¢¦ç•™äººç¡ã€‚æ˜æœˆæ¥¼é«˜ä¼‘ç‹¬å€šï¼Œé…’å…¥æ„è‚ ï¼ŒåŒ–ä½œç›¸æ€æ³ªã€‚ http://lib.xcz.im/work/57b8ee4a128fe10054c91757]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡12]]></title>
    <url>%2F2019%2F03%2F10%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8712%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS]Facebookç ”ç©¶äººå‘˜æå‡ºçš„ä¸¤ç§åŸºäºå·ç§¯çš„æ–¹æ³•å°è¯•æ›¿ä»£self-attentionåœ¨transformerä¸­çš„ä½œç”¨ï¼Œæ‹¥æœ‰æ›´å°‘çš„å‚æ•°ä»¥åŠæ›´å¿«çš„é€Ÿåº¦ï¼Œå¹¶ä¸”èƒ½å¤Ÿè¾¾åˆ°å¾ˆå¥½çš„æ•ˆæœã€‚ Lightweight convolutionèƒŒæ™¯ï¼šdepthwise convolutionæ¯ä¸ªchannelç‹¬ç«‹è¿›è¡Œå·ç§¯ï¼Œæ³¨æ„åˆ°æ”¾åˆ°NLPä»»åŠ¡ä¸Šchannelæ˜¯æŒ‡embeddingçš„æ¯ä¸€ç»´ã€‚ O_{i, c}=\text{DepthwiseConv}\left(X, W_{c, :}, i, c\right)=\sum_{j=1}^{k} W_{c, j} \cdot X_{\left(i+j-\left\lceil\frac{k+1}{2}\right]\right), c}å› æ­¤Lightweight convolutionçš„è®¡ç®—æ–¹æ³•ä¸ºï¼š \operatorname{LightConv}\left(X, W_{\left\lceil\frac{c H}{d}\right\rceil,:}, i, c\right)=\text { DepthwiseConv}\left(X, \text{softmax}(W_{\left\lceil\frac{c H}{d}\right\rceil,:}), i, c\right)æ¯ä¸€å±‚éƒ½æœ‰å›ºå®šçš„window sizeï¼Œè¿™å’Œself-attentionä¸åŒï¼Œself-attentionæ˜¯æ‰€æœ‰çš„contextéƒ½è¿›è¡Œäº¤äº’ã€‚ Weight sharing æ³¨æ„åˆ°è¿™é‡Œè®²æ¯d/Hä¸ªchannelçš„å‚æ•°è¿›è¡Œç»‘å®šï¼Œè¿›ä¸€æ­¥å‡å°‘å‚æ•°ã€‚ Softmax-normalization å¯¹channelä¸€ç»´è¿›è¡Œsoftmaxï¼Œç›¸å½“äºå½’ä¸€åŒ–æ¯ä¸ªè¯çš„æ¯ä¸€ç»´çš„çš„é‡è¦æ€§ï¼ˆæ¯”self-attentionæ›´ç²¾ç»†ï¼‰ã€‚å®éªŒè¯æ˜ï¼Œå¦‚æœæ²¡æœ‰softmaxæ²¡åŠæ³•æ”¶æ•›ã€‚ å› æ­¤æ€»ä½“çš„æ¶æ„ä¸ºï¼šinputâ€”&gt;linear â€”&gt; GLU(gated linear unit) â€”&gt; lightconv/dynamicConv â€”&gt; linear Dynamic convolutionä¸lightweight convolutionç›¸ä¼¼ï¼Œä½†åŠ äº†ä¸€ä¸ªåŠ¨æ€çš„kernel sizeã€‚ \text { DynamicConv}( X , i , c ) = \operatorname{LightConv}\left(X, f\left(X_{i}\right)_{h,:}, i, c\right)è¿™é‡Œçš„kernel sizeç®€å•ä½¿ç”¨çº¿æ€§æ˜ å°„ï¼š$f : \mathbb { R } ^ { d } \rightarrow \mathbb { R } ^ { H \times k }$å¦‚ï¼š$f\left(X_{i}\right)=\sum_{c=1}^{d} W_{h, j, c}^{Q} X_{i, c}$ 2ï¸âƒ£[Joint Embedding of Words and Labels for Text Classiï¬cation]æå‡ºä¸€ç§æœºåˆ¶å°†labelä½œä¸ºembeddingä¸è¯ä¸€åŒè®­ç»ƒï¼ŒåŒæ—¶å¼•å…¥labelå’Œwordçš„attentionæœºåˆ¶ï¼Œåœ¨åˆ†ç±»ä¸Šè·å¾—æ•ˆæœã€‚ ä¸Šå›¾ä¸­ï¼ŒCæ˜¯label embeddingï¼Œç»´åº¦ä¸º$P\times K$ ; Væ˜¯å¥å­æ‰€æœ‰è¯çš„embeddingçŸ©é˜µï¼Œç»´åº¦ä¸º$P\times L$ã€‚$\mathbf{G}$çš„è®¡ç®—å…¬å¼ä¸ºï¼š \mathbf{G}=\left(\mathbf{C}^{\top} \mathbf{V}\right) \oslash \hat{\mathbf{G}}$\oslash$è¡¨ç¤ºelement-wiseç›¸é™¤ã€‚$\hat{\mathbf{G}}$è¡¨ç¤ºl2 normï¼Œä¹Ÿå³ï¼š \hat{g}_{k l}=\left\|\boldsymbol{c}_{k}\right\|\left\|\boldsymbol{v}_{l}\right\|å› æ­¤å…¬å¼çš„æœ¬è´¨å³åœ¨è®¡ç®—labelä¸æ¯ä¸ªè¯çš„cosè·ç¦»ã€‚ åœ¨è·å¾—äº†$\mathbf{G}$åï¼Œä¸ºäº†è·å¾—æ›´é«˜çš„çš„è¡¨ç¤ºï¼Œå¦‚phraseï¼Œå°†ä¸€ä¸ªä¸€ä¸ªblockå–å‡ºï¼Œå¹¶è¿‡çº¿æ€§å±‚ï¼š \boldsymbol{u}_{l}=\operatorname{ReLU}\left(\mathbf{G}_{l-r : l+r} \mathbf{W}_{1}+\boldsymbol{b}_{1}\right)æ¥ç€å¯¹æ¯ä¸ª$\boldsymbol{u}_{l}$å–æœ€å¤§å€¼ï¼š m_{l}=\textbf{max-pooling}\left(\boldsymbol{u}_{l}\right)æ­¤æ—¶çš„$\mathbf{m}$æ˜¯ä¸€ä¸ªé•¿åº¦ä¸ºLçš„å‘é‡ã€‚æœ€ç»ˆå¯¹måšsoftmaxè·å¾—ä¸€ä¸ªåˆ†æ•°çš„åˆ†å¸ƒï¼š \boldsymbol{\beta}=\operatorname{SoftMax}(\boldsymbol{m})å°†è¯¥åˆ†æ•°å’Œæ¯ä¸ªè¯åšåŠ æƒæ±‚å’Œï¼Œè·å¾—æœ€ç»ˆçš„å‘é‡è¡¨ç¤ºï¼š \boldsymbol{z}=\sum_{l} \beta_{l} \boldsymbol{v}_{l}æ€è€ƒï¼šå°†labelä¸embeddingæ”¾åœ¨ä¸€èµ·è®­ç»ƒè¿™ä¸ªæ€è·¯ä¸é”™ã€‚ä½†æ•´åˆçš„æ–¹å¼æ˜¯å¦è¿‡äºç®€å•ç²—æš´äº†ï¼Ÿç‰¹åˆ«æ˜¯phraseçš„æå–å’Œéšåçš„max-poolingçš„å¯è§£é‡Šæ€§å¹¶ä¸å¼ºçš„æ ·å­ã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>attention</tag>
        <tag>Convolution</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>embedding</tag>
        <tag>text classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†18]]></title>
    <url>%2F2019%2F03%2F10%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8618%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Depthwise seperable convolution]Depthwise seperable convolution = depthwise + pointwiseå…ˆæ¯ä¸ªå·ç§¯æ ¸ç‹¬ç«‹å¯¹ä¸€ä¸ªfeature mapè¿›è¡Œå·ç§¯ï¼Œå†é€šè¿‡ä¸€ä¸ª$1\times 1 \times n$çš„å·ç§¯æ ¸å¯¹feature mapè¿›è¡Œæ•´åˆã€‚ https://blog.csdn.net/tintinetmilou/article/details/81607721 2ï¸âƒ£[å¦‚ä½•å¯»æ‰¾è¾ƒå¥½çš„lr]ä¸€ç§å¯å‘å¼çš„æ–¹æ³•ï¼š Over an epoch begin your SGD with a very low learning rate (like 10âˆ’8) but change it (by multiplying it by a certain factor for instance) at each mini-batch until it reaches a very high value (like 1 or 10). Record the loss each time at each iteration and once youâ€™re finished, plot those losses against the learning rate. https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Convolution</tag>
        <tag>learning rate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯18]]></title>
    <url>%2F2019%2F03%2F09%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D18%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£è¥¿æ±Ÿæœˆ Â· é£å…´[å®‹] è¾›å¼ƒç–¾é†‰é‡Œä¸”è´ªæ¬¢ç¬‘ï¼Œè¦æ„é‚£å¾—å·¥å¤«ã€‚è¿‘æ¥å§‹è§‰å¤äººä¹¦ï¼Œä¿¡è‘—å…¨æ— æ˜¯å¤„ã€‚æ˜¨å¤œæ¾è¾¹é†‰å€’ï¼Œé—®æ¾ã€Œæˆ‘é†‰ä½•å¦‚ã€ã€‚åªç–‘æ¾åŠ¨è¦æ¥æ‰¶ï¼Œä»¥æ‰‹æ¨æ¾æ›°ã€Œå»ã€ï¼ http://lib.xcz.im/work/57b935bcd342d3005ac8e63f 2ï¸âƒ£è¶æ‹èŠ±[å®‹] æ™æ®Šæ§›èŠæ„çƒŸå…°æ³£éœ²ï¼Œç½—å¹•è½»å¯’ï¼Œç‡•å­åŒé£å»ã€‚æ˜æœˆä¸è°™ç¦»æ¨è‹¦ï¼Œæ–œå…‰åˆ°æ™“ç©¿æœ±æˆ·ã€‚æ˜¨å¤œè¥¿é£å‡‹ç¢§æ ‘ï¼Œç‹¬ä¸Šé«˜æ¥¼ï¼Œæœ›å°½å¤©æ¶¯è·¯ã€‚æ¬²å¯„å½©ç¬ºå…¼å°ºç´ ï¼Œå±±é•¿æ°´é˜”çŸ¥ä½•å¤„ï¼Ÿ http://lib.xcz.im/work/57b318dd1532bc00618ffaff]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¦‚ä½•ä½¿ç”¨fairseqå¤ç°Transformer NMT]]></title>
    <url>%2F2019%2F01%2F28%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8fairseq%E5%A4%8D%E7%8E%B0Transformer%20NMT%2F</url>
    <content type="text"><![CDATA[åŸºäºTransformerçš„NMTè™½ç„¶ç»“æœå¥½ï¼Œä½†è¶…å‚éå¸¸éš¾è°ƒï¼Œåªè¦æœ‰ä¸€ä¸¤ä¸ªå‚æ•°å’Œè®ºæ–‡ä¸ä¸€æ ·ï¼Œå°±æœ‰å¯èƒ½å¾—åˆ°å’Œè®ºæ–‡ç›¸å»ç”šè¿œçš„ç»“æœã€‚fairseqæ˜¯ç°æœ‰æ¯”è¾ƒå®Œå–„çš„seq2seqåº“ï¼Œç”±äºæ˜¯å¤§å…¬å¸å‡ºå“ï¼Œå› æ­¤ä¹Ÿå†™å¾—è¾ƒä¸ºå®Œå–„ï¼Œä¸è®ºæ˜¯ä»£ç è¿˜æ˜¯æ–‡æ¡£ã€‚ æœ¬æ–‡è®¨è®ºå¦‚ä½•ä½¿ç”¨fairseqå¤ç°åŸºäºTransformerçš„ç¿»è¯‘ä»»åŠ¡ï¼Œä¹Ÿå³å¤ç°Vaswani, et al. çš„è®ºæ–‡ç»“æœã€‚æœ¬æ–‡å°½é‡ä¸è®¨è®ºå®ç°ç»†èŠ‚ï¼Œåªè®¨è®ºå¦‚ä½•å¤ç°å‡ºç»“æœã€‚ fairseqé¡¹ç›®åœ°å€ï¼šhttps://github.com/pytorch/fairseq ä½¿ç”¨æ•™ç¨‹åœ¨è¿™é‡Œæˆ‘ä»¬å‚è€ƒçš„æ˜¯18å¹´çš„æ–‡ç« Scaling Neural Machine Translationï¼ŒåŒæ ·æ˜¯åŸºäºTransformerçš„NMTã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨WMT16 EN-DEè€Œä¸æ˜¯Vaswani, et al.è®ºæ–‡ä¸­çš„WMT14 EN-DEã€‚äºŒè€…åªåœ¨ä¸€ä¸ªæ–‡ä»¶ï¼ˆcommoncrawlï¼‰ä¸Šæœ‰åŒºåˆ«ï¼Œå…¶ä»–æ˜¯ä¸€æ ·çš„ï¼Œç”±äºWMT16 EN-DEæœ‰é¢„å¤„ç†å¥½çš„æ•°æ®ï¼Œä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬å°±ä½¿ç”¨è¯¥ä»½æ•°æ®ï¼ˆä¸‹æ–‡ä¹Ÿæœ‰é¢„å¤„ç†WMT14æ•°æ®çš„æ–¹æ³•ï¼‰ å‡†å¤‡å·¥ä½œ å®‰è£…fairseqï¼Œåœ¨Readmeå†…æœ‰ é˜…è¯»Readmeï¼ˆoptionalï¼‰ é˜…è¯»docï¼ˆoptionalï¼‰ æ•°æ®é¢„å¤„ç†Step1æ•°æ®é¢„å¤„ç†ä¸»è¦æ˜¯ä¸‹è½½å¤šä¸ªæ–‡ä»¶å¹¶åˆå¹¶â€”&gt;æ¸…ç†/tokenizeæ•°æ®â€”&gt;å°†æ•°æ®åˆ†ä¸ºtrainã€validâ€”&gt;bpe(bype pair encoding)ã€‚fairseqæä¾›äº†ä¸€æ•´å¥—å¤„ç†æµç¨‹çš„è„šæœ¬ï¼Œåœ¨examples/translation/prepare-wmt14en2de.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139#!/bin/bash# Adapted from https://github.com/facebookresearch/MIXER/blob/master/prepareData.shecho 'Cloning Moses github repository (for tokenization scripts)...'git clone https://github.com/moses-smt/mosesdecoder.gitecho 'Cloning Subword NMT repository (for BPE pre-processing)...'git clone https://github.com/rsennrich/subword-nmt.gitSCRIPTS=mosesdecoder/scriptsTOKENIZER=$SCRIPTS/tokenizer/tokenizer.perlCLEAN=$SCRIPTS/training/clean-corpus-n.perlNORM_PUNC=$SCRIPTS/tokenizer/normalize-punctuation.perlREM_NON_PRINT_CHAR=$SCRIPTS/tokenizer/remove-non-printing-char.perlBPEROOT=subword-nmtBPE_TOKENS=40000URLS=( "http://statmt.org/wmt13/training-parallel-europarl-v7.tgz" "http://statmt.org/wmt13/training-parallel-commoncrawl.tgz" "http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz" "http://data.statmt.org/wmt17/translation-task/dev.tgz" "http://statmt.org/wmt14/test-full.tgz")FILES=( "training-parallel-europarl-v7.tgz" "training-parallel-commoncrawl.tgz" "training-parallel-nc-v12.tgz" "dev.tgz" "test-full.tgz")CORPORA=( "training/europarl-v7.de-en" "commoncrawl.de-en" "training/news-commentary-v12.de-en")# This will make the dataset compatible to the one used in "Convolutional Sequence to Sequence Learning"# https://arxiv.org/abs/1705.03122if [ "$1" == "--icml17" ]; then URLS[2]="http://statmt.org/wmt14/training-parallel-nc-v9.tgz" FILES[2]="training-parallel-nc-v9.tgz" CORPORA[2]="training/news-commentary-v9.de-en"fiif [ ! -d "$SCRIPTS" ]; then echo "Please set SCRIPTS variable correctly to point to Moses scripts." exitfisrc=entgt=delang=en-deprep=wmt14_en_detmp=$prep/tmporig=origdev=dev/newstest2013mkdir -p $orig $tmp $prepcd $origfor ((i=0;i&lt;$&#123;#URLS[@]&#125;;++i)); do file=$&#123;FILES[i]&#125; if [ -f $file ]; then echo "$file already exists, skipping download" else url=$&#123;URLS[i]&#125; wget "$url" if [ -f $file ]; then echo "$url successfully downloaded." else echo "$url not successfully downloaded." exit -1 fi if [ $&#123;file: -4&#125; == ".tgz" ]; then tar zxvf $file elif [ $&#123;file: -4&#125; == ".tar" ]; then tar xvf $file fi fidonecd ..echo "pre-processing train data..."for l in $src $tgt; do rm $tmp/train.tags.$lang.tok.$l for f in "$&#123;CORPORA[@]&#125;"; do cat $orig/$f.$l | \ perl $NORM_PUNC $l | \ perl $REM_NON_PRINT_CHAR | \ perl $TOKENIZER -threads 8 -a -l $l &gt;&gt; $tmp/train.tags.$lang.tok.$l donedoneecho "pre-processing test data..."for l in $src $tgt; do if [ "$l" == "$src" ]; then t="src" else t="ref" fi grep '&lt;seg id' $orig/test-full/newstest2014-deen-$t.$l.sgm | \ sed -e 's/&lt;seg id="[0-9]*"&gt;\s*//g' | \ sed -e 's/\s*&lt;\/seg&gt;\s*//g' | \ sed -e "s/\â€™/\'/g" | \ perl $TOKENIZER -threads 8 -a -l $l &gt; $tmp/test.$l echo ""doneecho "splitting train and valid..."for l in $src $tgt; do awk '&#123;if (NR%100 == 0) print $0; &#125;' $tmp/train.tags.$lang.tok.$l &gt; $tmp/valid.$l awk '&#123;if (NR%100 != 0) print $0; &#125;' $tmp/train.tags.$lang.tok.$l &gt; $tmp/train.$ldoneTRAIN=$tmp/train.de-enBPE_CODE=$prep/coderm -f $TRAINfor l in $src $tgt; do cat $tmp/train.$l &gt;&gt; $TRAINdoneecho "learn_bpe.py on $&#123;TRAIN&#125;..."python $BPEROOT/learn_bpe.py -s $BPE_TOKENS &lt; $TRAIN &gt; $BPE_CODEfor L in $src $tgt; do for f in train.$L valid.$L test.$L; do echo "apply_bpe.py to $&#123;f&#125;..." python $BPEROOT/apply_bpe.py -c $BPE_CODE &lt; $tmp/$f &gt; $tmp/bpe.$f donedoneperl $CLEAN -ratio 1.5 $tmp/bpe.train $src $tgt $prep/train 1 250perl $CLEAN -ratio 1.5 $tmp/bpe.valid $src $tgt $prep/valid 1 250for L in $src $tgt; do cp $tmp/bpe.test.$L $prep/test.$Ldone å¦‚æœå¸Œæœ›ä½¿ç”¨é¢„å¤„ç†å¥½çš„æ•°æ®ï¼Œåˆ™å¯ä»¥ä½¿ç”¨WMT16 EN-DEï¼Œåœ°å€ä¸ºï¼šhttps://drive.google.com/uc?export=download&amp;id=0B_bZck-ksdkpM25jRUN2X2UxMm8å¹¶è§£å‹ã€‚ Step2æ¥ä¸‹æ¥å¯¹æ•°æ®è¿›è¡ŒäºŒå€¼åŒ–(binarize): 12345678910TEXT=wmt16_en_de_bpe32kmkdir $TEXTtar -xzvf wmt16_en_de.tar.gz -C $TEXT # è§£å‹æ–‡ä»¶python preprocess.py --source-lang en --target-lang de \ --trainpref $TEXT/train.tok.clean.bpe.32000 \ --validpref $TEXT/newstest2013.tok.bpe.32000 \ --testpref $TEXT/newstest2014.tok.bpe.32000 \ --destdir data-bin/wmt16_en_de_bpe32k \ --nwordssrc 32768 --nwordstgt 32768 \ --joined-dictionary åˆ°è¿™é‡Œï¼Œéº»çƒ¦çš„é¢„å¤„ç†å°±ç»“æŸäº†ã€‚ è®­ç»ƒcdåˆ°fairseqç›®å½•ä¸‹ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š 1CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node 8 train.py data-bin/wmt16_en_de_bpe32k \ --arch transformer_wmt_en_de --share-all-embeddings \ --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \ --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 \ --lr 0.0007 --min-lr 1e-09 \ --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --weight-decay 0.0\ --max-tokens 4096 --save-dir checkpoints/en-de-base\ --no-progress-bar --log-format json --log-interval 50\ --save-interval-updates 1000 --keep-interval-updates 20 æ³¨æ„åˆ°è¯¥è®¾ç½®ä¸åŸè®ºæ–‡ä¸å¤§ä¸€è‡´ã€‚ä½†å·²è¯å®è¯¥è®¾ç½®å¯ä»¥å¤ç°è®ºæ–‡ç»“æœã€‚ å¦‚æœæ²¡æœ‰è¿™ä¹ˆå¤šå¡ï¼Œé‚£ä¹ˆå¯ä»¥è®¾ç½®update freqä»¥æ¨¡æ‹Ÿ8å¡è¡Œä¸ºã€‚å¦‚ï¼š 1234567891011CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node 4 \train.py data-bin/wmt16_en_de_bpe32k \ --arch transformer_wmt_en_de --share-all-embeddings \--optimizer adam --adam-betas '(0.9, 0.98)' \--clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 \--lr 0.0007 --min-lr 1e-09 --criterion label_smoothed_cross_entropy \--label-smoothing 0.1 --weight-decay 0.0 --max-tokens 4096 \--save-dir checkpoints/en-de-16-base \ --no-progress-bar --log-format json --log-interval 50 --save-interval-updates 1000 \--keep-interval-updates 20 --update-freq 2 |tee exp2.log 4å¼ å¡åˆ™è®¾update freq=2ï¼Œ2å¼ å¡åˆ™è®¾update freq=4ï¼Œä»¥æ­¤ç±»æ¨ã€‚ å¤§æ¦‚åœ¨100ä¸ªepochå†…èƒ½å¤Ÿæ”¶æ•›(å®é™…ä¸Šåº”è¯¥åœ¨150-200ä¸ªepochæ”¶æ•›ï¼Œ100epochçš„BLEUæ˜¯27.3ï¼Œ150-200epochçš„ç»“æœæ˜¯27.67)ï¼Œä¹Ÿå³åœ¨475000ä¸ªstepã€‚8å¼ 1080Tiåœ¨å¤§æ¦‚ä¸¤å¤©èƒ½å¤Ÿè®­ç»ƒå®Œæˆï¼Œ4å¼ 1080Tiå¤§æ¦‚4å¤©è®­ç»ƒå®Œæˆã€‚ å¼€å§‹è®­ç»ƒâ€¦ æœ€ååˆ™ä¼šè·å¾—checkpointï¼š æµ‹è¯•æµ‹è¯•åˆ†ä¸ºå‡ ä¸ªé˜¶æ®µï¼šé¦–å…ˆå°†å‡ ä¸ªcheckpointè¿›è¡Œå¹³å‡ï¼Œå®éªŒè¡¨æ˜ï¼Œè¿›è¡Œå¹³å‡èƒ½å¤Ÿæœ‰ä¸€å®šçš„æå‡ï¼›å…¶æ¬¡ï¼Œä½¿ç”¨å¹³å‡åçš„æ¨¡å‹å¯¹testé›†çš„å¥å­è¿›è¡Œç¿»è¯‘ï¼›æœ€ç»ˆå°†ç”Ÿæˆçš„å¥å­å’Œæ­£ç¡®çš„å¥å­è®¡ç®—bleuå€¼ã€‚ average checkpointåœ¨æµ‹è¯•é˜¶æ®µï¼Œè®ºæ–‡åœ¨Transformer-baseä¸­å¯¹æœ€åäº”ä¸ªcheckpointè¿›è¡Œå¹³å‡ï¼Œä¹Ÿå³å¯¹æƒå€¼è¿›è¡Œå¹³å‡ï¼š 123python scripts/average_checkpoints.py \--inputs checkpoints/en-de-base/ \--num-epoch-checkpoints 5 --output averaged_model.pt æœ€ç»ˆè·å¾—averaged_model.ptï¼Œæˆ‘ä»¬å°†ç”¨è¯¥æ–‡ä»¶è¿›è¡Œæµ‹è¯•ã€‚ generateæˆ‘ä»¬é‡‡ç”¨å’Œè®ºæ–‡ä¸€è‡´çš„è¶…å‚ï¼š 1234CUDA_VISIBLE_DEVICES=0 python generate.py \data-bin/wmt16_en_de_bpe32k/ --path /some_checkpoint \--remove-bpe --beam 4 --batch-size 64 --lenpen 0.6 \--max-len-a 1 --max-len-b 50|tee generate.out å…¶ä¸­lenpenæ˜¯ç”Ÿæˆå¥å­çš„é•¿åº¦æƒ©ç½šç³»æ•°ï¼›max-len-aå’Œmax-len-bæŒ‡çš„æ˜¯æ¯ä¸ªå¥å­çš„æœ€é•¿é•¿åº¦é™åˆ¶ï¼Œä¹Ÿå³ï¼šå‡è®¾æºå¥å­é•¿åº¦ä¸ºxï¼Œåˆ™ç›®æ ‡å¥å­çš„é•¿åº¦åº”å°äºax+b ã€‚ æœ€ç»ˆæˆ‘ä»¬ç¿»è¯‘å¥½çš„å¥å­ä»¥åŠç›¸å¯¹åº”çš„è¯¦ç»†ä¿¡æ¯éƒ½åœ¨generate.outé‡Œé¢ã€‚æˆ‘ä»¬éœ€è¦æå–æºè¯­è¨€å¥å­å’Œç›®æ ‡è¯­è¨€å¥å­ï¼Œä»¥æ–¹ä¾¿åé¢çš„è®¡ç®—ã€‚å› æ­¤ï¼š 123grep ^T generate.out | cut -f2- | perl -ple 's&#123;(\S)-(\S)&#125;&#123;$1 ##AT##-##AT## $2&#125;g' &gt; generate.refgrep ^H generate.out |cut -f3- | perl -ple 's&#123;(\S)-(\S)&#125;&#123;$1 ##AT##-##AT## $2&#125;g' &gt; generate.sys åˆ†åˆ«è¿è¡Œè¿™ä¸¤ä¸ªbashå‘½ä»¤ï¼Œæˆ‘ä»¬åˆ™è·å¾—äº†generate.refå’Œgenerate.sysï¼Œåˆ†åˆ«æ˜¯ç›®æ ‡å’Œæºè¯­è¨€çš„å¥å­ã€‚ æ³¨æ„åˆ°è¿™é‡Œæœ‰ä¸€ä¸ªéå¸¸é‡è¦çš„å°trickï¼Œä¹Ÿå³split compoundã€‚å› ä¸ºä¸€äº›å†å²åŸå› ï¼ˆæˆ‘ä¹Ÿä¸çŸ¥é“ä¸ºå•¥ï¼Œtensor2tensoré‡Œé¢çš„è„šæœ¬æœ‰æåˆ°ï¼‰ï¼Œè¯¥trickå·²ç»åœ¨ä¸Šé¢çš„è„šæœ¬å‘½ä»¤ä½“ç°å‡ºæ¥äº†ã€‚å®è·µè¯æ˜ï¼Œä½¿ç”¨è¯¥trickèƒ½å¤Ÿæé«˜bleuå€¼ 0.5ä¸ªç‚¹ä»¥ä¸Šã€‚ scoreæˆ‘ä»¬æ­¤æ—¶å°±å¯ä»¥è®¡ç®—bleuå€¼äº†ï¼Œfairseqæä¾›äº†è¯¥è„šæœ¬ï¼š 1python score.py --sys generate.sys --ref generate.ref å¤§åŠŸå‘Šæˆï¼æˆ‘ä»¬ç»ˆäºå¤ç°å‡ºç»“æœäº†ã€‚ä½œä¸ºå‚è€ƒï¼šæ ¹æ®æˆ‘çš„å®éªŒï¼Œåªä½¿ç”¨checkpointä¸­æœ€å¥½çš„ä¸€ä¸ªcheckpointï¼Œåœ¨ç»è¿‡äº†ä¸Šè¿°çš„æµç¨‹åï¼Œå¯ä»¥å¾—åˆ°27.30çš„ç»“æœã€‚ å…¶ä»–æ ¹æ®æˆ‘çš„éœ€æ±‚ï¼Œæˆ‘è¿˜éœ€è¦è¯¦ç»†è®°å½•ä¸­é—´ç»“æœï¼Œå¹¶æ‰“å°åœ¨tensorboardä¸Šæ–¹ä¾¿å¯è§†åŒ–ï¼Œå¦‚ï¼š fairseqå¹¶æ²¡æœ‰æä¾›è¿™ç§åŠŸèƒ½ï¼Œå› æ­¤éœ€è¦è‡ªå·±ä¿®æ”¹éƒ¨åˆ†æºä»£ç ã€‚åªéœ€è¦ä¿®æ”¹train.pyæºæ–‡ä»¶å³å¯ã€‚ â‘ åœ¨å¼€å¤´åŠ summary writer æ³¨æ„åˆ°æ¯æ¬¡å®éªŒéƒ½éœ€è¦ä¿®æ”¹å®éªŒçš„åå­—ã€‚ â‘¡ä¿®æ”¹trainå‡½æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ·»åŠ è®°å½•çš„ä»£ç ï¼š åœ¨epochç»“æŸï¼Œæ·»åŠ è®°å½•çš„ä»£ç ï¼š å¯¹validateçš„ä½¿ç”¨è¿›è¡Œä¿®æ”¹ï¼ˆæ·»åŠ äº†is_epochï¼‰ï¼š trainå‡½æ•°å…¨éƒ¨ä»£ç ï¼š 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273def train(args, trainer, task, epoch_itr): """Train the model for one epoch.""" # Update parameters every N batches if epoch_itr.epoch &lt;= len(args.update_freq): update_freq = args.update_freq[epoch_itr.epoch - 1] else: update_freq = args.update_freq[-1] # Initialize data iterator itr = epoch_itr.next_epoch_itr(fix_batches_to_gpus=args.fix_batches_to_gpus) itr = iterators.GroupedIterator(itr, update_freq) progress = progress_bar.build_progress_bar( args, itr, epoch_itr.epoch, no_progress_bar='simple', ) extra_meters = collections.defaultdict(lambda: AverageMeter()) first_valid = args.valid_subset.split(',')[0] max_update = args.max_update or math.inf for i, samples in enumerate(progress, start=epoch_itr.iterations_in_epoch): log_output = trainer.train_step(samples) if log_output is None: continue # log mid-epoch stats stats = get_training_stats(trainer) num_updates = stats['num_updates'] # print(type(num_updates)) # print(type(stats['loss'])) summary_writer.add_scalar('Training/training_loss_update', float(stats['loss']), num_updates) summary_writer.add_scalar('Training/training_nll_loss_update', float(stats['nll_loss']), num_updates) summary_writer.add_scalar('Training/training_ppl_update', float(stats['ppl']), num_updates) # ------record training metrics --- # for k, v in log_output.items(): if k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size']: continue # these are already logged above if 'loss' in k: extra_meters[k].update(v, log_output['sample_size']) else: extra_meters[k].update(v) stats[k] = extra_meters[k].avg progress.log(stats) # ignore the first mini-batch in words-per-second calculation if i == 0: trainer.get_meter('wps').reset() num_updates = trainer.get_num_updates() if args.save_interval_updates &gt; 0 and num_updates % args.save_interval_updates == 0 and num_updates &gt; 0: valid_losses = validate(args, trainer, task, epoch_itr, [first_valid], is_epoch=False) save_checkpoint(args, trainer, epoch_itr, valid_losses[0]) if num_updates &gt;= max_update: break # log end-of-epoch stats stats = get_training_stats(trainer) # ------record training metrics --- # summary_writer.add_scalar('Training/training_loss_epoch', float(stats['loss']), epoch_itr.epoch) summary_writer.add_scalar('Training/training_nll_loss_epoch', float(stats['nll_loss']), epoch_itr.epoch) summary_writer.add_scalar('Training/training_ppl_epoch', float(stats['ppl']), epoch_itr.epoch) for k, meter in extra_meters.items(): stats[k] = meter.avg progress.print(stats) # reset training meters for k in [ 'train_loss', 'train_nll_loss', 'wps', 'ups', 'wpb', 'bsz', 'gnorm', 'clip', ]: meter = trainer.get_meter(k) if meter is not None: meter.reset() â‘¢ä¿®æ”¹validateå‡½æ•°æ·»åŠ äº†ä¸€ä¸ªå‚æ•°is_epochï¼š æ·»åŠ è®°å½•çš„ä»£ç ï¼š validateå…¨éƒ¨ä»£ç ï¼š 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263def validate(args, trainer, task, epoch_itr, subsets, is_epoch=True): """Evaluate the model on the validation set(s) and return the losses.""" valid_losses = [] for subset in subsets: # Initialize data iterator itr = task.get_batch_iterator( dataset=task.dataset(subset), max_tokens=args.max_tokens, max_sentences=args.max_sentences_valid, max_positions=utils.resolve_max_positions( task.max_positions(), trainer.get_model().max_positions(), ), ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test, required_batch_size_multiple=8, seed=args.seed, num_shards=args.distributed_world_size, shard_id=args.distributed_rank, num_workers=args.num_workers, ).next_epoch_itr(shuffle=False) progress = progress_bar.build_progress_bar( args, itr, epoch_itr.epoch, prefix='valid on \'&#123;&#125;\' subset'.format(subset), no_progress_bar='simple' ) # reset validation loss meters for k in ['valid_loss', 'valid_nll_loss']: meter = trainer.get_meter(k) if meter is not None: meter.reset() extra_meters = collections.defaultdict(lambda: AverageMeter()) for sample in progress: log_output = trainer.valid_step(sample) for k, v in log_output.items(): if k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size']: continue extra_meters[k].update(v) # log validation stats stats = get_valid_stats(trainer) # ------record validate metrics --- # if is_epoch: # every epoch summary_writer.add_scalar('Validation/valid_loss_epoch', float(stats['valid_loss']), epoch_itr.epoch) summary_writer.add_scalar('Validation/valid_nll_loss_epoch', float(stats['valid_nll_loss']), epoch_itr.epoch) summary_writer.add_scalar('Validation/valid_ppl_epoch', float(stats['valid_ppl']), epoch_itr.epoch) else: # every n update num_updates = stats['num_updates'] summary_writer.add_scalar('Validation/valid_loss_update', float(stats['valid_loss']), num_updates / args.save_interval_updates) summary_writer.add_scalar('Validation/valid_nll_loss_update', float(stats['valid_nll_loss']), num_updates / args.save_interval_updates) summary_writer.add_scalar('Validation/valid_ppl_update', float(stats['valid_ppl']), num_updates / args.save_interval_updates) for k, meter in extra_meters.items(): stats[k] = meter.avg progress.print(stats) valid_losses.append(stats['valid_loss']) return valid_losses ReferenceReplicating results from â€œScaling Neural Machine Translationâ€ How to reproduce the result of WMT14 en-de on transformer BASE model?]]></content>
      <tags>
        <tag>æ•™ç¨‹</tag>
        <tag>Transformer</tag>
        <tag>NMT</tag>
        <tag>fairseq</tag>
        <tag>æœºå™¨ç¿»è¯‘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•15]]></title>
    <url>%2F2019%2F01%2F06%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9515%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[flatten multi-dimentional list]å¯¹å¤šå±‚åµŒå¥—çš„listè¿›è¡Œå±•å¹³ã€‚ 12345678910# é€’å½’def flatten(nestedList): def aux(listOrItem): if isinstance(listOrItem, list): for elem in listOrItem: for item in aux(elem): yield item else: yield listOrItem return list(aux(nestedList)) 2ï¸âƒ£[sorted index]ä½¿ç”¨å†…ç½®æ–¹æ³•è·å¾—æ’å¥½åºçš„index 123sorted_index=[i[0] for i in sorted(enumerate(sent_length), key=lambda x:x[1], reverse=self.reverse)]]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡11]]></title>
    <url>%2F2019%2F01%2F06%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8711%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Multi-Head Attention with Disagreement Regularization]EMNLPçš„çŸ­æ–‡ã€‚ é¼“åŠ±transformerä¸­headä¸headä¹‹é—´çš„å·®å¼‚ã€‚ åŠ äº†ä¸‰ç§æ­£åˆ™åŒ–æ–¹æ³•ï¼šâ‘ on subspace â‘¡on attention position â‘¢on output æ²¡ä»€ä¹ˆäº®ç‚¹ã€‚ 2ï¸âƒ£[Dropout: A Simple Way to Prevent Neural Networks from Overï¬tting]ç»å…¸è®ºæ–‡ã€‚dropoutæ–¹æ³•å¾ˆç®€å•ï¼Œä½†å¦‚ä½•æƒ³åˆ°ï¼Œå…¶èƒŒåçš„intuitionï¼Œä»¥åŠä¸€äº›ç°è±¡å¾ˆæœ‰å¯å‘æ„ä¹‰ã€‚ä»…ç½—åˆ—ä¸€äº›intuition/motivationä»¥åŠç°è±¡ï¼š ç½‘ç»œå¤æ‚å…³ç³»å­¦åˆ°å¾ˆå¤šå™ªå£°ï¼Œå¯¼è‡´overfitting æœ€å¥½çš„regularizationæ–¹æ³•æ˜¯å¯¹æ‰€æœ‰çš„parameter settingçš„ç»“æœè¿›è¡Œaverageã€‚è¿™å°±æ˜¯è´å¶æ–¯æ–¹æ³•ï¼Œ dropoutæ˜¯å¯¹è¯¥æ–¹æ³•è¿›è¡Œè¿‘ä¼¼ï¼Œè®ºæ–‡ä¹Ÿæåˆ°äº†model combination dropoutèƒ½å¤Ÿå‡å°‘unitä¹‹é—´å¤æ‚çš„co-adaptationï¼Œèƒ½å¤Ÿæ›´é²æ£’ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œä¸éœ€è¦ä¾èµ–å…¶ä»–unitå»çº æ­£è‡ªå·±çš„é”™è¯¯ã€‚each hidden unit in a neural network trained with dropout must learn to work with a randomly chosen sample of other units. This should make each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes dropoutçš„ç‰¹æ€§ï¼šsparsityã€‚æ ‡å‡†çš„ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå›ºåŒ–å…¶ä»–unitçš„é”™è¯¯ï¼Œå¯¼è‡´å¤æ‚çš„co-adaptationï¼Œä½†è¿™ç§å¤æ‚çš„adaptationä¼šå¯¼è‡´æ³›åŒ–æ€§çš„é™ä½ï¼Œå› ä¸ºå¯¹äºæœªè§åˆ°çš„æ•°æ®è¿™ç§å¤æ‚çš„adaptationæ˜¯æ²¡ç”¨çš„ã€‚å› æ­¤dropoutçš„ç½‘ç»œä¸­æ¯ä¸ªunitéƒ½è¦å­¦ä¼šè‡ªå·±çº æ­£è‡ªå·±çš„é”™è¯¯ï¼Œå› æ­¤æ¯ä¸ªunitèƒ½å¤Ÿç‹¬ç«‹å­¦åˆ°æ•°æ®çš„ä¸€éƒ¨åˆ†ç‰¹æ€§ã€‚dropoutä¼šå¯¼è‡´ç¨€ç–åŒ–ï¼Œæ¯æ¬¡éƒ½åªä¼šæœ‰ä¸€å°éƒ¨åˆ†çš„activationé«˜ã€‚ä½¿ç”¨dropouté…åˆé«˜çš„å­¦ä¹ ç‡æ¯”è¾ƒå¥½ï¼Œå› ä¸ºdropoutå¯èƒ½ä¼šå¯¼è‡´gradientä¹‹é—´äº’ç›¸cancelï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ä½¿ç”¨é«˜çš„momentumã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>attention</tag>
        <tag>dropout</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>regularization</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³äºPytorchä¸­index_copy_åŠå…¶æ€è€ƒ]]></title>
    <url>%2F2018%2F12%2F31%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADindex_copy_%E5%8F%8A%E5%85%B6%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[å‰å‡ æ—¥å› ä¸ºin-placeæ“ä½œçš„é—®é¢˜ï¼Œdebugäº†å¥½å‡ å¤©ï¼Œæœ€ç»ˆæ‰å‘ç°é—®é¢˜ã€‚ 12output,_=pad_packed_sequence(output,batch_first=True)output=output.index_copy(0,torch.tensor(sorted_index),output) å› ä¸ºPytorchä¸­pack_sequenceéœ€è¦å°†batchæŒ‰é•¿åº¦æ’åˆ—ï¼Œæˆ‘åœ¨è¿‡å®ŒGRUåéœ€è¦å°†å…¶é¡ºåºè¿˜åŸï¼Œåœ¨è¿™è¾¹sorted_indexå³æ˜¯è®°å½•åŸæ¥indexæ˜ å°„ã€‚ ç„¶è€Œæˆ‘åœ¨å†™çš„æ—¶å€™ï¼Œå‚è€ƒçš„æ˜¯å®˜æ–¹çš„exampleï¼š 123456789&gt;&gt;&gt; x = torch.zeros(5, 3)&gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)&gt;&gt;&gt; index = torch.tensor([0, 4, 2])&gt;&gt;&gt; x.index_copy_(0, index, t)tensor([[ 1., 2., 3.], [ 0., 0., 0.], [ 7., 8., 9.], [ 0., 0., 0.], [ 4., 5., 6.]]) å› æ­¤æˆ‘ä¹Ÿä¸å‡æ€ç´¢åœ°å†™ï¼š12output,_=pad_packed_sequence(output,batch_first=True)output=output.index_copy_(0,torch.tensor(sorted_index),output) å°±å› ä¸ºå¤šäº†ä¸€ä¸ª_ï¼Œå¯¼è‡´é€»è¾‘å’Œæˆ‘æƒ³è±¡ä¸­çš„ä¸ä¸€æ ·ã€‚ ä¸€ä¸ªç®€å•çš„ä¾‹å­å±•ç¤ºä¸ºä»€ä¹ˆè¿™ä¹ˆæ˜¯é”™çš„ï¼š 1234567891011import torchx=torch.Tensor([21,42,45,59])print(x) # tensor([21., 42., 45., 59.])index=torch.tensor([1,2,0,3])x=x.index_copy_(0,index,x)print(x) # tensor([21., 21., 21., 59.]) ç”±äºæ˜¯in-placeæ“ä½œï¼Œç¬¬ä¸€æ­¥ï¼Œå°†index=0çš„æ•°å€¼ï¼ˆä¹Ÿå³21ï¼‰å¤åˆ¶åˆ°index=1çš„åœ°æ–¹ï¼Œæ­¤æ—¶å˜æˆ[21,21,45,59]ï¼›æ¥ç€å°†index=1çš„æ•°å€¼å¤åˆ¶åˆ°index=2çš„ä½ç½®ä¸Šï¼Œæ³¨æ„åˆ°ä¹‹å‰å·²ç»æ˜¯in-placeæ“ä½œï¼Œå› æ­¤æ­¤æ—¶å–çš„ä¸æ˜¯æƒ³è±¡ä¸­çš„42ï¼Œè€Œæ˜¯å·²ç»è¢«æ›¿æ¢çš„21ã€‚åé¢çš„ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ æ­£ç¡®çš„åšæ³•åªéœ€è¦å»æ‰in-placeå³å¯ã€‚ å·²ç»å¥½å‡ æ¬¡é‡åˆ°in-placeçš„é—®é¢˜äº†ï¼Œåœ¨æ¯æ¬¡åšin-placeæ“ä½œæ—¶ï¼Œéƒ½è¦è­¦æƒ•ã€‚åº”å°½å¯èƒ½é¿å…in-placeæ“ä½œã€‚å®é™…ä¸ŠPytorchå®˜æ–¹ä¹Ÿä¸å»ºè®®ä½¿ç”¨in-placeæ“ä½œã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>é‡åˆ°çš„é—®é¢˜</tag>
        <tag>Pytorch</tag>
        <tag>index_coopy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•14]]></title>
    <url>%2F2018%2F12%2F29%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9514%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[shuffle list]shuffle listå¯ä»¥ä½¿ç”¨randomçš„shuffleå‡½æ•°ï¼Œäº¦å³ï¼š 12l=[1,2,3,4]shuffle(l) # in place operation è€Œæƒ³è¦shuffleä¸¤ä¸ªå¯¹åº”listï¼Œä¹Ÿå³ç­‰é•¿ä¸”ä¸€ä¸€å¯¹åº”çš„listï¼Œåˆ™å¯ä»¥ï¼š 123456789101112131415# borrow from stackoverflowimport randomdef shuffle(a,b): assert len(a) == len(b) start_state = random.getstate() random.shuffle(a) random.setstate(start_state) random.shuffle(b)a = [1,2,3,4,5,6,7,8,9]b = [11,12,13,14,15,16,17,18,19]shuffle(a,b)print(a) # [9, 7, 3, 1, 2, 5, 4, 8, 6]print(b) # [19, 17, 13, 11, 12, 15, 14, 18, 16] 2ï¸âƒ£[inverse tensor]Pytorchç›®å‰è¿˜ä¸æ”¯æŒæ­¥è¿›ä¸ºè´Ÿçš„æƒ…å†µï¼Œå› æ­¤ä¸èƒ½ä½¿ç”¨ç±»ä¼¼Pythonçš„l[::-1]çš„æ–¹æ³•reverse tensorã€‚ä¸€ç§è§£å†³æ–¹æ¡ˆï¼š 1234567import torchinv_idx = torch.arange(tensor.size(0)-1, -1, -1).long()# or equivalently torch.range(tensor.size(0)-1, 0, -1).long()inv_tensor = tensor.index_select(0, inv_idx)# or equivalentlyinv_tensor = tensor[inv_idx] 3ï¸âƒ£[GRU initialization]12345def _gru_init(self): # use orthogonal seems better nn.init.orthogonal_(self.word_RNN.weight_ih_l0.data) #æ²¡æœ‰dataä¸è¡Œï¼Œä¼šæŠ¥leaf variable in-placeé”™è¯¯ï¼Œå¯èƒ½weight_ih_l0ä¸æ˜¯parameter nn.init.orthogonal_(self.word_RNN.weight_hh_l0.data) self.word_RNN.bias_ih_l0.data.zero_() self.word_RNN.bias_hh_l0.data.zero_() 4ï¸âƒ£[sort counter]éœ€æ±‚ï¼šç»Ÿè®¡documentçš„å¥å­ä¸ªæ•°çš„åˆ†å¸ƒï¼Œå¹¶æŒ‰ç…§é•¿åº¦é¡ºåºæ’åˆ—ã€‚ 123n_sents=[len(sentences) for sentences in documents]n_lengths=Counter(n_sents)n_lengths=sorted(n_lengths.items())]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†17]]></title>
    <url>%2F2018%2F12%2F29%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8617%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Pytorch]åœ¨æœ‰RNNçš„ä»£ç ä¸­ï¼Œå¦‚æœå‡ºç° Cuda Error : RuntimeError: CUDNN_STATUS_EXECUTION_FAILED é‚£ä¹ˆå¯èƒ½çš„å‡ºé”™åŸå› æ˜¯æ²¡æœ‰å°†init stateæ”¾å…¥cudaä¸­ã€‚ Reference: https://discuss.pytorch.org/t/cuda-error-runtimeerror-cudnn-status-execution-failed/17625 2ï¸âƒ£[Pytorch]clone() â†’ TensorReturns a copy of the self tensor. The copy has the same size and data type as self.Unlike copy_(), this function is recorded in the computation graph. Gradients propagating to the cloned tensor will propagate to the original tensor. å¦‚æœéœ€è¦å¦ä¸€ä¸ªç›¸åŒçš„tensoråšå…¶ä»–è®¡ç®—ï¼Œåˆ™ä½¿ç”¨clone()è€Œä¸æ˜¯copy_() 123forward_vec=sent_vec# backward_vec=sent_vec wrongbackward_vec=sent_vec.clone() å½“ç„¶ä¹Ÿä¸èƒ½ç›´æ¥èµ‹å€¼ï¼Œå› ä¸ºèµ‹çš„åªæ˜¯æŒ‡é’ˆï¼Œæ”¹å˜backward_vecä¹Ÿä¼šæ”¹å˜åŸæ¥çš„å€¼ã€‚ 3ï¸âƒ£[Python]Pythonä¸­==å’Œisçš„åŒºåˆ«ï¼šisè¡¨ç¤ºæ˜¯å¦æ˜¯åŒä¸€ä¸ªobjectï¼›è€Œ==è¡¨ç¤ºæ˜¯å¦æ˜¯åŒä¸€ä¸ªå€¼ã€‚ 123456str='GRU'str == 'GRU' # Truestr is 'GRU' # Truestr=str.upper()str == 'GRU' # Falsestr is 'GRU' # True 4ï¸âƒ£[RNN]åœ¨RNNçš„åˆå§‹åŒ–ä¸­ï¼Œä½¿ç”¨æ­£äº¤åˆå§‹åŒ–ä¼šæ¯”å…¶ä»–æ–¹æ³•å¥½ä¸€äº›ï¼ˆå¾…å¯¹æ¯”å®éªŒæµ‹éªŒï¼‰ã€‚Reference: https://smerity.com/articles/2016/orthogonal_init.html 5ï¸âƒ£[Pytorch]åœ¨æä¾›é¢„è®­ç»ƒembeddingä½œä¸ºåˆå§‹åŒ–æ—¶ï¼Œæ­£ç¡®åšæ³•ï¼š 1234if pretrained_matrix is not None: pretrained_matrix=torch.from_numpy(pretrained_matrix).type(torch.FloatTensor) self.embedding.weight= nn.Parameter(pretrained_matrix, requires_grad=True) å¿…é¡»è¦æœ‰.type(torch.FloatTensor)ï¼Œå¦åˆ™ä¼šå‡ºé”™ï¼šCuDNN error: CUDNN_STATUS_EXECUTION_FAILED 6ï¸âƒ£[Pytorch]Pytorchä¸­ï¼Œå°†åˆå§‹hidden stateä½œä¸ºå¯å­¦ä¹ å‚æ•°å®è·µï¼šhttps://discuss.pytorch.org/t/solved-train-initial-hidden-state-of-rnns/2589/9https://discuss.pytorch.org/t/learn-initial-hidden-state-h0-for-rnn/10013/7]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>Python</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Day with Google]]></title>
    <url>%2F2018%2F12%2F23%2F%E8%A7%81%E9%97%BB%26%E6%83%B3%E6%B3%95%2FA%20Day%20with%20Google%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨ä¸€ä¹¡ä¸‹äººç»ˆäºå†æ¬¡è¿›åŸäº†ğŸ™ˆ æœ¬æ¬¡çš„ç›®çš„æ˜¯æ¥å‚è§‚Googleã€‚ é«˜æ¥¼æ—ç«‹ï¼š Here We are: å’•æœæ˜¯ä»€ä¹ˆé¬¼ï¼Ÿ å®£è®²ï¼š ä¸å¾—ä¸æ„Ÿæ…¨é£Ÿå ‚çœŸå¥½ğŸ¦†ï¼Œè¿˜æœ‰ä¸“é—¨åƒé¢çš„é£Ÿå ‚ã€‚è€Œä¸”è¿˜éƒ½ä¸ç”¨é’±ğŸ™‰ï¼Œå¯¹æ¯”å¼ æ±Ÿçš„é£Ÿå ‚ğŸ™‰ï¼š æºœäº†æºœäº†ï¼š]]></content>
      <tags>
        <tag>Google</tag>
        <tag>æ´»åŠ¨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡10]]></title>
    <url>%2F2018%2F12%2F23%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8710%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Regularization of Neural Networks using DropConnect]åœ¨dropoutçš„åŸºç¡€ä¸Šæå‡ºdropconnectã€‚ä¸dropoutä¸åŒçš„æ˜¯ï¼Œdropconnectå¯¹weightè¿›è¡Œdropè€Œä¸æ˜¯å¯¹layerè¿›è¡Œdropã€‚ åˆ›æ–°ä¹‹å¤„åœ¨äºinferenceçš„æ—¶å€™å’Œdropoutä¸åŒã€‚ è®­ç»ƒ inference åœ¨inferenceçš„æ—¶å€™é€šè¿‡é«˜æ–¯é‡‡æ ·çš„æ–¹æ³•å»æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„ä¼¯åŠªåˆ©åˆ†å¸ƒã€‚intuitionï¼šæœ¬æ–‡å¯¹dropoutåœ¨inferenceç®€å•å¯¹unitè¿›è¡Œç¼©æ”¾è¿›è¡Œåæ€ï¼Œè®¤ä¸ºè¿™åœ¨æ•°å­¦ä¸Šå¹¶ä¸åˆç†ï¼Œå› æ­¤æå‡ºç”¨é«˜æ–¯åˆ†å¸ƒå»é‡‡æ ·ã€‚ 2ï¸âƒ£[Attentive Pooling Networks]æå‡ºattentive poolingæœºåˆ¶ï¼Œç”¨ä»¥answer selectionã€‚ï¼ˆä»€ä¹ˆæ˜¯answer selectionï¼šç»™å®šä¸€ä¸ªé—®é¢˜ï¼Œç»™å®šå¤šä¸ªç­”æ¡ˆå€™é€‰ï¼Œè¦ä»ç­”æ¡ˆé€‰é¡¹ä¸­é€‰æ‹©æ­£ç¡®çš„ç­”æ¡ˆã€‚ï¼‰ ä¼ ç»Ÿanswer selectionï¼šé¦–å…ˆå°†è¯è½¬åŒ–æˆè¯å‘é‡ï¼Œæ¥ç€é€šè¿‡bi-LSTMæˆ–CNNè·å¾—ä¸€ä¸ªçŸ©é˜µè¡¨ç¤ºï¼Œæ¥ä¸‹æ¥å¯¹Qå’ŒAåˆ†åˆ«è¿›è¡Œmax-poolingè·å¾—å›ºå®šè¡¨ç¤ºï¼Œæœ€åé€šè¿‡cosè·ç¦»åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦æ˜¯æ­£ç¡®ç­”æ¡ˆï¼Œä»ç­”æ¡ˆå€™é€‰ä¸­é€‰æ‹©åˆ†æ•°æœ€é«˜çš„ã€‚ ä½†è¿™æ ·çš„é—®é¢˜åœ¨äºQå’ŒAä¹‹é—´æ²¡æœ‰äº¤äº’ã€‚ æœ¬æ–‡åˆ©ç”¨attentionä½œä¸ºQå’ŒAçš„äº¤äº’ã€‚ è·å¾—Qå’ŒAçŸ©é˜µçš„æ–¹å¼æ˜¯ä¸€è‡´çš„ã€‚æ¥ä¸‹æ¥ï¼Œé¦–å…ˆè®¡ç®—ä¸€ä¸ªGçŸ©é˜µï¼Œé€šè¿‡åŒçº¿æ€§attentionå…¬å¼è·å¾—ï¼š Gæ‰€ä»£è¡¨çš„æ„ä¹‰æ˜¯Qå’ŒAçš„æ¯ä¸ªè¯ä¹‹é—´çš„å¯¹é½ï¼šå¯¹äºç¬¬iè¡Œæ¥è¯´ï¼Œä»£è¡¨Qçš„ç¬¬iä¸ªè¯å’ŒAä¸­æ‰€æœ‰è¯çš„ä¸€ä¸ªåˆ†æ•°ï¼›å¯¹äºç¬¬jåˆ—æ¥è¯´ï¼Œä»£è¡¨ç¬¬jä¸ªè¯å’ŒQä¸­æ‰€æœ‰è¯çš„åˆ†æ•°ã€‚ æ¥ä¸‹æ¥å¯¹Gçš„è¡Œå’Œåˆ—åˆ†åˆ«è¿›è¡Œmax-poolingæ“ä½œï¼š æ­¤æ­¥ä»£è¡¨é€‰æ‹©ä¸æŸè¯å…³ç³»æœ€é‡è¦çš„è¯ã€‚ æ¥ä¸‹æ¥å¯¹gåˆ†åˆ«è¿›è¡Œsoftmaxï¼Œå†åˆ†åˆ«è¿›è¡Œç‚¹ç§¯ä»¥è·å¾—æœ€ç»ˆå‘é‡è¡¨ç¤ºï¼š åŒæ ·ï¼Œæœ€ç»ˆä½¿ç”¨cosè·ç¦»è®¡ç®—ç›¸ä¼¼åº¦ã€‚ 3ï¸âƒ£[Improved Regularization of Convolutional Neural Networks with Cutout]æ˜¯ä»æ•°æ®å¢å¼ºå’Œdropoutçš„è§’åº¦ï¼š dropout in convolutional layers simply acts to increase robustness to noisy inputs, rather than having the same model averaging effect that is observed in fully-connected layers æŸä¸ªè¾“å…¥è¢«ç§»å»ï¼Œæ‰€æœ‰åé¢ç›¸å…³çš„çš„feature mapéƒ½è¢«ç§»å»ï¼š In this sense, cutout is much closer to data augmentation than dropout, as it is not creating noise, but instead generating images that appear novel to the network å…¶å®åªæ˜¯å°†è¾“å…¥éšæœºdropæ‰ä¸€å—ã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>DropConnect</tag>
        <tag>Cutout</tag>
        <tag>Attentive Pooling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•13]]></title>
    <url>%2F2018%2F12%2F23%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9513%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[flatten list]å¯¹äºŒç»´listè¿›è¡Œå±•å¼€ã€‚ 12345678list2d = [[1,2,3],[4,5,6], [7], [8,9]]# â‘ flatten = [l for list in list2d for l in list]# â‘¡import itertoolsmerged = list(itertools.chain(*list2d))# ormerged = list(itertools.chain.from_iterable(list2d))]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†16]]></title>
    <url>%2F2018%2F12%2F23%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8616%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Softmax]åœ¨ä½¿ç”¨softmaxçš„æ—¶å€™ï¼Œè¦éå¸¸æ³¨æ„softmaxçš„è¡Œä¸ºã€‚åº”å°½é‡æ§åˆ¶softmaxå‰å…ƒç´ çš„è§„æ¨¡ï¼Œå¦åˆ™å®¹æ˜“å‡ºç°one-hotçš„æƒ…å†µï¼Œå¯¼è‡´è®­ç»ƒå›°éš¾ã€‚ åŒæ—¶ï¼Œå¯¹å…¨-infåšsoftmaxæ˜¯æœªå®šä¹‰çš„ï¼Œå› æ­¤ä¹Ÿä¼šå‡ºç°é—®é¢˜ï¼š 2ï¸âƒ£[slice]åœ¨å¯¹tensoræˆ–arrayæ“ä½œæ—¶ï¼Œå¦‚æœéœ€è¦å–æŸç»´çš„sliceï¼š 1234import torcha = torch.rand(2,5)a[:,1:3] # å–ç¬¬1åˆ—åˆ°ç¬¬2åˆ—çš„slicea[:][1:3] # wrongï¼Œè·å¾—çš„æ˜¯ç¬¬1è¡Œåˆ°ç¬¬2è¡Œçš„slice åŸå› æ˜¯ï¼Œa[:][1:3]æ˜¯å…ˆåša[:]æ“ä½œï¼Œè·å¾—äº†å…¨éƒ¨å…ƒç´ ï¼Œç„¶åå†åš[1:3]æ“ä½œï¼Œä¹Ÿå³è·å¾—ç¬¬1è¡Œåˆ°ç¬¬2è¡Œçš„å…ƒç´ ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Python</tag>
        <tag>Softmax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•12]]></title>
    <url>%2F2018%2F12%2F16%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9512%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[CUDA time]æ­£ç¡®æµ‹è¯•ä»£ç åœ¨cudaè¿è¡Œæ—¶é—´ã€‚éœ€è¦åŠ ä¸Štorch.cuda.synchronize()ã€‚ 12345678910111213141516171819202122232425262728293031323334import torchimport timea = torch.randint(high=1000, size=(20, 200, 256)).double().cuda()b = torch.randint(high=1000, size=(20, 200, 256)).double().cuda()torch.cuda.synchronize()start = time.time()M = torch.bmm(a, b.transpose(1, 2))torch.cuda.synchronize()end = time.time()print("bmm", end - start)print("max_mem", torch.cuda.max_memory_allocated())torch.cuda.synchronize()start = time.time()local_a = a.unsqueeze(2)local_b = b.unsqueeze(1)N = (local_a*local_b).sum(-1)torch.cuda.synchronize()end = time.time()print("element-wise", end - start)print("max_mem", torch.cuda.max_memory_allocated())print("output difference (should be 0)", (N - M).abs().max())print("In single precision this can fail because of the size of the tensors.")print("Using double should always work")]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†15]]></title>
    <url>%2F2018%2F12%2F16%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8615%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Pytorch]åœ¨0.41çš„pytorchä¸­ï¼Œbernoulliçš„é€Ÿåº¦ä¼šæ¯”éšæœºsampleçš„é€Ÿåº¦æ…¢å¾ˆå¤šï¼›åœ¨1.0ä¸­ä¿®å¤äº†è¯¥bugï¼Œä½†é€Ÿåº¦ä¸Šè¿˜æ˜¯éšæœºsampleå¿«ä¸€ç‚¹ç‚¹ã€‚ 1234567# Pytorch0.41Bernoulli 0.430371046066sample 0.24411702156# Pytorch1.0Bernoulli 0.256921529sample 0.25317035184 123# ä»¥ä¸‹äºŒè€…ç­‰ä»·mask = Bernoulli(gamma).sample(x.size()) # slowmask = (torch.rand_like(x)&lt;gamma).float() # faster Reference:https://github.com/pytorch/pytorch/issues/6940]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡9]]></title>
    <url>%2F2018%2F12%2F16%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%879%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Sentence-State LSTM for Text Representation]æå‡ºä¸€ç§æ–°å‹çš„encodeå¥å­çš„æ–¹æ³•ã€‚æœ‰ç‚¹ç±»ä¼¼gather-distributeçš„æƒ³æ³•ã€‚ æ¯ä¸ªæ—¶é—´æ­¥tæ‰€æœ‰çš„hä¸€èµ·æ›´æ–°ã€‚æ›´æ–°æ–¹å¼æ˜¯ä¸å…¶å·¦å³çš„ç‚¹è¿›è¡Œäº¤äº’ï¼ŒåŒæ—¶ä¸ä¸€ä¸ªglobal representationè¿›è¡Œäº¤äº’ã€‚è¿™æ ·å³è€ƒè™‘äº†localçš„ä¿¡æ¯ä¹Ÿè€ƒè™‘äº†globalçš„ä¿¡æ¯ã€‚æ¯æ¬¡æ›´æ–°éƒ½å¢åŠ äº†ä¿¡æ¯äº¤äº’ï¼Œä»3gramåˆ°5gramå†åˆ°7gramâ€¦ å…·ä½“æ¥è¯´ï¼šâ‘ å¦‚ä½•æ±‚$h_i$ ä»å…¬å¼å¯ä»¥çœ‹å‡ºï¼Œå¯¹äºä¸€ä¸ªç‰¹å®šçš„$h_i$ï¼ŒåŒæ—¶è€ƒè™‘å·¦å³ä¸¤ç‚¹ï¼Œä»¥åŠglobalä¿¡æ¯$g$ï¼Œä»¥åŠè¾“å…¥$x$ã€‚ â‘¡å¦‚ä½•æ±‚g é€šè¿‡averageåŒæ—¶è€ƒè™‘æ‰€æœ‰çš„è¯ï¼ŒåŒæ—¶è€ƒè™‘è‡ªå·±ä¸Šä¸€ä¸ªçŠ¶æ€ã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>LSTM</tag>
        <tag>Encode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pythonä¸­çš„+=æ“ä½œ]]></title>
    <url>%2F2018%2F12%2F09%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPython%E4%B8%AD%E7%9A%84%2B%3D%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[å‰å‡ æ—¥åœ¨å†™ä¸€æ®µPytorchä»£ç æ—¶ï¼Œåˆä¸€æ¬¡é‡åˆ°äº†in-placeæ“ä½œçš„é—®é¢˜ã€‚ 1output+=pos # posæ˜¯ä¸å¯æ›´æ–°çš„tensorï¼Œoutputæ˜¯å¯æ›´æ–°çš„tensor ç¨‹åºæŠ¥é”™ï¼šâ€œone of the variables needed for gradient computation has been modified by an inplace operationâ€ã€‚ æ— æ„ä¸­å°†ä»£ç æ”¹æˆoutput=output+posï¼Œç¨‹åºå°±ä¸ä¼šæŠ¥é”™äº†ã€‚ åœ¨æŸ¥é˜…äº†ç›¸å…³èµ„æ–™åï¼Œå°†æˆ‘çš„æ€è€ƒæ•´ç†ä¸‹æ¥ã€‚ åœ¨Pythonä¸­ï¼Œi=i+1å’Œi+=1æ˜¯ä¸åŒçš„ï¼Œå¦‚æœè¢«æ“ä½œæ•°æ²¡æœ‰éƒ¨ç½² â€™iaddâ€˜æ–¹æ³•ï¼Œåˆ™i=i+1å’Œi+=1æ˜¯ç­‰ä»·çš„ï¼Œâ€™+=â€˜å¹¶ä¸ä¼šäº§ç”Ÿin-placeæ“ä½œï¼›å½“è¢«æ“ä½œæ•°æœ‰éƒ¨ç½²è¯¥æ–¹æ³•ä¸”æ­£ç¡®éƒ¨ç½²ï¼Œåˆ™æ˜¯ä¼šäº§ç”Ÿin-placeæ“ä½œçš„ã€‚å½“æ²¡æœ‰in-placeæ“ä½œæ—¶ï¼Œi=i+1è¡¨ç¤ºå¯¹ié‡åˆ†é…ï¼Œä¹Ÿå³iæŒ‡å‘äº†å¦ä¸€ä¸ªç©ºé—´è€Œä¸æ˜¯åŸæ¥çš„ç©ºé—´ã€‚ æ‰€ä»¥ï¼Œè¿™æ ·çš„ä¾‹å­å°±èƒ½è§£é‡Šæ¸…æ¥šäº†ï¼š 12345678910import numpy as npA = np.arange(12).reshape(4,3)for a in A: a = a + 1# Aå¹¶æ²¡æœ‰è¢«æ”¹å˜B = np.arange(12).reshape(4,3)for b in B: b += 1# Bè¢«æ”¹å˜äº† åœ¨Pytorchä¸­ï¼Œä¹Ÿæœ‰éƒ¨ç½²â€™iadd()â€˜æ“ä½œï¼Œæ‰€ä»¥å¯¹äºoutput+=posï¼Œoutputå†…éƒ¨çš„å€¼è¢«æ”¹å˜äº†ï¼Œä¹Ÿå³åœ¨è®¡ç®—å›¾ä¸­å¼•å…¥äº†ç¯ï¼Œåœ¨åå‘æ±‚å¯¼æ—¶åˆ™ä¼šå‡ºé”™ã€‚ å› æ­¤ï¼Œåœ¨Pytorchä¸­ï¼Œåº”å½“é¿å…in-placeçš„æ“ä½œã€‚ Reference:https://stackoverflow.com/questions/41446833/what-is-the-difference-between-i-i-1-and-i-1-in-a-for-loop]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>é‡åˆ°çš„é—®é¢˜</tag>
        <tag>Pytorch</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡8]]></title>
    <url>%2F2018%2F12%2F09%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%878%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding]æå‡ºäº†ä¸¤ç§attentionæœºåˆ¶ï¼Œå³ multi-dimentional attentionå’Œdirectional self-attentionï¼Œåœ¨æ­¤åŸºç¡€ä¸Šæå‡ºæœ‰å‘è‡ªæ³¨æ„åŠ›ç½‘ç»œï¼ˆdirectional self-attention network) Multi-dimensional Attentionä¸ä¼ ç»Ÿçš„æ–¹æ³•ä¸åŒçš„æ˜¯ï¼Œå¯¹äºæ¯ä¸ªè¯å¯¹ï¼Œattentionå‡ºæ¥çš„ä¸æ˜¯æ ‡é‡è€Œæ˜¯å‘é‡ã€‚ è®¡ç®—å…¬å¼ï¼š $f$çš„ç»´åº¦ä¸$q$ç›¸åŒï¼Œæ¯ä¸€ç»´ä»£è¡¨çš„æ˜¯$x_i$åœ¨è¯¥ç»´å¯¹$q$çš„é‡è¦æ€§ã€‚ä¹Ÿå³feature-wiseçš„attentionã€‚å› æ­¤å¯¹äº$q$è€Œè¨€ï¼Œå…¶è·å¾—çš„åŠ æƒæ±‚å’Œå‘é‡ä¸ºï¼š ä½¿ç”¨feature-wiseçš„attentionèƒ½å¤Ÿè§£å†³ä¸€æ¬¡å¤šä¹‰çš„é—®é¢˜ï¼Œå› ä¸ºèƒ½å¤Ÿè®¡ç®—æ¯ä¸€ç»´çš„é‡è¦æ€§ï¼Œåœ¨ä¸åŒçš„contextä¸‹æœ‰ä¸åŒçš„é‡è¦æ€§ã€‚ å°†å…¶åº”ç”¨äºself-attentionä¸­ï¼Œæœ‰ä¸¤ç§å˜ä½“ï¼šâ‘ token2token å› æ­¤xåœ¨äº¤äº’å®Œæœ‰ï¼š â‘¡source2token ä¹Ÿå³$x_i$æ²¡æœ‰å’Œå…¶ä»–å…ƒç´ æœ‰äº¤äº’ã€‚å¯ç”¨ä½œè·å¾—sentence encodingï¼š Directional Self-Attentionä½¿ç”¨maskè¾¾åˆ°æœ‰å‘æ€§è¿™ä¸€ç›®çš„ï¼šé€šè¿‡maskçŸ©é˜µå°†ä½ç½®/æ–¹å‘ç¼–ç è¿›attentionï¼Œè§£å†³æ—¶åºä¸¢å¤±é—®é¢˜ã€‚é¦–å…ˆå°†xè¿‡ä¸€å±‚è·å¾—æ–°çš„hè¡¨ç¤ºï¼š æ¥ç€ä½¿ç”¨token2tokenæ±‚attentionï¼Œè¿™é‡Œä¸ºäº†å‡å°‘å‚æ•°ä½œäº†ä¸€å®šæ”¹åŠ¨ï¼Œå°†Wæ¢æˆcï¼Œtanhæ›¿æ¢Ïƒã€‚ $\textbf{1}$æ˜¯å…¨1çš„å‘é‡ã€‚Må°±æ˜¯maskçŸ©é˜µï¼Œä»£è¡¨iä¸jæ˜¯å¦è¿é€šï¼ŒMaskçŸ©é˜µæœ‰ï¼š ä¹Ÿå³ï¼š é¦–å…ˆmaskæ‰è‡ªå·±ï¼Œç¬¬äºŒï¼šåˆ†åˆ«maskæ‰forwardå’Œbackwardï¼Œç±»ä¼¼biLSTMï¼Œåªå’Œå‰é¢æˆ–åé¢çš„äº¤äº’ã€‚ Directional Self-Attention Networkåœ¨ä¸Šè¿°ä¸¤ä¸ªæ–¹æ³•çš„åŸºç¡€ä¸Šï¼Œæ­¤æ—¶å·²è·å¾—äº†ä¸Šä¸‹æ–‡ç›¸å…³çš„$s_i$ï¼Œå†å¼•å…¥fusion gateï¼š æ•´ä¸ªæµç¨‹ï¼š å°†å‰å‘å’Œåå‘çš„è¡¨ç¤ºæ‹¼æ¥èµ·æ¥ï¼Œè·å¾—æœ€ç»ˆçš„è¡¨ç¤º$[u^{fw};u^{bw}]$ï¼š å¯¹äºæ‰€è·å¾—çš„æ¯ä¸€ä¸ªè¡¨ç¤ºï¼Œé€šè¿‡source2tokenï¼Œè·å¾—æœ€ç»ˆçš„å¥å­è¡¨ç¤ºã€‚ è¿™ä¸€ç‚¹è®ºæ–‡ä¹Ÿæåˆ°äº†ï¼Œéå¸¸ç±»ä¼¼bi-LSTMã€‚ 2ï¸âƒ£[Targeted Dropout]ä¸€ç§ç½‘ç»œå‰ªææ–¹æ³•ï¼Œæƒ³æ³•ç®€å•æ˜“å®ç°ã€‚ç®€å•è¯´ï¼Œåœ¨æ¯æ¬¡æ›´æ–°æ—¶å¯¹æœ€ä¸é‡è¦çš„weightæˆ–è€…unitè¿›è¡Œéšæœºdropoutã€‚ Targeted DropoutDropoutç»™å®šè¾“å…¥Xï¼Œæƒé‡Wï¼Œè¾“å‡ºY Mä¸ºdropoutçš„maskçŸ©é˜µã€‚unit dropoutï¼š weight dropoutï¼š ä¹Ÿå³dropæ‰çš„æ˜¯layerä¹‹é—´çš„connectionã€‚ Magnitude-based pruningå‰ªæé€šå¸¸å¯¹æƒé‡æœ€å°çš„è¿›è¡Œå‰ªæï¼Œä¹Ÿå³ä¿ç•™topkä¸ªæœ€å¤§çš„æƒé‡ã€‚ Unit pruningï¼šç›´æ¥å‰ªæ‰çš„æ˜¯ä¸€æ•´åˆ—ï¼Œä¹Ÿå³ä¸€ä¸ªunit Weight pruningï¼šå¯¹Wçš„æ¯ä¸ªå…ƒç´ è¿›è¡Œå‰ªæã€‚æ³¨æ„æ˜¯å¯¹æ¯è¡Œçš„topkè¿›è¡Œä¿ç•™ å¯ä»¥ç†è§£æˆå¯¹ä¸€ä¸ªunitæ¥è¯´ï¼Œä¿ç•™æœ€é«˜çš„kä¸ªconnectionã€‚ æ–¹æ³•ç»“åˆdropoutå’Œå‰ªæã€‚ä¸»è¦æ€æƒ³ï¼šé¦–å…ˆé€‰æ‹©N-kæœ€ä¸é‡è¦çš„elementï¼Œç”±äºæˆ‘ä»¬å¸Œæœ›è¿™äº›low-valueçš„å…ƒç´ æœ‰æœºä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å˜å¾—é‡è¦ï¼Œå› æ­¤æˆ‘ä»¬å¯¹è¿™äº›elementè¿›è¡Œéšæœºdropoutã€‚ å¼•å…¥targeting proportion Î³å’Œdrop probability Î±ï¼Œäº¦å³ï¼šé€‰æ‹©æœ€ä½çš„Î³|Î¸|ä¸ªweightï¼Œå†æ ¹æ®Î±è¿›è¡Œdropoutã€‚è¿™æ ·åšçš„ç»“æœæ˜¯ï¼šå‡å°‘é‡è¦çš„å­ç½‘ç»œå¯¹ä¸é‡è¦çš„å­ç½‘ç»œçš„ä¾èµ–ã€‚ é™„å½•â‘ dropoutçš„intuitionï¼šå‡å°‘unitä¹‹é—´çš„ç›¸äº’é€‚åº”ã€‚when dropout is applied to a unit, the remaining network can no longer depend on that unitâ€™s contribution to the function and must learn to propagate that unitâ€™s information through a more reliable channelã€‚ä¹Ÿå¯ä»¥ç†è§£æˆï¼šä½¿å¾—unitä¹‹é—´çš„äº¤äº’ä¿¡æ¯è¾¾åˆ°æœ€å¤§ï¼Œåœ¨å¤±å»æŸä¸ªunitçš„æ—¶å€™å½±å“ä¸ä¼šé‚£ä¹ˆå¤§ã€‚ â‘¡targeted dropout intuitionï¼šthe important subnetwork is completely separated from the unimportant oneã€‚å‡è®¾ä¸€ä¸ªç½‘ç»œç”±ä¸¤ä¸ªä¸ç›¸äº¤çš„å­ç½‘ç»œç»„æˆï¼Œæ¯ä¸ªéƒ½èƒ½è¾“å‡ºæ­£ç¡®çš„ç»“æœï¼Œæ€»çš„ç½‘ç»œæ˜¯è¿™ä¸¤ä¸ªç½‘ç»œçš„å¹³å‡ã€‚æˆ‘ä»¬é€šè¿‡å¯¹ä¸é‡è¦çš„å­ç½‘ç»œè¿›è¡Œdropoutï¼ˆä¹Ÿå³å¾€å­ç½‘ç»œé‡ŒåŠ noiseï¼Œä¼šç ´åè¯¥å­ç½‘ç»œçš„è¾“å‡ºï¼Œç”±äºé‡è¦çš„å­ç½‘ç»œå·²ç»èƒ½å¤Ÿè¾“å‡ºæ­£ç¡®çš„ç»“æœï¼Œå› æ­¤ä¸ºäº†å‡å°‘æŸå¤±ï¼Œæˆ‘ä»¬éœ€è¦å‡å°‘ä¸é‡è¦ç½‘ç»œçš„è¾“å‡ºåˆ°0ï¼Œä¹Ÿå³killæ‰è¯¥å­ç½‘ç»œï¼Œå¹¶ä¸”åŠ å¼ºè¿™ä¸¤ä¸ªç½‘ç»œçš„åˆ†ç¦»ã€‚ï¼ˆä¸ºä»€ä¹ˆä¸ç›´æ¥èˆå¼ƒå‘¢ï¼Ÿå› ä¸ºæ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæœ‰å¯èƒ½ä¼šæœ‰å˜åŒ–ï¼‰è¿™ä¸ªè§£é‡Šè¿˜æ˜¯æ²¡å®Œå…¨æ‡‚ã€‚ 3ï¸âƒ£[A2-Nets: Double Attention Networks]å‘è¡¨äºNIPS2018ï¼Œä¸ªäººè®¤ä¸ºå¾ˆæœ‰å¯å‘ã€‚æå‡ºä¸€ç§æ–°çš„attentionæœºåˆ¶ï¼ŒåŸºäºâ€œæ”¶é›†-åˆ†å‘â€çš„æ€æƒ³ï¼Œèƒ½å¤Ÿè®©CNNè·å¾—æ›´å¤§çš„æ„Ÿå—é‡ã€‚ MotivationCNNæœ¬èº«ä¸»è¦æ˜¯æ•è·å±€éƒ¨ç‰¹å¾ä¸å…³ç³»ï¼Œä½†å¯¹äºé•¿è·ç¦»ä¹‹é—´çš„å…³ç³»åªèƒ½é€šè¿‡å †å å¤šå‡ å±‚æ‰èƒ½å®ç°ã€‚ä½†è¿™æ ·éœ€è¦æ›´é«˜çš„è®¡ç®—é‡ï¼Œä¸”å®¹æ˜“è¿‡æ‹Ÿåˆï¼›åŒæ—¶ï¼Œè¿œå¤„çš„ç‰¹å¾å®é™…ä¸Šæ˜¯æ¥è‡ªå¥½å‡ å±‚çš„å»¶è¿Ÿï¼Œå¯¼è‡´æ¨ç†çš„å›°éš¾ã€‚ é€šè¿‡å°†featureæ”¶é›†èµ·æ¥ï¼Œç„¶ååˆ†å‘ä¸‹å»ï¼Œä½¿å¾—featureä¹‹é—´æœ‰äº¤äº’ï¼Œè®©CNNè·å¾—æ›´å¤§çš„æ„Ÿå—é‡ï¼Œèƒ½å¤Ÿæ•è·é•¿è·ç¦»çš„ç‰¹å¾ã€‚ æ–¹æ³• ä¹Ÿå³ï¼š Xæ˜¯æ‰€æœ‰è¾“å…¥ï¼Œ$v_iæ˜¯$local featureã€‚ The First Attention Step: Feature Gatheringå¯¹äºä¸¤ä¸ªfeature map A,Bï¼Œæœ‰ï¼š å…¶ä¸­ï¼š å¦‚æœAã€Béƒ½æ¥è‡ªåŒä¸€ä¸ªXï¼Œå°†Bå½’ä¸€åŒ–softmaxï¼Œå°±ç±»ä¼¼transformerçš„attentionã€‚å…¶ä¸­ä¸Šå¼çš„æœ€å³è¾¹æ˜¯å¤–ç§¯çš„å½¢å¼ã€‚ æˆ‘ä»¬å°†Gæ‹†åˆ†æˆå‘é‡å½¢å¼ï¼šåŒæ—¶å°†Bé‡å†™æˆè¡Œå‘é‡å½¢å¼ï¼Œåˆ™æœ‰ï¼š åˆ™ä¼šæœ‰ï¼š ä¸Šå¼è®©æˆ‘ä»¬æœ‰ä¸€ä¸ªæ–°çš„ç†è§£è§’åº¦ï¼šGå®é™…ä¸Šå°±æ˜¯ a bag of visual primitivesã€‚æ¯ä¸ª$g_i$æ˜¯æ‰€æœ‰local featureåŠ æƒæ±‚å’Œï¼Œå…¶ä¸­$b_i$æ˜¯æ±‚å’Œçš„weightã€‚ å› æ­¤æˆ‘ä»¬å¯¹Båšsoftmaxï¼Œä¿è¯æƒé‡ä¸º1ï¼š The Second Attention Step: Feature Distributionåœ¨è·å¾—äº†å…¨å±€çš„feature Gåï¼Œç°åœ¨æ ¹æ®local featureå»è·å–å…¨å±€featureçš„éƒ¨åˆ†ï¼Œè¿™é€šè¿‡ä¸€ä¸ªæƒé‡æ§åˆ¶ï¼Œä¹Ÿå³$v_i$ï¼ˆlocal feature)çš„æ¯ä¸€ç»´ä½œä¸ºæƒé‡ã€‚å¯ä»¥ä¸å°†local feature $v_i$å½’ä¸€åŒ–ï¼Œä½†å½’ä¸€åŒ–èƒ½æ›´å¥½åœ°convergeã€‚ The Double Attention Blockæœ€ç»ˆå¾—åˆ°double attention blockï¼š æ•´ä¸ªæµç¨‹ï¼š æ‰€ä»¥å…¶å®æ˜¯æœ‰ä¸‰ä¸ªconvolution layerã€‚ ä¸Šå¼è¿˜å¯ä»¥å†™æˆï¼šæ•°å­¦ä¸Šç­‰ä»·ï¼Œä½†è®¡ç®—ä¸Šå·®å¾ˆå¤šã€‚ç¬¬ä¸€ä¸ªå¼å­ä¼šæœ‰æ›´ä½çš„å¤æ‚åº¦ã€‚ æ€è€ƒè™½ç„¶ç”¨äº†attentionï¼Œä½†è¿™é‡Œå’ŒTransformerè¿˜æ˜¯æœ‰éå¸¸å¤§çš„åŒºåˆ«çš„ã€‚Transformeræ¯ä¸ªå…ƒç´ éƒ½å’Œå…¶ä»–å…ƒç´ æœ‰äº¤äº’ï¼Œé€šè¿‡ç›´æ¥çš„è®¡ç®—å¾—åˆ°æƒé‡ã€‚è€Œè¿™è¾¹çš„æƒé‡ç”±featureæœ¬èº«æ¥å†³å®šã€‚å¹¶æ²¡æœ‰ç›´æ¥çš„äº¤äº’ã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>attention</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>Dropout</tag>
        <tag>DiSAN</tag>
        <tag>Targeted Dropout</tag>
        <tag>double attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†14]]></title>
    <url>%2F2018%2F12%2F09%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8614%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Pytorch]Pytorchçš„tensorå’ŒTensoræ˜¯æœ‰åŒºåˆ«çš„ï¼š 123import torcha = torch.tensor(2) # æ˜¯æ ‡é‡ï¼Œsizeä¸º[]b = torch.Tensor(2) # æ˜¯å‘é‡ï¼Œsizeä¸º[2]]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯17]]></title>
    <url>%2F2018%2F12%2F09%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D17%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£è™ç¾äºº[å®‹] å¶æ¢¦å¾—è½èŠ±å·²ä½œé£å‰èˆï¼Œåˆé€é»„æ˜é›¨ã€‚æ™“æ¥åº­é™¢åŠæ®‹çº¢ï¼ŒæƒŸæœ‰æ¸¸ä¸ï¼Œåƒä¸ˆè¢…æ™´ç©ºã€‚æ®·å‹¤èŠ±ä¸‹åŒæºæ‰‹ï¼Œæ›´å°½æ¯ä¸­é…’ã€‚ç¾äººä¸ç”¨æ•›è›¾çœ‰ï¼Œæˆ‘äº¦å¤šæƒ…ï¼Œæ— å¥ˆé…’é˜‘æ—¶ã€‚]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†13]]></title>
    <url>%2F2018%2F12%2F02%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8613%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[attention]æ‰€æœ‰attentionçš„æ€»ç»“ï¼šAttention? Attention! 2ï¸âƒ£[Pytorch]â‘ torch.no_gradèƒ½å¤Ÿæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œmodel.evalä¸èƒ½ã€‚å› ä¸ºevalä¸ä¼šå…³é—­å†å²è¿½è¸ªã€‚ model.eval() will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval model instead of training mode.torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you wonâ€™t be able to backprop (which you donâ€™t want in an eval script). Reference:Does model.eval() &amp; with torch.set_grad_enabled(is_train) have the same effect for grad history? â€˜model.eval()â€™ vs â€˜with torch.no_grad()â€™ â‘¡torch.full(â€¦) returns a tensor filled with value.]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•11]]></title>
    <url>%2F2018%2F12%2F02%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9511%2F</url>
    <content type="text"><![CDATA[â‘ éœ€æ±‚ï¼šå¯¹äºä¸¤ä¸ªå‘é‡$a$ã€$b$ï¼Œ$a,b \in R^d$ï¼Œå®šä¹‰ä¸€ç§å‡æ³•ï¼Œæœ‰ï¼š a-b=Må…¶ä¸­$M \in R^{d\times d}$ï¼Œ$M_{ij}=a_i-b_j$ åœ¨ä»£ç ä¸­å®é™…çš„ç»´åº¦ï¼š 12a=torch.rand(batch_size,sequence_len,dim)b=torch.rand(batch_size,sequence_len,dim) æ–¹æ³•â‘ ï¼šforå¾ªç¯ 123456M=torch.zeros(bz,seq_len,seq_len)for b_i in range(bz): for i in range(seq_len): for j in range(seq_len): M_ij=torch.norm(a[b_i][i]-b[b_i][j]) M[b][i][j]=M_ij æ–¹æ³•â‘¡ï¼šçŸ©é˜µè¿ç®— 123a=a.unsqueeze(2) # bz,seq_len,1,dimb=b.unsqueeze(1) # bz,1,seq_lens,dimM=torch.norm(a-b,dim=-1) # will broadcast â‘¡éœ€æ±‚ï¼Œç”Ÿæˆä¸€ä¸ªmaskçŸ©é˜µï¼Œæ¯ä¸€è¡Œæœ‰ä¸€æ®µè¿ç»­çš„ä½ç½®å¡«å……1ï¼Œå…¶ä¸­æ¯ä¸€è¡Œå¡«å……1çš„å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®éƒ½ä¸åŒã€‚å…·ä½“æ¥è¯´ï¼Œå…ˆç”Ÿæˆä¸€ä¸ªä¸­å¿ƒä½ç½®centerï¼Œåˆ™å¼€å§‹ä½ç½®ä¸ºcenter-windowï¼›ç»“æŸä½ç½®ä¸ºcenter+windowã€‚å…¶ä¸­å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®ä¸èƒ½è¶Šç•Œï¼Œä¹Ÿå³ä¸å°äº0å’Œå¤§äºè¡Œçš„æ€»é•¿åº¦ã€‚å¦‚ï¼š æ€è·¯ï¼šâ‘ å…ˆç”Ÿæˆnè¡Œæ¯è¡Œå¯¹åº”çš„éšæœºä¸­å¿ƒä½ç½®ï¼Œç„¶åå†è·å¾—å·¦å’Œå³è¾¹ç•Œ 12345678centers=torch.randint(low=0,high=query_len,size=(query_len,),dtype=torch.long)left=centers-self.windowleft=torch.max(left,torch.LongTensor([0])).unsqueeze(1) # query_len,1right=centers+self.windowright=torch.min(right,torch.LongTensor([query_len-1])).unsqueeze(1) # query_len,1 â‘¡ç”Ÿæˆä¸€ä¸ªæ¯è¡Œéƒ½ç”¨[0,n-1]å¡«å……çš„çŸ©é˜µï¼Œ[0,n-1]è¡¨ç¤ºçš„æ˜¯è¯¥å…ƒç´ çš„indexï¼Œäº¦å³ï¼š 1range_matrix=torch.range(0,query_len-1,dtype=torch.long).unsqueeze(0).expand(query_len,-1) # query_len,query_len â‘¢åˆ©ç”¨&lt;=å’Œ&gt;=è·å¾—ä¸€ä¸ªå·¦è¾¹ç•Œå’Œå³è¾¹ç•ŒçŸ©é˜µï¼Œå·¦è¾¹ç•ŒçŸ©é˜µè¡¨ç¤ºåœ¨è¯¥å·¦è¾¹ç•Œçš„å·¦è¾¹éƒ½æ˜¯å¡«å……çš„1ï¼›å³è¾¹ç•ŒçŸ©é˜µè¡¨ç¤ºåœ¨è¯¥å³è¾¹ç•Œå³è¾¹éƒ½æ˜¯å¡«å……çš„1ã€‚å†è¿›è¡Œå¼‚æˆ–æ“ä½œã€‚ 1234range_matrix=torch.range(0,query_len-1,dtype=torch.long).unsqueeze(0).expand(query_len,-1) # query_len,query_lenleft_matrix=range_matrix&lt;=leftright_matrix=range_matrix&lt;=rightfinal_matrix=left_matrix^right_matrix]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡7]]></title>
    <url>%2F2018%2F12%2F02%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%877%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Convolutional Self-Attention Network]å¯¹self-attentionè¿›è¡Œæ”¹è¿›ï¼Œå¼•å…¥CNNçš„local-biasï¼Œä¹Ÿå³å¯¹queryçš„é‚»è¿‘è¯è¿›è¡Œattentionè€Œä¸æ˜¯æ‰€æœ‰è¯ï¼›å°†self-attentionæ‰©å±•åˆ°2Dï¼Œä¹Ÿå³è®©ä¸åŒçš„headä¹‹é—´ä¹Ÿæœ‰attentionäº¤äº’ã€‚ Motivation1ï¸âƒ£the normalization in Softmax may inhibits the attention to neighboring information ä¹Ÿå³é‚»å±…çš„ä¿¡æ¯æ›´é‡è¦ï¼Œè¦åŠ å¼ºé‚»å±…çš„é‡è¦æ€§ 2ï¸âƒ£features can be better captured by modeling dependencies across different channels å¯¹äºä¸åŒçš„channel/headä¹Ÿå¢åŠ ä»–ä»¬ä¹‹é—´çš„äº¤äº’ã€‚ æ–¹æ³• å¯¹äº1Dçš„convolutionï¼šé€‰å–ä¸­å¿ƒè¯å‘¨å›´ä¸€ä¸ªwindowï¼š å¯¹äº2Dçš„convolutionï¼Œåˆ™æœ‰ï¼š åœ¨å…·ä½“å®è·µä¸­ï¼Œåªå¯¹å‰ä¸‰å±‚æ·»åŠ local biasï¼Œè¿™æ˜¯å› ä¸ºmodeling localityåœ¨åº•å±‚æ›´æœ‰æ•ˆï¼Œå¯¹äºé«˜å±‚åº”è¯¥æ•è·æ›´è¿œçš„ä¿¡æ¯ã€‚ 2ï¸âƒ£[Modeling Localness for Self-Attention Networks]å’Œä¸Šæ–‡ä¸€æ ·ï¼Œå¼•å…¥local biaså¯¹self-attentionè¿›è¡Œæ”¹è¿›ï¼Œä»è€Œæå‡äº†ç¿»è¯‘è¡¨ç°ã€‚å’Œä¸Šæ–‡æ˜¯åŒä¸€ä½œè€…ï¼Œå‘åœ¨EMNLPä¸Šã€‚ Motivation1ï¸âƒ£self-attentionå­˜åœ¨çš„é—®é¢˜ï¼šè™½ç„¶èƒ½å¤Ÿå¢åŠ é•¿ç¨‹å…³æ³¨ï¼Œä½†å› æ­¤ä¼šå¯¼è‡´æ³¨æ„åŠ›çš„åˆ†æ•£ï¼Œå¯¹é‚»å±…çš„ä¿¡å·ä¼šå¿½ç•¥ã€‚å®è·µè¯æ˜ï¼Œå¯¹local biaså»ºæ¨¡åœ¨self-attentionæœ‰æå‡ã€‚ 2ï¸âƒ£ä»ç›´è§‰ä¸Šæ¥è¯´ï¼Œåœ¨ç¿»è¯‘æ¨¡å‹ä¸­ï¼Œå½“ç›®æ ‡è¯iä¸æºè¯­è¨€è¯jæœ‰å¯¹é½å…³ç³»æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›è¯ièƒ½åŒæ—¶å¯¹è¯jå‘¨å›´çš„è¯è¿›è¡Œå¯¹é½ï¼Œä½¿å¾—èƒ½å¤Ÿæ•è·ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¦‚phraseçš„ä¿¡æ¯ã€‚ æ–¹æ³•åœ¨åŸæ¥çš„å…¬å¼ä¸Šæ·»åŠ Gï¼šä¹Ÿå³ï¼š Gæ˜¯ä¸€ä¸ªalignment position matrixï¼ˆå¯¹é½ä½ç½®çŸ©é˜µï¼‰ï¼Œå…ƒç´ ijä»£è¡¨ç›®æ ‡è¯iä¸æºè¯­è¨€è¯jä¹‹é—´çš„ç´§å¯†ç¨‹åº¦ã€‚æˆ‘ä»¬æ¯æ¬¡æ ¹æ®ç›®æ ‡è¯ié¢„æµ‹ä¸€ä¸ªæºè¯­è¨€çš„ä¸­å¿ƒè¯ï¼Œåˆ™$G_{ij}$åˆ™ä¸ºï¼š $P_i$å°±æ˜¯å¯¹äºç›®æ ‡è¯jè€Œè¨€æºè¯­è¨€çš„ä¸­å¿ƒè¯ã€‚ $\sigma$ æ‰‹åŠ¨è®¾å®šï¼Œé€šå¸¸æ˜¯$\frac{D}{2}$ï¼ŒDä»£è¡¨çª—å£å¤§å°ã€‚ ä¹Ÿå³æœ€ç»ˆæˆ‘ä»¬éœ€è¦è®¡ç®—çš„æ˜¯ï¼Œä¸­å¿ƒè¯$P_i$å’Œçª—å£$D$ã€‚ è®¡ç®—$P_i$åˆ©ç”¨å¯¹åº”çš„ç›®æ ‡è¯içš„queryå³å¯ï¼š$p_i$æ˜¯ä¸€ä¸ªå®æ•°ã€‚ è®¡ç®—window sizeâ‘ å›ºå®šçª—å£ï¼Œå°†å…¶ä½œä¸ºä¸€ä¸ªè¶…å‚ã€‚ â‘¡Layer-Speciï¬c Windowå°†è¯¥å±‚æ‰€æœ‰çš„keyå¹³å‡ï¼Œè®¡ç®—å‡ºä¸€ä¸ªå…±äº«çš„window sizeï¼š â‘¢Query-Speciï¬c Windowæ¯ä¸ªqueryéƒ½æœ‰è‡ªå·±çš„window size å®éªŒåˆ†æä¸ç»“è®ºâ‘ å°†model localityç”¨äºä½å±‚æ•ˆæœä¼šæ›´å¥½ï¼Œè¿™æ˜¯å› ä¸ºä½å±‚å¯¹ç›¸é‚»å»ºæ¨¡ï¼Œè€Œè¶Šé«˜å±‚è¶Šå…³æ³¨æ›´è¿œçš„è¯ã€‚ â‘¡å°†model localityæ”¾åœ¨encoderå’Œencoder-decoderéƒ¨åˆ†ä¼šæ›´å¥½ï¼ˆtransformeræœ‰ä¸‰ä¸ªåœ°æ–¹å¯ä»¥æ”¾ï¼‰ å› ä¸ºdecoderæœ¬èº«å°±å€¾å‘å…³æ³¨ä¸´è¿‘çš„è¯ï¼Œå¦‚æœç»§ç»­è®©å…¶å…³æ³¨ä¸´è¿‘çš„è¯ï¼Œé‚£ä¹ˆå°±éš¾ä»¥è¿›è¡Œé•¿ç¨‹å»ºæ¨¡ã€‚ â‘¢è¶Šé«˜å±‚ï¼Œwindow sizeï¼ˆscopeï¼‰è¶Šå¤§ã€‚ ä¹Ÿå³ï¼Œåœ¨åº•å±‚æ›´å€¾å‘äºæ•è·é‚»è¿‘è¯çš„è¯­ä¹‰ï¼›è€Œé«˜å±‚å€¾å‘æ•è·é•¿ç¨‹ä¾èµ–ã€‚ä½†è¿™ä¸åŒ…æ‹¬ç¬¬ä¸€å±‚ï¼Œç¬¬ä¸€å±‚æ˜¯embeddingï¼Œè¿˜æ²¡æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå› æ­¤å€¾å‘äºæ•è·å…¨å±€ä¿¡æ¯ã€‚ 3ï¸âƒ£[Effective Approaches to Attention-based Neural Machine Translation]æå‡ºä¸¤ç§attentionæœºåˆ¶çš„ç¿»è¯‘æ¨¡å‹ï¼Œglobalå’Œlocalã€‚ æœ¬æ–‡ä¸åŸç‰ˆçš„ç¿»è¯‘æ¨¡å‹ç•¥æœ‰ä¸åŒï¼š cæ˜¯contextï¼Œhæ˜¯decodeçš„éšå±‚ã€‚ global attention è®¡ç®—attentionåˆ†æ•°ï¼š scoreæœ‰å¤šç§é€‰æ‹©ï¼š æ³¨æ„åˆ°è¯¥æ¨¡å‹ä¸ç¬¬ä¸€ä¸ªæå‡ºattention basedçš„æ¨¡å‹ä¸åŒä¹‹å¤„ï¼š$h_t -&gt; a_t -&gt; c_t -&gt; \tilde{h_t}$åŸç‰ˆæ˜¯ï¼š$h_{t-1} -&gt; a_t -&gt; c_t -&gt; h_t$ local attention ç”±äºglobal attentionè®¡ç®—ä»£ä»·é«˜ï¼Œä¸”å¯¹äºé•¿å¥æ•ˆæœä¸å¥½ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸€éƒ¨åˆ†æ¥åšattentionã€‚é¦–å…ˆç”Ÿæˆä¸€ä¸ªå¯¹é½ä½ç½®$p_t$ï¼Œå†é€‰æ‹©ä¸€ä¸ªçª—å£$[p_t - D,p_t + D]$ï¼Œå…¶ä¸­Dæ˜¯è¶…å‚ã€‚ å¦‚ä½•è·å¾—$p_t$?â‘ ç›´æ¥å‡è®¾$p_t=t$ï¼Œä¹Ÿå³sourceå’Œtargetçš„ä½ç½®å¤§è‡´ä¸€ä¸€å¯¹åº”ã€‚ â‘¡åšé¢„æµ‹ï¼šå…¶ä¸­Sæ˜¯sourceçš„å¥å­é•¿åº¦ã€‚ æ¥ç€ï¼Œä»¥$p_t$ä¸ºä¸­å¿ƒï¼Œæ·»åŠ ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒã€‚æœ€ç»ˆattentionè®¡ç®—å…¬å¼ï¼š å…¶ä¸­alignå’Œä¸Šé¢ä¸€è‡´ï¼š ä¹Ÿå°±æ˜¯è¯´ï¼Œå°†ä½ç½®ä¿¡æ¯ä¹Ÿè€ƒè™‘è¿›æ¥ã€‚ Input-feeding Approachmotivationï¼šåœ¨ä¸‹ä¸€æ¬¡çš„alignmentï¼ˆä¹Ÿå°±æ˜¯è®¡ç®—attentionï¼‰ä¹‹å‰ï¼Œåº”å½“çŸ¥é“ä¹‹å‰çš„alignmentæƒ…å†µï¼Œæ‰€ä»¥åº”å½“ä½œä¸ºè¾“å…¥ä¿¡æ¯ä¼ è¿›ä¸‹ä¸€å±‚ï¼š æ³¨æ„è¿™é‡Œå’ŒBahdanauçš„ä¸åŒã€‚Bahdanauæ˜¯ç›´æ¥ç”¨ä¸Šä¸‹æ–‡å»æ„é€ éšå±‚ã€‚è¿™é‡Œæå‡ºçš„æ¨¡å‹ç›¸å¯¹æ›´ä¸ºé€šç”¨ï¼Œä¹Ÿå¯ä»¥è¢«åº”ç”¨äºéattentionçš„æ¨¡å‹ä¸­ï¼ˆä¹Ÿå°±æ˜¯æ¯æ¬¡å°†encoderçš„æœ€åä¸€å±‚ä½œä¸ºè¾“å…¥åœ¨æ¯ä¸ªtime stepéƒ½è¾“å…¥ï¼‰ 4ï¸âƒ£[Towards Linear Time Neural Machine Translation with Capsule Networks]æ€æƒ³ï¼šåˆ©ç”¨capsuleæå‰ç”Ÿæˆsource sentenceçš„å›ºå®šé•¿åº¦çš„è¡¨ç¤ºï¼Œåœ¨decodeçš„æ—¶å€™ç›´æ¥ä½¿ç”¨ï¼Œè€Œä¸éœ€è¦attentionï¼Œä»¥è¾¾åˆ°çº¿æ€§æ—¶é—´NMTçš„ç›®çš„ã€‚ Motivationï¼šattention-basedçš„NMTæ—¶é—´å¤æ‚åº¦ä¸º$|S|\times |T|$ï¼Œè€Œæœ¬æ–‡å¸Œæœ›èƒ½å¤Ÿå°†NMTå‡å°‘åˆ°çº¿æ€§æ—¶é—´ã€‚è€Œä¼ ç»Ÿä¸åŠ attentionçš„NMTé€šå¸¸ä½¿ç”¨LSTMæœ€åä¸€å±‚éšå±‚ä½œä¸ºæºè¯­è¨€çš„encodeä¿¡æ¯ä¼ å…¥decodeï¼Œä½†è¿™æ ·çš„ä¿¡æ¯å¹¶ä¸èƒ½å¾ˆå¥½åœ°ä»£è¡¨æ•´ä¸ªå¥å­ï¼Œå› æ­¤æœ¬æ–‡ä½¿ç”¨capsuleä½œä¸ºæå–source sentenceä¿¡æ¯çš„æ–¹æ³•ï¼Œåˆ©ç”¨capsuleç”Ÿæˆå›ºå®šé•¿åº¦è¡¨ç¤ºï¼Œç›´æ¥ä¼ å…¥decodeç«¯ï¼Œä»¥è¾¾åˆ°çº¿æ€§æ—¶é—´çš„ç›®çš„ã€‚ é—®é¢˜å®šä¹‰å¯¹äºembeddingï¼šå¸Œæœ›èƒ½å¤Ÿè½¬æ¢æˆå›ºå®šé•¿åº¦çš„è¡¨ç¤ºCï¼š æˆ‘ä»¬é¦–å…ˆé€šè¿‡ä¸€ä¸ªåŒå‘çš„LSTMï¼š ä¸€ç§ç®€å•çš„è·å–Cçš„æ–¹æ³•ï¼šå…¶ä¸­$h_1$å’Œ$h_L$æœ‰äº’è¡¥å…³ç³»ã€‚ æœ¬æ–‡ä½¿ç”¨capsuleæå–æ›´ä¸°å¯Œçš„ä¿¡æ¯ã€‚ åœ¨decodeé˜¶æ®µï¼Œç”±äºæ‹¥æœ‰å›ºå®šè¡¨ç¤ºï¼Œé‚£ä¹ˆå°±ä¸éœ€è¦attentionï¼š æ€»ä½“æ¶æ„ï¼š Aggregation layers with Capsule Networkså®é™…ä¸Šå°±æ˜¯dynamic routingé‚£ä¸€å¥—ï¼Œå¯¹ä¿¡æ¯è¿›è¡Œæå–ï¼ˆè®ºæ–‡å…¬å¼æœ‰è¯¯å°±ä¸è´´å›¾äº†ï¼‰ ç®—æ³•ï¼š æœ€ç»ˆè·å¾—äº†ï¼š 5ï¸âƒ£[DropBlock: A regularization method for convolutional networks]é‡è¯»äº†ä¸€éã€‚ä»‹ç»ä¸€ç§æ–°å‹çš„dropoutï¼Œå¯ç”¨äºå·ç§¯å±‚æé«˜è¡¨ç°ã€‚é€šè¿‡å¤§é‡çš„å®éªŒå¾—å‡ºè®¸å¤šæœ‰æ„ä¹‰çš„ç»“è®ºã€‚æœ¬æ–‡å‘è¡¨äºNIPS2018ã€‚ Motivationç”±äºå·ç§¯å±‚çš„featureç›¸äº’ä¹‹é—´æœ‰è”ç³»ï¼Œå³ä½¿ä½¿ç”¨äº†dropoutï¼Œä¿¡æ¯ä¹Ÿèƒ½å¤Ÿæ ¹æ®å‘¨å›´çš„featureä¼ åˆ°ä¸‹ä¸€å±‚ã€‚å› æ­¤ä½¿ç”¨dropblockï¼Œä¸€æ¬¡å°†ä¸€ä¸ªæ–¹å—å†…çš„éƒ½dropæ‰ã€‚ ç®—æ³• å…¶ä¸­æœ‰ä¸¤ä¸ªè¶…å‚ï¼šâ‘ block_sizeè¡¨ç¤ºå—çš„å¤§å°ï¼›Î³è¡¨ç¤ºæœ‰å¤šå°‘ä¸ªunitè¦dropæ‰ï¼Œç­‰ä»·ä¼ ç»Ÿçš„dropoutçš„pã€‚å½“block_size=1æ—¶ç­‰ä»·dropoutï¼›å½“block size=æ•´ä¸ªfeature mapï¼Œç­‰ä»·äºspatial dropoutã€‚ åœ¨å®è·µä¸­ï¼Œé€šè¿‡ä»¥ä¸‹å…¬å¼è®¡ç®—Î³ï¼š (why? é€šè¿‡è®¡ç®—æœŸæœ›çš„æ–¹å¼å°†ä¼ ç»Ÿdropoutçš„keep_probä¸å½“å‰çš„Î³è”ç³»èµ·æ¥ï¼Œå¾—åˆ°ä¸€ä¸ªç­‰å¼ï¼Œæ•´ç†å³å¯è·å¾—ä¸Šå¼ï¼‰ åœ¨å®éªŒä¸­ï¼Œè¿˜å¯ä»¥é€æ¸å‡å°keep_probä½¿å¾—æ›´åŠ é²æ£’æ€§ã€‚ å®éªŒ&amp;ç»“è®ºâ‘ æ•ˆæœ:dropout&lt; spatial dropout &lt; dropblock â‘¡dropblockèƒ½æœ‰æ•ˆå»æ‰semantic information â‘¢dropblockæ˜¯ä¸€ä¸ªæ›´åŠ å¼ºçš„regularization â‘£ä½¿ç”¨dropblockçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿå­¦ä¹ æ›´å¤šçš„åŒºåŸŸï¼Œè€Œä¸æ˜¯åªä¸“æ³¨äºä¸€ä¸ªåŒºåŸŸ å¯¹äºresnetï¼Œç›´æ¥å°†dropblockåº”ç”¨äºæ·»åŠ å®Œskip connectionåçš„featureèƒ½å¤Ÿæœ‰æ›´é«˜çš„è¡¨ç°ã€‚ 6ï¸âƒ£[Contextual String Embeddings for Sequence Labeling]æå‡ºä¸€ç§å»ºç«‹åœ¨characteråŸºç¡€ä¸Šçš„æ–°å‹çš„ä¸Šä¸‹æ–‡embedding(contextualized embeddingï¼‰ã€‚ç”¨äºsequence labelingã€‚æœ¬æ–‡å‘è¡¨äºcoling2018ã€‚ æ–¹æ³•æ•´ä½“æ¶æ„ï¼š é¦–å…ˆå°†characterä½œä¸ºåŸºæœ¬å•ä½ï¼Œè¿‡ä¸€ä¸ªåŒå‘LSTMï¼Œè¿›è¡Œlanguage modelçš„å»ºæ¨¡ã€‚ å¦‚ä½•æå–ä¸€ä¸ªè¯çš„è¯å‘é‡ï¼šæå–å‰å‘LSTMä¸­è¯¥è¯çš„æœ€åä¸€ä¸ªcharacterçš„åä¸€ä¸ªhidden stateï¼Œä»¥åŠåå‘LSTMä¸­ç¬¬ä¸€ä¸ªè¯çš„å‰ä¸€ä¸ªhidden stateï¼Œ å¦‚ä¸Šå›¾æ‰€ç¤ºã€‚æœ€ç»ˆæ‹¼èµ·æ¥å³å¯ï¼šå› æ­¤è¯¥è¯ä¸ä»…ä¸è¯å†…éƒ¨çš„characterç›¸å…³ï¼Œè¿˜è·Ÿå…¶å‘¨å›´çš„contextæœ‰å…³ã€‚ sequence labelingæˆ‘ä¸æ„Ÿå…´è¶£ï¼Œè¯¥éƒ¨åˆ†æ²¡çœ‹ã€‚ Discussionç›¸æ¯”word levelçš„language modelï¼Œcharacter-levelç‹¬ç«‹äºtokenizationå’Œfixed vocabularyï¼Œæ¨¡å‹æ›´å®¹æ˜“è¢«è®­ç»ƒï¼Œå› ä¸ºè¯è¡¨å°ä¸”è®­ç»ƒæ—¶é—´çŸ­ã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>attention</tag>
        <tag>capsule</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>NMT</tag>
        <tag>self-attention</tag>
        <tag>locality modeling</tag>
        <tag>dropblock</tag>
        <tag>contextualized embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯16]]></title>
    <url>%2F2018%2F12%2F01%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D16%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£è©è¨è›®[äº”ä»£åå›½] æç…œäººç”Ÿæ„æ¨ä½•èƒ½å…ï¼Œé”€é­‚ç‹¬æˆ‘æƒ…ä½•é™ï¼æ•…å›½æ¢¦é‡å½’ï¼Œè§‰æ¥åŒæ³ªå‚ã€‚é«™æ¥¼è°ä¸ä¸Šï¼Ÿé•¿è®°ç§‹æ™´æœ›ã€‚å¾€äº‹å·²æˆç©ºï¼Œè¿˜å¦‚ä¸€æ¢¦ä¸­ã€‚ è§‰(jue)æ¥ï¼šé†’æ¥ã€‚ 2ï¸âƒ£å—ä¹¡å­ Â· å’Œæ¨å…ƒç´ ï¼Œæ—¶ç§»å®ˆå¯†å·[å®‹] è‹è½¼ä¸œæ­¦æœ›é¦€æ­ï¼Œäº‘æµ·å¤©æ¶¯ä¸¤æ³èŒ«ã€‚ä½•æ—¥åŠŸæˆåé‚äº†ï¼Œè¿˜ä¹¡ï¼Œé†‰ç¬‘é™ªå…¬ä¸‰ä¸‡åœºã€‚ä¸ç”¨è¯‰ç¦»è§ï¼Œç—›é¥®ä»æ¥åˆ«æœ‰è‚ ã€‚ä»Šå¤œé€å½’ç¯ç«å†·ï¼Œæ²³å¡˜ï¼Œå •æ³ªç¾Šå…¬å´å§“æ¨ã€‚]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ— é¢˜]]></title>
    <url>%2F2018%2F12%2F01%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E4%BA%BA%E4%B8%8D%E5%8F%AF%E8%83%BD%E7%BB%8F%E5%8E%86%E4%B8%96%E7%95%8C%E4%B8%8A%E6%89%80%E6%9C%89%E7%83%AD%E9%97%B9%2F</url>
    <content type="text"><![CDATA[äººä¸å¯èƒ½ç»å†ä¸–ç•Œä¸Šæ‰€æœ‰çƒ­é—¹ï¼Œä½†å¯ä»¥ç”¨çœ¼ç›çœ‹ï¼Œç”¨å¿ƒæ„Ÿå—ï¼Œç”¨èƒ¸æ€€æ‰©å¼ ã€‚]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†12]]></title>
    <url>%2F2018%2F11%2F19%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8612%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Transformer]å¯¹Transformeræ–°ç†è§£ï¼š å¯ä»¥å°†Transformerç†è§£æˆä¸€å¼ å…¨è¿æ¥å›¾ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹ä¸å…¶ä»–èŠ‚ç‚¹çš„å…³ç³»é€šè¿‡attentionæƒé‡è¡¨ç°ã€‚å›¾å…³ç³»æ˜¯åºåˆ—å…³ç³»æˆ–è€…æ ‘å…³ç³»çš„ä¸€èˆ¬åŒ–ã€‚ ä¸ºä»€ä¹ˆè¦æœ‰multi-headï¼Ÿä¸ä»…ä»…æ˜¯è®ºæ–‡çš„è§£é‡Šï¼Œæˆ–è®¸è¿˜å¯ä»¥ç†è§£æˆï¼Œå¯¹ä¸€ä¸ªå‘é‡çš„ä¸åŒéƒ¨åˆ†ï¼ˆå¦‚ç¬¬1ç»´åˆ°20ç»´ï¼Œç¬¬21ç»´åˆ°40ç»´ç­‰ï¼‰æ–½ä»¥ä¸åŒçš„attentionæƒé‡ï¼Œå¦‚æœä¸ä½¿ç”¨multi-headï¼Œé‚£ä¹ˆå¯¹äºä¸€ä¸ªqueryï¼Œå°±åªä¼šæœ‰ä¸€ä¸ªæƒé‡ï¼Œè€Œä¸åŒçš„ç»´åº¦æœ‰ä¸åŒçš„é‡è¦æ€§ã€‚ 2ï¸âƒ£[attention&amp;capsule]attentionæ˜¯æ”¶ä¿¡æ¯ï¼Œqueryä»valueæŒ‰æƒé‡è·å–ä¿¡æ¯ï¼Œå…¶ä¸­æ‰€æœ‰valueçš„æƒé‡å’Œæ˜¯1ã€‚capsuleæ˜¯å‘ä¿¡æ¯ï¼Œå¯¹äº$l-1$å±‚çš„ä¸€ä¸ªcapsuleæ¥è¯´ï¼Œåœ¨ä¼ å…¥åˆ°$l$å±‚çš„kä¸ªcapsuleçš„ä¿¡æ¯ï¼Œå…¶æƒé‡å’Œä¸º1ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Transformer</tag>
        <tag>attention</tag>
        <tag>capsule</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡6]]></title>
    <url>%2F2018%2F11%2F19%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%876%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[A STRUCTURED SELF ATTENTIVE SENTENCE EMBEDDING]ä»‹ç»äº†ä¸€ç§ç”Ÿæˆsentence embeddingçš„æ–¹æ³•ã€‚ä¸å…¶ä»–sentence embeddingä¸åŒçš„åœ°æ–¹åœ¨äºï¼Œç”Ÿæˆçš„æ˜¯ä¸€ä¸ªçŸ©é˜µè€Œä¸æ˜¯ä¸€ä¸ªå‘é‡ã€‚é€šè¿‡çŸ©é˜µçš„å½¢å¼ï¼Œèƒ½å¤Ÿå…³æ³¨ä¸åŒéƒ¨åˆ†çš„è¯­ä¹‰è¡¨ç¤ºï¼Œç±»ä¼¼äºTransformerçš„multi-headã€‚ Contribution: å°†sentence embeddingæ‰©å±•ä¸ºçŸ©é˜µå½¢å¼ï¼Œèƒ½å¤Ÿè·å¾—æ›´å¤šçš„ä¿¡æ¯ã€‚ å¼•å…¥æ­£åˆ™åŒ–ï¼Œä½¿å¾—sentence matrixå…·æœ‰æ›´ä¸°å¯Œçš„å¤šæ ·æ€§ã€‚ æ–¹æ³•åŒå‘LSTM+self-attentionã€‚ åŒå‘çš„LSTMè·å¾—ä¸Šä¸‹æ–‡çš„è¡¨ç¤ºï¼š å› æ­¤å¯ä»¥è·å¾—attentionæƒé‡å‘é‡ï¼š å…¶ä¸­$H:n\times2u,W_{s1}:d_a\times2u ,w_{s2}:d_a$ ï¼Œ$d_a$æ˜¯è¶…å‚ã€‚ ç°å°†å‘é‡$w_{s2}$æ‰©å±•ä¸ºçŸ©é˜µï¼Œäº¦å³æœ‰Multi-hop attentionï¼š $W_{s2}$ç»´åº¦ä¸º$r\times d_a$ï¼Œ$r$ä»£è¡¨äº†headçš„ä¸ªæ•°ã€‚ å› æ­¤æœ€ç»ˆçš„sentence embeddingçŸ©é˜µä¸ºï¼š æ­£åˆ™åŒ–ä¸ºäº†è®©Aå°½å¯èƒ½æœ‰å¤šæ ·æ€§ï¼ˆå› ä¸ºå¦‚æœéƒ½æ˜¯ç›¸ä¼¼çš„ï¼Œé‚£ä¹ˆåˆ™ä¼šæœ‰å†—ä½™æ€§ï¼‰ï¼Œå¼•å…¥å¦‚ä¸‹çš„æ­£åˆ™åŒ–ï¼š åŸå› ï¼šå¯¹äºä¸åŒçš„head $a^i$ä¸$a^j$ï¼Œ$A A^T$æœ‰ï¼š å¦‚æœ$a^i$ä¸$a^j$å¾ˆç›¸ä¼¼é‚£ä¹ˆå°±ä¼šæ¥è¿‘äº1ï¼Œå¦‚æœéå¸¸ä¸ç›¸ä¼¼(no overlay)åˆ™ä¼šæ¥è¿‘äº0ã€‚å› æ­¤æ•´ä¸ªå¼å­å°±æ˜¯:å¸Œæœ›å¯¹è§’çº¿éƒ¨åˆ†æ¥è¿‘äº0ï¼ˆå› ä¸ºå‡äº†å•ä½é˜µï¼‰ï¼Œè¿™å°±ç›¸å½“äºå°½å¯èƒ½focuså°éƒ¨åˆ†çš„è¯ï¼›åŒæ—¶å…¶ä»–éƒ¨åˆ†å°½å¯èƒ½æ¥è¿‘äº0ï¼Œä¹Ÿå³ä¸åŒçš„headä¹‹é—´æ²¡æœ‰overlapã€‚ å¦‚ä½•ä½¿ç”¨æ–‡ç« æåˆ°ï¼Œåœ¨åšåˆ†ç±»çš„æ—¶å€™å¯ä»¥ç›´æ¥å°†çŸ©é˜µMå±•å¼€ï¼Œè¿‡å…¨è¿æ¥å±‚å³å¯ã€‚ 2ï¸âƒ£[Attention-over-Attention Neural Networks for Reading Comprehension]åœ¨å®Œå½¢å¡«ç©ºä»»åŠ¡(Cloze-style Reading Comprehension)ä¸Šæå‡ºä¸€ç§æ–°çš„attentionï¼Œå³nested-attentionã€‚ ä»»åŠ¡æè¿°ä¸‰å…ƒç»„ $ D,Q,A $ï¼Œdocumentï¼Œquestionï¼Œanswerã€‚å…¶ä¸­answerä¸€èˆ¬æ˜¯documentçš„ä¸€ä¸ªè¯ã€‚ æ–¹æ³•æœ¬æ–‡æå‡ºçš„attentionæœºåˆ¶ï¼Œæ˜¯é€šè¿‡ä¸€ä¸ªæ–°çš„attentionå»æŒ‡ç¤ºå¦ä¸€ä¸ªattentionçš„é‡è¦ç¨‹åº¦ã€‚ é¦–å…ˆé€šè¿‡ä¸€å±‚å…±äº«çš„embeddingå±‚ï¼Œå°†documentå’Œqueryéƒ½encodeæˆword embeddingï¼Œç„¶åé€šè¿‡åŒå‘çš„GRUï¼Œå°†éšå±‚æ‹¼æ¥èµ·æ¥æˆä¸ºæ–°çš„è¡¨ç¤ºã€‚ æ¥ç€è·å¾—pair-wise matching matrixï¼š å…¶ä¸­$h$ä»£è¡¨ä¸Šè¿°æåˆ°çš„æ‹¼æ¥èµ·æ¥çš„è¡¨ç¤ºï¼Œ$M(i,j)$ä»£è¡¨äº†documentçš„è¯$i$å’Œquestionçš„è¯$j$ä¹‹é—´çš„åŒ¹é…ç¨‹åº¦ã€‚ æ¥ç€å¯¹columnåšsoftmaxï¼šå…¶ä»£è¡¨çš„æ„ä¹‰å³query-to-document attentionï¼Œäº¦å³å¯¹äºä¸€ä¸ªqueryå†…çš„è¯ï¼Œdocumentçš„æ¯ä¸ªè¯ä¸å…¶åŒ¹é…çš„æƒé‡ã€‚ æ¥ä¸‹æ¥ï¼Œå¯¹rowè¿›è¡Œsoftmaxæ“ä½œï¼šä»£è¡¨çš„æ˜¯ç»™å®šä¸€ä¸ªdocumentçš„è¯ï¼Œqueryçš„å“ªä¸ªè¯æ›´ä¸ºé‡è¦ã€‚ æ¥ä¸‹æ¥æˆ‘ä»¬å°†Î²å¹³å‡èµ·æ¥ï¼Œè·å¾—ä¸€ä¸ªå‘é‡ï¼šè¿™ä¸ªå‘é‡ä»æœ‰attentionçš„æ€§è´¨ï¼Œå³æ‰€æœ‰å…ƒç´ åŠ å’Œä¸º1ã€‚ä»£è¡¨çš„æ˜¯ä»å¹³å‡æ¥çœ‹ï¼Œqueryè¯çš„é‡è¦æ€§ã€‚ æœ€åï¼Œæˆ‘ä»¬å¯¹Î±å’ŒÎ²åšç‚¹ç§¯ä»¥è·å¾—attended document-level attentionï¼š å…¶ä¸­$s$çš„ç»´åº¦æ˜¯$D\times 1$ã€‚sä»£è¡¨çš„æ„ä¹‰å³â€œa weighted sum of each individual document-level attention Î±(t) when looking at query word at time tâ€ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹Î±è¿›è¡ŒåŠ æƒï¼Œä»£è¡¨query wordçš„å¹³å‡é‡è¦ç¨‹åº¦ã€‚ æœ€ç»ˆåœ¨åšå®Œå‹å¡«ç©ºçš„é¢„æµ‹æ—¶ï¼š ä¸ªäººè§‰å¾—è¿™ç§attention-over-attentionçš„æƒ³æ³•è¿˜æ˜¯æŒºæœ‰åˆ›æ–°çš„ã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>attention</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>sentence embedding</tag>
        <tag>nested attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç½‘ç»œä¼˜åŒ–ä¸æ­£åˆ™åŒ–æ€»ç»“]]></title>
    <url>%2F2018%2F11%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[å¤§é‡å‚è€ƒè‡ªã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹ ä¼˜åŒ–ç®—æ³•å¯¹äºæ ‡å‡†çš„SGDï¼Œå¸¸è§çš„æ”¹è¿›ç®—æ³•ä»ä¸¤ä¸ªæ–¹é¢è¿›è¡Œï¼šå­¦ä¹ ç‡è¡°å‡&amp;æ¢¯åº¦æ–¹å‘ä¼˜åŒ–ã€‚è®°$g_t$ä¸ºtæ—¶åˆ»çš„å¯¼æ•°ï¼š å­¦ä¹ ç‡è¡°å‡AdaGradç®—æ³•é€šè¿‡è®¡ç®—å†æ¬¡çš„æ¢¯åº¦å¹³æ–¹ç´¯è®¡å€¼è¿›è¡Œå­¦ä¹ ç‡è¡°å‡ã€‚$G_t$æ˜¯ç´¯è®¡å€¼ï¼š æ›´æ–°å€¼åˆ™ä¸ºï¼š ç¼ºç‚¹ï¼šéšç€è¿­ä»£æ¬¡æ•°çš„å¢åŠ å­¦ä¹ ç‡é€’å‡ã€‚åœ¨ç»è¿‡ä¸€å®šæ¬¡æ•°çš„è¿­ä»£ä¾ç„¶æ²¡æœ‰æ‰¾åˆ°æœ€ä¼˜ç‚¹æ—¶ï¼Œç”±äºè¿™æ—¶çš„å­¦ä¹ ç‡å·²ç»éå¸¸å°ï¼Œå¾ˆéš¾å†ç»§ç»­æ‰¾åˆ°æœ€ä¼˜ç‚¹ã€‚ RMSpropç®—æ³•å¯¹AdaGradçš„æ”¹è¿›ï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äº$G_t$çš„è®¡ç®—ï¼Œå°†å†å²ä¿¡æ¯å’Œå½“å‰ä¿¡æ¯è¿›è¡Œçº¿æ€§åŠ æƒï¼Œä½¿å¾—å­¦ä¹ ç‡å¯ä»¥åŠ¨æ€æ”¹å˜è€Œä¸æ˜¯å•è°ƒé€’å‡ï¼š Î²ä¸ºè¡°å‡ç‡ï¼Œé€šå¸¸å–0.9ã€‚ä¹Ÿå³å†å²ä¿¡æ¯å ä¸»å¯¼ã€‚ AdaDeltaç®—æ³•åŒæ ·æ˜¯å¯¹AdaGradçš„æ”¹è¿›ã€‚æ¯æ¬¡è®¡ç®—ï¼š ä¹Ÿå³å†å²æ›´æ–°å·®å’Œä¸Šä¸€æ—¶åˆ»çš„æ›´æ–°å·®çš„åŠ æƒï¼ˆRMSpropæ˜¯å†å²æ¢¯åº¦å’Œå½“å‰æ¢¯åº¦ï¼‰ã€‚ æœ€ç»ˆæ›´æ–°å·®å€¼ä¸ºï¼š å…¶ä¸­$G_t$è®¡ç®—æ–¹æ³•å’ŒRMSpropä¸€è‡´ã€‚ æ¢¯åº¦æ–¹å‘ä¼˜åŒ–åˆ©ç”¨å†å²çš„æ¢¯åº¦ï¼ˆæ–¹å‘ï¼‰è°ƒæ•´å½“å‰æ—¶åˆ»çš„æ¢¯åº¦ã€‚ åŠ¨é‡ï¼ˆMomentumï¼‰æ³•åŠ¨é‡æ³•ï¼ˆMomentum Methodï¼‰æ˜¯ç”¨ä¹‹å‰ç§¯ç´¯åŠ¨é‡æ¥æ›¿ä»£çœŸæ­£çš„æ¢¯åº¦ã€‚æ¯æ¬¡è¿­ä»£çš„æ¢¯åº¦å¯ä»¥çœ‹ä½œæ˜¯åŠ é€Ÿåº¦ã€‚ ä¹Ÿå³ä¸Šä¸€æ—¶åˆ»çš„æ›´æ–°å·®å€¼å’Œå½“å‰æ¢¯åº¦å…±åŒå†³å®šå½“å‰çš„æ›´æ–°å·®å€¼ã€‚$Ï$ä¸ºåŠ¨é‡å› å­ï¼Œé€šå¸¸ä¸º0.9ã€‚ä¹Ÿå³åŠ¨é‡å äº†ä¸»å¯¼ã€‚ å½“æŸä¸ªå‚æ•°åœ¨æœ€è¿‘ä¸€æ®µæ—¶é—´å†…çš„æ¢¯åº¦æ–¹å‘ä¸ä¸€è‡´æ—¶ï¼Œå…¶çœŸå®çš„å‚æ•°æ›´æ–°å¹…åº¦å˜å°ï¼›ç›¸åï¼Œå½“åœ¨æœ€è¿‘ä¸€æ®µæ—¶é—´å†…çš„æ¢¯åº¦æ–¹å‘éƒ½ä¸€è‡´æ—¶ï¼Œå…¶çœŸå®çš„å‚æ•°æ›´æ–°å¹…åº¦å˜å¤§ï¼Œèµ·åˆ°åŠ é€Ÿä½œç”¨ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œåœ¨è¿­ä»£åˆæœŸï¼Œæ¢¯åº¦æ–¹æ³•éƒ½æ¯”è¾ƒä¸€è‡´ï¼ŒåŠ¨é‡æ³•ä¼šèµ·åˆ°åŠ é€Ÿä½œç”¨ï¼Œå¯ä»¥æ›´å¿«åœ°åˆ°è¾¾æœ€ä¼˜ç‚¹ã€‚åœ¨è¿­ä»£åæœŸï¼Œæ¢¯åº¦æ–¹æ³•ä¼šå–å†³ä¸ä¸€è‡´ï¼Œåœ¨æ”¶æ•›å€¼é™„è¿‘éœ‡è¡ï¼ŒåŠ¨é‡æ³•ä¼šèµ·åˆ°å‡é€Ÿä½œç”¨ï¼Œå¢åŠ ç¨³å®šæ€§ã€‚ NesterovåŠ é€Ÿæ¢¯åº¦åŠ¨é‡æ³•çš„æ”¹è¿›ç‰ˆæœ¬ã€‚ å‰é¢æåˆ°çš„åŠ¨é‡æ³•ï¼Œæ˜¯ä¸Šä¸€æ­¥çš„æ›´æ–°æ–¹å‘$\Delta \theta_{t-1}$ä¸å½“å‰æ¢¯åº¦$-g_t$çš„åŠ å’Œã€‚å› æ­¤å¯ä»¥ç†è§£æˆï¼Œå…ˆæ ¹æ®$âˆ†Î¸_{tâˆ’1}$æ›´æ–°ä¸€æ¬¡å¾—åˆ°å‚æ•°Î¸ï¼Œå†ç”¨$g_t$è¿›è¡Œæ›´æ–°ã€‚äº¦å³ï¼šä¸Šå¼çš„ç¬¬äºŒæ­¥ä¸­ï¼Œ$g_t$æ˜¯åœ¨$ \theta_{t-1}$ä¸Šçš„æ¢¯åº¦ã€‚æˆ‘ä»¬å°†è¯¥æ­¥æ”¹ä¸ºåœ¨$\theta_{t}$çš„æ¢¯åº¦ã€‚å› æ­¤ï¼Œæœ‰ï¼š å’ŒåŠ¨é‡æ³•ç›¸æ¯”ï¼Œç›¸å½“äºæå‰èµ°äº†ä¸€æ­¥ã€‚ Adam&amp;NadamAdamä¸€æ–¹é¢è®¡ç®—æ¢¯åº¦å¹³æ–¹çš„åŠ æƒï¼ŒåŒæ—¶è¿˜è®¡ç®—æ¢¯åº¦çš„åŠ æƒï¼šé€šå¸¸$Î²_1=0.9$ï¼Œ$Î²_2=0.99$ä¹Ÿå³å†å²ä¿¡æ¯å äº†ä¸»å¯¼ã€‚ åœ¨åˆæœŸ$M_t$ä¸$G_t$ä¼šæ¯”çœŸå®å‡å€¼å’Œæ–¹å·®è¦å°ï¼ˆæƒ³è±¡$M_0=0$ï¼Œ$G_0=0$æ—¶ï¼‰ã€‚å› æ­¤å¯¹å…¶è¿›è¡Œä¿®æ­£ï¼Œå³ï¼šå› æ­¤æœ€ç»ˆæœ‰ï¼š åŒç†æœ‰Nadamã€‚ Adam = Momentum + RMSpropNadam = Nesterov + RMSprop æ¢¯åº¦æˆªæ–­ gradient clippingåˆ†ä¸ºæŒ‰å€¼æˆªæ–­ä¸æŒ‰æ¨¡æˆªæ–­ã€‚ å‚æ•°åˆå§‹åŒ–åˆå§‹å€¼é€‰å–å¾ˆå…³é”®ã€‚å‡è®¾å…¨éƒ¨åˆå§‹åŒ–ä¸º0ï¼Œåˆ™åç»­æ›´æ–°å¯¼è‡´æ‰€æœ‰çš„æ¿€æ´»å€¼ç›¸åŒï¼Œä¹Ÿå³å¯¹ç§°æƒé‡ç°è±¡ã€‚ åŸåˆ™ï¼šä¸èƒ½è¿‡å¤§ï¼Œå¦åˆ™æ¿€æ´»å€¼ä¼šå˜å¾—é¥±å’Œï¼Œå¦‚sigmoidï¼›ä¸èƒ½è¿‡å°ï¼Œå¦åˆ™ç»è¿‡å¤šå±‚ä¿¡å·ä¼šé€æ¸æ¶ˆå¤±ï¼Œå¹¶ä¸”å¯¼è‡´sigmoidä¸¢å¤±éçº¿æ€§çš„èƒ½åŠ›ï¼ˆåœ¨0é™„è¿‘åŸºæœ¬è¿‘ä¼¼çº¿æ€§ï¼‰ã€‚å¦‚æœä¸€ä¸ªç¥ç»å…ƒçš„è¾“å…¥è¿æ¥å¾ˆå¤šï¼Œå®ƒçš„æ¯ä¸ªè¾“å…¥è¿æ¥ä¸Šçš„æƒé‡å°±åº”è¯¥å°ä¸€äº›ï¼Œè¿™æ˜¯ä¸ºäº†é¿å…è¾“å‡ºè¿‡å¤§ã€‚ Gaussianåˆ†å¸ƒåˆå§‹åŒ–åŒæ—¶è€ƒè™‘è¾“å…¥è¾“å‡ºï¼Œå¯ä»¥æŒ‰ $N(0,\sqrt{\frac{2}{n_{in} + n_{out}}})$ é«˜æ–¯åˆ†å¸ƒæ¥åˆå§‹åŒ–ã€‚ å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–åœ¨$[-r,r]$åŒºé—´å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–ï¼Œå…¶ä¸­rå¯ä»¥æŒ‰ç…§ç¥ç»å…ƒæ•°é‡è‡ªé€‚åº”è°ƒæ•´ã€‚ Xavieråˆå§‹åŒ–æ–¹æ³•è‡ªåŠ¨è®¡ç®—è¶…å‚rã€‚rçš„å…¬å¼ä¸ºï¼šå…¶ä¸­$n^l$ä»£è¡¨ç¬¬$l$å±‚çš„ç¥ç»å…ƒä¸ªæ•°ã€‚ ä¸ºä»€ä¹ˆæ˜¯è¿™ä¸ªå¼å­ï¼ˆæ¨å¯¼è§å‚è€ƒèµ„æ–™ï¼‰ï¼šç»¼åˆè€ƒè™‘äº†â‘ è¾“å…¥è¾“å‡ºçš„æ–¹å·®è¦ä¸€è‡´ï¼›â‘¡åå‘ä¼ æ’­ä¸­è¯¯å·®ä¿¡å·çš„æ–¹å·®ä¸è¢«æ”¾å¤§æˆ–ç¼©å°ã€‚ å½’ä¸€åŒ–å°†æ•°æ®åˆ†å¸ƒå½’ä¸€åŒ–ï¼Œä½¿å¾—åˆ†å¸ƒä¿æŒç¨³å®šã€‚å‡è®¾æ•°æ®æœ‰å››ç»´(N,C,H,W)ã€‚Nä»£è¡¨batchï¼›Cä»£è¡¨channelï¼›H,Wä»£è¡¨heightå’Œwidthã€‚ Batch Normalizationæ²¿ç€é€šé“è¿›è¡Œå½’ä¸€åŒ–ï¼Œäº¦å³æ¯ä¸ªé€šé“éƒ½æœ‰è‡ªå·±çš„å‡å€¼å’Œæ–¹å·®ã€‚å…¶ä¸­ç¼©æ”¾å¹³ç§»å˜é‡æ˜¯å¯å­¦ä¹ çš„ã€‚ ç¼ºç‚¹ï¼šâ‘ å¯¹batch sizeæ•æ„Ÿï¼Œbatch sizeå¤ªå°åˆ™æ–¹å·®å‡å€¼ä¸è¶³ä»¥ä»£è¡¨æ•°æ®åˆ†å¸ƒâ‘¡å¯¹äºä¸ç­‰é•¿çš„è¾“å…¥å¦‚RNNæ¥è¯´ï¼Œæ¯ä¸€ä¸ªtimestepéƒ½éœ€è¦ä¿å­˜ä¸åŒçš„ç‰¹å¾ã€‚ Layer Normalizationå¯¹ä¸€ä¸ªè¾“å…¥è¿›è¡Œæ­£åˆ™åŒ–ï¼Œäº¦å³æ¯ä¸ªè¾“å…¥éƒ½æœ‰è‡ªå·±çš„æ–¹å·®ã€å‡å€¼ã€‚è¿™æ ·ä¸ä¾èµ–äºbatchå¤§å°å’Œè¾“å…¥sequenceçš„æ·±åº¦ã€‚ å¯¹RNNæ•ˆæœæ¯”è¾ƒæ˜æ˜¾ï¼Œä½†CNNä¸­ä¸å¦‚BN Instance Normalizationå¯¹HWè¿›è¡Œå½’ä¸€åŒ– Group Normalizationå°†channelåˆ†ä¸ºå¤šä¸ªgroupï¼Œæ¯ä¸ªgroupå†…åšå½’ä¸€åŒ– Referenceã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹https://blog.csdn.net/liuxiao214/article/details/81037416]]></content>
      <tags>
        <tag>æ·±åº¦å­¦ä¹ ğŸ¤–</tag>
        <tag>ä¼˜åŒ–ç®—æ³•</tag>
        <tag>å‚æ•°åˆå§‹åŒ–</tag>
        <tag>Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•10]]></title>
    <url>%2F2018%2F11%2F11%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9510%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[get_sinusoid_encoding_table]Transformerç»å¯¹ä½ç½®ã€‚ 12345678910111213141516def get_sinusoid_encoding_table(n_position, d_hid, padding_idx=None): def cal_angle(position, hid_idx): return position / np.power(10000, 2 * (hid_idx // 2) / d_hid) def get_posi_angle_vec(position): return [cal_angle(position, hid_j) for hid_j in range(d_hid)] sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)]) sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) sinusoid_table[:, 0::2] = np.cos(sinusoid_table[:, 0::2]) if padding_idx is not None: sinusoid_table[padding_idx] = 0. return torch.FloatTensor(sinusoid_table) # n_position,embed_dim]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†11]]></title>
    <url>%2F2018%2F11%2F11%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8611%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Optimizer]https://zhuanlan.zhihu.com/p/32262540https://zhuanlan.zhihu.com/p/32338983 Adamç­‰è‡ªé€‚åº”å­¦ä¹ ç‡ç®—æ³•å¯¹äºç¨€ç–æ•°æ®å…·æœ‰ä¼˜åŠ¿ï¼Œä¸”æ”¶æ•›é€Ÿåº¦å¾ˆå¿«ï¼›ä½†ç²¾è°ƒå‚æ•°çš„SGDï¼ˆ+Momentumï¼‰å¾€å¾€èƒ½å¤Ÿå–å¾—æ›´å¥½çš„æœ€ç»ˆç»“æœã€‚ å»ºè®®ï¼šå‰æœŸç”¨Adamï¼Œäº«å—Adamå¿«é€Ÿæ”¶æ•›çš„ä¼˜åŠ¿ï¼›åæœŸåˆ‡æ¢åˆ°SGDï¼Œæ…¢æ…¢å¯»æ‰¾æœ€ä¼˜è§£ã€‚ä»€ä¹ˆæ—¶å€™ä»Adamåˆ‡æ¢åˆ°SGDï¼Ÿå½“SGDçš„ç›¸åº”å­¦ä¹ ç‡çš„ç§»åŠ¨å¹³å‡å€¼åŸºæœ¬ä¸å˜çš„æ—¶å€™ã€‚ 2ï¸âƒ£[Pytorch]LongTensoré™¤ä»¥æµ®ç‚¹æ•°ï¼Œä¼šå¯¹é™¤æ•°è¿›è¡Œå–æ•´ï¼Œå†åšé™¤æ³•ã€‚ 3ï¸âƒ£[Pytorch]ä½¿ç”¨Pytorchçš„DataParallel 1234567891011121314device = torch.device('cuda:' + str( config.CUDA_VISIBLE_DEVICES[0]) if config.use_cuda else 'cpu') # æŒ‡å®šç¬¬ä¸€ä¸ªè®¾å¤‡model = ClassifyModel( vocab_size=len(vocab), max_seq_len=config.max_sent_len, embed_dim=config.embed_dim, n_layers=config.n_layers, n_head=config.n_head, d_k=config.d_k, d_v=config.d_v, d_model=config.d_model, d_inner=config.d_inner_hid, n_label=config.n_label, dropout=config.dropout).to(device)model = DataParallel(model, device_ids=config.CUDA_VISIBLE_DEVICES) # æ˜¾å¼å®šä¹‰device_ids æ³¨æ„åˆ°ï¼šdevice_idsçš„èµ·å§‹ç¼–å·è¦ä¸ä¹‹å‰å®šä¹‰çš„deviceä¸­çš„â€œcuda:0â€ç›¸ä¸€è‡´ï¼Œä¸ç„¶ä¼šæŠ¥é”™ã€‚ å¦‚æœä¸æ˜¾å¼åœ¨ä»£ç ä¸­çš„DataParallelæŒ‡å®šè®¾å¤‡ï¼Œé‚£ä¹ˆéœ€è¦åœ¨å‘½ä»¤è¡Œå†…æŒ‡å®šã€‚å¦‚æœæ˜¯åœ¨å‘½ä»¤è¡Œé‡Œé¢è¿è¡Œçš„ï¼Œä¸”deviceä¸æ˜¯ä»0å¼€å§‹ï¼Œåº”å½“æ˜¾å¼è®¾ç½®GPU_idï¼Œå¦åˆ™ä¼šå‡ºé”™â€˜AssertionError: Invalid device idâ€™ï¼Œæ­£ç¡®çš„å‘½ä»¤ï¼š 1CUDA_VISIBLE_DEVICES=4,5 python -u classify_main.py --gpu_id 0,1]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>Optimizer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³äºsparse gradient]]></title>
    <url>%2F2018%2F11%2F11%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8Esparse%20gradient%2F</url>
    <content type="text"><![CDATA[å‰å‡ å¤©åœ¨çœ‹AllenAIåœ¨EMNLPçš„pptæ—¶ï¼Œæœ‰ä¸€é¡µå†™é“ï¼š ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§æƒ…å†µï¼Ÿ Embeddingæ˜¯ä¸€ä¸ªå¾ˆå¤§çš„çŸ©é˜µï¼Œæ¯ä¸€æ¬¡å…¶å®éƒ½åªæœ‰ä¸€ä¸ªå°éƒ¨åˆ†è¿›è¡Œäº†æ›´æ–°ï¼Œå¯¹äºä¸€äº›è¯æ¥è¯´ï¼Œå‡ºç°çš„é¢‘ç‡ä¸é«˜ï¼Œæˆ–è€…è¯´ï¼Œå…¶å®å¤§éƒ¨åˆ†çš„è¯åœ¨ä¸€ä¸ªloop/epochä¸­ï¼Œè¢«æ›´æ–°çš„æ¬¡æ•°æ˜¯è¾ƒå°‘çš„ã€‚ä½†æ˜¯ï¼Œæ³¨æ„åˆ°ä¸€èˆ¬çš„optimizerç®—æ³•ï¼Œæ˜¯ä»¥matrixä¸ºå•ä½è¿›è¡Œæ›´æ–°çš„ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸€æ¬¡éƒ½æ˜¯$W^{t+1}=W^{t}-\eta \frac{\partial L}{\partial{W}}$ è€ŒAdamç®—æ³•ï¼š åŠ¨é‡å äº†ä¸»å¯¼ã€‚ä½†è¿™æ ·ï¼Œæ¯æ¬¡batchæ›´æ–°ï¼Œé‚£äº›æ²¡è¢«æ›´æ–°çš„è¯ï¼ˆä¹Ÿå³gradient=0ï¼‰çš„åŠ¨é‡ä»ç„¶ä¼šè¢«è¡°å‡ï¼Œæ‰€ä»¥è¿™æ ·å½“åˆ°è¿™ä¸ªè¯æ›´æ–°çš„æ—¶å€™ï¼Œä»–çš„åŠ¨é‡å·²ç»è¢«è¡°å‡å®Œäº†ï¼Œæ‰€ä»¥æ›´æ–°çš„gradientå°±å¾ˆå°ã€‚ è§£å†³æ–¹æ¡ˆï¼š â‘ åœ¨PyTorchä¸­ï¼ŒEmbeddingçš„APIï¼štorch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False, _weight=None) å…¶ä¸­sparse (bool, optional) â€“ if True, gradient w.r.t. weight matrix will be a sparse tensor. å°†sparseè®¾ä¸ºTrueå³å¯ã€‚ â‘¡é’ˆå¯¹sparseçŸ©é˜µï¼Œä½¿ç”¨ä¸åŒçš„optimizerï¼Œå¦‚torch.optim.SparseAdamï¼š Implements lazy version of Adam algorithm suitable for sparse tensors.In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.]]></content>
      <tags>
        <tag>sparse gradient</tag>
        <tag>ä»£ç å®è·µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡5]]></title>
    <url>%2F2018%2F11%2F10%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%875%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Neural Turing Machine]é€šè¿‡æ¨¡ä»¿å†¯è¯ºä¾æ›¼æœºï¼Œå¼•å…¥å¤–éƒ¨å†…å­˜(externel memory)ã€‚ å’Œæ™®é€šç¥ç»ç½‘ç»œä¸€æ ·ï¼Œä¸å¤–ç•Œäº¤äº’ï¼Œè·å¾—ä¸€ä¸ªè¾“å…¥ï¼Œäº§ç”Ÿä¸€ä¸ªè¾“å‡ºã€‚ä½†ä¸åŒçš„æ˜¯ï¼Œå†…éƒ¨è¿˜æœ‰ä¸€ä¸ªmemoryè¿›è¡Œè¯»å†™ã€‚å‡è®¾memoryæ˜¯ä¸€ä¸ªN Ã— Mçš„çŸ©é˜µï¼ŒNæ˜¯å†…å­˜çš„ä½ç½®æ•°é‡ã€‚ è¯»å†™memoryâ‘ è¯»å…¶ä¸­è¯»çš„æ—¶å€™å¯¹å„å†…å­˜ä½ç½®çº¿æ€§åŠ æƒã€‚wæ˜¯å½’ä¸€åŒ–æƒé‡ã€‚ â‘¡å†™$e_t$æ˜¯æ“¦é™¤å‘é‡ï¼ˆerase vectorï¼‰ $a_t$æ˜¯åŠ å’Œå‘é‡(add vector) å…·ä½“å¦‚ä½•è·å¾—æƒé‡å°±ä¸è¯´äº†ã€‚ Controller networkä¸­é—´çš„controller networkå¯ä»¥æ˜¯ä¸€ä¸ªæ™®é€šçš„feed forwardæˆ–è€…RNNã€‚ åœ¨å®é™…ä¸­NTMç”¨å¾—å¹¶ä¸å¤šã€‚ 2ï¸âƒ£[Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling]ELMoçš„ç²¾ç®€ç‰ˆï¼Œé€šè¿‡å³æ’å³ç”¨çš„æ–¹æ³•æ¥å‹ç¼©è¯­è¨€æ¨¡å‹ï¼Œå¯¹ç‰¹å®šä»»åŠ¡å‰ªæä¸åŒçš„å±‚ï¼Œä½¿å¾—èƒ½å¤Ÿå‡å°‘inferenceçš„æ—¶é—´ã€‚è¿™ç¯‡çš„ideaæŒºæœ‰åˆ›æ–°çš„ï¼Œä½†ä¼¼ä¹æœ‰äº›trivialçš„æ„Ÿè§‰ã€‚ RNN and Dense Connectivityæ¯ä¸€å±‚çš„è¾“å‡ºéƒ½ä¼šä¼ åˆ°æ‰€æœ‰å±‚ä½œä¸ºè¾“å…¥ï¼Œå› æ­¤å¯¹äºLå±‚çš„è¾“å…¥ï¼š è¿™æ ·æˆ‘ä»¬å°±èƒ½å¤Ÿéšæ„åœ°å»æ‰ä»»æ„ä¸­é—´å±‚äº†ã€‚åŒæ—¶ä¸€äº›è¯­è¨€ä¿¡æ¯ä¹Ÿåˆ†æ•£åˆ°å„ä¸ªå±‚ï¼Œå³ä½¿å»æ‰æŸäº›å±‚ä¹Ÿæ²¡æœ‰å…³ç³»ã€‚ åˆ™æœ€ç»ˆçš„outputä¸ºï¼š æœ€ç»ˆä½œprojectionåˆ°æ­£å¸¸ç»´åº¦ï¼ˆåœ¨æ¯å±‚éƒ½ä¼šè¿™ä¹ˆåšï¼Œå°†è¾“å…¥é™ç»´åˆ°æ­£å¸¸ç»´åº¦å†è¾“å…¥ï¼‰ï¼š å†åšä¸€ä¸ªsoftmaxï¼š ç”±äº $h^{â€»}$ ç”¨äºsoftmaxï¼Œæ‰€ä»¥å¯èƒ½å’Œtarget wordï¼Œä¹Ÿå³ä¸‹ä¸€ä¸ªè¯æ¯”è¾ƒç›¸ä¼¼ï¼Œå› æ­¤å¯èƒ½æ²¡æœ‰å¾ˆå¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ æ‰€ä»¥æœ€ç»ˆæˆ‘ä»¬ä½¿ç”¨$h_t$ï¼Œä»¥åŠåå‘çš„$h_t^r$ï¼Œå†è¿‡ä¸€å±‚çº¿æ€§å±‚è·å¾—æœ€ç»ˆçš„embeddingï¼ˆå’ŒELMoæœ‰äº›ä¸åŒï¼ŒELMoæ˜¯ç›´æ¥æ‹¼èµ·æ¥ï¼‰ï¼š Layer Selectionæˆ‘ä»¬åœ¨æ¯å±‚çš„outputéƒ½åŠ ä¸€ä¸ªæƒé‡ç³»æ•°ã€‚ æˆ‘ä»¬å¸Œæœ›åœ¨target taskä¸Šç”¨çš„æ—¶å€™ï¼Œéƒ¨åˆ†zèƒ½å¤Ÿå˜æˆ0ï¼Œè¾¾åˆ°layer selectionçš„æ•ˆæœï¼ŒåŠ å¿«inferenceçš„é€Ÿåº¦ã€‚ äº¦å³ï¼š ä¸€ç§ç†æƒ³çš„æ–¹æ³•æ˜¯L0æ­£åˆ™åŒ–ï¼š ä½†ç”±äºæ²¡åŠæ³•æ±‚å¯¼ï¼Œå› æ­¤ï¼Œé‡‡ç”¨L1æ­£åˆ™åŒ–ï¼šä½†ä½¿ç”¨L1æ­£åˆ™åŒ–æœ‰ä¸€å®šçš„é£é™©ï¼Œå› ä¸ºå¦‚æœè®©æ‰€æœ‰zéƒ½è¿œç¦»1ï¼Œé‚£ä¹ˆä¼šå½±å“performanceã€‚ å¼•å…¥æ–°çš„æ­£åˆ™åŒ–æ–¹æ³•$R_2 =\delta(|z|_0&gt;\lambda_1) |z|_1$äº¦å³ï¼Œåªæœ‰åœ¨éé›¶zçš„ä¸ªæ•°å¤§äºæŸä¸ªé˜ˆå€¼æ—¶ï¼Œæ‰èƒ½æœ‰æ­£åˆ™åŒ–æ•ˆæœï¼Œä¿è¯éé›¶çš„ä¸ªæ•°ã€‚â€™it can be â€œturned-offâ€ after achieving a satisfying sparsityâ€™. è¿›ä¸€æ­¥å¼•å…¥$R_3=\delta(|z|_0&gt;\lambda_1) |z|_1 + |z(1-z)|_1$å…¶ä¸­ç¬¬äºŒé¡¹ä¸ºäº†é¼“åŠ±zå‘0æˆ–1èµ°ã€‚ Layer-wise Dropoutéšæœºåˆ é™¤éƒ¨åˆ†layerï¼Œè¿™äº›layerçš„è¾“å‡ºä¸ä¼šä¼ å…¥ä¹‹åçš„å±‚ï¼Œä½†ä»ç„¶ä¼šå‚ä¸æœ€åçš„representationè®¡ç®—ã€‚ è¿™ç§dropoutä¼šè®©perplexityæ›´é«˜ï¼Œä½†å¯¹ç”Ÿæˆæ›´å¥½çš„representationæœ‰å¸®åŠ©ã€‚ 3ï¸âƒ£[Constituency Parsing with a Self-Attentive Encoder]å…¶ä¸­çš„positional encodingæˆ‘æ¯”è¾ƒæ„Ÿå…´è¶£ã€‚åŸç‰ˆçš„positional encodingæ˜¯ç›´æ¥å’Œembeddingç›¸åŠ çš„ã€‚äº¦å³ï¼šé‚£ä¹ˆåœ¨selt-attentionæ—¶ï¼Œæœ‰ï¼šè¿™æ ·ä¼šæœ‰äº¤å‰é¡¹ï¼šè¯¥é¡¹æ²¡æœ‰ä»€ä¹ˆæ„ä¹‰ï¼Œä¸”å¯èƒ½ä¼šå¸¦æ¥è¿‡æ‹Ÿåˆã€‚ å› æ­¤åœ¨è¿™è¾¹å°†positional encodingå’Œembeddingæ‹¼èµ·æ¥ï¼Œäº¦å³ï¼š å¹¶ä¸”ï¼Œåœ¨è¿›å…¥multi-headæ—¶çš„çº¿æ€§å±‚ä¹Ÿåšæ”¹å˜ï¼š è¿™æ ·åœ¨ç›¸ä¹˜çš„æ—¶å€™å°±ä¸ä¼šæœ‰äº¤å‰é¡¹äº†ã€‚ å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æœ‰ä¸€å®šçš„æå‡ã€‚ 4ï¸âƒ£[DropBlock: A regularization method for convolutional networks]å¤§è‡´ç¿»äº†ä¸€ä¸‹ã€‚Motivation:åœ¨CNNä¸­ï¼Œdropoutå¯¹convolutional layerçš„ä½œç”¨ä¸å¤§ï¼Œä¸€èˆ¬éƒ½åªç”¨åœ¨å…¨è¿æ¥å±‚ã€‚ä½œè€…æ¨æµ‹ï¼Œå› ä¸ºæ¯ä¸ªfeature mapéƒ½æœ‰ä¸€ä¸ªæ„Ÿå—é‡èŒƒå›´ï¼Œä»…ä»…å¯¹å•ä¸ªåƒç´ è¿›è¡Œdropoutå¹¶ä¸èƒ½é™ä½feature mapå­¦ä¹ çš„ç‰¹å¾èŒƒå›´ï¼Œäº¦å³ç½‘ç»œä»å¯ä»¥é€šè¿‡è¯¥ä½ç½®çš„ç›¸é‚»ä½ç½®å…ƒç´ å»å­¦ä¹ å¯¹åº”çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä¹Ÿå°±ä¸ä¼šä¿ƒä½¿ç½‘ç»œå»å­¦ä¹ æ›´åŠ é²æ£’çš„ç‰¹å¾ã€‚ å› æ­¤ä½œè€…çš„åšæ³•æ˜¯ï¼Œdropoutä¸€æ•´å—ä½ç½®ã€‚ 5ï¸âƒ£[Accelerating Neural Transformer via an Average Attention Network]æå‡ºäº†AAN(average attention network)ï¼Œå¯¹transformerç¿»è¯‘æ¨¡å‹çš„decodeéƒ¨åˆ†è¿›è¡Œæ”¹è¿›ï¼ŒåŠ é€Ÿäº†è¿‡ç¨‹ã€‚ ç”±äºTransformeråœ¨decodeé˜¶æ®µéœ€è¦ç”¨åˆ°å‰é¢æ‰€æœ‰çš„yï¼Œä¹Ÿå³è‡ªå›å½’(auto-regressive)çš„æ€§è´¨ï¼Œæ‰€ä»¥æ— æ³•å¹¶è¡Œï¼š è¿‡ç¨‹ç»™å®šyï¼š é¦–å…ˆå°†ä»–ä»¬åŠ èµ·æ¥ï¼Œè¿‡ä¸€å±‚å…¨è¿æ¥ï¼šè¿™ä¹Ÿç›¸å½“äºå°±æ˜¯è®©æ‰€æœ‰çš„yæœ‰ç›¸åŒçš„æƒé‡ï¼Œæ­¤æ—¶gå°±æ˜¯ä¸Šä¸‹æ–‡ç›¸å…³çš„è¡¨ç¤ºã€‚ æ¥ä¸‹æ¥æ·»åŠ ä¸€ä¸ªgatingï¼šæ§åˆ¶äº†ä»è¿‡å»ä¿å­˜å¤šå°‘ä¿¡æ¯å’Œè·å–å¤šå°‘æ–°çš„ä¿¡æ¯ã€‚ å’ŒTransformeråŸç‰ˆè®ºæ–‡ä¸€æ ·ï¼Œæ·»åŠ ä¸€ä¸ªresidual connectionï¼š å¦‚å›¾æ•´ä¸ªè¿‡ç¨‹ï¼š æ€»ç»“ï¼šAAN=average layer+gating layer åŠ é€Ÿâ‘ è€ƒè™‘åˆ°åŠ å’Œæ“ä½œæ˜¯åºåˆ—åŒ–çš„ï¼Œåªèƒ½ä¸€ä¸ªä¸€ä¸ªæ¥ï¼Œä¸èƒ½å¹¶è¡Œï¼Œåœ¨è¿™é‡Œä½¿ç”¨ä¸€ä¸ªmaskçš„trickï¼Œä½¿å¾—åœ¨è®­ç»ƒæ—¶ä¹Ÿèƒ½å¤Ÿå¹¶è¡Œï¼š â‘¡åœ¨inferenceæ—¶çš„åŠ é€Ÿï¼š è¿™æ ·Transformerå°±èƒ½å¤Ÿç±»ä¼¼RNNï¼Œåªè€ƒè™‘å‰ä¸€ä¸ªçš„stateï¼Œè€Œä¸æ˜¯å‰é¢æ‰€æœ‰çš„stateã€‚ æœ€ç»ˆçš„æ¨¡å‹ï¼š]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>Transformer</tag>
        <tag>dropout</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>self-attention</tag>
        <tag>NTM</tag>
        <tag>ELMo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯15]]></title>
    <url>%2F2018%2F11%2F10%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D15%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£èœ€ç›¸[å”] æœç”«ä¸ç›¸ç¥ å ‚ä½•å¤„å¯»ï¼Œé”¦å®˜åŸå¤–æŸæ£®æ£®ã€‚æ˜ é˜¶ç¢§è‰è‡ªæ˜¥è‰²ï¼Œéš”å¶é»„é¹‚ç©ºå¥½éŸ³ã€‚ä¸‰é¡¾é¢‘çƒ¦å¤©ä¸‹è®¡ï¼Œä¸¤æœå¼€æµè€è‡£å¿ƒã€‚å‡ºå¸ˆæœªæ·èº«å…ˆæ­»ï¼Œé•¿ä½¿è‹±é›„æ³ªæ»¡è¥Ÿã€‚ http://m.xichuangzhu.com/work/57b93410a633bd00665efd4a]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡4]]></title>
    <url>%2F2018%2F11%2F04%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%874%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Character-Level Language Modeling with Deeper Self-Attention]å°†transformerç”¨äºcharacter-levelçš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œé€šè¿‡æ·»åŠ å¤šä¸ªlossæ¥æé«˜å…¶è¡¨ç°ä»¥åŠåŠ å¿«æ‹Ÿåˆé€Ÿåº¦ï¼ŒåŒæ—¶åŠ æ·±transformerçš„å±‚æ•°ï¼Œæå¤§æå‡è¡¨ç°ï¼Œ12å±‚çš„transformer layerèƒ½è¾¾åˆ°SOTAï¼Œè€Œ64å±‚åˆ™æœ‰æ›´å¤šçš„æå‡ã€‚ æ™®é€šRNNç”¨äºcharacter-level language modelï¼šå°†å¥å­æŒ‰characterä¸ºå•ä½ç»„æˆå¤šä¸ªbatchï¼Œæ¯ä¸ªbatché¢„æµ‹æœ€åä¸€ä¸ªè¯ï¼Œç„¶åå°†è¯¥batchçš„éšçŠ¶æ€ä¼ å…¥ä¸‹ä¸€ä¸ªbatchã€‚ä¹Ÿå³â€œtruncated backpropagation through timeâ€ (TBTT)ã€‚ å¦‚æœç”¨åœ¨Transformerï¼Œå¦‚ä¸‹å›¾ï¼Œæˆ‘ä»¬åªé¢„æµ‹$t_4$ã€‚ æœ¬æ–‡çš„ä¸€å¤§è´¡çŒ®æ˜¯å¤šåŠ äº†ä¸‰ç§lossï¼Œå¹¶ä¸”æœ‰äº›lossçš„æƒå€¼ä¼šéšç€è®­ç»ƒçš„è¿‡ç¨‹è€Œé€æ¸å‡å°ï¼Œæ¯ä¸ªlosséƒ½ä¼šè‡ªå·±çš„scheduleã€‚è¿™äº›lossåŠ å¿«äº†æ‹Ÿåˆé€Ÿåº¦ï¼ŒåŒæ—¶ä¹Ÿæå‡äº†è¡¨ç°ã€‚ LossMultiple Positionså¯¹äºbatchå†…è€Œè¨€ï¼Œæ¯ä¸ªæ—¶é—´æ­¥téƒ½è¦é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚ Intermediate Layer Lossesè¦æ±‚ä¸­é—´å±‚ä¹Ÿåšå‡ºé¢„æµ‹ï¼š åœ¨è¿™é‡Œï¼Œè¶Šåº•å±‚çš„layerå…¶lossæƒå€¼è¶Šä½ã€‚ Multiple Targetsæ¯ä¸€ä¸ªpositionï¼Œä¸ä»…ä»…è¦é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œè¿˜è¦é¢„æµ‹ä¸‹å‡ ä¸ªè¯ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯å’Œé¢„æµ‹ä¸‹å‡ ä¸ªè¯çš„åˆ†ç±»å™¨æ˜¯ç‹¬ç«‹çš„ã€‚ Positional embeddingæ¯ä¸€å±‚çš„éƒ½æ·»åŠ ä¸€ä¸ªä¸å…±äº«çš„å¯å­¦ä¹ çš„positional embeddingã€‚ 2ï¸âƒ£[Self-Attention with Relative Position Representations]æå‡ºä½¿ç”¨ç›¸å¯¹ä½ç½®æ›¿ä»£Transformerçš„ç»å¯¹ä½ç½®ä¿¡æ¯ï¼Œå¹¶åœ¨NMTä¸Šæœ‰ä¸€å®šçš„æå‡ã€‚ åˆ†è§£ï¼šåœ¨åŸå…ˆçš„self-attentionä¸­ï¼Œè¾“å‡ºä¸ºï¼š å…¶ä¸­ï¼š ç°åœ¨æˆ‘ä»¬è€ƒè™‘æ·»åŠ ç›¸å¯¹ä½ç½®ï¼Œå…¶ä¸­ç›¸å¯¹ä½ç½®ä¿¡æ¯åœ¨å„å±‚éƒ½æ˜¯å…±äº«çš„ï¼š $a_{ij}^K$çš„å…·ä½“å½¢å¼ï¼šä¸Šå¼ä¸ºäº†é™ä½å¤æ‚åº¦ï¼Œä¸è€ƒè™‘é•¿äºkçš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚ è€ƒè™‘åˆ°transformerçš„å¹¶è¡Œæ€§ï¼Œä¸ºäº†å¹¶è¡Œæ€§ï¼Œæˆ‘ä»¬è€ƒè™‘å¦‚ä¸‹å¼å­ï¼šå…¶ä¸­ï¼Œç¬¬ä¸€é¡¹å’ŒåŸæ¥çš„Transformerä¸€è‡´ï¼›ç¬¬äºŒé¡¹ï¼Œé€šè¿‡reshapeå¯ä»¥è¾¾åˆ°å¹¶è¡Œçš„æ•ˆæœï¼Œç„¶åä¸¤é¡¹ç›´æ¥åŠ èµ·æ¥ã€‚ å®éªŒè¯æ˜ï¼Œä½¿ç”¨ç›¸å¯¹ä½ç½®æ•ˆæœæ˜¯æœ‰ä¸€å®šçš„æå‡çš„ï¼Œè€ŒåŒæ—¶ä½¿ç”¨ç»å¯¹ä½ç½®å’Œç›¸å¯¹ä½ç½®å¹¶æ²¡æœ‰æå‡ã€‚ 3ï¸âƒ£[WEIGHTED TRANSFORMER NETWORK FOR MACHINE TRANSLATION]è¿™ç¯‡è¢«ICLRæ‹’äº†ï¼Œä½†æœ‰å®¡ç¨¿äººæ‰“äº†9åˆ†çš„é«˜åˆ†ã€‚ å¯¹Transformerè¿›è¡Œæ”¹è¿›ï¼Œæ‹¥æœ‰æ›´å¥½çš„æ•ˆæœå’Œæ›´å°çš„è®¡ç®—ä»£ä»·ã€‚ ä¼ ç»Ÿçš„Transformerï¼š Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})Vhead_i=Attention(QW_i^Q,KW_i^K,VW_i^V)MultiHead(Q,K,V)=Concat_i (head_i)W^OFFN(x)=max(0,xW_1+b_1)W_2 + b_2åœ¨æœ¬æ–‡ä¸­ï¼Œå…ˆå¯¹headè¿›è¡Œå‡ç»´å¹¶ä¹˜ä»¥æƒé‡ï¼Œè¿‡äº†FNNåï¼Œå†ä¹˜ä»¥å¦ä¸€ä¸ªæƒé‡ã€‚å…¶ä¸­æƒé‡$\alpha$ $ \kappa$ä¸ºå¯å­¦ä¹ å‚æ•°ï¼š head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)\overline{head_i}=head_i W^{O_i} \times \kappa_iBranchedAttention(Q,K,V)=\sum_{i=1}^{M} \alpha_i FFN(\overline{head}_i)å…¶ä¸­è¦æ±‚æƒé‡ä¹‹å’Œä¸º1ã€‚å³$\sum_{i=1}^{M}\alpha_i=1$,$\sum_{i=1}^{M}\kappa_i=1$ã€‚ æ–‡ä¸­å¯¹$\kappa$å’Œ$\alpha$ä½œäº†è§£é‡Šã€‚ Îº can be interpreted as a learned concatenation weight and Î± as the learned addition weight é€šè¿‡å®éªŒï¼Œå‘ç°è¯¥æ¨¡å‹ä¼šæœ‰æ›´å¥½çš„æ­£åˆ™åŒ–ç‰¹æ€§ã€‚åŒæ—¶æ•ˆæœä¹Ÿæœ‰ä¸€å®šæå‡ï¼Œæ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼š 4ï¸âƒ£[You May Not Need Attention]ç²—ç•¥åœ°è¿‡äº†ä¸€éï¼Œä¸€äº›ç»†èŠ‚æ²¡æœ‰å¼„æ˜ç™½ã€‚ æå‡ºä¸€ç§å°†encoder-decoderèåˆèµ·æ¥çš„æ¨¡å‹ï¼Œä¹Ÿå³eager translation modelï¼Œä¸éœ€è¦attentionï¼Œèƒ½å¤Ÿå®ç°å³æ—¶çš„ç¿»è¯‘ï¼Œä¹Ÿå³è¯»å…¥ä¸€ä¸ªè¯å°±èƒ½ç¿»è¯‘ä¸€ä¸ªè¯ï¼ŒåŒæ—¶ä¸éœ€è¦è®°å½•encoderçš„æ‰€æœ‰è¾“å‡ºï¼Œå› æ­¤éœ€è¦å¾ˆå°‘çš„å†…å­˜ã€‚ åˆ†ä¸ºä¸‰æ­¥ï¼šâ‘ pre-processingè¿›è¡Œé¢„å¤„ç†ï¼Œä½¿å¾—æºå¥å­å’Œç›®æ ‡å¥å­æ»¡è¶³eager feasible for every aligned pair of words $(s_i , t_j ), i â‰¤ j$ã€‚ é¦–å…ˆé€šè¿‡ç°æˆçš„å·¥å…·è¿›è¡Œå¯¹é½æ“ä½œ(alignment)ï¼Œç„¶åå¯¹äºé‚£äº›ä¸ç¬¦åˆeager feasibleçš„æœ‰å…·ä½“ç®—æ³•ï¼ˆæ²¡è®¤çœŸçœ‹ï¼‰è¿›è¡Œè¡¥paddingã€‚å¦‚å›¾ æˆ‘ä»¬è¿˜å¯ä»¥åœ¨target sentenceçš„å¼€å¤´æ·»åŠ bä¸ªpaddingï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¼€å§‹é¢„æµ‹ä¹‹å‰è·å–æ›´å¤šçš„source sentenceçš„è¯ã€‚ â‘¡æ¨¡å‹ä¸¤å±‚çš„LSTMï¼Œè¾“å…¥æ˜¯ä¸Šä¸€æ¬¡çš„yå’Œå½“å‰çš„xæ‹¼æ¥èµ·æ¥ç›´æ¥ä¼ è¿›å»ã€‚ â‘¢post processingåœ¨æœ€ç»ˆç»“æœä¹‹å‰ï¼Œå°†paddingå»æ‰ã€‚ åœ¨inferenceï¼ˆä¹Ÿå³beam searchï¼‰æ—¶ï¼Œè¿˜æœ‰å‡ ä¸ªæ“ä½œ/trickï¼š Padding limit Source padding injection SPI å®éªŒè¡¨æ˜ï¼Œeager modelåœ¨é•¿çš„å¥å­è¡¨ç°è¶…è¿‡ä¼ ç»Ÿå¸¦attentionçš„NMTï¼Œè€Œé•¿å¥å­çš„å»ºæ¨¡æ­£æ˜¯attention-based çš„æ¨¡å‹çš„ä¸€å¤§æŒ‘æˆ˜ï¼›è€Œåœ¨çŸ­å¥å­ä¸Šå°±ä¸å¦‚attention-basedçš„NMTã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>Transformer</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>NMT</tag>
        <tag>Language Modeling</tag>
        <tag>self-attention</tag>
        <tag>relative position</tag>
        <tag>positional encoding</tag>
        <tag>eager translation model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯14]]></title>
    <url>%2F2018%2F11%2F04%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D14%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£é¹¤å†²å¤©[å®‹] æŸ³æ°¸é»„é‡‘æ¦œä¸Šï¼Œå¶å¤±é¾™å¤´æœ›ã€‚æ˜ä»£æš‚é—è´¤ï¼Œå¦‚ä½•å‘ï¼Ÿæœªé‚é£äº‘ä¾¿ï¼Œäº‰ä¸æ£æ¸¸ç‹‚è¡ã€‚ä½•é¡»è®ºå¾—ä¸§ï¼Ÿæ‰å­è¯äººï¼Œè‡ªæ˜¯ç™½è¡£å¿ç›¸ã€‚çƒŸèŠ±å··é™Œï¼Œä¾çº¦ä¸¹é‘å±›éšœã€‚å¹¸æœ‰æ„ä¸­äººï¼Œå ªå¯»è®¿ã€‚ä¸”æåçº¢å€šç¿ ï¼Œé£æµäº‹ï¼Œå¹³ç”Ÿç•…ã€‚é‘æ˜¥éƒ½ä¸€é¥·ã€‚å¿æŠŠæµ®åï¼Œæ¢äº†æµ…æ–Ÿä½å”±ï¼ æ£ï¼ˆzÃ¬ï¼‰ï¼šæ”¾çºµï¼Œéšå¿ƒæ‰€æ¬²ã€‚æï¼ˆnÃ¨nï¼‰ï¼šå¦‚æ­¤ã€‚ http://m.xichuangzhu.com/work/57aeff68a633bd0057f7d406]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•9]]></title>
    <url>%2F2018%2F11%2F04%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%959%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[collate_fn]å°†ä¸ç­‰é•¿å¥å­ç»„åˆæˆbatchã€‚ 1234567891011121314151617def collate_fn(insts): ''' Pad the instance to the max seq length in batch ''' max_len = max(len(inst) for inst in insts) batch_seq = np.array([ inst + [Constants.PAD] * (max_len - len(inst)) for inst in insts]) batch_pos = np.array([ [pos_i + 1 if w_i != Constants.PAD else 0 for pos_i, w_i in enumerate(inst)] for inst in batch_seq]) # ä½ç½®ä¿¡æ¯ batch_seq = torch.LongTensor(batch_seq) batch_pos = torch.LongTensor(batch_pos) return batch_seq, batch_pos]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[â€œè±æ–¯æ¯â€æŒ‘æˆ˜èµ›æœ‰æ„Ÿ]]></title>
    <url>%2F2018%2F10%2F30%2F%E8%A7%81%E9%97%BB%26%E6%83%B3%E6%B3%95%2F%E2%80%9C%E8%8E%B1%E6%96%AF%E6%9D%AF%E2%80%9D%E6%8C%91%E6%88%98%E8%B5%9B%E6%9C%89%E6%84%9F%2F</url>
    <content type="text"><![CDATA[å†æ—¶ä¸‰ä¸ªæœˆçš„â€œè±æ–¯æ¯â€å…¨å›½ç¬¬ä¸€å±Šâ€œå†›äº‹æ™ºèƒ½Â·æœºå™¨é˜…è¯»â€æŒ‘æˆ˜èµ›ç»ˆäºè½ä¸‹å¸·å¹•ï¼Œå‰å‡ æ—¥ï¼ˆ10.26-10.28ï¼‰æœ‰å¹¸åœ¨å—äº¬é’æ—…å®¾é¦†å‚ä¸å†³èµ›ï¼Œä½“éªŒå¤šå¤šï¼Œæ”¶è·æ»¡æ»¡ï¼Œå¿ƒä¸­äº¦æœ‰ä¸€äº›æ„Ÿæƒ³ã€‚ ä¸€ä¸ªæ˜¯å—äº¬æ€»å¸¦ç»™æˆ‘ä¸€ç§å›å®¶çš„æ„Ÿè§‰ï¼Œå¯¹å—äº¬çš„äº‹ç‰©æ€»æœ‰äº²åˆ‡æ„Ÿã€‚ç¬¬ä¸€æ¬¡æ¥å—äº¬æ˜¯ä¸€å¹´åŠå‰ï¼Œä¹Ÿæ˜¯æ¥å‚åŠ æ¯”èµ›ã€‚å‘¨äº”æ™šä¸Šçš„å¤œæ¸¸ç§¦æ·®ï¼Œè®©æˆ‘æ„Ÿå—åˆ°è®¸ä¹…æœªæ›¾æ„Ÿå—åˆ°çš„çƒŸç«æ°”æ¯ã€‚ ç¬¬äºŒä¸ªæ˜¯æ­¤æ¬¡ä¸»åŠæ–¹æä¾›çš„é£Ÿå®¿ä»¤äººæƒŠå–œã€‚ä¸€å¼€å§‹å¬åˆ°é’æ—…å®¾é¦†ï¼Œæˆ‘å·²ç»åšå¥½äº†è‰°è‹¦å¥‹æˆ˜çš„å‡†å¤‡äº†ï¼Œç„¶è€Œé…’åº—æ˜¯æ˜Ÿçº§é…’åº—çš„ï¼Œåƒæ–¹é¢ç›´æ¥åˆ°æ¥¼ä¸‹çš„è‡ªåŠ©ã€‚å¯ä»¥çœ‹å‡ºä¸»åŠæ–¹æ­¤æ¬¡ç¡®å®ç”¨å¿ƒåœ¨ä¸¾åŠè¿™æ¬¡æ¯”èµ›ã€‚ ç¬¬ä¸‰ç‚¹æ˜¯å…³äºæ¯”èµ›çš„ï¼Œå…³äºæ¯”èµ›çš„æ•´ä¸ªå†ç¨‹æˆ‘è¿˜æ˜¯é¢‡æœ‰æ„Ÿè§¦ã€‚æˆ‘ä»¬æ˜¯ä»¥ç¬¬9åçš„æˆç»©æŒºè¿›å†³èµ›ï¼Œå…¶å®åœ¨åæœŸæ¯”èµ›ä¸­ï¼Œæˆ‘ä»¬éƒ½æœ‰æ‰€æ‡ˆæ€ äº†ï¼Œå‡ ä¹æ²¡æœ‰èŠ±æ—¶é—´åœ¨è¿™ä¸Šé¢ï¼Œ10æœˆåˆå‘å¸ƒå†³èµ›çš„æ•°æ®é›†ï¼Œè€Œæˆ‘ä»¬åœ¨10æœˆ20æ—¥æ‰å¾—çŸ¥è¿™ä¸€äº‹æƒ…ï¼Œæ­¤æ—¶ç¦»å†³èµ›åªå‰©ä¸€å‘¨æ—¶é—´ã€‚å› æ­¤æˆ‘ä»¬ç¡®å®å‡†å¤‡ä¸è¶³ã€‚å½“ç„¶æˆ‘ä»¬ä¹Ÿæ²¡æœ‰é¢„æ–™åˆ°æˆ‘ä»¬çš„å†³èµ›æˆç»©ä¼šè¿™ä¹ˆé å‰ï¼Œå¦åˆ™æˆ‘ä»¬è‚¯å®šä¼šæ›´åŠ å……åˆ†å»å‡†å¤‡ã€‚è¿™ç¡®å®æ˜¯æˆ‘ä»¬çš„å¤±è¯¯ã€‚ æˆ‘ä»¬åœ¨æ¯”èµ›è¿‡ç¨‹ä¸­ï¼Œä¸€ç›´å°è¯•åœ¨ä½¿ç”¨ELMoï¼Œè¿™æ­£æ˜¯æˆ‘è´Ÿè´£çš„éƒ¨åˆ†ã€‚ä¸€å¼€å§‹ä½¿ç”¨å®˜æ–¹TensorFlowçš„ä»£ç ï¼Œè´¹äº†ä¹ç‰›äºŒè™ä¹‹åŠ›æˆ‘æ‰è·‘é€šä»£ç ï¼Œä½†å› ä¸ºé˜Ÿé•¿ä½¿ç”¨çš„æ˜¯pytorchï¼Œè€ŒäºŒè€…åœ¨cudaç‰ˆæœ¬ä¸Šä¸å…¼å®¹ï¼Œå› æ­¤åœ¨åˆèµ›æˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨ELMoã€‚è€Œåœ¨æœ€åå‡ å¤©ï¼Œæˆ‘å°è¯•ä½¿ç”¨å“ˆå·¥å¤§çš„pytorchè®­ç»ƒä»£ç ï¼Œä½†å› ä¸ºinferenceé€Ÿåº¦å®åœ¨å¤ªæ…¢ï¼Œæˆ‘ä»¬æœ€ç»ˆè¿˜æ˜¯å¼ƒç”¨äº†è¿™ä¸ªæ–¹æ¡ˆã€‚è€Œåœ¨å†³èµ›ç°åœºï¼Œæˆ‘ä»¬å‘ç°ä¹Ÿç¡®å®æ˜¯å› ä¸ºé€Ÿåº¦å’Œèµ„æºçš„åŸå› ï¼Œå¤§å®¶éƒ½æ²¡æœ‰ä½¿ç”¨ELMoï¼Œé™¤äº†ä¸€ç»„ã€‚è¯¥ç»„æ­£æ˜¯å‡­å€Ÿäº†ELMoå¼¯é“è¶…è½¦ä»ç¬¬7å‡åˆ°äº†ç¬¬ä¸€ï¼Œæ‹¿èµ°äº†20ä¸‡å¤§å¥–ã€‚è¿™ä¹Ÿæ˜¯æˆ‘ä»¬éå¸¸é—æ†¾çš„ä¸€ä¸ªåœ°æ–¹ï¼Œæˆ‘ä»¬åœ¨é‡åˆ°å›°éš¾æ—¶æ²¡æœ‰å°è¯•è§£å†³ï¼Œè€Œæ˜¯ç›´æ¥å¼ƒç”¨ï¼Œæœ€ç»ˆæ²¡æœ‰å–å¾—æ›´å¥½çš„æˆç»©ã€‚ æ­¤æ¬¡æˆ‘ä»¬çš„æˆç»©æ’åç¬¬4(ä¸‰ç­‰å¥–)ï¼Œæ˜¯æœ‰ä¸€å®šçš„è¿›æ­¥çš„ï¼Œä½†æœ‰ä¸€ç‚¹é—æ†¾çš„æ˜¯ï¼Œæˆ‘ä»¬ä»…å·®0.18ç™¾åˆ†ç‚¹ï¼Œå°±èƒ½è¶…è¿‡ç¬¬ä¸‰åæ‹¿åˆ°5ä¸‡çš„å¥–é‡‘äº†ã€‚åé¢æˆ‘ä»¬åˆ†æäº†ä¸€ä¸‹ï¼Œè¿˜æ˜¯å› ä¸ºæˆ‘ä»¬å¯¹æ¯”èµ›æ‡ˆæ€ çš„æ€åº¦ï¼Œå…¶ä»–ç»„éƒ½å¯¹æ•°æ®è¿›è¡Œäº†åˆ†æå¹¶æœ‰é’ˆå¯¹æ€§çš„æ”¹è¿›ï¼Œè€Œæˆ‘ä»¬å¹¶æ²¡æœ‰åšè¿™ä¸€æ­¥ã€‚ Anywayï¼Œç¬¬ä¸€æ¬¡ç»„é˜Ÿå‚åŠ æ¯”èµ›å°±æœ‰æ”¶è·ï¼Œå¢é•¿äº†è§è¯†ï¼Œä»äº¤æµä¸­ä¹Ÿè·å¾—äº†è®¸å¤šã€‚è¿™ä¸ªæ¯”èµ›ä¹‹åï¼Œå°±å¾—å¥½å¥½çœ‹paperäº†ã€‚ __(:Ğ·ã€âˆ )_]]></content>
      <tags>
        <tag>æœ‰æ„Ÿ</tag>
        <tag>æ¯”èµ›</tag>
        <tag>è±æ–¯æ¯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯13]]></title>
    <url>%2F2018%2F10%2F29%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D13%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£è¡Œè·¯éš¾ä¸‰é¦–[å”] æç™½ã€å…¶ä¸€ã€‘é‡‘æ¨½æ¸…é…’æ–—ååƒï¼Œç‰ç›˜çç¾ç›´ä¸‡é’±ã€‚åœæ¯æŠ•ç®¸ä¸èƒ½é£Ÿï¼Œæ‹”å‰‘å››é¡¾å¿ƒèŒ«ç„¶ã€‚æ¬²æ¸¡é»„æ²³å†°å¡å·ï¼Œå°†ç™»å¤ªè¡Œé›ªæ»¡å±±ã€‚é—²æ¥å‚é’“ç¢§æºªä¸Šï¼Œå¿½å¤ä¹˜èˆŸæ¢¦æ—¥è¾¹ã€‚è¡Œè·¯éš¾ï¼Œè¡Œè·¯éš¾ï¼Œå¤šæ­§è·¯ï¼Œä»Šå®‰åœ¨ï¼Ÿé•¿é£ç ´æµªä¼šæœ‰æ—¶ï¼Œç›´æŒ‚äº‘å¸†æµæ²§æµ·ï¼ æ³¨é‡Šï¼šã€Œé—²æ¥å‚é’“ç¢§æºªä¸Šï¼Œå¿½å¤ä¹˜èˆŸæ¢¦æ—¥è¾¹ã€‚ã€å¥ï¼šæš—ç”¨å…¸æ•…ï¼šå§œå¤ªå…¬å•å°šæ›¾åœ¨æ¸­æ°´çš„ç£»æºªä¸Šé’“é±¼ï¼Œå¾—é‡å‘¨æ–‡ç‹ï¼ŒåŠ©å‘¨ç­å•†ï¼›ä¼Šå°¹æ›¾æ¢¦è§è‡ªå·±ä¹˜èˆ¹ä»æ—¥æœˆæ—è¾¹ç»è¿‡ï¼Œåè¢«å•†æ±¤è˜è¯·ï¼ŒåŠ©å•†ç­å¤ã€‚è¿™ä¸¤å¥è¡¨ç¤ºè¯—äººè‡ªå·±å¯¹ä»æ”¿ä»æœ‰æ‰€æœŸå¾…ã€‚ç¢§ï¼Œä¸€ä½œã€Œåã€ã€‚ 2ï¸âƒ£ç™»ç§‘å[å”] å­ŸéƒŠæ˜”æ—¥é¾Œé¾Šä¸è¶³å¤¸ï¼Œä»Šæœæ”¾è¡æ€æ— æ¶¯ã€‚æ˜¥é£å¾—æ„é©¬è¹„ç–¾ï¼Œä¸€æ—¥çœ‹å°½é•¿å®‰èŠ±ã€‚ æ³¨é‡Šï¼šé¾Œé¾Šï¼ˆwÃ² chuÃ²ï¼‰ï¼šåŸæ„æ˜¯è‚®è„ï¼Œè¿™é‡ŒæŒ‡ä¸å¦‚æ„çš„å¤„å¢ƒã€‚ä¸è¶³å¤¸ï¼šä¸å€¼å¾—æèµ·ã€‚æ”¾è¡ï¼ˆdÃ ngï¼‰ï¼šè‡ªç”±è‡ªåœ¨ï¼Œä¸å—çº¦æŸã€‚æ€æ— æ¶¯ï¼šå…´è‡´é«˜æ¶¨ã€‚ http://m.xichuangzhu.com/work/57add198a633bd0057eefa8a]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡3]]></title>
    <url>%2F2018%2F10%2F29%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%873%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[A Neural Probabilistic Language Model]ç¬¬ä¸€ç¯‡ä½¿ç”¨ç¥ç»ç½‘ç»œè·å¾—è¯å‘é‡çš„paperã€‚ é€šè¿‡å¯¹language modelå»ºæ¨¡ï¼Œå°†è¯æ˜ å°„åˆ°ä½ç»´è¡¨ç¤ºï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒæ—¶è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥åŠæ¯ä¸ªè¯çš„è¯å‘é‡ã€‚ å°†ä¸­å¿ƒè¯çš„å‰nä¸ªæ‹¼æ¥èµ·æ¥ $x=(C(w_{t-1},C(w_{t-2}),â€¦,C(w_{t-n+1}))$å°†$x$é€å…¥ç¥ç»ç½‘ç»œä¸­è·å¾—$y=b+Wx+Utanh(d+Hx)$ï¼Œæœ€ååšä¸€ä¸ªsoftmaxå³å¯ã€‚ 2ï¸âƒ£[Adaptive Computation Time for Recurrent Neural Networks]ä¸€ç§å…è®¸RNNåŠ¨æ€å †å å±‚æ•°çš„ç®—æ³•ã€‚ Motivationè¯æ®è¯æ˜ï¼ŒRNNçš„å †å å±‚æ•°å¤šï¼Œæ•ˆæœä¼šæœ‰æå‡ã€‚ä½†æ˜¯ï¼Œå¯¹äºä¸åŒçš„ä»»åŠ¡ï¼Œè¦æ±‚ä¸åŒçš„è®¡ç®—å¤æ‚åº¦ã€‚æˆ‘ä»¬éœ€è¦å…ˆéªŒæ¥å†³å®šç‰¹å®šä»»åŠ¡çš„è®¡ç®—å¤æ‚åº¦ã€‚å½“ç„¶æˆ‘ä»¬å¯ä»¥ç²—æš´åœ°ç›´æ¥å †å æ·±å±‚çš„ç½‘ç»œã€‚ACT(Adaptive Computation Time)èƒ½å¤ŸåŠ¨æ€å†³å®šæ¯ä¸ªè¾“å…¥tæ‰€éœ€çš„è®¡ç®—æ¬¡æ•°ã€‚ æ–¹æ³•å°†RNNæ¯ä¸€æ­¥çš„è¾“å‡ºè¿‡ä¸€ä¸ªç½‘ç»œ+sigmoidå±‚ï¼Œè·å¾—ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œä¹Ÿå³ä»€ä¹ˆæ—¶å€™åº”å½“åœæ­¢ä¸å†ç»§ç»­å¾€ä¸Šå †å ï¼Œç›´åˆ°æ¦‚ç‡åŠ å’Œä¸º1ã€‚åŒæ—¶ä¸ºäº†å°½å¯èƒ½æŠ‘åˆ¶å±‚æ•°çš„æ— é™å¢é•¿ï¼Œåœ¨lossæ·»åŠ ä¸€é¡¹æƒ©ç½šã€‚ æ¨¡å‹å¯¹äºæ™®é€šçš„RNNï¼š sæ˜¯éšè—å±‚ï¼›yæ˜¯è¾“å‡ºã€‚ å¯¹äºACTçš„RNNï¼Œæœ‰ï¼š ä¸Šæ ‡næ˜¯æŒ‡çš„tæ—¶åˆ»çš„å±‚æ•°ï¼›å…¶ä¸­ï¼š $Î´$æ˜¯flatï¼ŒæŒ‡ç¤ºxæ˜¯ç¬¬å‡ æ¬¡è¾“å…¥ã€‚ å¼•å…¥æ–°çš„ç½‘ç»œï¼Œè¾“å…¥æ—¶éšçŠ¶æ€ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼š é‚£ä¹ˆæ¯ä¸€å±‚çš„æ¦‚ç‡æ˜¯ï¼š å…¶ä¸­$R(t)$æ˜¯åœ¨æ¯ä¸€å±‚æ¦‚ç‡æ±‚å’Œè¶…è¿‡1æ—¶çš„å‰©ä½™æ¦‚ç‡ï¼ˆä¸ºäº†ä¿è¯æ¦‚ç‡å’Œä¸º1ï¼Œå¯ä»¥è¯•ç€ä¸¾ä¸€ä¸ªä¾‹å­æ¥è¯æ˜ï¼‰ Îµæ˜¯ä¸ºäº†è§£å†³ç¬¬ä¸€æ¬¡è¾“å‡ºæ—¶å°±è¶…è¿‡1-Îµçš„æƒ…å†µï¼ŒÎµä¸€èˆ¬å–å¾ˆå°ã€‚ æœ€ç»ˆï¼ŒåŠ æƒæ±‚å’Œï¼Œä½œä¸ºæœ€ç»ˆçš„ç»“æœï¼Œä¼ å…¥ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ï¼š æ™®é€šRNNä¸ACTçš„RNNå¯¹æ¯”ï¼š æŸå¤±å‡½æ•°ä¸ºäº†é˜²æ­¢æ¨¡å‹å±‚æ•°æ— é™å¢é•¿ï¼Œæ·»åŠ ä¸€é¡¹æƒ©ç½šé¡¹ä»¥æŠ‘åˆ¶ã€‚ è®°æ¯ä¸€æ­¥çš„æƒ©ç½šé¡¹ä¸ºï¼š æ€»çš„æƒ©ç½šé¡¹åˆ™ä¸ºï¼š Loss functionåˆ™ä¸ºï¼š å› ä¸ºN(t)æ˜¯ä¸å¯å¯¼çš„ï¼Œæˆ‘ä»¬åœ¨å®é™…è¿‡ç¨‹ä¸­åªå»æœ€å°åŒ–R(t) ï¼ˆæˆ‘è§‰å¾—ä¸ç”šåˆç†ï¼Œä¸€ç§è§£è¯»æ˜¯å¦‚æœæˆ‘ä»¬ä¸æ–­æœ€å°åŒ–R(t)ç›´åˆ°å˜æˆ0ï¼Œé‚£ä¹ˆç›¸å½“äºN(t)å°‘äº†ä¸€å±‚ï¼Œæ¥ç€R(t)å°±ä¼šå˜å¾—å¾ˆå¤§ï¼Œç„¶ååˆç»§ç»­æœ€å°åŒ–R(t)â€¦ï¼‰ 3ï¸âƒ£[Universal Transformers]æå‡ºä¸€ç§æ–°å‹é€šç”¨çš„transformerã€‚ MotivationTransformerçš„é—®é¢˜ï¼šRNNçš„å½’çº³åç½®(inductive bias)åœ¨ä¸€äº›ä»»åŠ¡ä¸Šå¾ˆé‡è¦ï¼Œä¹Ÿå³RNNçš„å¾ªç¯å­¦ä¹ çš„è¿‡ç¨‹ï¼›Transformeråœ¨ä¸€äº›é—®é¢˜ä¸Šè¡¨ç°ä¸å¥½ï¼Œå¯èƒ½æ˜¯å½’çº³åç½®çš„åŸå› ã€‚ Notably, however, the Transformer foregoes the RNNâ€™s inductive bias towards learning iterative or recursive transformations.Our experiments indicate that this inductive bias may be crucial for several algorithmic and language understanding tasks of varying complexity: in contrast to models such as the Neural Turing Machine [13], the Neural GPU [17] or Stack RNNs [16], the Transformer does not generalize well to input lengths not encountered during training. å› æ­¤åœ¨Transformerå†…å¼•å…¥å½’çº³åç½® ç‰¹ç‚¹ æ¯ä¸€å±‚çš„æƒé‡æ˜¯å…±äº«çš„ï¼Œä¹Ÿå³multi-headä¸Šçš„æƒé‡ä»¥åŠtransition functionåœ¨æ¯ä¸€å±‚æ˜¯ä¸€è‡´çš„ã€‚è¿™ä¸€ç‚¹å’ŒRNNã€CNNä¸€è‡´ã€‚ åŠ¨æ€å±‚æ•°ï¼ˆACT mechanism ï¼‰ï¼šå¯¹äºæ¯ä¸ªè¯éƒ½ä¼šæœ‰ä¸åŒçš„å¾ªç¯æ¬¡æ•°ï¼›ä¹Ÿå³æœ‰äº›è¯éœ€è¦æ›´å¤šçš„refineï¼›è€Œæœ‰äº›è¯ä¸éœ€è¦ã€‚å’Œå›ºå®šå±‚æ•°çš„transformerç›¸æ¯”ï¼Œä¼šæœ‰æ›´å¥½çš„é€šç”¨æ€§ã€‚ æ¨¡å‹æ€»ä½“æ¶æ„ è¿‡ç¨‹ï¼š å’Œæ™®é€šTransformerä¸åŒçš„åœ°æ–¹åœ¨äºï¼š åŠ äº†ä¸€å±‚Transitionå±‚ï¼ŒTransitionå¯ä»¥æ˜¯depth-wise separable convolutionï¼ˆæ˜¯ä»€ä¹ˆï¼Ÿï¼‰æˆ–è€…å…¨è¿æ¥å±‚ã€‚ æ¯å±‚éƒ½æ·»åŠ äº†position embeddingï¼›ä»¥åŠtimestep embeddingï¼Œç”¨ä»¥æŒ‡ç¤ºå±‚æ•°ã€‚ ACTç”±äºä¸€ä¸ªå¥å­ä¸­é—´ï¼Œæœ‰äº›è¯æ¯”å…¶ä»–è¯æ›´éš¾å­¦ä¼šï¼Œéœ€è¦æ›´å¤šè®¡ç®—é‡ï¼Œä½†å †å å¤ªå¤šå±‚ä¼šå¤§å¤§å¢åŠ è®¡ç®—é‡ï¼Œä¸ºäº†èŠ‚çœè®¡ç®—é‡ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥ACTæ¥åŠ¨æ€åˆ†é…è®¡ç®—é‡ã€‚ ACTåŸæ¥ç”¨äºRNNï¼Œåœ¨Transformerä¸­ï¼Œå½“halting unitæŒ‡ç¤ºè¯tåº”å½“åœæ­¢æ—¶ï¼Œç›´æ¥è®²è¯¥è¯çš„çŠ¶æ€å¤åˆ¶åˆ°ä¸‹ä¸€ä¸ªtime stepï¼Œç›´åˆ°æ‰€æœ‰çš„è¯éƒ½åœæ­¢ã€‚]]></content>
      <tags>
        <tag>Embedding</tag>
        <tag>Paper</tag>
        <tag>Transformer</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>ACT</tag>
        <tag>Language Modeling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯12]]></title>
    <url>%2F2018%2F10%2F21%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D12%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£æœ›æµ·æ½®[å®‹] æŸ³æ°¸ä¸œå—å½¢èƒœï¼Œä¸‰å´éƒ½ä¼šï¼Œé’±å¡˜è‡ªå¤ç¹åã€‚çƒŸæŸ³ç”»æ¡¥ï¼Œé£å¸˜ç¿ å¹•ï¼Œå‚å·®åä¸‡äººå®¶ã€‚äº‘æ ‘ç»•å ¤æ²™ï¼Œæ€’æ¶›å·éœœé›ªï¼Œå¤©å ‘æ— æ¶¯ã€‚å¸‚åˆ—ç ç‘ï¼Œæˆ·ç›ˆç½—ç»®ï¼Œç«è±ªå¥¢ã€‚é‡æ¹–å å·˜æ¸…å˜‰ï¼Œæœ‰ä¸‰ç§‹æ¡‚å­ï¼Œåé‡Œè·èŠ±ã€‚ç¾Œç®¡å¼„æ™´ï¼Œè±æ­Œæ³›å¤œï¼Œå¬‰å¬‰é’“åŸè²å¨ƒã€‚åƒéª‘æ‹¥é«˜ç‰™ï¼Œä¹˜é†‰å¬ç®«é¼“ï¼ŒåŸèµçƒŸéœã€‚å¼‚æ—¥å›¾å°†å¥½æ™¯ï¼Œå½’å»å‡¤æ± å¤¸ã€‚ å å·˜ï¼ˆyÇnï¼‰ï¼šå±‚å±‚å å çš„å±±å³¦ã€‚ http://m.xichuangzhu.com/work/57b318228ac247005f2223db]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•8]]></title>
    <url>%2F2018%2F10%2F21%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%958%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[batchify]å¿«é€Ÿå°†æ•°æ®åˆ†æˆbatchã€‚ 12345678def batchify(data, bsz): # Work out how cleanly we can divide the dataset into bsz parts. nbatch = data.size(0) // bsz # Trim off any extra elements that wouldn't cleanly fit (remainders). data = data.narrow(0, 0, nbatch * bsz) # Evenly divide the data across the bsz batches. data = data.view(bsz, -1).t().contiguous() return data.to(device)]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRMLç¬¬å››ç«  åˆ†ç±»çš„çº¿æ€§æ¨¡å‹]]></title>
    <url>%2F2018%2F10%2F21%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FPRML%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%88%86%E7%B1%BB%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[åˆ¤åˆ«å‡½æ•° â€”-æœªå®Œâ€”-]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡2]]></title>
    <url>%2F2018%2F10%2F20%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%872%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling]æœ¬æ–‡è´¡çŒ®ï¼šæå‡ºä¸€ç§æ–°çš„æ¨¡å‹TCNï¼ˆTemporal Convolutional Networksï¼‰è¿›è¡Œlanguage modelå»ºæ¨¡ã€‚ Dilated convolutionæ¯ä¸€å±‚çš„æ„Ÿå—é‡éƒ½å¯ä»¥æ˜¯ä¸åŒçš„ï¼Œä¹Ÿå³ï¼ŒåŒæ ·çš„kernel sizeï¼Œé«˜å±‚çš„å¯ä»¥è·³ç€çœ‹ã€‚ æ¯å±‚çš„dé€æ¸å¢å¤§ï¼ˆä¹Ÿå³è·³çš„æ­¥æ•°ï¼‰ï¼Œä¸€èˆ¬æŒ‰æŒ‡æ•°å¢å¤§ã€‚ï¼ˆæˆ‘è§‰å¾—è¿™æ ·å¾ˆæœ‰é“ç†ï¼Œå¦‚æœæ¯ä¸€å±‚çš„déƒ½æ˜¯ä¸€æ ·çš„ï¼Œé‚£captureåˆ°çš„ä¿¡æ¯å°±ä¼šæœ‰é‡å¤ï¼Œèƒ½çœ‹åˆ°çš„è§†é‡ä¹Ÿä¸å¦‚é€æ¸å¢å¤§çš„å¤šï¼‰ Residual block è¿™è¾¹çš„residual blockæ¯”è¾ƒå¤æ‚ï¼›ä¸€ä¸ªå€¼å¾—ä¸»æ„çš„ç»†èŠ‚æ˜¯ï¼Œå› ä¸ºæ„Ÿå—é‡çš„ä¸åŒï¼Œä¸Šå±‚çš„æ„Ÿå—é‡æ€»æ˜¯æ¯”ä¸‹å±‚çš„å¤§å¾ˆå¤šï¼Œå› æ­¤ä¸åº”è¯¥ç›´æ¥å°†ä¸‹å±‚çš„åŠ åˆ°ä¸Šå±‚ï¼Œè€Œæ˜¯å¯ä»¥ä½¿ç”¨ä¸€ä¸ª1*1çš„convolutionå¯¹ä¸‹å±‚çš„xè¿›è¡Œå·ç§¯ï¼Œè¿™å°±ç±»ä¼¼scaleå¯¹è¾“å…¥è¿›è¡Œæ”¾ç¼©ã€‚ 2ï¸âƒ£[Dissecting Contextual Word Embeddingsï¼š Architecture and Representation]ä¸€ç¯‡åˆ†æçš„æ–‡ç« ã€‚ELMoä½œè€…çš„åˆä¸€ç¯‡æ–‡ç« ã€‚ å¯¹æ¯”ä¸‰ç§ä¸åŒçš„å»ºæ¨¡æ–¹å¼ï¼ˆLSTM/GCNN/Transformerï¼‰è·å¾—çš„è¯å‘é‡ï¼Œä»¥åŠåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼›ä»¥åŠä¸åŒå±‚è·å¾—çš„ä¸åŒä¿¡æ¯â€¦è·å¾—äº†ä¸åŒçš„ç»“è®ºã€‚ â‘ biLM ä¸“æ³¨äºword morphologyè¯çš„å½¢æ€ï¼›åº•å±‚çš„LMå…³æ³¨local syntaxï¼›è€Œé«˜å±‚çš„LMå…³æ³¨semantic contentï¼› â‘¡ä¸åŒçš„ä»»åŠ¡ä¼šæœ‰ä¸åŒçš„æ­£åˆ™åŒ–sçš„å€¾å‘ã€‚ 3ï¸âƒ£[Transformer-XL: Language modeling with longer-term dependency]åˆ©ç”¨Transformerè¿›è¡Œlanguage modelï¼Œä¸æ™®é€šçš„Transformerå»ºæ¨¡ä¸åŒçš„æ˜¯ï¼ŒTransformer-XLæ·»åŠ äº†å†å²ä¿¡æ¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è¡¨ç°ã€‚è¿™ç¯‡è¿˜åœ¨ICLR2019å®¡ç¨¿ä¸­ã€‚ è´¡çŒ®ï¼šæœ¬æ–‡æå‡ºäº†èƒ½å¤Ÿè¿›è¡Œé•¿ç¨‹ä¾èµ–çš„åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ Transformer-XLï¼›å¼•å…¥ç›¸å¯¹ä½ç½®çš„positional encodingã€‚ ç»“æ„åŸå…ˆçš„transformer language modelæ˜¯å°†å¥å­åˆ†ä¸ºä¸€ä¸ªä¸€ä¸ªsegmentã€‚segmentä¹‹é—´æ˜¯æ²¡æœ‰è”ç³»çš„ã€‚ï¼ˆä¸ºä»€ä¹ˆä¸ç›´æ¥æŒ‰åŸç‰ˆçš„Transformeré‚£æ ·æ‰€æœ‰çš„è¯éƒ½ç›¸äº’åšself-attentionï¼Ÿå› ä¸ºè€ƒè™‘åˆ°æ•ˆç‡é—®é¢˜ï¼Œå¥å­é•¿åº¦å¯èƒ½ä¼šå¾ˆé•¿ï¼‰ è®­ç»ƒé˜¶æ®µï¼š è€Œåœ¨æµ‹è¯•é˜¶æ®µï¼Œæ¯æ¬¡å‘å³æ»‘åŠ¨ä¸€æ ¼ï¼šè¿™æ ·æ¯ä¸€ä¸ªæ—¶é—´æ­¥éƒ½è¦é‡æ–°è®¡ç®—ä¸€éï¼Œå†å²ä¿¡æ¯æ²¡æœ‰åˆ©ç”¨åˆ°ã€‚æ˜¾ç„¶é€Ÿåº¦å¾ˆæ…¢ã€‚ åœ¨Transformerå¼•å…¥recurrenceï¼Œä¹Ÿå³å¼•å…¥å†å²ä¿¡æ¯ã€‚åŸºäºè¿™æ ·çš„æƒ³æ³•ï¼Œæå‡ºçš„æ–°æ¨¡å‹Transformer-XLã€‚åœ¨ç»“æ„ä¸ŠåŒæ ·åˆ†ä¸ºæ¯ä¸ªsegmentï¼Œä½†åœ¨æ¯ä¸ªé˜¶æ®µéƒ½æ¥æ”¶ä¸Šä¸€ä¸ªï¼ˆç”šè‡³ä¸ŠLä¸ªï¼‰å†å²ä¿¡æ¯ã€‚ è®­ç»ƒé˜¶æ®µï¼š è€Œåœ¨æµ‹è¯•é˜¶æ®µï¼ŒåŒæ ·åˆ†ä¸ºsegmentï¼Œä½†å› ä¸ºæ¥æ”¶äº†å†å²ä¿¡æ¯ï¼Œä¸éœ€è¦æ¯æ¬¡æ»‘åŠ¨ä¸€æ ¼ä¹Ÿèƒ½è·å¾—å¤§é‡ä¿¡æ¯ã€‚ å…·ä½“æ¥è¯´ï¼šSGä»£è¡¨stop gradientï¼Œå’Œè¯¥é˜¶æ®µçš„hidden stateè¿›è¡Œæ‹¼æ¥ã€‚ RELATIVE POSITIONAL ENCODINGSå¦‚æœæˆ‘ä»¬ä½¿ç”¨äº†absolute positional encodingsï¼ˆä¹Ÿå³åŸç‰ˆçš„positional encodingsï¼‰é‚£ä¹ˆä¼šå‡ºç°è¿™ç§æƒ…å†µ åœ¨åŒä¸€å±‚ä¹‹é—´çš„å‰ä¸€ä¸ªsegmentå’Œåä¸€ä¸ªsegmentä½¿ç”¨äº†åŒæ ·çš„ç»å¯¹ä½ç½®ä¿¡æ¯ï¼Œå¯¹äºå½“å‰segmentçš„é«˜å±‚ï¼Œå¯¹äºåŒä¸€ä¸ªä½ç½®iï¼Œæ— æ³•åŒºåˆ†è¯¥ä½ç½®ä¿¡æ¯æ˜¯æ¥è‡ªå½“å‰segmentçš„è¿˜æ˜¯ä¸Šä¸€ä¸ªsegmentçš„ï¼ˆå› ä¸ºéƒ½æ˜¯åŒæ ·çš„ç»å¯¹ä½ç½®ï¼‰ã€‚ å› æ­¤æˆ‘ä»¬å¼•å…¥ç›¸å¯¹ä½ç½®ä¿¡æ¯Rï¼Œå…¶ä¸­ç¬¬iè¡Œä»£è¡¨ç›¸å¯¹è·ç¦»içš„encodingã€‚ å…·ä½“æ¥è¯´ï¼š é¦–å…ˆæˆ‘ä»¬åœ¨ä¼ ç»Ÿçš„è®¡ç®—$query_i$å’Œ$key_j$çš„attentionåˆ†æ•°æ—¶ï¼Œå¯ä»¥æ‹†è§£æˆï¼š ï¼ˆå› ä¸ºquery=(embedding E +positional embedding Uï¼‰ï¼Œkeyä¹Ÿä¸€æ ·ï¼Œå°†å¼å­æ‹†å¼€å°±èƒ½è·å¾—ä¸Šè¿°å¼å­) æˆ‘ä»¬å°†è¯¥å¼å­è¿›è¡Œä¿®æ”¹ï¼š ç¬¬ä¸€ï¼Œå°†å‡ºç°äº†absolute positional embedding $U$çš„åœ°æ–¹ï¼Œç»Ÿç»Ÿæ”¹æˆ$R_{i-j}$ï¼Œä¹Ÿå³åœ¨bå’Œdé¡¹ã€‚å…¶ä¸­è¿™é‡Œçš„Rå’ŒåŸç‰ˆçš„Transformerçš„ä½ç½®è®¡ç®—å…¬å¼ç›¸åŒã€‚ ç¬¬äºŒï¼Œåœ¨cé¡¹ä¸­ï¼Œä½¿ç”¨ä¸€ä¸ª$u$æ›¿ä»£äº†$U_i W_q$ï¼Œè¿™ä¸€é¡¹åŸæœ¬çš„æ„ä¹‰åœ¨äºï¼Œ$query_i$çš„positional encodingå¯¹$key_j$çš„embeddingè¿›è¡Œattentionï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¯¥é¡¹è¡¨ç°äº†$query_i$ä½ç½®å¯¹å“ªäº›$key_j$çš„å†…å®¹æœ‰å…´è¶£ï¼Œä½œè€…è®¤ä¸ºqueryä¸ç®¡åœ¨å“ªä¸ªä½ç½®ä¸Šéƒ½æ˜¯ä¸€æ ·çš„ï¼Œä¹Ÿå°±æ˜¯è¯´queryçš„ä½ç½®ä¿¡æ¯åº”å½“æ²¡å½±å“ï¼Œæ‰€ä»¥ç»Ÿç»Ÿæ›¿æ¢æˆä¸€ä¸ªå¯å­¦ä¹ çš„$u$ã€‚åŸºäºç±»ä¼¼çš„ç†ç”±dé¡¹æ¢æˆäº†$v$ã€‚ ç¬¬ä¸‰ï¼Œå°†$W_k$ç»†åˆ†æˆäº†ä¸¤ä¸ª$W_{k,E}$å’Œ$W_{k,R}$ã€‚è¿™æ˜¯æ ¹æ®queryæ˜¯Embeddingè¿˜æ˜¯positional encodingæ¥åŒºåˆ†çš„ã€‚for producing the content-based key vectors and location-based key vectors respectively æ¯ä¸€é¡¹ç°åœ¨éƒ½æœ‰äº†ä¸åŒçš„æ„ä¹‰ï¼š Under the new parameterization, each term has an intuitive meaning: term (a) represents contentbased addressing, term (b) captures a content-dependent positional bias, term (c) governs a global content bias, and (d) encodes a global positional bias. æœ€åæ€»ç»“ä¸€ä¸‹æ•´ä¸ªç»“æ„ï¼š ä¸åŸç‰ˆTransformerä¸åŒçš„æ˜¯ï¼ŒTransformer-XLåœ¨æ¯ä¸€å±‚éƒ½æ·»åŠ äº†ä½ç½®ä¿¡æ¯ã€‚ 4ï¸âƒ£[Trellis Networks for Sequence Modeling]ä¸€ç§ç»“åˆRNNå’ŒCNNçš„è¯­è¨€å»ºæ¨¡æ–¹å¼ã€‚ æœ€å°çš„å•å…ƒç»“æ„ï¼š ä¹Ÿå³ï¼š æ¥ä¸‹æ¥å†å¤„ç†éçº¿æ€§ï¼š å› ä¸ºæ¯å±‚éƒ½è¦è¾“å…¥xï¼Œä¸”Wæ˜¯å…±äº«çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æå‰è®¡ç®—å¥½è¿™ä¸€é¡¹ï¼Œåé¢ç›´æ¥ç”¨å³å¯ã€‚ æœ€ç»ˆåœ¨å®ç°çš„æ—¶å€™æ˜¯ï¼š æ€»ä½“æ¡†æ¶ï¼š ä¸TCNï¼ˆtemporal convolution networkï¼‰ä¸åŒä¹‹å¤„ï¼šâ‘ filter weightä¸ä»…åœ¨time stepä¹‹é—´å…±äº«ï¼Œåœ¨ä¸åŒå±‚ä¹‹é—´ä¹Ÿå…±äº«ï¼›â‘¡åœ¨æ¯ä¸€å±‚éƒ½æ·»åŠ äº†è¾“å…¥ ä¼˜ç‚¹ï¼šå…±äº«äº†Wï¼Œæ˜¾è‘—å‡å°‘äº†å‚æ•°ï¼›â€˜Weight tying can be viewed as a form of regularization that can stabilize trainingâ€™ æˆ‘ä»¬è¿˜å¯ä»¥æ‰©å±•è¯¥ç½‘ç»œï¼Œå¼•å…¥gateï¼š 5ï¸âƒ£[Towards Decoding as Continuous Optimisation in Neural Machine Translation]ä¸€ç¯‡å¾ˆæœ‰æ„æ€çš„paperã€‚ç”¨äºNMT decodeçš„inferenceé˜¶æ®µã€‚è¿™ç¯‡æœ‰ä¸€å®šçš„éš¾åº¦ï¼Œä»¥ä¸‹åªæ˜¯æˆ‘çš„ç†è§£ã€‚ æ€æƒ³Motivationï¼šNMTä¸­çš„decode inferenceé˜¶æ®µï¼Œé€šå¸¸éƒ½æ˜¯ä»å·¦åˆ°å³çš„ï¼Œè¿™æ ·æœ‰ä¸ªç¼ºç‚¹ï¼Œå°±æ˜¯æ•´ä½“çš„targetä¹‹é—´çš„ä¾èµ–æ˜¯æ²¡æœ‰è¢«å……åˆ†åˆ©ç”¨åˆ°çš„ï¼Œæ¯”å¦‚è¯´ç”Ÿæˆçš„è¯çš„å³è¾¹æ˜¯æ²¡æœ‰ç”¨åˆ°çš„ã€‚é‚£ä¹ˆæˆ‘ä»¬ä¸ºä»€ä¹ˆä¸ç›´æ¥å…¨éƒ¨ç”Ÿæˆå‘¢ï¼Ÿç„¶åä¸æ–­æ›´æ–°ã€‚ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬å°†ç¦»æ•£ï¼ˆdiscreteï¼‰çš„decodeè¿‡ç¨‹å˜æˆä¸€ä¸ªè¿ç»­çš„è¿‡ç¨‹ï¼ˆcontinuous optimizationï¼‰ã€‚ å‡è®¾æˆ‘ä»¬å·²ç»è®­ç»ƒå¥½æ¨¡å‹ï¼Œç»™å®šä¸€ä¸ªå¥å­ï¼Œæˆ‘ä»¬è¦ç¿»è¯‘æˆç›®æ ‡å¥å­ï¼Œä¸”å‡è®¾æˆ‘ä»¬å·²çŸ¥è¦ç”Ÿæˆçš„å¥å­é•¿åº¦æ˜¯lï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ‰ï¼šæˆ‘ä»¬è¦æ‰¾åˆ°ä¸€ä¸ªæœ€ä¼˜çš„åºåˆ—$y$ï¼Œä½¿å¾—$-log$æœ€å°ã€‚ ç­‰ä»·äºï¼šå…¶ä¸­$\widetilde{y}_i$æ˜¯one-hotã€‚å…¶å®è¿™é‡Œå°±æ˜¯å‡è®¾æœ‰è¿™ä¹ˆä¸€ä¸ªground truthï¼Œä½†å®é™…ä¸Šæ˜¯æ²¡æœ‰çš„ã€‚ æˆ‘ä»¬å°†$\widetilde{y}_i$æ˜¯one-hotè¿™ä¸ªæ¡ä»¶æ”¾å®½ä¸€äº›ï¼Œå˜æˆæ˜¯ä¸€ä¸ªæ¦‚ç‡å•çº¯å‹ï¼ˆå…¶å®å°±æ˜¯æ‰€æœ‰å…ƒç´ åŠ èµ·æ¥æ˜¯1ï¼Œä¸”éƒ½å¤§äºç­‰äº0ï¼‰ã€‚ é‚£ä¹ˆå°±å˜æˆäº†ï¼š è¿™ä¸ªæ”¹å˜çš„æœ¬è´¨æ˜¯ï¼š å°±æ˜¯è¯´åŸæ¥one-hotçš„$\widetilde{y}_i$ç”Ÿæˆåä¸¢åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œå–äº†ä¸€ä¸ªè¯å‘é‡ï¼Œæ¥ç€è®¡ç®—ã€‚ç°åœ¨æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ$\hat{y}_i$ä¸¢è¿›æ¥ï¼Œå°±ç›¸å½“äºå–äº†å¤šä¸ªè¯å‘é‡çš„åŠ æƒæ±‚å’Œã€‚ åœ¨åˆ©ç”¨ä¸‹è¿°çš„æ›´æ–°ç®—æ³•æ›´æ–°å®Œ$\hat{y}_i$ä¹‹åï¼Œå¯¹äºæ¯ä¸ªæ—¶é—´æ­¥tï¼Œæˆ‘ä»¬æ‰¾$\hat{y}_i$ä¸­å…ƒç´ æœ€å¤§çš„å€¼å¯¹åº”çš„è¯ä½œä¸ºç”Ÿæˆçš„è¯ã€‚ æœ‰ä¸¤ç§æ–¹æ³•Exponentiated Gradient å’Œ SGDã€‚å®é™…ä¸Šæ–¹æ³•å€’åœ¨å…¶æ¬¡äº†ï¼Œä¸»è¦æ˜¯å‰é¢æ‰€è¿°çš„continuous optimizationè¿™ç§æ€æƒ³ã€‚ ç®—æ³•Exponentiated Gradientå…·ä½“è§è®ºæ–‡ SGDå› ä¸ºæˆ‘ä»¬è¦ä¿è¯å•çº¯å½¢çš„çº¦æŸä¸å˜ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥ä¸€ä¸ªrï¼Œç„¶ååšä¸€ä¸ªsoftmax åº”ç”¨è¿™ç§è¿ç»­decodeå¯ä»¥ç”¨åœ¨å“ªï¼Ÿ Bidirectional Ensembleå¯ä»¥å¾ˆæ–¹ä¾¿åœ°è¿›è¡ŒåŒå‘çš„ç”Ÿæˆï¼š è€Œåœ¨ä¼ ç»Ÿçš„æ–¹æ³•ä¸­æ²¡åŠæ³•ï¼ˆå¾ˆéš¾ï¼‰åšåˆ° Bilingual Ensembleæˆ‘ä»¬å¸Œæœ›æºè¯­è¨€åˆ°ç›®æ ‡è¯­è¨€å’Œç›®æ ‡åˆ°æºè¯­è¨€éƒ½ç”Ÿæˆå¾—å¥½ é—®é¢˜$\hat{y}_i$çš„åˆå§‹åŒ–å¾ˆé‡è¦ï¼Œä¸€ä¸å°å¿ƒå°±ä¼šé™·å…¥local minimaï¼›ç”Ÿæˆçš„é€Ÿåº¦æ…¢ 6ï¸âƒ£[Universal Language Model Fine-tuning for Text Classiï¬cation]å’ŒELMoã€OpenAI GPTä¸€æ ·ï¼Œéƒ½æ˜¯é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè¿ç§»åˆ°å…¶ä»–ä»»åŠ¡ä¸Šï¼ˆè¿™é‡Œæ˜¯åˆ†ç±»ä»»åŠ¡ï¼‰ã€‚å¯ä»¥åœ¨éå¸¸å°çš„æ•°æ®é›†ä¸Šæœ‰å¾ˆå¥½çš„æ•ˆæœã€‚ è´¡çŒ®ï¼š è¿ç§»å­¦ä¹ æ¨¡å‹ULMFiT æå‡ºå‡ ç§trickï¼šdiscriminative ï¬ne-tuning, slanted triangular learning rates,gradual unfreezing ï¼Œæœ€å¤§ä¿è¯çŸ¥è¯†çš„ä¿ç•™ã€‚ æ¨¡å‹ ä¸‰éƒ¨æ›²ï¼š é€šç”¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ ç›®æ ‡ä»»åŠ¡çš„è¯­è¨€æ¨¡å‹fine-tuning ç›®æ ‡ä»»åŠ¡çš„åˆ†ç±»fine-tuning trickDiscriminative ï¬ne-tuningMotivationï¼šä¸åŒå±‚æœ‰ä¸åŒçš„ä¿¡æ¯ï¼›åº”å½“fine-tune ä¸åŒç¨‹åº¦ï¼Œä¹Ÿå³ä½¿ç”¨ä¸åŒçš„learning rateã€‚ ä½œè€…å‘ç°ä¸Šä¸€å±‚çš„å­¦ä¹ ç‡æ˜¯ä¸‹ä¸€å±‚çš„2.6å€æ—¶æ•ˆæœæ¯”è¾ƒå¥½ã€‚ Slanted triangular learning rates (STLR) å…·ä½“å…¬å¼ï¼š Gradual unfreezingä»é¡¶å±‚åˆ°åº•å±‚ï¼Œä¸€æ­¥ä¸€æ­¥unfreezeï¼Œä¹Ÿå³ä»ä¸Šåˆ°ä¸‹fine-tuneã€‚è¿™æ˜¯å› ä¸ºæœ€ä¸Šä¸€å±‚æœ‰æœ€å°‘çš„general knowledgeã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>TCN</tag>
        <tag>Transformer-XL</tag>
        <tag>Trellis Networks</tag>
        <tag>continuous decoding</tag>
        <tag>ULMFiT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è®ºæ–‡1]]></title>
    <url>%2F2018%2F10%2F14%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%871%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Learned in Translation: Contextualized Word Vectors]CoVeæ˜¯ç¬¬ä¸€ä¸ªå¼•å…¥åŠ¨æ€è¯å‘é‡çš„æ¨¡å‹ã€‚Motivationï¼šç¿»è¯‘æ¨¡å‹èƒ½å¤Ÿä¿å­˜æœ€å¤šçš„ä¿¡æ¯ï¼Œå› ä¸ºå¦‚æœä¿å­˜ä¿¡æ¯ä¸å¤Ÿå¤šï¼Œdecoderæ¥æ”¶åˆ°çš„ä¿¡æ¯ä¸è¶³ï¼Œç¿»è¯‘æ•ˆæœå°±ä¸ä¼šå¥½ã€‚ï¼ˆä½†å®é™…ä¸Šï¼Œæˆ‘ä¸ªäººè®¤ä¸ºï¼Œdecoderçš„è¡¨ç°è¿˜å’Œlanguage modelæœ‰å…³ï¼Œå¦‚æœdecoderæ˜¯ä¸€ä¸ªå¥½çš„language modelï¼Œä¹Ÿæœ‰å¯èƒ½ç¿»è¯‘å‡ºä¸é”™çš„ç»“æœï¼‰ åšæ³•ï¼šä½¿ç”¨ä¼ ç»ŸNMTçš„encoder-decoderçš„åšæ³•ç¿»è¯‘æ¨¡å‹ï¼Œåªæ˜¯å°†(bi)LSTMæ‰€å¾—åˆ°çš„éšå±‚çŠ¶æ€è¡¨ç¤ºå–å‡ºæ¥å’Œembeddingæ‹¼æ¥èµ·æ¥ï¼Œä½œä¸ºä¸€ä¸ªè¯çš„è¡¨ç¤ºï¼š w=[GloVe(w); CoVe(w)] 2ï¸âƒ£[Language Modeling with Gated Convolutional Networks]ä½¿ç”¨CNNå¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå»ºæ¨¡ï¼Œæé«˜å¹¶è¡Œæ€§ã€‚ è´¡çŒ®ï¼šä½¿ç”¨äº†CNNè¿›è¡Œlanguage modelå»ºæ¨¡ï¼›æå‡ºäº†ç®€åŒ–ç‰ˆçš„gateæœºåˆ¶åº”ç”¨åœ¨CNNä¸­ã€‚ åšæ³•ï¼š å®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªè¾“å…¥ä¸¤ä¸ªfilterï¼Œå·ç§¯å‡ºæ¥çš„åšä¸€ä¸ªgateçš„æ“ä½œ$H_0 = AâŠ—Ïƒ(B)$ï¼Œæ§åˆ¶æµå‘ä¸‹ä¸€å±‚çš„æ•°æ®ã€‚ ä¸€ä¸ªå°ç»†èŠ‚æ˜¯ï¼Œä¸ºäº†ä¸è®©language modelçœ‹åˆ°ä¸‹ä¸€ä¸ªè¯ï¼Œæ¯ä¸€å±‚åœ¨å¼€å§‹å·ç§¯çš„æ—¶å€™ä¼šåœ¨å·¦è¾¹æ·»åŠ kernel_size-1ä¸ªpaddingã€‚ æ‰©å±•ï¼šå› ä¸ºCNNçš„å¹¶è¡Œæ€§é«˜ï¼Œå¯ä»¥ä½¿ç”¨CNNæ¥å¯¹language modelå»ºæ¨¡æ›¿ä»£ELMoï¼ŒåŒæ ·å¯ä»¥è·å¾—åŠ¨æ€è¯å‘é‡ã€‚è¿™ä¸ªæƒ³æ³•å·²ç»ç”±æå‡ºELMoçš„å›¢é˜Ÿåšå‡ºæ¥å¹¶è¿›è¡Œå¯¹æ¯”äº†ã€‚è®ºæ–‡ï¼šDissecting Contextual Word Embeddings: Architecture and Representation ç›®å‰æ­£åœ¨å¤ç°è¯¥è®ºæ–‡ ã€‚ 3ï¸âƒ£[Attention is All you need]éå¸¸ç»å…¸çš„è®ºæ–‡ã€‚æå‡ºäº†Transformerã€‚ä¸ºäº†è¯»BERTé‡æ¸©äº†ä¸€éã€‚ 4ï¸âƒ£[Improving Language Understanding by Generative Pre-Training]BERTå°±æ˜¯followè¿™ç¯‡æ–‡ç« çš„å·¥ä½œã€‚ä½¿ç”¨Transformeré¢„è®­ç»ƒä¸€ä¸ªlanguage modelè¿›è¡Œè¿ç§»å­¦ä¹ ã€‚ è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤æ­¥ï¼šâ‘ ä½¿ç”¨æœªæ ‡è®°æ•°æ®è®­ç»ƒlanguage modelï¼›â‘¡ä½¿ç”¨æœ‰æ ‡è®°æ•°æ®è¿›è¡Œfine-tune Motivationï¼šELMoæ˜¯è®­ç»ƒå¥½language modelï¼Œç„¶åè·å¾—åŠ¨æ€è¯å‘é‡å†ç”¨åˆ°å…¶ä»–ä»»åŠ¡ä¸Šï¼Œè¿™æ ·å°±ä¼šå¤šäº†å¾ˆå¤šå‚æ•°ã€‚å’ŒELMoä¸åŒçš„æ˜¯ï¼Œè¿™é‡Œä½¿ç”¨ä¸€ä¸ªTransformeræ¨¡å‹è§£å†³å¤šç§ä»»åŠ¡ï¼ˆåˆ©ç”¨è¿ç§»å­¦ä¹ ï¼‰ã€‚ è´¡çŒ®ï¼šä½¿ç”¨Transformerè¿›è¡Œlanguage modelå»ºæ¨¡ï¼›å°è¯•åˆ©ç”¨language modelè¿›è¡Œè¿ç§»å­¦ä¹ è€Œä¸æ˜¯å¦ä¸€ç§æ€è·¯ï¼ˆELMoï¼‰åªæå–è¯å‘é‡ã€‚ â‘ æ— ç›‘ç£å­¦ä¹ language model å…·ä½“åˆ°Transformerå°±æ˜¯ï¼š â‘¡ç›‘ç£å­¦ä¹ ï¼ˆfine-tuneï¼‰æ ¹æ®è¾“å…¥é¢„æµ‹æ ‡ç­¾ å…·ä½“å°±æ˜¯ï¼š å°†ä¸¤ä¸ªä»»åŠ¡ä¸€èµ·è®­ç»ƒï¼Œåˆ™æœ‰ï¼š å¯¹äºä¸åŒä»»åŠ¡ï¼Œå¯¹è¾“å…¥è¿›è¡Œä¸€å®šçš„æ”¹åŠ¨ä»¥é€‚åº”Transformerç»“æ„ï¼š 5ï¸âƒ£[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]åˆ·çˆ†å„æ¦œå•çš„ä¸€ç¯‡ç¥æ–‡ã€‚ä½¿ç”¨Transformeré¢„è®­ç»ƒä¸€ä¸ªlanguage modelè¿›è¡Œè¿ç§»å­¦ä¹ ã€‚ Motivationï¼šä¹‹å‰çš„language modelåªèƒ½æ ¹æ®å‰é¢çš„è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªï¼ˆå³ä½¿ELMoæ˜¯åŒå‘çš„LSTMï¼Œä¹Ÿæ˜¯åˆ†åˆ«è®­ç»ƒä¸€ä¸ªå‰å‘å’Œä¸€ä¸ªåå‘çš„ï¼‰ï¼Œé™åˆ¶äº†åŒå‘çš„contextï¼›å› æ­¤æå‡ºäº†åŒå‘çš„language modelã€‚ åšæ³•ï¼šæ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šâ‘ masked LMï¼šå› ä¸ºä½¿ç”¨äº†ä¸¤è¾¹çš„contextï¼Œè€Œlanguage modelçš„ç›®çš„æ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œè¿™æ ·æ¨¡å‹ä¼šæå‰çœ‹åˆ°ä¸‹ä¸€ä¸ªè¯ï¼Œä¸ºäº†è§£å†³è¯¥é—®é¢˜ï¼Œè®­ç»ƒçš„æ—¶å€™è®²éƒ¨åˆ†è¯maskæ‰ï¼Œæœ€ç»ˆåªé¢„æµ‹è¢«maskæ‰çš„è¯ã€‚ â‘¡Next Sentence Predictionï¼šéšæœº50%ç”Ÿæˆä¸¤ä¸ªå¥å­æ˜¯æœ‰ä¸Šä¸‹å¥å…³ç³»çš„ï¼Œ50%ä¸¤ä¸ªå¥å­æ˜¯æ²¡æœ‰å…³ç³»çš„ï¼Œç„¶ååšåˆ†ç±»ï¼›å…·ä½“æ¥è¯´æ˜¯æ‹¿ç¬¬ä¸€ä¸ªè¯[CLS]ï¼ˆè¿™æ˜¯æ‰‹åŠ¨æ·»åŠ çš„ï¼‰çš„è¡¨ç¤ºï¼Œè¿‡ä¸€ä¸ªsoftmaxå±‚å¾—åˆ°ã€‚ è”åˆè®­ç»ƒè¿™ä¸¤ä¸ªä»»åŠ¡ã€‚ æ¥ä¸‹æ¥æ˜¯é€šè¿‡å…·ä½“çš„ä»»åŠ¡è¿›è¡Œfine-tuneã€‚ä¸€ä¸ªæ¨¡å‹è§£å†³å¤šç§é—®é¢˜ï¼š æœ¬æ–‡è´¡çŒ®ï¼šä½¿ç”¨Transformerè¿›è¡ŒåŒå‘çš„language modelå»ºæ¨¡ã€‚è®ºæ–‡æåˆ°çš„ä¸€äº›ç»†èŠ‚/trickséå¸¸å€¼å¾—è®¨è®ºï¼Œæ¯”å¦‚å¯¹token embeddingæ·»åŠ äº†è®¸å¤šä¿¡æ¯ï¼Œéå¸¸ç®€å•ç²—æš´ã€‚]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>Transformer</tag>
        <tag>æ¯å‘¨è®ºæ–‡é˜…è¯»</tag>
        <tag>CoVe</tag>
        <tag>GCNN</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 18:Deep Reinforcement Learning]]></title>
    <url>%2F2018%2F10%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2018%3A%20Deep%20Reinforcement%20Learning%2F</url>
    <content type="text"><![CDATA[è®°å·ï¼š $a$æ˜¯actionï¼Œ$s$å³å¤–éƒ¨çŠ¶æ€stateï¼Œ$\pi_{\theta}(s)$ä¹Ÿå³ä»$s$æ˜ å°„åˆ°$a$çš„å‡½æ•°ï¼›$r$æ˜¯rewardï¼Œæ¯é‡‡å–ä¸€ä¸ªåŠ¨ä½œï¼Œä¼šæœ‰ä¸€ä¸ªrewardï¼Œåˆ™æ€»çš„rewardä¸º R_\theta = \sum_{t=1}^{T} r_tæˆ‘ä»¬ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ‹Ÿåˆ$\pi$ï¼Œä¸€ä¸ªeposide $\tau$æ˜¯ä¸€ä¸ªæµç¨‹ä¸‹æ¥çš„çš„æ‰€æœ‰stateã€actionå’Œrewardçš„é›†åˆã€‚ \tau = \{s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T \}å¦‚æœæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„actorè¿è¡Œnæ¬¡ï¼Œåˆ™æ¯ä¸ª$\tau$ä¼šæœ‰ä¸€å®šçš„æ¦‚ç‡è¢«é‡‡æ ·åˆ°ï¼Œé‡‡æ ·æ¦‚ç‡è®°ä¸º$P(\tau|\theta)$ï¼Œåˆ™æˆ‘ä»¬å¯ä»¥é€šè¿‡é‡‡æ ·çš„æ–¹å¼æ¥å¯¹æœŸæœ›rewardè¿›è¡Œä¼°è®¡ï¼š \overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta) â‰ˆ \frac{1}{N} \sum_{n=1}^{N} R(\tau^n)é‚£ä¹ˆæˆ‘ä»¬æ¥ä¸‹æ¥çš„ç›®æ ‡å°±æ˜¯æœ€å¤§åŒ–æœŸæœ›rewardï¼Œå…¶ä¸­æœŸæœ›rewardæ˜¯ï¼š \overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta)æˆ‘ä»¬åŒæ ·ä½¿ç”¨æ¢¯åº¦ä¸Šå‡ï¼šå…¶ä¸­ä¸$Î¸$ç›¸å…³çš„æ˜¯$P$ï¼Œåˆ™å¯ä»¥å†™æˆï¼š \nabla \overline{R}_\theta = \sum_\tau R(\tau) \nabla P(\tau|\theta)= \sum_\tau R(\tau) P(\tau|\theta) \frac{\nabla P(\tau|\theta)}{P(\tau|\theta)}ç”±äº$\dfrac {d\log \left( f\left( x\right) \right) }{dx}=\dfrac {1}{f\left( x\right) }\dfrac {df(x)}{dx}$ï¼Œåˆ™å‰å¼å¯å†™æˆï¼š \nabla \overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta) \nabla log P(\tau | \theta) â‰ˆ \frac{1}{N} \sum_{n=1}^{N} R(\tau^n) log P(\tau ^n| \theta)å¦‚ä½•æ±‚æ¢¯åº¦ï¼Ÿç”±äºï¼š P(\tau | \theta)=p(s_1)p(a_1|s_1,\theta)p(r_1,s_2|s_1,a_1)p(a_2|s_2,\theta)p(r_2,s_3|s_2,a_2)... \\=p(s_1)\prod_{t=1}^{T}p(a_t|s_t,\theta)p(r_t , s_{t+1}| s_t,a_t)å®é™…ä¸Šï¼Œå…¶ä¸­ä¸æ¢¯åº¦ç›¸å…³çš„åªæœ‰ä¸­é—´é¡¹$p(a_t|s_t,\theta)$ï¼Œè¯¥é¡¹ä¹Ÿå³$Ï€$å‡½æ•°ï¼Œä»stateåˆ°actionçš„æ˜ å°„ã€‚å–logå¹¶æ±‚å¯¼ï¼Œæœ‰ï¼š \nabla log P(\tau | \theta)= \sum_{t=1}^{T} \nabla log p(a_t|s_t,\theta)ä»£å›ï¼Œå› æ­¤æœ€ç»ˆ$\overline{R}_\theta$çš„æ¢¯åº¦ä¸ºï¼š \nabla \overline{R}_\theta = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^n) \nabla log p(a_{t}^n | s_t^n,\theta)æ³¨æ„åˆ°è¯¥å¼å­å‘Šè¯‰æˆ‘ä»¬ï¼Œåº”è€ƒè™‘æ•´ä½“çš„rewardè€Œä¸åº”è¯¥åªè€ƒè™‘æ¯ä¸€æ­¥çš„rewardï¼›å¹¶ä¸”å–logçš„åŸå› å¯ä»¥ç†è§£æˆæ˜¯å¯¹actionå–å½’ä¸€åŒ–ï¼Œå› ä¸ºï¼š \frac{\nabla p(a_t^n | s_t^n,\theta)}{p(a_t^n | s_t^n,\theta)}ä¹Ÿå°±æ˜¯è¯´å¯¹äºé‚£äº›å‡ºç°æ¬¡æ•°è¾ƒå¤šçš„actionï¼Œè¦è¡¡é‡ä»–ä»¬å¯¹rewardçš„çœŸæ­£å½±å“ï¼Œåº”å½“å¯¹ä»–ä»¬å½’ä¸€åŒ–ã€‚ ä¸ºäº†è®©é‚£äº›å‡ºç°å¯èƒ½æ€§è¾ƒä½çš„actionä¸ä¼šå› ä¸ºæ²¡è¢«sampleåˆ°è€Œåœ¨æ›´æ–°åè¢«é™ä½ä»–ä»¬çš„æ¦‚ç‡ï¼Œå¯ä»¥æ·»åŠ ä¸€ä¸ªbaselineï¼Œåªæœ‰è¶…è¿‡$b$çš„rewardæ‰ä¼šå¢åŠ ä»–ä»¬å‡ºç°çš„æ¦‚ç‡ã€‚ \nabla \overline{R}_\theta = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} (R(\tau^n)-b) \nabla log p(a_{t}^n | s_t^n,\theta)]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>Deep Reinforcement Learning</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 17:Ensemble]]></title>
    <url>%2F2018%2F10%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2017%3A%20Ensemble%2F</url>
    <content type="text"><![CDATA[Baggingå¯¹äºå¤æ‚æ¨¡å‹ï¼Œå¾€å¾€varianceä¼šå¤§ï¼Œé€šè¿‡å¯¹å¤šä¸ªæ¨¡å‹çš„å¹³å‡ï¼Œèƒ½å¤Ÿå‡å°varianceï¼š baggingçš„æ€æƒ³æ˜¯å¤šæ¬¡æœ‰æ”¾å›åœ°é‡‡æ ·Nâ€™ä¸ªç‚¹ï¼ˆé€šå¸¸Nâ€™=Nï¼‰ï¼Œç„¶åå¯¹é‡‡æ ·çš„å‡ ä¸ªæ•°æ®é›†åˆ†åˆ«è®­ç»ƒä¸€ä¸ªæ¨¡å‹ æµ‹è¯•çš„æ—¶å€™å†å¯¹å‡ ä¸ªæ¨¡å‹è¿›è¡Œå¹³å‡æˆ–æŠ•ç¥¨ BoostingåŸºæœ¬æ€æƒ³æ˜¯å¯¹å‡ ä¸ªå¼±åˆ†ç±»å™¨çº¿æ€§åŠ æƒï¼Œå¾—åˆ°å¼ºåˆ†ç±»å™¨ã€‚åˆ†ç±»å™¨æŒ‰å…ˆåé¡ºåºè®­ç»ƒï¼Œæ¯æ¬¡è®­ç»ƒå®Œï¼Œå¯¹æ–°æ¨¡å‹åˆ†ç±»é”™è¯¯çš„æ•°æ®è¿›è¡Œè°ƒé«˜æƒé‡ï¼Œè€Œæ­£ç¡®çš„æ•°æ®åˆ™é™ä½æƒé‡ã€‚ å¯ä»¥ä¿è¯ï¼šåªè¦åˆ†ç±»å™¨çš„é”™è¯¯ç‡å°äº50%ï¼Œåœ¨boostingåèƒ½å¤Ÿæœ‰100%çš„æ­£ç¡®ç‡ï¼ˆåœ¨è®­ç»ƒé›†ï¼‰ã€‚ è¯æ˜è¿‡ç¨‹ç•¥ã€‚ Ensemble: StackingåŸºæœ¬æ€æƒ³ï¼šä½¿ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒå¤šä¸ªåˆçº§åˆ†ç±»å™¨ï¼Œå°†åˆçº§åˆ†ç±»å™¨çš„è¾“å‡ºä½œä¸ºæ¬¡çº§åˆ†ç±»å™¨çš„è¾“å…¥ï¼Œè·å¾—æœ€ç»ˆçš„è¾“å‡ºã€‚æˆ‘ä»¬åº”å½“ä½¿ç”¨ä¸åŒçš„è®­ç»ƒæ•°æ®æ¥è®­ç»ƒæ¬¡çº§åˆ†ç±»å™¨]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Ensemble</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æµ…è°ˆmaskçŸ©é˜µ]]></title>
    <url>%2F2018%2F10%2F12%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%B5%85%E8%B0%88mask%E7%9F%A9%E9%98%B5%2F</url>
    <content type="text"><![CDATA[ä¸ªäººç›®å‰å¯¹maskçŸ©é˜µçš„ä¸€ç‚¹ç†è§£ã€‚ æ˜¯ä»€ä¹ˆmaskçŸ©é˜µæ˜¯ä»€ä¹ˆï¼Ÿæ˜¯ä¸€ä¸ªç”±0å’Œ1ç»„æˆçš„çŸ©é˜µã€‚ä¸€ä¸ªä¾‹å­æ˜¯ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä¸­ï¼Œå¥å­çš„é•¿åº¦æ˜¯ä¸ç­‰é•¿çš„ï¼Œä½†å› ä¸ºæˆ‘ä»¬ç»å¸¸å°†å¥å­ç»„æˆmini-batchç”¨ä»¥è®­ç»ƒï¼Œå› æ­¤é‚£äº›é•¿åº¦è¾ƒçŸ­çš„å¥å­éƒ½ä¼šåœ¨å¥å°¾è¿›è¡Œå¡«å……0ï¼Œä¹Ÿå³paddingçš„æ“ä½œã€‚ä¸€ä¸ªmaskçŸ©é˜µå³ç”¨ä»¥æŒ‡ç¤ºå“ªäº›æ˜¯çœŸæ­£çš„æ•°æ®ï¼Œå“ªäº›æ˜¯paddingã€‚å¦‚ï¼šå›¾ç‰‡æ¥æºï¼šTheanoï¼šLSTMæºç è§£æ å…¶ä¸­maskçŸ©é˜µä¸­1ä»£è¡¨çœŸå®æ•°æ®ï¼›0ä»£è¡¨paddingæ•°æ®ã€‚ ä¸ºä»€ä¹ˆä¸ºä»€ä¹ˆè¦ä½¿ç”¨maskçŸ©é˜µï¼Ÿä½¿ç”¨maskçŸ©é˜µæ˜¯ä¸ºäº†è®©é‚£äº›è¢«maskæ‰çš„tensorä¸ä¼šè¢«æ›´æ–°ã€‚è€ƒè™‘ä¸€ä¸ªtensor Tçš„size(a,b)ï¼ŒåŒæ ·å¤§å°çš„maskçŸ©é˜µMï¼Œç›¸ä¹˜åï¼Œåœ¨åå‘å›ä¼ çš„æ—¶å€™åœ¨Tå¯¹åº”maskä¸º0çš„åœ°æ–¹ï¼Œ0çš„æ¢¯åº¦ä»ä¸º0ã€‚å› æ­¤ä¸ä¼šè¢«æ›´æ–°ã€‚ æ€ä¹ˆåšæ¥ä¸‹æ¥ä»‹ç»å‡ ç§ï¼ˆå¯èƒ½ä¸å…¨ï¼‰ä½¿ç”¨maskçš„åœºæ™¯ã€‚ å¯¹è¾“å…¥è¿›è¡Œmaskè€ƒè™‘NLPä¸­å¸¸è§çš„å¥å­ä¸ç­‰é•¿çš„æƒ…å†µã€‚è®¾æˆ‘ä»¬çš„è¾“å…¥çš„batch I:(batch_size,max_seqlen)ï¼Œæˆ‘ä»¬åœ¨è¿‡ä¸€å±‚Embeddingå±‚ä¹‹å‰ï¼Œåœ¨è¿‡äº†ä¸€å±‚Embeddingå±‚ï¼Œåˆ™æœ‰ E:(batch_size,max_seqlen,embed_dim)ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›Embeddingæ˜¯æ›´æ–°çš„(æ¯”å¦‚æˆ‘ä»¬çš„Embeddingæ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œé‚£å½“ç„¶Embeddingéœ€è¦æ›´æ–°)ï¼Œä½†æˆ‘ä»¬åˆä¸å¸Œæœ›paddingæ›´æ–°ã€‚ä¸€ç§æ–¹æ³•å³ä»¤Eä¸Mç›¸ä¹˜ã€‚å…¶ä¸­Mæ˜¯maskçŸ©é˜µ(batch_size,max_seqlen,1) (1æ˜¯å› ä¸ºè¦broadcastï¼‰ï¼Œè¿™æ ·åœ¨Embeddingæ›´æ–°æ¢¯åº¦æ—¶ï¼Œå› ä¸ºmaskçŸ©é˜µçš„å…³ç³»ï¼Œpaddingä½ç½®ä¸Šçš„æ¢¯åº¦å°±æ˜¯0ã€‚å½“ç„¶åœ¨Pytorchä¸­è¿˜å¯ä»¥ç›´æ¥æ˜¾å¼åœ°å†™ï¼š1self.embedding = nn.Embedding(vocab_size, embed_dim,padding_idx=0) è€Œæ­¤æ—¶åº”å½“å°†paddingæ˜¾å¼æ·»åŠ åˆ°è¯å…¸çš„ç¬¬ä¸€ä¸ªã€‚ å¯¹æ¨¡å‹ä¸­é—´è¿›è¡Œmaskä¸€ä¸ªå¾ˆç»å…¸çš„åœºæ™¯å°±æ˜¯dropoutã€‚å¯¹äºå‚æ•°çŸ©é˜µW:(h,w)ï¼ŒåŒæ ·å¤§å°çš„maskçŸ©é˜µMï¼Œåœ¨å‰å‘ä¼ æ’­æ—¶ä»¤Wâ€™=W*Mï¼Œåˆ™åœ¨åå‘ä¼ æ’­æ—¶ï¼ŒMä¸­ä¸º0çš„éƒ¨åˆ†ä¸è¢«æ›´æ–°ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è°ƒç”¨PyTorchä¸­çš„åŒ…nn.Dropout() 123m = nn.Dropout(p=0.2)input = torch.randn(20, 16)output = m(input) å¯¹lossè¿›è¡Œmaskè€ƒè™‘NLPä¸­çš„language modelï¼Œæ¯ä¸ªè¯éƒ½éœ€è¦é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œåœ¨ä¸€ä¸ªbatchä¸­å¥å­æ€»æ˜¯æœ‰é•¿æœ‰çŸ­ï¼Œå¯¹äºä¸€ä¸ªçŸ­å¥ï¼Œæ­¤æ—¶åœ¨è®¡ç®—lossçš„æ—¶å€™ï¼Œä¼šå‡ºç°è¿™æ ·çš„åœºæ™¯ï¼š&lt;pad&gt;è¯è¦é¢„æµ‹ä¸‹ä¸€ä¸ª&lt;pad&gt;è¯ã€‚ä¸¾ä¸ªä¾‹å­ï¼šä¸‰ä¸ªå¥å­[a,b,c,d],[e,f,g],[h,i]ï¼Œåœ¨ç»„æˆbatchåï¼Œä¼šå˜æˆXï¼š a b c d e f g &lt;pad&gt; h i &lt;pad&gt; &lt;pad&gt; Yï¼š b c d &lt;pad&gt; f g &lt;eos&gt; &lt;pad&gt; i &lt;eos&gt; &lt;pad&gt; &lt;pad&gt; Xæ˜¯è¾“å…¥ï¼ŒYæ˜¯é¢„æµ‹ã€‚é‚£ä¹ˆä»ç¬¬ä¸‰è¡Œå¯ä»¥çœ‹å‡ºï¼Œ&lt;pad&gt;åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ª&lt;pad&gt;ã€‚è¿™æ˜¾ç„¶æ˜¯æœ‰é—®é¢˜çš„ã€‚ä¸€ç§è§£å†³æ–¹æ¡ˆå°±æ˜¯ä½¿ç”¨maskçŸ©é˜µï¼Œåœ¨lossçš„è®¡ç®—æ—¶ï¼Œå°†é‚£äº›æœ¬ä¸åº”è¯¥è®¡ç®—çš„maskæ‰ï¼Œä½¿å¾—å…¶lossä¸º0ï¼Œè¿™æ ·å°±ä¸ä¼šåå‘å›ä¼ äº†ã€‚å…·ä½“å®è·µï¼šåœ¨PyTorchä¸­ï¼Œä»¥CrossEntropyä¸ºä¾‹ï¼š 12class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=â€™elementwise_meanâ€™ å¦‚æœreduction=Noneåˆ™ä¼šè¿”å›ä¸€ä¸ªä¸è¾“å…¥åŒæ ·å¤§å°çš„çŸ©é˜µã€‚åœ¨ä¸maskçŸ©é˜µç›¸ä¹˜åï¼Œå†å¯¹æ–°çŸ©é˜µè¿›è¡Œmeanæ“ä½œã€‚åœ¨PyTorchå®è·µä¸Šè¿˜å¯ä»¥å¯ä»¥è¿™ä¹ˆå†™ï¼š 123masked_outputs = torch.masked_select(dec_outputs, mask)masked_targets = torch.masked_select(targets, mask)loss = my_criterion(masked_outputs, masked_targets) å¦ä¸€ç§æ›´ä¸ºç®€å•çš„è§£å†³æ–¹æ¡ˆæ˜¯ï¼Œç›´æ¥åœ¨CrossEntropyä¸­è®¾ignore_index=0ï¼Œè¿™æ ·ï¼Œåœ¨è®¡ç®—lossçš„æ—¶å€™ï¼Œå‘ç°target=0æ—¶ï¼Œä¼šè‡ªåŠ¨ä¸å¯¹å…¶è¿›è¡Œlossçš„è®¡ç®—ã€‚å…¶æœ¬è´¨å’ŒmaskçŸ©é˜µæ˜¯ä¸€è‡´çš„ã€‚ æ€»ç»“maskçŸ©é˜µå¯ä»¥ç”¨åœ¨ä»»ä½•åœ°æ–¹ï¼Œåªè¦å¸Œæœ›ä¸ä¹‹ç›¸ä¹˜çš„tensorç›¸å¯¹åº”çš„åœ°æ–¹ä¸æ›´æ–°å°±å¯ä»¥è¿›è¡Œmaskæ“ä½œã€‚]]></content>
      <tags>
        <tag>ä»£ç å®è·µ</tag>
        <tag>maskçŸ©é˜µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ·±åº¦ç‚¼ä¸¹tricksåˆé›†]]></title>
    <url>%2F2018%2F10%2F07%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%B7%B1%E5%BA%A6%E7%82%BC%E4%B8%B9tricks%E5%90%88%E9%9B%86%2F</url>
    <content type="text"><![CDATA[â€”-Deprecatedâ€”- è°ƒå‚æŠ€å·§æ•°æ®å¢å¼ºé¢„å¤„ç†1ï¸âƒ£zero-center[9]å°†æ•°æ®ä¸­å¿ƒåŒ– åˆå§‹åŒ–1ï¸âƒ£Xavier initialization[7]æ–¹æ³•é€‚ç”¨[9]äºæ™®é€šæ¿€æ´»å‡½æ•°(tanh,sigmoid)ï¼šscale = np.sqrt(3/n) 2ï¸âƒ£He initialization[8]æ–¹æ³•é€‚ç”¨[9]äºReLUï¼šscale = np.sqrt(6/n) 3ï¸âƒ£Batch normalization[10]4ï¸âƒ£RNN/LSTM init hidden stateHinton[3]æåˆ°å°†RNN/LSTMçš„åˆå§‹hidden stateè®¾ç½®ä¸ºå¯å­¦ä¹ çš„weight è®­ç»ƒæŠ€å·§1ï¸âƒ£Gradient Clipping[5,6]2ï¸âƒ£learning rateåŸåˆ™ï¼šå½“validation losså¼€å§‹ä¸Šå‡æ—¶ï¼Œå‡å°‘å­¦ä¹ ç‡ã€‚[1]Time/Drop-based/Cyclical Learning Rate 3ï¸âƒ£batch size[2]ä¸­è¯¦ç»†è®ºè¿°äº†å¢åŠ batch sizeè€Œä¸æ˜¯å‡å°learning rateèƒ½å¤Ÿæå‡æ¨¡å‹è¡¨ç°ã€‚ä¿æŒå­¦ä¹ ç‡ä¸å˜ï¼Œæé«˜batch sizeï¼Œç›´åˆ°batch size~è®­ç»ƒé›†/10ï¼Œæ¥ä¸‹æ¥å†é‡‡ç”¨å­¦ä¹ ç‡ä¸‹é™çš„ç­–ç•¥ã€‚ Reference[1]How to make your model happy again â€” part 1 [2]Donâ€™t Decay the Learning Rate, Increase the Batch Size [3]CSC2535 2013: Advanced Machine Learning Lecture 10 Recurrent neural networks [4]https://zhuanlan.zhihu.com/p/25110150 [5]On the difficulty of training Recurrent Neural Networks [6]Language Modeling with Gated Convolutional Networks [7]Understanding the difficulty of training deep feedforward neural networks [8]Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification [9]çŸ¥ä¹ï¼šä½ æœ‰å“ªäº›deep learningï¼ˆrnnã€cnnï¼‰è°ƒå‚çš„ç»éªŒï¼Ÿ [10]Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift]]></content>
      <tags>
        <tag>è°ƒå‚</tag>
        <tag>tricks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯11]]></title>
    <url>%2F2018%2F10%2F07%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D11%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£èµ‹å¾—å¤åŸè‰é€åˆ«[å”] ç™½å±…æ˜“ç¦»ç¦»åŸä¸Šè‰ï¼Œä¸€å²ä¸€æ¯è£ã€‚é‡ç«çƒ§ä¸å°½ï¼Œæ˜¥é£å¹åˆç”Ÿã€‚è¿œèŠ³ä¾µå¤é“ï¼Œæ™´ç¿ æ¥è’åŸã€‚åˆé€ç‹å­™å»ï¼Œè‹è‹æ»¡åˆ«æƒ…ã€‚ è‹è‹ï¼ˆqÄ«ï¼‰ï¼šå½¢å®¹è‰æœ¨é•¿å¾—èŒ‚ç››çš„æ ·å­ã€‚ http://m.xichuangzhu.com/work/57b8a5371532bc005b99da51]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRMLç¬¬ä¸‰ç«  å›å½’çš„çº¿æ€§æ¨¡å‹]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FPRML%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%9B%9E%E5%BD%92%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[çº¿æ€§åŸºå‡½æ•°æ¨¡å‹ åç½®-â½…å·®åˆ†è§£ è´å¶æ–¯çº¿æ€§å›å½’ è´å¶æ–¯æ¨¡å‹â½è¾ƒ è¯æ®è¿‘ä¼¼]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 16:SVM]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2016%3A%20SVM%2F</url>
    <content type="text"><![CDATA[Hinge Loss+kernel method = SVM Hinge LossSVMä¸logistic regressionçš„åŒºåˆ«å³åœ¨äºloss functionçš„ä¸åŒï¼Œlogisticæ˜¯cross entropyï¼Œè€ŒSVMæ˜¯hinge loss ä¹Ÿå³å¦‚æœåˆ†ç±»é—´éš”å¤§äº1ï¼Œåˆ™ $L(m_i)=max(0,1âˆ’m_i(w))$ï¼Œåˆ™æŸå¤±ä¸º0ã€‚å› æ­¤SVMæ›´å…·é²æ£’æ€§ï¼Œå› ä¸ºå¯¹ç¦»ç¾¤ç‚¹ä¸æ•æ„Ÿã€‚ å¯¹äºlinear SVMï¼š å®šä¹‰å‡½æ•° $f(x)=\sum_i w_i x_i +b=w^T x$ å®šä¹‰æŸå¤±å‡½æ•° $L(f)=\sum_n l(f(x^n),\hat{y}^n)+\lambda ||w||_2$ï¼Œå…¶ä¸­$l(f(x^n),\hat{y}^n)=max(0,1-\hat{y}^n f(x))$ æ¢¯åº¦ä¸‹é™æ±‚è§£ï¼ˆçœç•¥äº†æ­£åˆ™åŒ–ï¼‰ \frac{\partial{l(f(x^n),\hat{y}^n})}{\partial{w_i}}= \frac{\partial{l(f(x^n),\hat{y}^n})}{\partial{f(x^n)}} \frac{\partial{f(x^n)}}{\partial{w_i}} x_i^n è€Œ f(x^n)=w^T \cdot x^n \frac{\partial{max(0,1-\hat{y}^n f(x^n)})}{\partial{f(x^n)}}= \left\{ \begin{array}{**lr**} -\hat{y}^n & if \hat{y}^n f(x^n)]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 15:Transfer Learning]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2015%3A%20Transfer%20Learning%2F</url>
    <content type="text"><![CDATA[Model Fine-tuningå‡è®¾æˆ‘ä»¬æœ‰å¾ˆå¤šçš„source data $(x^s,y^s )$ï¼Œä¸ä»»åŠ¡ç›¸å…³çš„target data $(x^t,y^t )$ å¾ˆå°‘ã€‚æˆ‘ä»¬åˆ©ç”¨source dataè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œç„¶åç”¨target dataæ¥fine tuneæ¨¡å‹ã€‚ conservative training æˆ‘ä»¬å¯ä»¥ç”¨source dataè®­ç»ƒå¥½çš„æ¨¡å‹çš„weightä½œä¸ºæ–°çš„æ¨¡å‹çš„weightï¼Œç„¶åè®¾å®šä¸€äº›é™åˆ¶ï¼Œæ¯”å¦‚source dataä½œä¸ºè¾“å…¥çš„outputåº”å’Œtarget dataä½œä¸ºè¾“å…¥çš„outputå°½é‡ç›¸ä¼¼ï¼Œæˆ–è€…å‚æ•°å°½é‡ç›¸ä¼¼ç­‰ã€‚ layer transferä¹Ÿå°±æ˜¯æ–°æ¨¡å‹æœ‰å‡ å±‚æ˜¯ç›´æ¥copyæ—§æ¨¡å‹çš„ï¼Œåªè®­ç»ƒå…¶å®ƒå±‚ã€‚æ³¨æ„åˆ°ä¸åŒä»»åŠ¡æ‰€åº”copyçš„å±‚æ˜¯ä¸åŒçš„ï¼Œè¯­éŸ³ä»»åŠ¡æœ€åå‡ å±‚æ•ˆæœå¥½ï¼Œå›¾åƒè¯†åˆ«å‰é¢å‡ å±‚æ•ˆæœå¥½ Multitask Learningä¸åŒä»»åŠ¡ä¹‹é—´å…±äº«ç›¸åŒçš„ä¸­é—´å±‚ï¼Œå¦‚ï¼š è¿˜æœ‰ä¸€ç§progressive neural networksï¼šé¦–å…ˆè®­ç»ƒå¥½ç¬¬ä¸€ä¸ªä»»åŠ¡çš„æ¨¡å‹ï¼Œç„¶ååœ¨è®­ç»ƒç¬¬äºŒä¸ªæ¨¡å‹çš„æ—¶å€™å°†ç¬¬ä¸€ä¸ªæ¨¡å‹çš„éšå±‚åŠ å…¥åˆ°ç¬¬äºŒä¸ªæ¨¡å‹çš„éšå±‚ä¸­ï¼›è®­ç»ƒç¬¬ä¸‰ä¸ªæ¨¡å‹åˆ™å°†ç¬¬äºŒä¸ªå’Œç¬¬ä¸€ä¸ªæ¨¡å‹çš„éšå±‚åŠ å…¥åˆ°ç¬¬ä¸‰ä¸ªæ¨¡å‹çš„éšå±‚ä¸­ï¼Œä»¥æ­¤ç±»æ¨ Domain-adversarial trainingsource dataæ˜¯æœ‰æ ‡ç­¾çš„ï¼Œè€Œtarget dataæ˜¯æ— æ ‡ç­¾çš„ï¼Œéƒ½å±äºåŒä¸€ä¸ªä»»åŠ¡ï¼Œä½†æ•°æ®æ˜¯mismatchçš„ï¼Œå¦‚ï¼š å› ä¸ºNNçš„éšå±‚å¯ä»¥ç†è§£æˆæ˜¯åœ¨æŠ½å–å›¾åƒçš„ç‰¹å¾ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿåœ¨è®­ç»ƒNNçš„è¿‡ç¨‹ä¸­å»æ‰source dataçš„ä¸€äº›domain specificçš„ç‰¹æ€§ï¼Œè¿™æ ·å°±å¯ä»¥ç”¨åœ¨target dataä¸Šäº†ã€‚å› æ­¤æˆ‘ä»¬åœ¨feature exactoråé¢è¿æ¥ä¸¤ä¸ªæ¨¡å—ï¼š ä¸€æ–¹é¢æˆ‘ä»¬å¸Œæœ›æŠ½å–çš„ç‰¹å¾èƒ½å¤Ÿä½¿å¾—åˆ†ç±»å™¨æ­£ç¡®åœ°åˆ†ç±»ï¼Œå¦ä¸€æ–¹é¢æˆ‘ä»¬å¸Œæœ›è¿™äº›ç‰¹å¾èƒ½å¤Ÿè®©domain classifierèƒ½å¤Ÿæ— æ³•è¯†åˆ«ç‰¹å¾æ˜¯ä»å“ªäº›dataæŠ½å–å¾—åˆ°çš„ï¼Œè¿™æ ·å¾—åˆ°çš„ç‰¹å¾å°±æ˜¯è¢«å»æ‰domain specificç‰¹å¾çš„ã€‚ å…·ä½“è®­ç»ƒï¼š Zero-shot Learningsource dataæœ‰æ ‡ç­¾ï¼Œtarget dataæ— æ ‡ç­¾ï¼Œä½†ä»»åŠ¡ä¸åŒï¼Œå¦‚ï¼š Representing each class by its attributesä¸€ç§æ–¹æ³•æ˜¯å°†æ¯ä¸€ä¸ªç±»éƒ½ç”¨ç‰¹å¾è¡¨ç¤ºï¼Œä½†ç‰¹å¾è¦è¶³å¤Ÿä¸°å¯Œï¼š åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œè¾“å…¥æ˜¯å›¾ç‰‡ï¼Œè¾“å‡ºåˆ™æ˜¯è¿™äº›ç‰¹å¾ï¼šè¿™æ ·åœ¨å°†target dataæ”¾å…¥è®­ç»ƒå¥½çš„NNåä¹Ÿä¼šå¾—åˆ°ä¸€ä¸ªè¿™æ ·çš„attributeï¼ŒæŸ¥è¡¨å³å¯æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ç‰¹å¾å¯¹åº”çš„ç±»ã€‚ Attribute embeddingå¦‚æœç‰¹å¾ç»´åº¦å¤ªé«˜ï¼Œä¹Ÿå¯ä»¥å°†ç‰¹å¾å‹ç¼©æˆä¸€ä¸ªå‘é‡è¡¨ç¤ºï¼Œè¿™æ ·åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œè¾“å‡ºåˆ™æ˜¯è¿™æ ·çš„å‘é‡ç‰¹å¾ï¼Œè¾“å…¥target dataï¼Œè¾“å‡ºå‘é‡ç‰¹å¾ï¼Œæ‰¾åˆ°æœ€è¿‘çš„ç‰¹å¾å¯¹åº”çš„ç±»å³å¯ Attribute embedding + word embeddingå¦‚æœæ²¡æœ‰attributeæ•°æ®ï¼Œåˆ©ç”¨word embeddingä¹Ÿå¯ä»¥è¾¾åˆ°ä¸é”™çš„æ•ˆæœã€‚åœ¨zero-shot learningä¸­ï¼Œå…‰æ˜¯è®©ç›¸åŒç±»çš„få’Œgç›¸ä¼¼æ˜¯ä¸å¤Ÿçš„ï¼Œè¿˜åº”è¯¥è®©ä¸åŒçš„få’Œgå°½é‡è¿œã€‚ f^âˆ—,g^âˆ—=arg min_{(f,g)}â¡âˆ‘_nmax(0,kâˆ’f(x^n )\cdot g(y^n )+max_{(mâ‰ n)} â¡f(x^m )\cdot g(x^m ) )]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Transfer Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 14:Unsupervised Learning:Generation]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2014%3A%20Unsupervised%20Learning%3A%20Generation%2F</url>
    <content type="text"><![CDATA[Component-by-componentå¯¹äºå›¾åƒæ¥è¯´ï¼Œæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªpixelï¼šPixelRNN VAEæ¶æ„ï¼š å…¶ä¸­eæ˜¯å™ªå£°ï¼ŒÏƒæ˜¯æ–¹å·®ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–reconstruction errorï¼Œä»¥åŠä¸€ä¸ªé™åˆ¶ã€‚è¯¥é™åˆ¶çš„ç›®çš„å³é˜²æ­¢Ïƒ=0ï¼Œmæ˜¯æ­£åˆ™åŒ–é¡¹ã€‚ ä¸­é—´çš„æ¨å¯¼ä»¥åŠä¸ºä»€ä¹ˆæ˜¯è¿™æ ·çš„æ¶æ„æˆ‘è¿˜ä¸æ˜¯å¾ˆæ‡‚ï¼Œä¹‹åå†æ›´æ–°ã€‚å®é™…ä¸Šå¯ä»¥è¿™ä¹ˆç†è§£ï¼Œæœ‰å‡ ä¸ªè¦ç‚¹ï¼š é¦–å…ˆæˆ‘ä»¬æ˜¯åŸºäºè¿™ä¹ˆä¸€ä¸ªå‡è®¾ï¼šä¸­é—´çš„codeåº”å½“æ˜¯æœä»æ­£æ€åˆ†å¸ƒçš„ï¼Œè€Œencoderçš„ä½œç”¨å³åœ¨äºæ‹Ÿåˆè¯¥æ­£æ€åˆ†å¸ƒçš„å‡å€¼ä¸æ–¹å·®çš„å¯¹æ•°ï¼ˆå› ä¸ºæ–¹å·®åº”å½“æ’ä¸ºæ­£ï¼Œä½†ç¥ç»ç½‘ç»œçš„è¾“å‡ºå¯èƒ½æœ‰æ­£æœ‰è´Ÿï¼‰ å¦‚æœç”Ÿæˆå‡ºæ¥çš„codeä¸ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼Œä¼šæœ‰ä¸€ä¸ªæƒ©ç½šé¡¹ï¼Œä¹Ÿå°±æ˜¯ä¸Šå›¾çš„constraintï¼ˆå¯ä»¥é€šè¿‡KLæ•£åº¦æ¨å¯¼è·å¾—ï¼‰ æŒ‰ç†è¯´ï¼Œåº”å½“æ˜¯åœ¨ç”Ÿæˆäº†å‡å€¼å’Œæ–¹å·®åï¼Œå®šä¹‰å¥½è¯¥æ­£æ€åˆ†å¸ƒï¼Œç„¶åå†ä»ä¸­é‡‡æ ·ï¼Œä½†æ˜¯è¿™æ ·æ²¡åŠæ³•å›ä¼ æ›´æ–°æ¢¯åº¦ï¼Œå› æ­¤è¿™é‡Œä½¿ç”¨é‡å‚æ•°æŠ€å·§(Reparameterization Trick)ï¼Œä¹Ÿå³ä»$N(\mu,\sigma^2)$ä¸­é‡‡æ ·$Z$ï¼Œç›¸å½“äºä»$N(0,I)$ä¸­é‡‡æ ·$\varepsilon$ï¼Œç„¶åè®©$Z=\mu + \varepsilon \times \mu$ Reference:https://www.sohu.com/a/226209674_500659 VAEçš„ä¸»è¦é—®é¢˜åœ¨äºï¼Œç½‘ç»œåªè¯•å›¾å»è®°ä½è§è¿‡çš„å›¾åƒï¼Œä½†æ²¡æ³•çœŸæ­£å»ç”Ÿæˆæ²¡è§è¿‡çš„å›¾åƒã€‚ Generative Adversarial Network (GAN)GANåŒ…å«ä¸€ä¸ªdiscriminatorå’Œä¸€ä¸ªgeneratorï¼Œgeneratorè¯•å›¾ç”Ÿæˆèƒ½å¤Ÿéª—è¿‡discriminatorçš„æ ·æœ¬ï¼Œè€Œgeneratorè¯•å›¾èƒ½å¤Ÿå°†generatorç”Ÿæˆçš„æ ·æœ¬å’ŒçœŸå®çš„æ ·æœ¬åŒºåˆ†ã€‚ ä¹‹åä¼šæœ‰è¯¦ç»†çš„ä»‹ç»ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Generation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 13:Unsupervised Learning:Auto-encoder]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2013%3A%20Unsupervised%20Learning%3A%20Auto-encoder%2F</url>
    <content type="text"><![CDATA[Auto-encoderç”±ä¸€ä¸ªencoderå’Œä¸€ä¸ªdecoderç»„æˆï¼Œencoderè´Ÿè´£å°†è¾“å…¥è½¬æˆä¸€ä¸ªå‘é‡è¡¨ç¤ºï¼ˆç»´åº¦é€šå¸¸å°äºè¾“å…¥ï¼‰ï¼Œdecoderè´Ÿè´£å°†è¿™æ®µå‘é‡è¡¨ç¤ºæ¢å¤æˆåŸæ¥çš„è¾“å…¥ã€‚é‚£ä¹ˆä¸­é—´çš„codeå°±å¯ä»¥ä½œä¸ºè¾“å…¥çš„ä¸€ä¸ªä½ç»´è¡¨ç¤ºï¼š Auto-encoder for CNN Unpoolingæœ‰ä¸¤ç§æ–¹æ³•ï¼Œä¸€ç§åœ¨poolingçš„æ—¶å€™è®°å½•æœ€å¤§å€¼çš„ä½ç½®ï¼Œåœ¨unpoolingæ—¶åœ¨ç›¸å¯¹ä½ç½®å¡«å……æœ€å¤§å€¼ï¼Œå…¶ä»–ä½ç½®å¡«å……0ï¼›å¦ä¸€ç§ä¸è®°å½•æœ€å¤§å€¼ä½ç½®ï¼Œç›´æ¥åœ¨poolingåŒºåŸŸå…¨éƒ¨å¡«å……æœ€å¤§å€¼ã€‚ Deconvolutionå…¶å®æœ¬è´¨å°±æ˜¯convolutionã€‚ è¿™æ˜¯convolution: æˆ‘ä»¬æœŸå¾…çš„convolutionï¼š å®é™…ä¸Šå°±ç­‰ä»·åœ¨ä¸¤è¾¹åšpaddingï¼Œç„¶åç›´æ¥convolutionï¼š Auto-encoderçš„ç”¨å¤„å¯ä»¥é¢„è®­ç»ƒæ¯ä¸€å±‚çš„DNNï¼š åŒç†å…¶å®ƒå±‚ä¹Ÿæ˜¯ä¸€æ ·ï¼Œæ¯æ¬¡fixä½å…¶ä»–å±‚ç„¶ååšAuto-encoderã€‚é‚£ä¹ˆåœ¨bpçš„æ—¶å€™åªéœ€è¦fine-tuneå°±è¡Œã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Auto-encoder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 12:Unsupervised Learning:Neighbor Embedding]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2012%3A%20Unsupervised%20Learning%3A%20Neighbor%20Embedding%2F</url>
    <content type="text"><![CDATA[Locally Linear Embedding (LLE)ä¸€ç§é™ç»´æ–¹æ³•æ€æƒ³ï¼šå‡è®¾æ¯ä¸ªç‚¹å¯ä»¥ç”±å…¶å‘¨å›´çš„ç‚¹æ¥è¡¨ç¤º æˆ‘ä»¬éœ€è¦æ‰¾åˆ°è¿™æ ·çš„$w_{ij}$ï¼Œä½¿å¾—ï¼š âˆ‘_iâ€–x^iâˆ’âˆ‘_j w_{ij} x^j â€–_2è¿™æ ·åœ¨é™ç»´çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä»ç„¶ä¿æŒxä¹‹é—´çš„è¿™æ ·çš„å…³ç³»: Laplacian Eigenmapsä¸€ç§é™ç»´æ–¹æ³•åŸºæœ¬æ€æƒ³ï¼šå¦‚æœ$x^1$ä¸$x^2$åœ¨é«˜ç»´ç©ºé—´ä¸­ç›¸è¿‘ï¼Œåˆ™é™ç»´åä¹Ÿåº”è¯¥æ¥è¿‘ï¼š S=1/2 âˆ‘_{i,j} w_{i,j} (z^iâˆ’z^j )^2å…¶ä¸­ï¼š å¦‚æœå°†zå…¨è®¾ä¸º0ï¼Œæ˜¾ç„¶Sæœ€å°ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ç»™zä¸€ä¸ªé™åˆ¶ï¼šzåº”å½“å……æ»¡ç©ºé—´ï¼Œä¹Ÿå³å‡å¦‚zæ˜¯Mç»´ï¼Œé‚£ä¹ˆ$\{z^1,z^2â€¦,z^N\}$çš„ç§©åº”è¯¥ç­‰äºM T-distributed Stochastic Neighbor Embedding (t-SNE)ä¹Ÿæ˜¯ä¸€ç§é™ç»´æ–¹æ³•å‰é¢æåˆ°çš„æ–¹æ³•æœ‰ä¸€ä¸ªé—®é¢˜ï¼šåŒä¸€ç±»çš„ç‚¹ç¡®å®èšåœ¨ä¸€èµ·ï¼Œä½†ä¸åŒç±»çš„ç‚¹å¹¶æ²¡æœ‰å°½é‡åˆ†å¼€ t-SNEçš„ä¸»è¦æ€æƒ³ï¼šå°†æ•°æ®ç‚¹æ˜ å°„åˆ°æ¦‚ç‡åˆ†å¸ƒï¼Œæˆ‘ä»¬å¸Œæœ›é™ç»´å‰å’Œé™ç»´åï¼Œæ•°æ®åˆ†å¸ƒçš„æ¦‚ç‡åº”å½“å°½å¯èƒ½ä¸€è‡´ã€‚t-SNEæ„å»ºä¸€ä¸ªé«˜ç»´å¯¹è±¡ä¹‹é—´çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä½¿å¾—ç›¸ä¼¼çš„å¯¹è±¡æœ‰æ›´é«˜çš„æ¦‚ç‡è¢«é€‰æ‹©ï¼Œè€Œä¸ç›¸ä¼¼çš„å¯¹è±¡æœ‰è¾ƒä½çš„æ¦‚ç‡è¢«é€‰æ‹©ã€‚t-SNEåœ¨ä½ç»´ç©ºé—´é‡Œåœ¨æ„å»ºè¿™äº›ç‚¹çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä½¿å¾—è¿™ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´å°½å¯èƒ½çš„ç›¸ä¼¼ã€‚ å¦‚ä½•åšï¼Ÿåœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰ï¼š P(x^j |x^i )=\frac{S(x^i,x^j )}{âˆ‘_{kâ‰ i}S(x^i,x^k )}å…¶ä¸­Sè¡¨ç¤ºiä¸jä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚ åœ¨ä½ç»´ç©ºé—´ä¸­ï¼ŒåŒæ ·æœ‰ï¼š Q(z^j |z^i )=\frac{Sâ€²(z^i,z^j )}{âˆ‘_{kâ‰ i}Sâ€²(z^i,z^k )}ä½¿ç”¨KLæ•£åº¦å»è®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼š L=âˆ‘_i KL(P(âˆ—|x^i )||Q(âˆ—|z^i )) =âˆ‘_iâˆ‘_j P(x^j |x^i )\frac{log P(x^j |x^i )}{Q(z^j |z^i )}t-SNEä¸­ï¼Œé«˜ç»´ç©ºé—´å’Œä½ç»´ç©ºé—´è®¡ç®—ç›¸ä¼¼åº¦çš„å…¬å¼ä¸å¤§ä¸€æ ·ï¼š S(x^i,x^j )=exp(âˆ’â€–x^iâˆ’x^j â€–_2 )Sâ€²(z^i,z^j )=\frac{1}{(1+â€–z^iâˆ’z^j â€–_2)}ä¸¤ä¸ªå…¬å¼çš„å›¾ç¤ºï¼š ä¹Ÿå³ä½ç»´ç©ºé—´ä¼šæ‹‰é•¿è·ç¦»ï¼Œä½¿å¾—è·ç¦»è¿œçš„ç‚¹å°½å¯èƒ½è¢«æ‹‰å¼€ã€‚ t-SNEçš„é—®é¢˜åœ¨äºï¼št-SNEæ— æ³•å¯¹æ–°çš„æ•°æ®ç‚¹è¿›è¡Œé™ç»´ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Neighbor Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 11:Unsupervised Learning:Linear Dimension Reduction]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2011%3A%20Unsupervised%20Learning%3A%20Linear%20Dimension%20Reduction%2F</url>
    <content type="text"><![CDATA[ClusteringK-meansç®—æ³•æ­¥éª¤ï¼š è¿­ä»£æ›´æ–°ä½¿å¾—æœ€åèšç±»ä¸­å¿ƒæ”¶æ•›ã€‚ä½†äº‹å…ˆéœ€è¦å®šå¥½æœ‰å¤šå°‘ç±»ã€‚ Hierarchical Agglomerative Clustering (HAC)è‡ªä¸‹è€Œä¸Šï¼Œæ¯æ¬¡é€‰ä¸¤ä¸ªæœ€è¿‘çš„èšä¸ºä¸€ç±»ï¼Œç›´åˆ°æ‰€æœ‰çš„éƒ½åˆ†æˆä¸€ç±»æœ€åé€‰æ‹©ä¸€ä¸ªé˜ˆå€¼åˆ’åˆ†ï¼Œå¦‚è“è‰²ç»¿è‰²å’Œçº¢è‰²çš„çº¿ Dimension Reductionæ‰¾åˆ°ä¸€ä¸ªæ˜ å°„ï¼Œä½¿å¾—xèƒ½å¤Ÿæ˜ å°„åˆ°ä½ç»´z Principle Component Analysis (PCA)ç›®çš„æ˜¯æ‰¾åˆ°ä¸€ä¸ªç»´åº¦ï¼Œä½¿å¾—æŠ•å½±å¾—åˆ°çš„varianceæœ€å¤§ï¼Œä¹Ÿå³æœ€å¤§ç¨‹åº¦ä¿ç•™æ•°æ®çš„å·®å¼‚æ€§ã€‚ å½¢å¼åŒ–å¯ä»¥å†™æˆï¼ˆä¸€ç»´æƒ…å½¢ï¼‰ï¼š Var(z_1 )=\frac{1}{N} âˆ‘_{z_1}(z_1âˆ’\overline{z_1} )^2å…¶ä¸­ï¼š â€–w^1 â€–_2=1z_1=w^1 \cdot x$\overline{z_1}$è¡¨ç¤ºzçš„å‡å€¼ å‡å¦‚æˆ‘ä»¬è¦æŠ•å½±åˆ°å¤šç»´ï¼Œå…¶ä»–ç»´åº¦ä¹Ÿæœ‰åŒæ ·çš„ç›®æ ‡ã€‚å…¶ä¸­æ¯ä¸ªç»´åº¦ä¹‹é—´éƒ½åº”è¯¥æ˜¯ç›¸äº’æ­£äº¤çš„ã€‚ å¦‚ä½•åšï¼Ÿæ‰¾åˆ°$ \frac{1}{N}âˆ‘(xâˆ’\overline{x} ) (xâˆ’\overline{x})^T$çš„å‰kä¸ªæœ€å¤§çš„ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œç»„åˆèµ·æ¥å³æ˜¯æˆ‘ä»¬è¦æ‰¾çš„$W$ è¯æ˜â€”-Warning of Mathâ€”-ç›®çš„ï¼š$Var(z_1 )=\frac{1}{N} âˆ‘_{z_1}(z_1âˆ’\overline{z_1} )^2 $å…¶ä¸­ $\overline{z_1} =\frac{1}{N} âˆ‘{z_1} = \frac{1}{N} âˆ‘ w^1 \cdot x=w^1\cdot \overline{x}$ æ¨å¯¼ï¼šæ”¹å˜ç¬¦å· $S=Cov(x)$ åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•ï¼Œæœ‰ï¼š$Sw^1=Î±w^1$ç­‰å¼ä¸¤è¾¹å„å·¦ä¹˜$(w^1)^T$ï¼Œæœ‰ï¼š$(w^1 )^T Sw^1=Î±(w^1 )^T w^1=Î±$ ä¹Ÿå³ï¼Œ$Î±$æ˜¯$S$çš„ç‰¹å¾å€¼ï¼Œé€‰æ‹©æœ€å¤§çš„ç‰¹å¾å€¼ï¼Œå°±èƒ½å¤Ÿæœ€å¤§åŒ–æˆ‘ä»¬çš„ç›®æ ‡ã€‚ åŒç†ï¼Œæˆ‘ä»¬è¦æ‰¾$w^2$ï¼Œæœ€å¤§åŒ–$(w^2 )^T Sw^2$ï¼Œå…¶ä¸­æœ‰ï¼š$(w^2 )^T w^2=1$$(w^2 )^T w^1=0$ ï¼ˆä¸ç¬¬ä¸€ç»´æ­£äº¤ï¼‰ å› æ­¤åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•ï¼š g(w^2 )= (w^2 )^T Sw^2âˆ’Î±((w^2 )^T w^2âˆ’1)âˆ’Î²((w^2 )^T w^1âˆ’0)æœ€ç»ˆå¾—åˆ°ï¼Œw2å¯¹åº”ç¬¬äºŒå¤§çš„ç‰¹å¾å€¼çš„ç‰¹å¾å‘é‡ã€‚ ä»¥æ­¤ç±»æ¨ï¼Œå…¶ä»–ç»´ä¹ŸåŒç†ã€‚â€”-End of Mathâ€”- PCAçš„å…¶ä»–å®é™…ä¸Šæœ€ç»ˆå¾—åˆ°çš„zï¼Œæ¯ä¸€ç»´ä¹‹é—´çš„åæ–¹å·®éƒ½ä¸º0 è¯æ˜å¦‚ä¸‹ï¼š PCAä¹Ÿå¯ä»¥ç”¨SVDæ¥åšï¼š Uä¸­ä¿å­˜äº†Kä¸ªç‰¹å¾å‘é‡ã€‚ ä»å¦ä¸€ç§è§’åº¦ç†è§£PCAï¼Œä¹Ÿå¯ä»¥è®¤ä¸ºPCAæ˜¯ä¸€ç§autoencoderï¼š PCAçš„é—®é¢˜PCAæ˜¯æ— ç›‘ç£å­¦ä¹ ï¼Œå¦‚æœæœ‰æ ‡ç­¾ï¼Œåˆ™æ— æ³•æŒ‰ç…§ç±»åˆ«æ¥è¿›è¡Œæ­£ç¡®é™ç»´ï¼Œå¦‚ï¼š ç¬¬äºŒå°±æ˜¯PCAæ˜¯çº¿æ€§å˜æ¢ï¼Œå¯¹äºä¸€äº›éœ€è¦éçº¿æ€§å˜æ¢çš„æ— èƒ½ä¸ºåŠ› Matrix Factorizationå®šä¹‰ï¼šçŸ©é˜µåˆ†è§£ï¼Œå°±æ˜¯å°†ä¸€ä¸ªçŸ©é˜µDåˆ†è§£ä¸ºUå’ŒVçš„ä¹˜ç§¯ï¼Œå³å¯¹äºä¸€ä¸ªç‰¹å®šçš„è§„æ¨¡ä¸ºm*nçš„çŸ©é˜µDï¼Œä¼°è®¡å‡ºè§„æ¨¡åˆ†åˆ«ä¸ºm*kå’Œn*kçš„çŸ©é˜µUå’ŒVï¼Œä½¿å¾—$UV^T$çš„å€¼å°½å¯èƒ½é€¼è¿‘çŸ©é˜µDã€‚å¸¸ç”¨äºæ¨èç³»ç»Ÿã€‚ æ€æƒ³ï¼šå‡å¦‚æœ‰ä¸€ä¸ªçŸ©é˜µï¼š å‡è®¾æ¨ªè½´å’Œçºµè½´æ¯ä¸€ç»´éƒ½æœ‰ä¸€ä¸ªå‘é‡ä»£è¡¨è¯¥ç»´ï¼ŒçŸ©é˜µçš„æ¯ä¸ªå…ƒç´ å°±æ˜¯æ¨ªè½´å’Œçºµè½´å¯¹åº”ç»´çš„ç‚¹ç§¯ã€‚æˆ‘ä»¬çš„ç›®çš„æ˜¯å°½å¯èƒ½å‡å°ï¼š L=\sum_{(i,j)} (r^i \cdot r^j -n_{ij})^2å…¶ä¸­$r_i$ $r_j$å°±æ˜¯å‘é‡è¡¨ç¤ºï¼Œ$n_{ij}$å°±æ˜¯çŸ©é˜µçš„å†…å®¹ã€‚ å¯ä»¥ä½¿ç”¨SVDæ±‚è§£ä¸Šå¼ï¼š å®é™…ä¸Šï¼Œè€ƒè™‘æ¯ä¸€è¡Œæˆ–åˆ—æœ¬èº«çš„ç‰¹æ€§ï¼Œæˆ‘ä»¬å¯¹Lossè¿›è¡Œæ‰©å±•ï¼š Minimizing \ \ L=\sum_{(i,j)} (r^i \cdot r^j +b_i+b_j-n_{ij})^2ä½¿ç”¨SGDå¯ä»¥æ±‚è§£ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Linear Dimension Reduction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¢¯åº¦æ¶ˆå¤±ä¸æ¢¯åº¦çˆ†ç‚¸çš„æ¨å¯¼]]></title>
    <url>%2F2018%2F10%2F07%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[è®°RNNä¸­æ¯ä¸€æ­¥çš„æŸå¤±ä¸º$E_t$ï¼Œåˆ™æŸå¤±å¯¹$h_{t-1}$çš„æƒé‡$W$çš„å¯¼æ•°æœ‰ï¼š \frac{\partial{E_t}}{\partial{W}}=\sum_{k=1}^{t} \frac{\partial{E_t}}{\partial{y_t}} \frac{\partial{y_t}}{\partial{h_t}} \frac{\partial{h_t}}{\partial{h_k}} \frac{\partial{h_k}}{\partial{W}}å…¶ä¸­$\frac{\partial{h_t}}{\partial{h_k}}$ä½¿ç”¨é“¾å¼æ³•åˆ™æœ‰ï¼š \frac{\partial{h_t}}{\partial{h_k}} = \prod_{j=k+1}^{t} \frac{\partial{h_j}}{\partial{h_{j-1}}} = \prod_{j=k+1}^{t} W^T \times diag[f^{\prime}(h_{j-1})]å…¶ä¸­$\frac{\partial{h_j}}{\partial{h_{j-1}}}$ æ˜¯é›…å…‹æ¯”çŸ©é˜µã€‚å¯¹å…¶å–æ¨¡(norm)ï¼Œæœ‰ï¼š \rVert \frac{\partial{h_j}}{\partial{h_{j-1}}}\rVert â‰¤ \rVert W^T \rVert \rVert diag[f^{\prime}(h_{j-1})] \rVert â‰¤ \beta_W \beta_hå½“$f$ä¸ºsigmoidæ—¶ï¼Œ$f^{\prime}(h_{j-1})$æœ€å¤§å€¼ä¸º1ã€‚ æœ€ç»ˆæˆ‘ä»¬æœ‰ï¼š \rVert \frac{\partial{h_t}}{\partial{h_{k}}}\rVert â‰¤ \rVert \prod_{j=k+1}^{t} \frac{\partial{h_j}}{\partial{h_{j-1}}} \rVert â‰¤ (\beta_W \beta_h)^{t-k}ä»ä¸Šå¼å¯ä»¥çœ‹å‡ºï¼Œå½“t-kè¶³å¤Ÿå¤§æ—¶ï¼Œå¦‚æœ$(\beta_W \beta_h)$å°äº1åˆ™$(\beta_W \beta_h)^{t-k}$åˆ™ä¼šå˜å¾—éå¸¸å°ï¼Œç›¸åï¼Œè‹¥$(\beta_W \beta_h)$å¤§äº1åˆ™$(\beta_W \beta_h)^{t-k}$åˆ™ä¼šå˜å¾—éå¸¸å¤§ã€‚ åœ¨è®¡ç®—æœºä¸­ï¼Œå½“æ¢¯åº¦å€¼å¾ˆå¤§æ—¶ï¼Œä¼šé€ æˆä¸Šæº¢(NaN)ï¼Œä¹Ÿå³æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œå½“æ¢¯åº¦å€¼å¾ˆå°æ—¶ï¼Œä¼šå˜æˆ0ï¼Œä¹Ÿå³æ¢¯åº¦æ¶ˆå¤±ã€‚æ³¨æ„åˆ°ï¼Œt-kçš„æŸå¤±å®é™…ä¸Šè¯„ä¼°çš„æ˜¯ä¸€ä¸ªè¾ƒè¿œçš„è¯å¯¹å½“å‰tçš„è´¡çŒ®ï¼Œæ¢¯åº¦æ¶ˆå¤±ä¹Ÿå³æ„å‘³ç€å¯¹å½“å‰çš„è´¡çŒ®æ¶ˆå¤±ã€‚ Reference:CS224d: Deep Learning for NLP Lecture4]]></content>
      <tags>
        <tag>æ¢¯åº¦æ¶ˆå¤±</tag>
        <tag>æ¢¯åº¦çˆ†ç‚¸</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†10]]></title>
    <url>%2F2018%2F10%2F07%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8610%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[æ­£æ€åˆ†å¸ƒ]é«˜ç»´æ­£æ€åˆ†å¸ƒæ˜¯ä»ä¸€ç»´å‘å±•è€Œæ¥çš„ï¼š https://www.zhihu.com/question/36339816 2ï¸âƒ£[RNN]from https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf é€šå¸¸è€Œè¨€ï¼Œæˆ‘ä»¬éƒ½ä¼šå°†RNNçš„initial stateè®¾ä¸ºå…¨0ï¼Œä½†åœ¨Hintonçš„slideä¸­æåˆ°ï¼Œæˆ‘ä»¬å¯ä»¥å°†åˆå§‹çŠ¶æ€ä½œä¸ºå¯å­¦ä¹ çš„å˜é‡ï¼Œå’Œæˆ‘ä»¬åœ¨å­¦ä¹ æƒé‡çŸ©é˜µä¸€æ ·ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>æ­£æ€åˆ†å¸ƒ</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRMLç¬¬äºŒç«  æ¦‚ç‡åˆ†å¸ƒ]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FPRML%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[äºŒå…ƒå˜é‡ å¤šé¡¹å¼åˆ†å¸ƒ é«˜æ–¯åˆ†å¸ƒ æŒ‡æ•°æ—åˆ†å¸ƒ éå‚æ•°ä¼˜åŒ–]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 10:Semi-supervised learning]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2010%3A%20Semi-supervised%2F</url>
    <content type="text"><![CDATA[ä»€ä¹ˆæ˜¯semi-supervised learning ç»™å®šæ•°æ®${(x^r,\hat{y}^r)}_{r=1}^{R},{(x_u)}_{u=R}^{R+U}$ï¼Œå…¶ä¸­æœªæ ‡è®°æ•°æ®è¿œè¿œå¤šäºæ ‡è®°æ•°æ® $U&gt;&gt;R$ ä¸ºä»€ä¹ˆåŠç›‘ç£å­¦ä¹ æœ‰ç”¨ï¼Ÿå› ä¸ºæœªæ ‡è®°æ•°æ®çš„åˆ†å¸ƒå¯èƒ½èƒ½å¤Ÿç»™æˆ‘ä»¬ä¸€äº›ä¿¡æ¯ã€‚ ç”Ÿæˆæ¨¡å‹çš„åŠç›‘ç£å­¦ä¹ ç»™å®šä¸¤ç±»$C_1$ã€$C_2$ï¼Œè¦æ±‚å¾—åˆ°åéªŒæ¦‚ç‡åˆ†å¸ƒ P(C_1 |x)=\frac{P(x|C_1 )P(C_1 )}{(P(x|C_1 )P(C_1 )+P(x|C_2 )P(C_2 ) )}å…¶ä¸­è”åˆæ¦‚ç‡åˆ†å¸ƒæœä»é«˜æ–¯åˆ†å¸ƒã€‚æœªæ ‡è®°æ•°æ®æ­¤æ—¶çš„ä½œç”¨å³å¸®æˆ‘ä»¬é‡æ–°ä¼°è®¡$P(C_1),P(C_2),\mu,\Sigma$ å¦‚ä½•åš?å…ˆåˆå§‹åŒ–$P(C_1),P(C_2),\mu,\Sigma$ï¼Œé€šå¸¸å¯ä»¥å…ˆç”¨æœ‰æ ‡è®°æ•°æ®è¿›è¡Œä¼°è®¡ è®¡ç®—æ¯ä¸ªæœªæ ‡è®°æ•°æ®çš„åéªŒæ¦‚ç‡åˆ†å¸ƒ ä»¥è¯¥æ¦‚ç‡åˆ†å¸ƒæ›´æ–°æ¨¡å‹ä¸æ–­é‡å¤ç›´è‡³æ‹Ÿåˆ åŸå› ï¼šå½“æˆ‘ä»¬åœ¨åšç›‘ç£å­¦ä¹ æ—¶ï¼Œä½¿ç”¨æœ€å¤§ä¼¼ç„¶æ±‚è§£ï¼š logL(Î¸)=âˆ‘_{x^r,\hat{y}^r} logP_Î¸ (x^r |\hat{y}^r )åŠ ä¸Šäº†æœªæ ‡è®°æ•°æ®åï¼ŒåŒæ ·ä¹Ÿè¦åšæœ€å¤§ä¼¼ç„¶ï¼š logL(Î¸)=âˆ‘_{(x^r,\hat{y}^r)} logP_Î¸ (x^r |\hat{y}^r )+âˆ‘_{x^u} logP_Î¸ (x^u)Low-density Separationå‡è®¾ä¸åŒç±»åˆ«ä¹‹é—´æœ‰ä¸€æ¡æ˜æ˜¾çš„åˆ†ç•Œçº¿ï¼Œä¹Ÿå³å­˜åœ¨ä¸€ä¸ªåŒºåŸŸï¼Œå…¶å¯†åº¦æ¯”å…¶ä»–åŒºåŸŸå° Self-trainingå¦‚ä½•åš? å…ˆç”¨æœ‰æ ‡ç­¾æ•°æ®è®­ç»ƒä¸€ä¸ªæ¨¡å‹$f$ï¼› åˆ©ç”¨æ¨¡å‹å¯¹æœªæ ‡è®°æ•°æ®è¿›è¡Œæ ‡è®°ï¼Œè¿™äº›æ ‡ç­¾ç§°ä¸ºä¼ªæ ‡ç­¾ï¼ˆpseudo-labelï¼‰ å°†éƒ¨åˆ†æœ‰ä¼ªæ ‡ç­¾çš„æ•°æ®æ”¾å…¥æœ‰æ ‡ç­¾æ•°æ®ä¸­ï¼Œé‡æ–°è®­ç»ƒé‡å¤ç›´åˆ°æ‹Ÿåˆ è¿™ç§æ–¹å¼å’Œç”Ÿæˆæ¨¡å‹çš„åŒºåˆ«ï¼šè¯¥æ–¹æ³•ä½¿ç”¨çš„æ˜¯hard labelè€Œç”Ÿæˆæ¨¡å‹ä½¿ç”¨çš„æ˜¯soft label Entropy-based Regularizationå°†æœªæ ‡è®°æ•°æ®å……å½“æ­£åˆ™åŒ–çš„æ•ˆæœï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹é¢„æµ‹æ ‡ç­¾çš„æ¦‚ç‡è¾ƒä¸ºé›†ä¸­ï¼Œä¹Ÿå³ç†µåº”è¯¥å°½å¯èƒ½å°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæœªæ ‡è®°æ•°æ®ä½¿å¾—åˆ†ç±»è¾¹ç•Œå°½å¯èƒ½åˆ’åœ¨ä½å¯†åº¦åŒºåŸŸã€‚ Smoothness Assumptionå‡è®¾ï¼šä½äºç¨ å¯†æ•°æ®åŒºåŸŸçš„ä¸¤ä¸ªè·ç¦»å¾ˆè¿‘çš„æ ·ä¾‹çš„ç±»æ ‡ç­¾ç›¸ä¼¼ï¼Œé€šè¿‡high density pathè¿æ¥ã€‚ x1ä¸x2ä¹‹é—´è¾ƒä¸ºç¨ å¯†ï¼Œå› æ­¤x2ä¸x1æ¯”x2ä¸x3æ›´ä¸ºæ¥è¿‘ã€‚ å¦‚ä½•çŸ¥é“x1ä¸x2é€šè¿‡high density pathè¿æ¥ï¼Ÿ åŸºäºå›¾çš„æ–¹æ³•ï¼š å®šä¹‰xiä¸xjä¹‹é—´çš„ç›¸ä¼¼åº¦$s(x^i,x^j)$ æ·»åŠ è¾¹ï¼Œæœ‰ä¸¤ç§é€‰æ‹© k nearest neighbor e-neighborhood è¾¹ä¹‹é—´çš„æƒé‡é€šè¿‡ç›¸ä¼¼åº¦æ¥è¡¡é‡ã€‚å¦‚ï¼š $s(x^i,x^j )=exp(âˆ’Î³â€–x^iâˆ’x^jâ€–^2)$ è¯¥æ–¹æ³•æœ¬è´¨å³åˆ©ç”¨æœ‰æ ‡ç­¾æ•°æ®å»å½±å“æœªæ ‡è®°æ•°æ®ï¼Œé€šè¿‡å›¾çš„ä¼ æ’­ã€‚ä½†ä¸€ä¸ªé—®é¢˜æ˜¯å¦‚æœæ•°æ®ä¸å¤Ÿå¤šï¼Œå°±å¯èƒ½æ²¡åŠæ³•ä¼ æ’­ã€‚å¦‚ï¼š åœ¨å»ºç«‹å¥½å›¾åï¼Œå¦‚ä½•ä½¿ç”¨? å®šä¹‰å›¾çš„å¹³æ»‘ç¨‹åº¦ï¼Œ$y$è¡¨ç¤ºæ ‡ç­¾ã€‚$S$è¶Šå°è¡¨ç¤ºè¶Šå¹³æ»‘ã€‚S=1/2âˆ‘_{i,j} w_{i,j} (y^iâˆ’y^j )^2=y^T Lyy=[â‹¯y^iâ‹¯y^jâ‹¯]^TL=Dâˆ’W Dæ˜¯é‚»æ¥çŸ©é˜µï¼Œç¬¬ijä¸ªå…ƒç´ å³xiä¸xjä¹‹é—´çš„weightï¼ŒWæ˜¯å¯¹è§’çŸ©é˜µï¼Œiiä¸ªå…ƒç´ æ˜¯Dçš„ç¬¬iè¡Œçš„åŠ å’Œï¼›Lç§°ä¸ºGraph Laplacian æˆ‘ä»¬æœ€ç»ˆåœ¨è®¡ç®—Lossçš„æ—¶å€™è¦åŠ ä¸Šè¿™é¡¹æ­£åˆ™é¡¹L=âˆ‘_{x^r}C(y^r,\hat{y}^r ) +Î»S]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>Semi-supervised learning</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 7:Tips for DL]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%207%3A%20Tips%20for%20DL%2F</url>
    <content type="text"><![CDATA[å¤§çº² new activation functionæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼šç”±äºsigmoidä¼šå°†å€¼å‹ç¼©ï¼Œæ‰€ä»¥åœ¨åå‘ä¼ æ’­æ—¶ï¼Œè¶Šåˆ°åé¢å€¼è¶Šå°ã€‚ æ‰€ä»¥åå±‚çš„æ›´æ–°ä¼šæ¯”å‰å±‚çš„æ›´æ–°æ›´å¿«ï¼Œå¯¼è‡´å‰å±‚è¿˜æ²¡convergeï¼Œåå±‚å°±æ ¹æ®å‰å±‚çš„æ•°æ®ï¼ˆrandomï¼‰è¾¾åˆ°convergeäº† ReLUèƒ½å¤Ÿå¿«é€Ÿè®¡ç®—ï¼Œä¸”èƒ½å¤Ÿè§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚ å› ä¸ºä¼šæœ‰éƒ¨åˆ†neuronçš„å€¼æ˜¯0ï¼Œæ‰€ä»¥ç›¸å½“äºæ¯æ¬¡è®­ç»ƒä¸€ä¸ªç˜¦é•¿çš„ç¥ç»ç½‘ç»œã€‚ ReLUçš„å˜ä½“ Maxouté¦–å…ˆå°†å‡ ä¸ªneuronå½’ä¸ºä¸€ç»„ï¼Œç„¶åæ¯æ¬¡å‰å‘ä¼ æ’­æ—¶å–æœ€å¤§çš„ä½œä¸ºè¾“å‡ºã€‚ å®é™…ä¸ŠReLUæ˜¯maxoutçš„ä¸€ç§ç‰¹æ®Šå½¢å¼ï¼š æ›´ä¸€èˆ¬çš„ï¼Œæœ‰ï¼š å› ä¸ºwå’Œbçš„å˜åŒ–ï¼Œæ‰€ä»¥è¯¥activation functionå®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªlearnable activation function è¿™æ ·ä¸€ä¸ªlearnable activation functionæœ‰è¿™æ ·çš„ç‰¹ç‚¹ï¼š Activation function in maxout network can be any piecewise linear convex functionHow many pieces depending on how many elements in a group å¦‚ï¼š maxoutåº”å¦‚ä½•è®­ç»ƒï¼Ÿ å®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªæ™®é€šçš„ç˜¦é•¿networkï¼Œå¸¸è§„è®­ç»ƒå³å¯ã€‚ Adaptive learning rateåœ¨adagradä¸­: è¶Šåˆ°åé¢learning rateè¶Šæ¥è¶Šå°ï¼Œä½†å®é™…ä¸Šåœ¨dlé‡Œé¢ï¼Œerror surfaceæ˜¯éå¸¸å¤æ‚çš„ï¼Œè¶Šæ¥è¶Šå°çš„learning rateå¯èƒ½ä¸é€‚ç”¨äºdlã€‚å¦‚ï¼š RMSprop$Ïƒ^t$æ˜¯å†å²ä¿¡æ¯ï¼Œä¹Ÿå°±æ˜¯è¯´$Ïƒ^t$å‚è€ƒäº†è¿‡å»çš„æ¢¯åº¦å’Œå½“å‰çš„æ¢¯åº¦è·å¾—ä¸€ä¸ªæ–°çš„æ”¾ç¼©å¤§å° Momentumå¼•å…¥æƒ¯æ€§ä½œä¸ºå‚è€ƒï¼Œä¹Ÿå³å‚è€ƒäº†ä¸Šä¸€æ¬¡æ¢¯åº¦çš„æ–¹å‘ã€‚å¼•å…¥æƒ¯æ€§åï¼Œå¯èƒ½æœ‰æœºä¼šè¶Šè¿‡local minimumã€‚æ™®é€šçš„gradient descent:æ¯æ¬¡æœç€æ¢¯åº¦çš„åæ–¹å‘èµ°ã€‚ Momentum: è€ƒè™‘äº†ä¸Šä¸€æ­¥èµ°çš„æ–¹å‘ã€‚ å…·ä½“ç®—æ³•ï¼š Adamç»“åˆäº†RMSpropå’ŒMomentumï¼Œä¹Ÿå³ç»¼åˆè€ƒè™‘äº†å†å²ä¿¡æ¯å†³å®šå½“å‰æ­¥é•¿ï¼›è€ƒè™‘äº†ä¸Šä¸€æ­¥çš„æ–¹å‘å†³å®šå½“å‰èµ°çš„æ–¹å‘ã€‚å…·ä½“ç®—æ³•ï¼š Early Stoppingå°±æ˜¯åœ¨validation setçš„lossä¸å†å‡å°æ—¶åœæ­¢ RegularizationL2æ­£åˆ™åŒ–å…¶ä¸­å› æ­¤æ›´æ–°å…¬å¼ä¸ºï¼š ä¹Ÿå³æ¯æ¬¡ä»¥$1-\eta \lambda$å¯¹wè¿›è¡Œæ”¾ç¼©ï¼Œä½¿wæ›´æ¥è¿‘0æ­£åˆ™åŒ–åœ¨DLä¸­ä¹Ÿç§°ä¸ºweight decay L1æ­£åˆ™åŒ– åˆ™æ›´æ–°å…¬å¼ä¸ºï¼š ä¹Ÿå³æ¯æ¬¡ä»¥$Î·Î»sgn(w)$ ä½¿wå¾€0é ï¼ˆsgnè¡¨ç¤ºç¬¦å·å‡½æ•°ï¼‰ å¯ä»¥çœ‹å‡ºï¼ŒL1æ¯æ¬¡éƒ½åŠ å‡ç›¸åŒçš„å€¼ï¼Œè€ŒL2æŒ‰æ¯”ä¾‹è¿›è¡Œç¼©æ”¾ã€‚å› æ­¤L1æ›´ä¸ºç¨€ç–(sparse)ã€‚ Dropoutè®­ç»ƒçš„æ—¶å€™æ¯ä¸€å±‚é‡‡æ ·p%çš„ç¥ç»å…ƒè®¾ä¸º0ï¼Œè®©å…¶ä¸å·¥ä½œ å®é™…ä¸Šå°±æ˜¯æ¯ä¸ªbatchæ”¹å˜äº†ç½‘ç»œç»“æ„ï¼Œä½¿å¾—ç½‘ç»œæ›´ç»†é•¿ æµ‹è¯•çš„æ—¶å€™æ‰€æœ‰çš„weightéƒ½ä¹˜ä»¥1-p% ä»ensembleçš„è§’åº¦çœ‹å¾…dropoutï¼šåœ¨è®­ç»ƒçš„æ—¶å€™è®­ç»ƒä¸€å †ä¸åŒç»“æ„çš„networkï¼Œæœ€å¤šæœ‰$2^N$ç§ç»„åˆï¼ŒNä¸ºneuronä¸ªæ•°ï¼Œå¯ä»¥ç§°ä¸ºç»ˆæçš„ensembleæ–¹æ³•äº†ã€‚è€Œåœ¨æµ‹è¯•çš„æ—¶å€™å¯¹è¿™äº›ä¸åŒçš„ç½‘ç»œè¿›è¡Œå¹³å‡ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Tips for DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[é‡‡æ ·æµ…æ]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F%E9%87%87%E6%A0%B7%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[æ€»ç»“åœ¨NLPä¸­çš„é‡‡æ ·æ–¹æ³•ï¼ˆæŒç»­æ›´æ–°ï¼‰ã€‚ é‡‡æ ·æ–¹æ³•1ï¸âƒ£é€†å˜æ¢é‡‡æ ·(Inverse Sampling)ç›®çš„ï¼šå·²çŸ¥ä»»æ„æ¦‚ç‡åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°æ—¶ï¼Œç”¨äºä»è¯¥åˆ†å¸ƒä¸­ç”Ÿæˆéšæœºæ ·æœ¬ã€‚ â€”-ä»€ä¹ˆæ˜¯ç´¯ç§¯åˆ†å¸ƒå‡½æ•°(CDF)â€”-æ˜¯æ¦‚ç‡å¯†åº¦å‡½æ•°(PDF)çš„ç§¯åˆ†ï¼Œå®šä¹‰ï¼š F_X(x)=P(Xâ‰¤x)=\int_{-âˆ}^{x}f_X(t)dtâ€”-ENDâ€”- æƒ³è±¡æˆ‘ä»¬çŸ¥é“é«˜æ–¯åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œæˆ‘ä»¬åº”è¯¥å¦‚ä½•é‡‡æ ·ï¼Ÿæœ¬è´¨ä¸Šæˆ‘ä»¬åªèƒ½å¯¹å‡åŒ€åˆ†å¸ƒè¿›è¡Œç›´æ¥é‡‡æ ·ï¼ˆé«˜æ–¯åˆ†å¸ƒæœ‰ç®—æ³•å¯ä»¥ç”Ÿæˆé‡‡æ ·ï¼Œä½†æ— æ³•ä¸€èˆ¬åŒ–ï¼‰ã€‚å¯¹äºè¿™ç§è¿ç»­çš„éšæœºå˜é‡ï¼Œæˆ‘ä»¬åªèƒ½é€šè¿‡é—´æ¥çš„æ–¹æ³•è¿›è¡Œé‡‡æ ·ã€‚ é€†å˜æ¢é‡‡æ ·å³æ˜¯é€šè¿‡ç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„åå‡½æ•°æ¥é‡‡æ ·ã€‚å› ä¸ºç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„å€¼åŸŸä¸º$[0,1]$ï¼Œå› æ­¤æˆ‘ä»¬é€šè¿‡åœ¨$[0,1]$ä¸Šè¿›è¡Œé‡‡æ ·ï¼Œå†æ˜ å°„åˆ°åŸåˆ†å¸ƒã€‚ä¾‹å­:æ˜ å°„å…³ç³»å¦‚å›¾ï¼š 2ï¸âƒ£é‡è¦æ€§é‡‡æ ·(Importance Sampling)ç›®çš„ï¼šå·²çŸ¥æŸä¸ªåˆ†å¸ƒ$P$ï¼Œå¸Œæœ›èƒ½ä¼°è®¡$f(x)$çš„æœŸæœ›ã€‚äº¦å³ï¼š E[f(x)]=\int_{x}f(x)p(x)dxâ‰ˆ\frac{1}{n}\sum_{i=1}^{n}f(x_i)å…¶ä¸­$x\sim p$ã€‚å‡è®¾$p(x)$çš„åˆ†å¸ƒå¤æ‚æˆ–æ ·æœ¬ä¸å¥½ç”Ÿæˆï¼Œå¦ä¸€åˆ†å¸ƒ$q(x)$æ–¹ä¾¿ç”Ÿæˆæ ·æœ¬ã€‚å› æ­¤æˆ‘ä»¬å¼•å…¥$q(x)$å¯¹åŸå…ˆåˆ†å¸ƒè¿›è¡Œä¼°è®¡ã€‚ E[f(x)]=\int_{x}f(x)p(x)dx=\int_{x}f(x)\frac{p(x)}{q(x)}q(x)dxâ‰ˆ\frac{1}{n}\sum_{i=1}^{n}f(x_i)\frac{p(x_i)}{q(x_i)}å…¶ä¸­ï¼Œ$x \sim q$ã€‚$w(x)=\frac{p(x)}{q(x)}$ç§°ä¸ºImportance Weight æ ¹æ®ä¸Šå¼ï¼Œå®é™…ä¸Šå°±æ˜¯æ¯æ¬¡é‡‡æ ·çš„åŠ æƒæ±‚å’Œã€‚ Referenceé€†å˜æ¢é‡‡æ ·https://zh.wikipedia.org/wiki/%E9%80%86%E5%8F%98%E6%8D%A2%E9%87%87%E6%A0%B7 é‡è¦æ€§é‡‡æ ·https://www.youtube.com/watch?v=S3LAOZxGcnk â€”â€”æŒç»­æ›´æ–°â€”â€”]]></content>
      <tags>
        <tag>é‡‡æ ·</tag>
        <tag>sampling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†9]]></title>
    <url>%2F2018%2F09%2F30%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%869%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Pytorch]Pytorchä¸­ä¿å­˜checkpointæ˜¯ä¸€ä¸ªdictå½¢å¼ï¼Œå¯ä»¥ä¿å­˜ä»»æ„å¤šä¸ªæ¨¡å‹åˆ°ä¸€ä¸ªcheckpointä¸­ã€‚1234567import torch#savetorch.save(&#123; 'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': loss, ... &#125;, PATH)#loadmodel = TheModelClass(*args, **kwargs) optimizer = TheOptimizerClass(*args, **kwargs)checkpoint = torch.load(PATH) model.load_state_dict(checkpoint['model_state_dict']) optimizer.load_state_dict(checkpoint['optimizer_state_dict']) epoch = checkpoint['epoch'] loss = checkpoint['loss']model.eval() # - or - model.train() 2ï¸âƒ£[Pytorch]Pytorchå¯ä»¥loadéƒ¨åˆ†æ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯åªloadè¿›æ¥éƒ¨åˆ†æˆ‘ä»¬éœ€è¦çš„å±‚ï¼Œè¿™åœ¨transfer learningä¸­ç”¨åˆ°ã€‚123torch.save(modelA.state_dict(), PATH)modelB = TheModelBClass(*args, **kwargs) modelB.load_state_dict(torch.load(PATH), strict=False)]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[no title]]></title>
    <url>%2F2018%2F09%2F30%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%9C%89%E5%8A%9B%E9%87%8F%E7%9A%84%E6%96%87%E5%AD%97%2F</url>
    <content type="text"><![CDATA[æ¯å½“æˆ‘é‡åˆ°è‡ªå·±ä¸æ•¢ç›´è§†çš„å›°éš¾æ—¶ï¼Œæˆ‘å°±ä¼šé—­ä¸ŠåŒçœ¼ï¼Œæƒ³è±¡è‡ªå·±æ˜¯ä¸€ä¸ª80å²çš„è€äººï¼Œä¸ºäººç”Ÿä¸­æ›¾æ”¾å¼ƒå’Œé€ƒé¿è¿‡çš„æ— æ•°å›°éš¾è€Œæ‡Šæ‚”ä¸å·²ï¼Œæˆ‘ä¼šå¯¹è‡ªå·±è¯´ï¼Œèƒ½å†å¹´è½»ä¸€æ¬¡è¯¥æœ‰å¤šå¥½ï¼Œç„¶åæˆ‘çå¼€çœ¼ç›ï¼šç °ï¼æˆ‘åˆå¹´è½»ä¸€æ¬¡äº†ï¼]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯10]]></title>
    <url>%2F2018%2F09%2F30%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D10%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£æ¬¡åŒ—å›ºå±±ä¸‹[å”] ç‹æ¹¾å®¢è·¯é’å±±å¤–ï¼Œè¡ŒèˆŸç»¿æ°´å‰ã€‚æ½®å¹³ä¸¤å²¸é˜”ï¼Œé£æ­£ä¸€å¸†æ‚¬ã€‚æµ·æ—¥ç”Ÿæ®‹å¤œï¼Œæ±Ÿæ˜¥å…¥æ—§å¹´ã€‚ä¹¡ä¹¦ä½•å¤„è¾¾ï¼Œå½’é›æ´›é˜³è¾¹ã€‚ æ¬¡ï¼šæ—…é€”ä¸­æš‚æ—¶åœå®¿ï¼Œè¿™é‡Œæ˜¯åœæ³Šçš„æ„æ€ã€‚ http://m.xichuangzhu.com/work/57b95de92e958a005fa8919e 2ï¸âƒ£å°†èµ´å´å…´ç™»ä¹æ¸¸åŸ[å”] æœç‰§æ¸…æ—¶æœ‰å‘³æ˜¯æ— èƒ½ï¼Œé—²çˆ±å­¤äº‘é™çˆ±åƒ§ã€‚æ¬²æŠŠä¸€éº¾æ±Ÿæµ·å»ï¼Œä¹æ¸¸åŸä¸Šæœ›æ˜­é™µã€‚ æ— èƒ½ï¼šæ— æ‰€ä½œä¸ºã€‚ http://m.xichuangzhu.com/work/57b99db9165abd005a6da742]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRMLç¬¬ä¸€ç«  ç»ªè®º]]></title>
    <url>%2F2018%2F09%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FPRML%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E7%BB%AA%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[è®°å½•PRMLå­¦ä¹ è¿‡ç¨‹ã€‚ç¬”è®°å…±äº«é“¾æ¥ï¼šhttps://1drv.ms/u/s!Apsp2510NHF6rIRjMclFB16v7B0FWg æ¦‚ç‡è®º å†³ç­–è®º ä¿¡æ¯è®º]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 6:Backpropagation]]></title>
    <url>%2F2018%2F09%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%206%3A%20Backpropagation%2F</url>
    <content type="text"><![CDATA[Chain RuleåŸºæœ¬å…¬å¼ forward passå’Œbackward passå¯ä»¥å°†backpropagationåˆ†ä¸ºä¸¤æ­¥ forward passåœ¨å‰å‘ä¼ æ’­çš„æ—¶å€™æå‰è®¡ç®—/ä¿å­˜å¥½ï¼Œå› ä¸ºè¯¥æ¢¯åº¦å¾ˆç®€å• æ¯”å¦‚zå¯¹w1çš„æ¢¯åº¦å°±æ˜¯x1ï¼Œå°±æ˜¯å’Œw1ç›¸è¿çš„é¡¹ backward passå›ä¼ çš„æ—¶å€™é€å±‚ç›¸ä¹˜ä¸‹å»ï¼Œç±»ä¼¼åŠ¨æ€è§„åˆ’ï¼Œè·å¾—äº†åä¸€å±‚çš„æ¢¯åº¦æ‰èƒ½æ±‚å‡ºå‰ä¸€å±‚çš„æ¢¯åº¦ã€‚ æ€»ç»“ å…ˆå‰å‘ï¼Œæå‰ç®—å‡ºæœ€é‚»è¿‘çš„æ¢¯åº¦ï¼Œç›´åˆ°output layerï¼Œè®¡ç®—å®Œè¯¥æ¢¯åº¦ï¼Œå†ä¸æ–­å›ä¼ é€å±‚ç›¸ä¹˜è·å¾—outputå¯¹å„å±‚çš„æ¢¯åº¦ã€‚ ä»£ç å®ç°ä¾‹å­reluå®ç°forward passå’Œbackward pass1234567891011121314151617181920212223242526272829303132import torchclass MyReLU(torch.autograd.Function): """ We can implement our own custom autograd Functions by subclassing torch.autograd.Function and implementing the forward and backward passes which operate on Tensors. """ @staticmethod def forward(ctx, input): """ In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. """ ctx.save_for_backward(input) #ä¸ºäº†ä¹‹åçš„backwardè®¡ç®— return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): """ In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. """ input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input &lt; 0] = 0 return grad_input]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Backpropagation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 5:Classification:Logistic Regression]]></title>
    <url>%2F2018%2F09%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%205%20Classification%3A%20Logistic%20Regression%2F</url>
    <content type="text"><![CDATA[logistic regressionå¦‚ä½•åšï¼Ÿstep1: å®šä¹‰function set step2: æ›´æ–°ä½¿ç”¨æœ€å¤§ä¼¼ç„¶æ›´æ–° L(w,b)=f_{w,b}(x^1 )f_{w,b}(x^2 )(1âˆ’f_{w,b} (x^3 ))â‹¯f_{w,b} (x^N )æ‰¾åˆ°wï¼Œbä½¿å¾—Læœ€å¤§ å¯¹ä¼¼ç„¶å‡½æ•°å–è´Ÿå¯¹æ•°ï¼Œåˆ™æœ‰ï¼š å°†å¼å­çš„æ¯ä¸ªå…ƒç´ å†™æˆä¼¯åŠªåˆ©åˆ†å¸ƒå½¢å¼ï¼š ä¸Šå¼å°±æ˜¯cross-entropyæŸå¤±å‡½æ•°ã€‚ æ±‚å¯¼è¯¥å¼å­å¯å¾—ï¼šæ›´æ–°å…¬å¼ï¼šå¯ä»¥çœ‹å‡ºä¸Šå¼å¾ˆç›´è§‚ï¼šå’Œç­”æ¡ˆå·®è·è¶Šå¤§ï¼Œæ›´æ–°æ­¥ä¼è¶Šå¤§ã€‚ åŒæ—¶å‘ç°ä¸Šå¼å’Œlinear regressionçš„æ›´æ–°å…¬å¼æ˜¯ä¸€è‡´çš„ã€‚ ä¸ºä»€ä¹ˆä¸åƒlinear regressioné‚£æ ·è®¾lossä¸ºsquareï¼Ÿå‡è®¾æˆ‘ä»¬ä½¿ç”¨square lossï¼Œåˆ™æ±‚å¯¼å¾—åˆ°çš„æ¢¯åº¦ï¼šä¸Šå¼å¯ä»¥çœ‹å‡ºï¼Œå½“æ¥è¿‘targetæ—¶ï¼Œæ¢¯åº¦å°ï¼›è¿œç¦»targetæ—¶ï¼Œæ¢¯åº¦ä¹Ÿå°ã€‚éš¾ä»¥è¾¾åˆ°å…¨å±€æœ€å° ä¸‹å›¾æ˜¯cross entropyå’Œsquare errorçš„å›¾åƒç¤ºæ„ï¼š å¦‚å›¾ï¼Œsquare losséš¾ä»¥åˆ°è¾¾å…¨å±€æœ€å°ã€‚ ç”Ÿæˆå¼æ¨¡å‹ä¸åˆ¤åˆ«å¼æ¨¡å‹çš„åŒºåˆ«ç”Ÿæˆå¼å¯¹è”åˆæ¦‚ç‡åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå†é€šè¿‡è´å¶æ–¯å®šç†è·å¾—åéªŒæ¦‚ç‡ï¼›è€Œåˆ¤åˆ«å¼æ¨¡å‹ç›´æ¥å¯¹åéªŒæ¦‚ç‡å»ºæ¨¡ã€‚äºŒè€…æ‰€å®šä¹‰çš„function setæ˜¯ä¸€è‡´çš„ï¼Œä½†åŒä¸€ç»„æ•°æ®å¯èƒ½ä¼šå¾—åˆ°ä¸åŒçš„wå’Œbã€‚ äºŒè€…ä¼˜åŠ£å¯¹æ¯”ï¼š æ•°æ®é‡å¤šæ—¶ï¼Œä¸€èˆ¬æ¥è¯´åˆ¤åˆ«å¼æ¨¡å‹ä¼šæ›´å¥½ã€‚å› ä¸ºåˆ¤åˆ«å¼æ¨¡å‹æ²¡æœ‰å…ˆéªŒå‡è®¾ï¼Œå®Œå…¨ä¾èµ–äºæ•°æ®ã€‚ä½†å¦‚æœæ•°æ®æœ‰å™ªå£°ï¼Œå®¹æ˜“å—å½±å“ã€‚ ç”Ÿæˆå¼æ¨¡å‹æ˜¯æœ‰ä¸€å®šçš„å‡è®¾çš„ï¼Œå½“å‡è®¾é”™è¯¯ï¼Œä¼šå½±å“åˆ†ç±»æ•ˆæœã€‚ æ­£å› ä¸ºæœ‰ä¸€å®šçš„å…ˆéªŒå‡è®¾ï¼Œå½“æ•°æ®é‡å¾ˆå°‘æ—¶ï¼Œå¯èƒ½æ•ˆæœä¼šä¸é”™ï¼›å¯¹äºå™ªå£°æ›´å…·æœ‰é²æ£’æ€§ã€‚ å…ˆéªŒå¯ä»¥ä»å…¶ä»–æ•°æ®æºè·å¾—æ¥å¸®åŠ©ç‰¹å®šä»»åŠ¡ï¼Œå¦‚è¯­éŸ³è¯†åˆ«é—®é¢˜ã€‚ logisticçš„å±€é™æœ¬è´¨ä»æ˜¯ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨ï¼Œæ²¡åŠæ³•åˆ†ç±»éçº¿æ€§çš„æ•°æ®ã€‚å¦‚ä½•è§£å†³è¯¥é—®é¢˜?å°†logistic regression modelæ‹¼æ¥èµ·æ¥ï¼Œå‰é¢çš„modelå¯¹æ•°æ®è¿›è¡Œfeature transformationï¼Œç„¶åå†å¯¹æ–°çš„featureè¿›è¡Œåˆ†ç±»ã€‚ logisticä¸deep learningçš„è”ç³»ï¼šå¦‚æœå°†logistic regressionçš„ä¸€ä¸ªå•å…ƒç§°ä¸ºneuronï¼Œæ‹¼èµ·æ¥å°±æ˜¯neural networkäº†ï¼ï¼ï¼]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Classification</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†8]]></title>
    <url>%2F2018%2F09%2F23%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%868%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Pytorch]torch.max()æœ‰ä¸¤ç§ä¸åŒå†™æ³•ã€‚torch.max(input) â†’ Tensor è¿”å›å…¶ä¸­æœ€å¤§çš„å…ƒç´ torch.max(input, dim, keepdim=False, out=None) â†’ (Tensor, LongTensor) è¿”å›è¯¥ç»´åº¦ä¸Šæœ€å¤§å€¼ï¼Œä»¥åŠå¯¹åº”çš„index 2ï¸âƒ£[Pytorch]å°†æ¨¡å‹åŒæ—¶éƒ¨ç½²åˆ°å¤šå¼ å¡ä¸Šè®­ç»ƒï¼Œæœ¬è´¨å°±æ˜¯å°†ä¸€ä¸ªbatchçš„æ•°æ®splitï¼Œé€åˆ°å„ä¸ªmodelï¼Œç„¶ååˆå¹¶ç»“æœã€‚ 123456model = nn.DataParallel(model)device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")model.to(device)for data in rand_loader: input = data.to(device) output = model(input) 3ï¸âƒ£[æ±‚å¯¼]æ ‡é‡ã€å‘é‡ã€çŸ©é˜µä¹‹é—´çš„æ±‚å¯¼æœ‰ä¸¤ç§å¸ƒå±€ï¼Œå³åˆ†å­å¸ƒå±€å’Œåˆ†æ¯å¸ƒå±€ã€‚åˆ†å­å¸ƒå±€å’Œåˆ†æ¯å¸ƒå±€åªå·®ä¸€ä¸ªè½¬ç½®ã€‚æˆ‘çš„è®°æ³•ï¼šåœ¨æ±‚å¯¼è¿‡ç¨‹ä¸­ï¼Œå‡è®¾åˆ†æ¯ä¸ºm*nï¼Œåˆ†å­ä¸º k*nï¼Œåˆ™å¯¼æ•°çŸ©é˜µåº”è¯¥ä¸º k*m ã€‚ä¸€äº›ç‰¹æ®Šçš„å¦‚æ ‡é‡å¯¹çŸ©é˜µæ±‚å¯¼ç­‰é™¤å¤–ã€‚å…·ä½“ç›´æ¥æŸ¥è¡¨ï¼šhttps://en.m.wikipedia.org/wiki/Matrix_calculus æŒ‰ä½è®¡ç®—æ±‚å¯¼ï¼šå‡è®¾ä¸€ä¸ªå‡½æ•°$f(x)$çš„è¾“å…¥æ˜¯æ ‡é‡$x$ã€‚å¯¹äºä¸€ç»„Kä¸ªæ ‡é‡$x_1,Â·Â·Â· ,x_K$ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡$f(x)$å¾—åˆ°å¦å¤–ä¸€ç»„Kä¸ªæ ‡é‡$z_1,Â·Â·Â· ,z_K$ï¼Œ$z_k = f(x_k),âˆ€k = 1,Â·Â·Â· ,K$å…¶ä¸­ï¼Œ$f(x)$æ˜¯æŒ‰ä½è¿ç®—çš„ï¼Œå³$[f(x)]_i = f(x_i)$å…¶å¯¼æ•°æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼š Referenceï¼šhttps://en.m.wikipedia.org/wiki/Matrix_calculushttps://blog.csdn.net/uncle_gy/article/details/78879131https://nndl.github.io/chap-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.pdf]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>æ±‚å¯¼</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•7]]></title>
    <url>%2F2018%2F09%2F23%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%957%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£softmaxçš„numpyå®ç°123def softmax(x,axis=0): """Compute softmax values for each sets of scores in x.""" return np.exp(x) / np.sum(np.exp(x), axis=axis) 2ï¸âƒ£numpy æ‰‹åŠ¨æ±‚å¯¼relu123456789101112131415161718192021222324252627282930313233343536import numpy as np# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random input and output datax = np.random.randn(N, D_in)y = np.random.randn(N, D_out)# Randomly initialize weightsw1 = np.random.randn(D_in, H)w2 = np.random.randn(H, D_out)learning_rate = 1e-6for t in range(500): # Forward pass: compute predicted y h = x.dot(w1) h_relu = np.maximum(h, 0) y_pred = h_relu.dot(w2) # Compute and print loss loss = np.square(y_pred - y).sum() print(t, loss) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.T.dot(grad_y_pred) grad_h_relu = grad_y_pred.dot(w2.T) grad_h = grad_h_relu.copy() grad_h[h &lt; 0] = 0 grad_w1 = x.T.dot(grad_h) # Update weights w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 3ï¸âƒ£Pytorchå®ç°relu1234567891011121314151617181920212223242526272829303132import torchclass MyReLU(torch.autograd.Function): """ We can implement our own custom autograd Functions by subclassing torch.autograd.Function and implementing the forward and backward passes which operate on Tensors. """ @staticmethod def forward(ctx, input): """ In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. """ ctx.save_for_backward(input) #ä¸ºäº†ä¹‹åçš„backwardè®¡ç®— return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): """ In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. """ input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input &lt; 0] = 0 return grad_input 4ï¸âƒ£Pytorchåœ¨å¤šå¼ å¡ä¸Šéƒ¨ç½²123456model = nn.DataParallel(model)device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")model.to(device)for data in rand_loader: input = data.to(device) output = model(input)]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¬è¾¾è§‚æ¯ç°åœºç­”è¾©æœ‰æ„Ÿ]]></title>
    <url>%2F2018%2F09%2F19%2F%E8%A7%81%E9%97%BB%26%E6%83%B3%E6%B3%95%2F%E5%90%AC%E8%BE%BE%E8%A7%82%E6%9D%AF%E7%8E%B0%E5%9C%BA%E7%AD%94%E8%BE%A9%E6%9C%89%E6%84%9F%2F</url>
    <content type="text"><![CDATA[å‰å‡ æ—¥ï¼ˆå‘¨æ—¥ï¼‰å»äº†è¾¾è§‚æ¯ç­”è¾©ç°åœºå¬äº†å‰10ååšäº†æŠ¥å‘Šï¼Œæœ‰äº†ä¸€äº›æ„Ÿæƒ³ï¼Œä½†ä¸€ç›´æ²¡æœ‰æŠ½å‡ºæ—¶é—´å†™ä¸€ä¸‹è‡ªå·±çš„æ„Ÿæƒ³ï¼ˆæ‡’ï¼‰ã€‚ è‡ªå·±å¤§æ¦‚èŠ±äº†åæ¥å¤©åšäº†ä¸€ä¸‹æ¯”èµ›ï¼Œå®é™…ä¸Šä¹Ÿå°±æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»çš„æ¯”èµ›ï¼Œå› ä¸ºæ²¡æœ‰æ¯”èµ›ç»éªŒçš„ç¼˜æ•…ï¼Œèµ°äº†å¾ˆå¤šå¼¯è·¯ã€‚ä¸è¿‡ä¹Ÿå­¦åˆ°äº†ä¸€äº›ä¸œè¥¿ã€‚ ç°è®°å½•å‰ååçš„ä¸€äº›idea/trickï¼š æ•°æ®å¢å¼º å› ä¸ºç»™çš„å¥å­é•¿åº¦å¾ˆé•¿ï¼Œå› æ­¤åœ¨åšæˆªæ–­çš„æ—¶å€™åé¢çš„å°±æ²¡æ³•è®­ç»ƒåˆ°äº†ï¼Œå¯ä»¥å°†æ–‡æœ¬å€’åºä½œä¸ºæ–°çš„æ•°æ®è®­ç»ƒæ¨¡å‹ã€‚å¯ä»¥å……åˆ†åˆ©ç”¨åˆ°æ•°æ® å°†æ•°æ®æ‰“ä¹±ã€éšæœºåˆ é™¤ï¼Œå®é™…ä¸Šå°±æ˜¯å¯¹ä¸€ä¸ªå¥å­çš„è¯è¿›è¡Œsampleå†ç»„åˆ æ‰“ä¹±è¯åºä»¥å¢åŠ æ•°æ®é‡ ä½¿ç”¨pseudo labelingï¼Œä½†æœ‰çš„é˜Ÿä¼ä½¿ç”¨è¿™ä¸ªåšå‡ºæ•ˆæœäº†ï¼Œä½†æœ‰çš„æ²¡æœ‰ ç‰¹å¾å·¥ç¨‹ å‡è®¾å¼€å¤´ä¸­é—´ç»“å°¾çš„ä¿¡æ¯å¯¹åˆ†ç±»æœ‰å¸®åŠ©ï¼Œå› æ­¤æˆªå–è¯¥éƒ¨åˆ†ä¿¡æ¯åšè®­ç»ƒ æ”¹è¿›baselineçš„tfidfçš„ç‰¹å¾å·¥ç¨‹æ–¹æ³•ï¼Œä½¿ç”¨åŸºäºç†µçš„è¯æƒé‡è®¡ç®— é™ç»´ï¼Œç•™ä¸‹æœ€é‡è¦çš„ç‰¹å¾ã€‚å…ˆç”¨å¡æ–¹åˆ†å¸ƒé™åˆ°20ä¸‡ï¼Œå†ç”¨SVDé™åˆ°8000 å°†word2vecå’ŒGloVeæ‹¼æ¥èµ·æ¥ä½œä¸ºdeep learningæ¨¡å‹çš„è¾“å…¥ å°†æ–‡ç« åˆ†æ®µï¼Œæ¯æ®µå–å‰20å20æ‹¼èµ·æ¥ æ¨¡å‹èåˆ æ‰€æœ‰é˜Ÿä¼éƒ½æ— ä¸€ä¾‹å¤–ä½¿ç”¨äº†æ¨¡å‹èåˆï¼Œstackingæˆ–è€…ç®€å•çš„æŠ•ç¥¨ DL+ML â€”&gt; lgbm model â€”&gt; voting æ·±åº¦æ¨¡å‹+ä¼ ç»Ÿæ¨¡å‹ï¼Œåœ¨æ·±åº¦æ¨¡å‹æœ€åä¸€å±‚åŠ å…¥ä¼ ç»Ÿæ¨¡å‹çš„ä¿¡æ¯/feature åå‘é€‰æ‹©å‰”é™¤å†—ä½™æ¨¡å‹ DL&amp;å…¶ä»– HANï¼Œé€‰æ‹©10ä¸ªattention vector å¯¹æ˜“é”™ç±»å¢åŠ æƒé‡ï¼Œé€šè¿‡æ”¹å˜æŸå¤±å‡½æ•°æ¥å¢åŠ æƒé‡ CNN, [1,2,3,4,5,6]*600 æå‡ºæ–°çš„æ¨¡å‹ï¼ˆç¬¬ä¸€åï¼‰ å…¶å®é™¤äº†ä¸€äº›trickï¼Œæˆ‘è¿˜æ˜¯æœ‰äº›å¤±æœ›çš„ï¼Œå› ä¸ºéƒ½æ˜¯ç”¨æ¨¡å‹èåˆå †å‡ºæ¥çš„ï¼Œè¿™ä¹Ÿè®©æˆ‘å¯¹æ¯”èµ›å¤±å»äº†ä¸€äº›å…´è¶£ã€‚è™½ç„¶èƒ½ç†è§£ç°åœ¨çš„æ¯”èµ›éƒ½æ˜¯è¿™æ ·çš„ï¼Œä½†æ„Ÿè§‰å®åœ¨å¤ªæš´åŠ›äº†ã€‚å½“ç„¶ï¼Œå…¶ä¸­è¿˜æ˜¯æœ‰ä¸€äº›äº®ç‚¹çš„ï¼Œæœ‰ä¸€æ”¯é˜Ÿä¼ç«‹æ„å¾ˆé«˜ï¼Œä»ç†è§£ä¸šåŠ¡çš„è§’åº¦å‡ºå‘è€Œä¸æ˜¯å †æ¨¡å‹ï¼Œä¹Ÿå–å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼›è¿˜æœ‰ä¸€ä¸ªä½¿ç”¨äº†æœ€æ–°è®ºæ–‡ä¸­çš„ç‰¹å¾å·¥ç¨‹æ”¹è¿›æ–¹æ³•ï¼Œä»¤æˆ‘è€³ç›®ä¸€æ–°ï¼›ä»¥åŠç¬¬ä¸€ååœ¨æ¯”èµ›è¿‡ç¨‹ä¸­æå‡ºæ¥ä¸‰ä¸ªæ–°çš„æ¨¡å‹ã€‚ Anywayï¼Œæˆ‘ç›®å‰è¿˜æ˜¯å¤ªèœäº†ï¼Œè¿˜æ˜¯å®‰å¿ƒæç§‘ç ”å§ã€‚_(:Ğ·ã€âˆ )]]></content>
      <tags>
        <tag>æœ‰æ„Ÿ</tag>
        <tag>è¾¾è§‚æ¯</tag>
        <tag>æ¯”èµ›</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†7]]></title>
    <url>%2F2018%2F09%2F16%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%867%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Pytorch]åªæœ‰ä¸€ä¸ªå…ƒç´ çš„tensorï¼Œå¯ç”¨.item()æ¥è·å–å…ƒç´  tensor &lt;â€”&gt; numpy ç›¸äº’è½¬åŒ–ä¼šå…±äº«å†…éƒ¨æ•°æ®ï¼Œå› æ­¤æ”¹å˜å…¶ä¸­ä¸€ä¸ªä¼šæ”¹å˜å¦ä¸€ä¸ª å¯ç”¨ä½¿ç”¨ .to æ¥ç§»åŠ¨åˆ°è®¾å¤‡ .detech() detach it from the computation history, and to prevent future computation from being tracked. å°†å…¶ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»ï¼Œå˜ä¸ºå¶å­èŠ‚ç‚¹ï¼Œå¹¶ä¸”requires_grad=False Function è®°å½•äº†è¿™ä¸ªtensoræ˜¯æ€ä¹ˆæ¥çš„ï¼Œæ‰€æœ‰çš„tensoréƒ½æœ‰ï¼Œé™¤éæ˜¯ç”¨æˆ·è‡ªå®šä¹‰çš„ï¼š 2ï¸âƒ£[åæ–¹å·®]å…³äºåæ–¹å·®çš„ç†è§£ï¼Œxä¸yå…³äºæŸä¸ªè‡ªå˜é‡çš„å˜åŒ–ç¨‹åº¦ï¼Œå³åº¦é‡äº†xä¸yä¹‹é—´çš„è”ç³»ã€‚https://www.zhihu.com/question/20852004]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>åæ–¹å·®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 4:Classification:Probabilistic Generative Model]]></title>
    <url>%2F2018%2F09%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%204%20Classification%20%20Probabilistic%20Generative%20Model%2F</url>
    <content type="text"><![CDATA[ä¸ºä»€ä¹ˆä¸ä½¿ç”¨regressionæ¥åˆ†ç±»ï¼Ÿ1ï¸âƒ£å¦‚æœä½¿ç”¨regressionçš„æ€æƒ³æ¥åˆ†ç±»ï¼Œä¼šå¯¹ç¦»è¾¹ç•Œè¾ƒè¿œçš„ç‚¹è¿›è¡Œæƒ©ç½šï¼š 2ï¸âƒ£å¦‚æœå¤šåˆ†ç±»ä½¿ç”¨regressionï¼Œå¦‚class 1, class 2, class 3ï¼›åˆ™éšå¼åœ°å‡è®¾äº†class 1 å’Œ class 2è¾ƒä¸ºæ¥è¿‘ï¼Œå¦‚æœæ²¡æœ‰è¿™ç§æ¥è¿‘å…³ç³»ï¼Œåˆ™åˆ†ç±»ä¼šä¸æ­£ç¡®ã€‚ é—®é¢˜æè¿°ä¸å®šä¹‰ å½“På¤§äº0.5åˆ™æ˜¯C1ç±»ï¼Œåä¹‹æ˜¯C2ç±»å…ˆéªŒP(C1)å’ŒP(C2)éƒ½å¥½è®¡ç®—ï¼Œè®¡ç®—C1å æ€»çš„æ¯”ä¾‹å³å¯å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—çš„å°±æ˜¯p(x|C) è¿™ä¸€æƒ³æ³•ï¼Œæœ¬è´¨æ˜¯å¾—åˆ°äº†ç”Ÿæˆå¼æ¨¡å‹ï¼š åŸç†æ¦‚è¿°ç°å‡è®¾è®­ç»ƒæ•°æ®ç‚¹çš„åˆ†å¸ƒæœä»é«˜æ–¯åˆ†å¸ƒï¼šï¼ˆæ˜¾ç„¶å¯ä»¥è‡ªå·±è®¾ä»»ä½•åˆ†å¸ƒï¼‰å³æ•°æ®ä»é«˜æ–¯åˆ†å¸ƒé‡‡æ ·å¾—åˆ°ï¼š æ ¹æ®æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œå¯ä»¥è·å¾—æ¯ä¸ªç±»åˆ«çš„Î¼å’ŒÎ£ï¼š å¾—åˆ°äº†å‚æ•°åï¼Œå³å¯ä»£å…¥å¾—åˆ°P(C|x) ï¼š åˆšåˆšå‡è®¾$Î£$å¯¹äºä¸åŒç±»åˆ«ä¸åŒï¼Œç°æˆ‘ä»¬ä»¤ä¸åŒç±»åˆ«å…±äº«ç›¸åŒ$Î£$ï¼šï¼ˆå› ä¸ºåæ–¹å·®ä»£è¡¨çš„æ˜¯ä¸åŒfeatureä¹‹é—´çš„è”ç³»ï¼Œå¯ä»¥è®¤ä¸ºæ˜¯å’Œç±»åˆ«æ— å…³çš„ï¼‰ $Î£$çš„è®¡ç®—å…¬å¼æ˜¯åŠ æƒæ±‚å’Œï¼š åœ¨ä½¿ç”¨äº†ç›¸åŒçš„åæ–¹å·®çŸ©é˜µåï¼Œè¾¹ç•Œå°±æ˜¯çº¿æ€§çš„ï¼ˆåé¢ä¼šæåˆ°ä¸ºä»€ä¹ˆæ˜¯è¿™æ ·ï¼‰ï¼š æ€»ç»“ï¼š ä¸‰æ­¥èµ°ï¼Œå®šä¹‰function setï¼Œè®¡ç®—Î¼å’Œåæ–¹å·®çŸ©é˜µï¼Œå¾—åˆ°best functionï¼š æ³¨æ„åˆ°ï¼Œå¦‚æœæˆ‘ä»¬è®¤ä¸ºï¼Œä¸åŒfeatureä¹‹é—´æ²¡æœ‰å…³ç³»ï¼Œæ¯ä¸ªfeatureç¬¦åˆç‰¹å®šçš„é«˜æ–¯åˆ†å¸ƒï¼Œåˆ™è¯¥åˆ†ç±»å™¨åˆ™æ˜¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼š åˆ†ç±»ä¸logistics regressionç°æ¨å¯¼ï¼Œè¯¥åˆ†ç±»é—®é¢˜ä¸logistics regressionä¹‹é—´çš„è”ç³»ï¼šå³ï¼š å‡è®¾æ•°æ®æœä»é«˜æ–¯åˆ†å¸ƒï¼Œå…±äº«$Î£$ æ¨å¯¼â‘ æ€»æ¡†æ¶ï¼š ä»¤ åˆ™æœ‰ï¼š â‘¡zçš„è¿›ä¸€æ­¥æ¨å¯¼ä¸ç®€åŒ–ï¼š å°†zå±•å¼€ï¼š è€Œç¬¬ä¸€éƒ¨åˆ†æœ‰ï¼š ç¬¬ä¸€éƒ¨åˆ†ç›¸é™¤ï¼Œæœ‰ï¼š å†è¿›è¡Œå±•å¼€ï¼Œæœ‰ï¼š æœ€ç»ˆzçš„å…¬å¼ä¸ºï¼š ç”±äºå…±äº«åæ–¹å·®çŸ©é˜µï¼Œåˆ™å¯ä»¥æ¶ˆå»éƒ¨åˆ†ï¼Œå¾—åˆ°ï¼š æ›¿æ¢æˆwå’Œbï¼š â‘¢æœ€ç»ˆï¼Œå°†zå¸¦å›åˆ°åŸå¼ï¼š æ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦å†ä¼°è®¡N1,N2,Î¼å’ŒÎ£ï¼Œç›´æ¥è®¡ç®—wå’Œbå³å¯ã€‚ä¹Ÿå› æ­¤ï¼Œåˆ†ç•Œçº¿æ˜¯çº¿æ€§çš„ã€‚ å…¨è¿‡ç¨‹ï¼š]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Classification</tag>
        <tag>Probabilistic Generative Model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 3:Gradient Descent]]></title>
    <url>%2F2018%2F09%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%203%20Gradient%20Descent%2F</url>
    <content type="text"><![CDATA[Gradient Descent tipstip 1ï¼šAdaptive Learning RatesAdagradåŸºæœ¬æ€æƒ³ å…¶ä¸­Ïƒæ˜¯ä¹‹å‰æ‰€æœ‰çš„æ¢¯åº¦çš„å¹³æ–¹æ ¹ åŒ–ç®€å½¢å¼ï¼š ä¸ºä»€ä¹ˆè¦æ€ä¹ˆåšï¼Ÿè€ƒè™‘ä¸€ä¸ªå¼€å£å‘ä¸Šçš„äºŒæ¬¡å‡½æ•° ä¹Ÿå³ï¼Œæœ€å¥½çš„æ­¥é•¿æ˜¯ä¸€æ¬¡å¯¼é™¤ä»¥äºŒæ¬¡å¯¼ï¼Œä½†äºŒæ¬¡å¯¼è®¡ç®—é‡å¤§ï¼Œå› æ­¤ä½¿ç”¨è¿‘ä¼¼çš„æ–¹å¼ï¼šå¯¹ä¸€æ¬¡å¯¼ä½œå¤šæ¬¡çš„sampleã€‚ä¸‹å›¾æ˜¾ç¤ºï¼Œå¦‚æœäºŒæ¬¡å¯¼å°ï¼Œé‚£ä¹ˆå¤šæ¬¡sampleè·å¾—çš„ä¸€æ¬¡å¯¼ä¹Ÿå°ï¼Œåä¹‹åˆ™å¤§ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€æ¬¡å¯¼åœ¨æŸç§ç¨‹åº¦ä¸Šå¯ä»¥åæ˜ äºŒæ¬¡å¯¼çš„å¤§å°ï¼Œæ‰€ä»¥ç›´æ¥ç”¨ä¸€æ¬¡å¯¼è¿‘ä¼¼ï¼Œå¯ä»¥å‡å°‘è®¡ç®—é‡ã€‚ tip 2ï¼šfeature scaling èƒ½å¤Ÿæ”¹å˜lossçš„åˆ†å¸ƒï¼Œä¸Šå›¾1ä¸­w2å¯¹lossçš„å½±å“è¾ƒå¤§ï¼Œåˆ™è¾ƒé™¡å³­ï¼Œå‚æ•°æ›´æ–°å°±è¾ƒå›°éš¾ï¼Œéœ€è¦adaptive learning rateï¼›å¦‚æœè¿›è¡Œfeature scalingï¼Œèƒ½å¤Ÿæ›´å¥½è¾¾åˆ°local optimal Gradient Descent Theoryå¦ä¸€ç§è§’åº¦çœ‹gradient descentï¼š åŸºæœ¬æ€æƒ³ï¼šæˆ‘ä»¬å¸Œæœ›æ¯ä¸€æ¬¡éƒ½åœ¨å½“å‰ç‚¹é™„è¿‘æ‰¾åˆ°ä¸€ä¸ªæœ€å°çš„ç‚¹ï¼Œå³åœ¨ä¸€ä¸ªèŒƒå›´å†…ï¼š åº”è¯¥å¦‚ä½•æ‰¾åˆ°è¯¥æœ€å°ç‚¹ï¼Ÿ æˆ‘ä»¬çŸ¥é“ï¼Œæ³°å‹’çº§æ•°çš„å½¢å¼ï¼š å½“xæ¥è¿‘x0æ—¶ï¼Œä¼šæœ‰å¦‚ä¸‹è¿‘ä¼¼ï¼š æ¨å¹¿åˆ°å¤šå…ƒæ³°å‹’çº§æ•°åˆ™æœ‰ï¼š é‚£ä¹ˆï¼Œå¦‚å‰æ‰€è¿°ï¼Œxæ¥è¿‘x0ï¼Œå¯¹äºå›¾ä¸­ï¼Œå³åœ†åœˆè¶³å¤Ÿå°æ—¶ï¼š ç®€åŒ–ç¬¦å·ï¼š æ‰€ä»¥å¯ä»¥ç®€å†™æˆï¼š ç”±äºs,u,véƒ½æ˜¯å¸¸æ•°ï¼Œåœ¨åœ†åœˆèŒƒå›´å†…å¯»æ‰¾æœ€å°å€¼å¯¹åº”çš„å‚æ•°å¯ä»¥ç®€åŒ–æˆï¼š å†åº¦ç®€åŒ–ï¼Œå¯ä»¥è¡¨è¾¾æˆï¼š åœ¨å›¾ä¸­å¯ä»¥ç”»ä¸ºä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯ æ˜¾ç„¶ï¼Œå½“åæ–¹å‘æ—¶ï¼Œæœ€å°ï¼š ä¹Ÿå³ï¼š æœ€ç»ˆå®Œæ•´çš„å¼å­ï¼š å› æ­¤ï¼Œå½“learning rateä¸å¤Ÿå°æ—¶ï¼Œæ˜¯ä¸æ»¡è¶³æ³°å‹’çº§æ•°è¿‘ä¼¼çš„ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>Gradient Descent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 2:Bias and Variance]]></title>
    <url>%2F2018%2F09%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%202%20Bias%20and%20Variance%2F</url>
    <content type="text"><![CDATA[å¦‚ä½•ç†è§£bias&amp;variancebiasæ˜¯function spaceä¸­å¿ƒç¦»optimal modelçš„å·®è·ï¼Œvarianceæ˜¯æŸæ¬¡å®éªŒæ‰€å¾—æ¨¡å‹ç¦»function spaceä¸­å¿ƒçš„è·ç¦»ã€‚ æ¯”å¦‚è¯´ï¼Œç®€å•åœ°æ¨¡å‹çš„function spaceå°ï¼Œéšæœºæ€§å°ï¼Œå› æ­¤varianceå°ï¼Œä½†ä¹Ÿå› ä¸ºfunction spaceå°ï¼Œè¡¨ç¤ºèƒ½åŠ›æœ‰é™ï¼Œå› æ­¤biaså¤§ã€‚ å¦‚å›¾ï¼šè¯¥å›¾ä¸­è“è‰²åœˆä»£è¡¨æ¨¡å‹æ‰€èƒ½è¡¨è¾¾çš„èŒƒå›´ã€‚ å¦‚ä½•è§£å†³varianceå¤§çš„é—®é¢˜â‘ æ›´å¤šçš„dataâ‘¡regularizationï¼šå¼ºè¿«functionæ›´å¹³æ»‘ï¼Œå› æ­¤å‡å°varianceï¼Œä½†å› ä¸ºè°ƒæ•´äº†function spaceï¼Œå¯èƒ½ä¼šå¢åŠ biasã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ ğŸ¤–</tag>
        <tag>æå®æ¯…æœºå™¨å­¦ä¹ è¯¾ç¨‹</tag>
        <tag>bias&amp;variance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯9]]></title>
    <url>%2F2018%2F09%2F16%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D9%2F</url>
    <content type="text"><![CDATA[ç™½é›ªæ­Œé€æ­¦åˆ¤å®˜å½’äº¬[å”] å²‘å‚åŒ—é£å·åœ°ç™½è‰æŠ˜ï¼Œèƒ¡å¤©å…«æœˆå³é£é›ªã€‚å¿½å¦‚ä¸€å¤œæ˜¥é£æ¥ï¼Œåƒæ ‘ä¸‡æ ‘æ¢¨èŠ±å¼€ã€‚æ•£å…¥ç å¸˜æ¹¿ç½—å¹•ï¼Œç‹è£˜ä¸æš–é”¦è¡¾è–„ã€‚å°†å†›è§’å¼“ä¸å¾—æ§ï¼Œéƒ½æŠ¤é“è¡£å†·éš¾ç€ã€‚ç€šæµ·é˜‘å¹²ç™¾ä¸ˆå†°ï¼Œæ„äº‘æƒ¨æ·¡ä¸‡é‡Œå‡ã€‚ä¸­å†›ç½®é…’é¥®å½’å®¢ï¼Œèƒ¡ç´çµç¶ä¸ç¾Œç¬›ã€‚çº·çº·æš®é›ªä¸‹è¾•é—¨ï¼Œé£æ£çº¢æ——å†»ä¸ç¿»ã€‚è½®å°ä¸œé—¨é€å›å»ï¼Œå»æ—¶é›ªæ»¡å¤©å±±è·¯ã€‚å±±å›è·¯è½¬ä¸è§å›ï¼Œé›ªä¸Šç©ºç•™é©¬è¡Œå¤„ã€‚ http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290 ç»å‘½è¯—è°­å—£åŒæœ›é—¨æŠ•æ­¢æ€å¼ ä¿­ï¼Œå¿æ­»é¡»è‡¾å¾…æœæ ¹ã€‚æˆ‘è‡ªæ¨ªåˆ€å‘å¤©ç¬‘ï¼Œå»ç•™è‚èƒ†ä¸¤æ˜†ä»‘ï¼]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch backward()æµ…æ]]></title>
    <url>%2F2018%2F09%2F16%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPytorch%20backward()%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[æœ€è¿‘åœ¨çœ‹pytorchæ–‡æ¡£çš„æ—¶å€™ï¼Œçœ‹åˆ°backwardå†…æœ‰ä¸€ä¸ªå‚æ•°gradientï¼Œåœ¨ç»è¿‡æŸ¥é˜…äº†ç›¸å…³èµ„æ–™å’Œè¿›è¡Œäº†å®éªŒåï¼Œå¯¹backwardæœ‰äº†æ›´æ·±çš„è®¤è¯†ã€‚ backward1ï¸âƒ£å¦‚æœè°ƒç”¨backwardçš„æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œå¦‚ï¼šloss.backward()åˆ™gradientä¸éœ€è¦æ‰‹åŠ¨ä¼ å…¥ï¼Œä¼šè‡ªåŠ¨æ±‚å¯¼ã€‚ä¾‹å­:$a=[x_1,x_2],b=\frac{x_1+x_2}{2}$åˆ™bå¯¹aæ±‚å¯¼ï¼Œæœ‰ï¼š$\dfrac {\partial b}{\partial x_{1}}=\frac{1}{2}ï¼Œ\dfrac {\partial b}{\partial x_{2}}=\frac{1}{2}$ 123456import torcha=torch.Tensor([2,3])a.requires_grad=Trueb=torch.mean(a) #tensor(2.5000, grad_fn=&lt;MeanBackward1&gt;)b.backward()a.grad #tensor([0.5000, 0.5000]) gradientæ­¤æ—¶åªæ˜¯åœ¨ç¼©æ”¾åŸgradçš„å¤§å°ï¼Œä¹Ÿå³ä¸æŒ‡å®šgradientå’Œgradient=1æ˜¯ç­‰ä»·çš„ å½“ç„¶ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šgradientï¼Œå…¶ä¸­æŒ‡å®šgradientçš„shapeå¿…é¡»å’Œbçš„ç»´åº¦ç›¸åŒ123gradient=torch.tensor(10.0)b.backward(gradient)a.grad #tensor([5., 5.]) 2ï¸âƒ£å¦‚æœè°ƒç”¨backwardçš„æ˜¯ä¸€ä¸ªå‘é‡ä¾‹å­ï¼š$a=[x_1,x_2],b=[b_1,b_2]$, å…¶ä¸­ $b_1=x_1+x_2,b_2=x_1*x_2$bå¯¹aæ±‚å¯¼ï¼Œæœ‰ï¼š$\dfrac {\partial b_1}{\partial x_{1}}=1,\dfrac {\partial b_1}{\partial x_{2}}=1$ $\dfrac {\partial b_2}{\partial x_{1}}=x_2,\dfrac {\partial b_2}{\partial x_{2}}=x_1$ åœ¨backwardçš„æ—¶å€™åˆ™å¿…é¡»æŒ‡å®šgradientã€‚ 1234567891011121314151617181920import torcha=torch.FloatTensor([2,3])a.requires_grad=Trueb=torch.zeros(2)b[0]=a[0]+a[1]b[1]=a[0]*a[1] # b=tensor([5., 6.], grad_fn=&lt;CopySlices&gt;)gradient=torch.tensor([1.0,0.0])b.backward(gradient,retain_graph=True)a.grad #tensor([1., 1.])ï¼Œè¯´æ˜æ˜¯å¯¹b_1è¿›è¡Œæ±‚å¯¼a.grad.zero_() #å°†æ¢¯åº¦æ¸…ç©ºï¼Œå¦åˆ™ä¼šå åŠ #-------------- #gradient=torch.tensor([0.0,1.0])b.backward(gradient,retain_graph=True)a.grad # tensor([3., 2.])ï¼Œè¯´æ˜å¯¹b_2è¿›è¡Œæ±‚å¯¼a.grad.zero_()# ------------- #gradient=torch.tensor([1.0,1.0])b.backward(gradient,retain_graph=True)a.grad # tensor([4., 3.])ï¼Œå³b_1,b_2çš„å¯¼æ•°çš„å åŠ a.grad.zero_() æ³¨æ„åˆ°b.backward()æ—¶éœ€è¦retain_graphè®¾ä¸ºTrueï¼Œå¦åˆ™åœ¨è®¡ç®—å®Œåä¼šè‡ªåŠ¨é‡Šæ”¾è®¡ç®—å›¾çš„å†…å­˜ï¼Œè¿™æ ·å°±æ²¡æ³•è¿›è¡ŒäºŒæ¬¡åå‘ä¼ æ’­äº†ã€‚ Referencehttps://www.pytorchtutorial.com/pytorch-backward/]]></content>
      <tags>
        <tag>Pytorch</tag>
        <tag>backward</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯8]]></title>
    <url>%2F2018%2F09%2F09%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D8%2F</url>
    <content type="text"><![CDATA[æœ›æœˆæ€€è¿œ[å”] å¼ ä¹é¾„æµ·ä¸Šç”Ÿæ˜æœˆï¼Œå¤©æ¶¯å…±æ­¤æ—¶ã€‚æƒ…äººæ€¨é¥å¤œï¼Œç«Ÿå¤•èµ·ç›¸æ€ã€‚ç­çƒ›æ€œå…‰æ»¡ï¼ŒæŠ«è¡£è§‰éœ²æ»‹ã€‚ä¸å ªç›ˆæ‰‹èµ ï¼Œè¿˜å¯æ¢¦ä½³æœŸã€‚ é¥å¤œï¼Œé•¿å¤œã€‚ http://m.xichuangzhu.com/work/57aca120a341310060e2a09f æ— é¢˜è¨é•‡å†°äº”åä¸ƒè½½çŠ¹å¦‚æ¢¦ï¼Œä¸¾å›½æ²¦äº¡ç¼˜æ±‰åŸã€‚é¾™æ¸¸æµ…æ°´å‹¿è‡ªå¼ƒï¼Œç»ˆæœ‰æ‰¬çœ‰åæ°”å¤©ã€‚ 1951å¹´ï¼Œä¸­å›½äººæ°‘å¿—æ„¿å†›åœ¨æŠ—ç¾æ´æœæˆ˜äº‰ç¬¬ä¸‰æ¬¡æˆ˜å½¹åæ‰“è¿›äº†æ±‰åŸï¼Œè¨é•‡å†°å¾—çŸ¥æ­¤äº‹ï¼Œå›æƒ³èµ·57å¹´å‰çš„ç”²åˆæ‚²æ­Œï¼Œå½“å³ä½œè¯—ä¸€é¦–ã€‚ ç™½é›ªæ­Œé€æ­¦åˆ¤å®˜å½’äº¬[å”] å²‘å‚åŒ—é£å·åœ°ç™½è‰æŠ˜ï¼Œèƒ¡å¤©å…«æœˆå³é£é›ªã€‚å¿½å¦‚ä¸€å¤œæ˜¥é£æ¥ï¼Œåƒæ ‘ä¸‡æ ‘æ¢¨èŠ±å¼€ã€‚æ•£å…¥ç å¸˜æ¹¿ç½—å¹•ï¼Œç‹è£˜ä¸æš–é”¦è¡¾è–„ã€‚å°†å†›è§’å¼“ä¸å¾—æ§ï¼Œéƒ½æŠ¤é“è¡£å†·éš¾ç€ã€‚ç€šæµ·é˜‘å¹²ç™¾ä¸ˆå†°ï¼Œæ„äº‘æƒ¨æ·¡ä¸‡é‡Œå‡ã€‚ä¸­å†›ç½®é…’é¥®å½’å®¢ï¼Œèƒ¡ç´çµç¶ä¸ç¾Œç¬›ã€‚çº·çº·æš®é›ªä¸‹è¾•é—¨ï¼Œé£æ£çº¢æ——å†»ä¸ç¿»ã€‚è½®å°ä¸œé—¨é€å›å»ï¼Œå»æ—¶é›ªæ»¡å¤©å±±è·¯ã€‚å±±å›è·¯è½¬ä¸è§å›ï¼Œé›ªä¸Šç©ºç•™é©¬è¡Œå¤„ã€‚ http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯7]]></title>
    <url>%2F2018%2F09%2F02%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D7%2F</url>
    <content type="text"><![CDATA[æ»•ç‹é˜åºé¥è¥Ÿç”«ç•…ï¼Œé€¸å…´é„é£ã€‚çˆ½ç±å‘è€Œæ¸…é£ç”Ÿï¼Œçº¤æ­Œå‡è€Œç™½äº‘éã€‚ç¢å›­ç»¿ç«¹ï¼Œæ°”å‡Œå½­æ³½ä¹‹æ¨½ï¼›é‚ºæ°´æœ±åï¼Œå…‰ç…§ä¸´å·ä¹‹ç¬”ã€‚å››ç¾å…·ï¼ŒäºŒéš¾å¹¶ã€‚ç©·ç‡çœ„äºä¸­å¤©ï¼Œæå¨±æ¸¸äºæš‡æ—¥ã€‚å¤©é«˜åœ°è¿¥ï¼Œè§‰å®‡å®™ä¹‹æ— ç©·ï¼›å…´å°½æ‚²æ¥ï¼Œè¯†ç›ˆè™šä¹‹æœ‰æ•°ã€‚æœ›é•¿å®‰äºæ—¥ä¸‹ï¼Œç›®å´ä¼šäºäº‘é—´ã€‚åœ°åŠ¿æè€Œå—æºŸæ·±ï¼Œå¤©æŸ±é«˜è€ŒåŒ—è¾°è¿œã€‚å…³å±±éš¾è¶Šï¼Œè°æ‚²å¤±è·¯ä¹‹äººï¼›èæ°´ç›¸é€¢ï¼Œå°½æ˜¯ä»–ä¹¡ä¹‹å®¢ã€‚æ€€å¸é˜è€Œä¸è§ï¼Œå¥‰å®£å®¤ä»¥ä½•å¹´ï¼Ÿ æ³¨é‡Šï¼šé¥è¥Ÿç”«ç•…ï¼Œé€¸å…´é„ï¼ˆchuÃ¡nï¼‰é£ï¼šç™»é«˜æœ›è¿œçš„èƒ¸æ€€é¡¿æ—¶èˆ’ç•…ï¼Œé£˜æ¬²è„±ä¿—çš„å…´è‡´æ²¹ç„¶è€Œç”Ÿã€‚ çˆ½ç±ï¼ˆlÃ iï¼‰å‘è€Œæ¸…é£ç”Ÿï¼Œçº¤æ­Œå‡è€Œç™½äº‘éï¼šå®´ä¼šä¸Šï¼Œæ’ç®«å“èµ·ï¼Œå¥½åƒæ¸…é£æ‹‚æ¥ï¼›æŸ”ç¾çš„æ­Œå£°ç¼­ç»•ä¸æ•£ï¼Œéæ­¢äº†ç™½äº‘é£åŠ¨ã€‚çˆ½ï¼šå½¢å®¹ç±çš„å‘éŸ³æ¸…è„†ã€‚ç±ï¼šæ’ç®«ï¼Œä¸€ç§ç”±å¤šæ ¹ç«¹ç®¡ç¼–æ’è€Œæˆçš„ç®¡ä¹å™¨ã€‚ ç¢ï¼ˆsuÄ«ï¼‰å›­ç»¿ç«¹ï¼Œæ°”å‡Œå½­æ³½ä¹‹æ¨½ï¼šä»Šæ—¥çš„å®´ä¼šï¼Œå¥½æ¯”å½“å¹´ç¢å›­ç«¹æ—çš„èšä¼šï¼Œåœ¨åº§çš„æ–‡äººé›…å£«ï¼Œè±ªçˆ½å–„é¥®çš„æ°”æ¦‚è¶…è¿‡äº†é™¶æ¸Šæ˜ã€‚ç¢å›­ï¼šè¥¿æ±‰æ¢å­ç‹åœ¨ç¢æ°´æ—ä¿®å»ºçš„ç«¹å›­ï¼Œä»–å¸¸å’Œä¸€äº›æ–‡äººåœ¨æ­¤é¥®é…’èµ‹è¯—ã€‚ é‚ºï¼ˆyÃ¨ï¼‰æ°´æœ±åï¼Œå…‰ç…§ä¸´å·ä¹‹ç¬”ï¼šè¿™æ˜¯å€Ÿè¯—äººæ›¹æ¤ã€è°¢çµè¿æ¥æ¯”æ‹Ÿå‚åŠ å®´ä¼šçš„æ–‡äººã€‚é‚ºï¼šä»Šæ²³åŒ—ä¸´æ¼³ï¼Œæ˜¯æ›¹é­å…´èµ·çš„åœ°æ–¹ã€‚æ›¹æ¤æ›¾åœ¨è¿™é‡Œä½œè¿‡ã€Šå…¬å®´è¯—ã€‹ï¼Œè¯—ä¸­æœ‰â€œæœ±åå†’ç»¿æ± â€çš„å¥å­ã€‚ä¸´å·ä¹‹ç¬”ï¼šæŒ‡è°¢çµè¿ï¼Œä»–æ›¾ä»»ä¸´å·ï¼ˆä»Šå±æ±Ÿè¥¿ï¼‰å†…å²ã€‚ å››ç¾ï¼šæŒ‡è‰¯è¾°ã€ç¾æ™¯ã€èµå¿ƒã€ä¹äº‹ã€‚ äºŒéš¾ï¼šè´¤ä¸»ã€å˜‰å®¾ã€‚ åœ°åŠ¿æè€Œå—æºŸæ·±ï¼Œå¤©æŸ±é«˜è€ŒåŒ—è¾°è¿œï¼šåœ°åŠ¿åè¿œï¼Œå—æµ·æ·±é‚ƒï¼›å¤©æŸ±é«˜è€¸ï¼ŒåŒ—ææ˜Ÿè¿œæ‚¬ã€‚ å¸é˜ï¼ˆhÅ«nï¼‰ï¼šåŸæŒ‡å¤©å¸çš„å®ˆé—¨è€…ã€‚è¿™é‡ŒæŒ‡çš‡å¸çš„å®«é—¨ã€‚ å¥‰å®£å®¤ä»¥ä½•å¹´ï¼šä»€ä¹ˆæ—¶å€™æ‰èƒ½åƒè´¾è°Šé‚£æ ·å»ä¾å¥‰å›ç‹å‘¢]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†6]]></title>
    <url>%2F2018%2F09%2F02%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%866%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[dropout]dropoutå½¢å¼:RNNçš„å½¢å¼æœ‰å¤šç§ï¼š recurrent dropoutRNN: $h_t=f(W_h âŠ™ [x_t,h_{t-1}]+b_h)$åŠ ä¸Šdropoutçš„RNNï¼š$h_t=f(W_h âŠ™ [x_t,d(h_{t-1})]+b_h)$ï¼Œå…¶ä¸­$d(\cdot)$ä¸ºdropoutå‡½æ•°åŒç†ï¼šLSTM:$c_t=f_t âŠ™c_{t-1} + i_t âŠ™ d(g_t)$GRU:$h_t=(1-z_t)âŠ™c_{t-1}+z_tâŠ™d(g_t)$ å‚ç›´è¿æ¥çš„dropoutdropoutçš„ä½œç”¨å³æ˜¯å¦å…è®¸Lå±‚æŸä¸ªLSTMå•å…ƒçš„éšçŠ¶æ€ä¿¡æ¯æµå…¥L+1å±‚å¯¹åº”å•å…ƒã€‚ Reference:https://blog.csdn.net/falianghuang/article/details/72910161 2ï¸âƒ£[Pytorch]pack_padded_sequenceç”¨äºRNNä¸­ï¼Œå°†paddingçŸ©é˜µå‹ç¼©:è¿™æ ·å°±å¯ä»¥å®ç°åœ¨RNNä¼ è¾“è¿‡ç¨‹ä¸­çŸ­å¥æå‰ç»“æŸã€‚ pad_packed_sequenceæ˜¯pack_padded_sequenceçš„é€†è¿ç®—ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>dropout</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æˆ‘æ²¡æœ‰è¯´è¯]]></title>
    <url>%2F2018%2F08%2F29%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%88%91%E6%B2%A1%E6%9C%89%E8%AF%B4%E8%AF%9D%2F</url>
    <content type="text"><![CDATA[ã€Šæˆ‘æ²¡æœ‰è¯´è¯ã€‹ çº³ç²¹æ€å…±äº§å…šæ—¶ï¼Œæˆ‘æ²¡æœ‰å‡ºå£°â€”â€”å› ä¸ºæˆ‘ä¸æ˜¯å…±äº§å…šå‘˜ï¼›æ¥ç€ä»–ä»¬è¿«å®³çŠ¹å¤ªäººï¼Œæˆ‘æ²¡æœ‰å‡ºå£°â€”â€”å› ä¸ºæˆ‘ä¸æ˜¯çŠ¹å¤ªäººï¼›ç„¶åä»–ä»¬æ€å·¥ä¼šæˆå‘˜ï¼Œæˆ‘æ²¡æœ‰å‡ºå£°â€”â€”å› ä¸ºæˆ‘ä¸æ˜¯å·¥ä¼šæˆå‘˜ï¼›åæ¥ä»–ä»¬è¿«å®³å¤©ä¸»æ•™å¾’ï¼Œæˆ‘æ²¡æœ‰å‡ºå£°â€”â€”å› ä¸ºæˆ‘æ˜¯æ–°æ•™å¾’ï¼›æœ€åå½“ä»–ä»¬å¼€å§‹å¯¹ä»˜æˆ‘çš„æ—¶å€™ï¼Œå·²ç»æ²¡æœ‰äººèƒ½ç«™å‡ºæ¥ä¸ºæˆ‘å‘å£°äº†]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning NLP best practicesç¬”è®°]]></title>
    <url>%2F2018%2F08%2F26%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FDeep%20Learning%20NLP%20best%20practices%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[åšå®¢åœ°å€ï¼šhttp://ruder.io/deep-learning-nlp-best-practices/index.htmlä¸ªäººè§‰å¾—è¿™ç¯‡æ–‡ç« å†™å¾—å¾ˆå¥½ï¼Œæœ‰è®¸å¤šå®è·µå¾—åˆ°çš„ç»éªŒï¼Œé€šè¿‡è¿™ç¯‡å¯ä»¥é¿å…èµ°ä¸€äº›å¼¯è·¯ã€‚ PracticesWord Embedding The optimal dimensionality of word embeddings is mostly task-dependent: a smaller dimensionality works better for more syntactic tasks such as named entity recognition or part-of-speech (POS) tagging, while a larger dimensionality is more useful for more semantic tasks such as sentiment analysis. å¯¹äºåå‘è¯­æ³•çš„ï¼Œä½¿ç”¨ç»´åº¦ä½ä¸€äº›çš„è¯å‘é‡ï¼›è€Œå¯¹äºåå‘è¯­ä¹‰å†…å®¹çš„ï¼Œä½¿ç”¨ç»´åº¦å¤§ä¸€äº›çš„è¯å‘é‡ï¼Œå¦‚æƒ…æ„Ÿåˆ†æã€‚ LSTM Depth performance improvements of making the model deeper than 2 layers are minimal LSTMæ·±åº¦æœ€å¥½ä¸è¦è¶…è¿‡ä¸¤å±‚ã€‚ Optimization It is often thought that Adam clearly outperforms vanilla stochastic gradient descent (SGD). However, while it converges much faster than SGD, it has been observed that SGD with learning rate annealing slightly outperforms Adam. Recent work furthermore shows that SGD with properly tuned momentum outperforms Adam . Adamå¯ä»¥æ›´æ—©æ‹Ÿåˆï¼Œè€ŒSGDæ•ˆæœå¯èƒ½ä¼šæ›´å¥½ä¸€äº›ã€‚ å¯ä»¥é‡‡ç”¨ä¼˜åŒ–ç­–ç•¥ï¼Œæ¯”å¦‚è¯´ä½¿ç”¨Adamè®­ç»ƒç›´åˆ°æ‹Ÿåˆï¼Œç„¶åå°†å­¦ä¹ ç‡å‡åŠï¼Œå¹¶é‡æ–°å¯¼å…¥ä¹‹å‰è®­ç»ƒå¥½çš„æœ€å¥½çš„æ¨¡å‹ã€‚è¿™æ ·Adamèƒ½å¤Ÿå¿˜è®°ä¹‹å‰çš„ä¿¡æ¯å¹¶é‡æ–°å¼€å§‹è®­ç»ƒã€‚ Denkowski &amp; Neubig (2017) show that Adam with 2 restarts and learning rate annealing is faster and performs better than SGD with annealing Ensembling Combining multiple models into an ensemble by averaging their predictions is a proven strategy to improve model performance. Ensemblingå¾ˆé‡è¦çš„ä¸€ç‚¹æ˜¯éœ€è¦ä¿è¯å¤šæ ·æ€§ï¼š Ensembling is an important way to ensure that results are still reliable if the diversity of the evaluated models increases (Denkowski &amp; Neubig, 2017). While ensembling different checkpoints of a model has been shown to be effective (Jean et al., 2015; Sennrich et al., 2016) [51, 52], it comes at the cost of model diversity. Cyclical learning rates can help to mitigate this effect LSTM tricks åœ¨initial stateä¸­æˆ‘ä»¬å¸¸å¸¸ä½¿ç”¨å…¨0å‘é‡ï¼Œå®é™…ä¸Šå¯ä»¥å°†å…¶ä½œä¸ºå‚æ•°å­¦ä¹ ã€‚ Instead of fixing the initial state, we can learn it like any other parameter, which can improve performance å°†inputå’Œoutput embeddingçš„å‚æ•°å…±äº«ï¼Œå¦‚æœæ˜¯åšlanguage modelæˆ–è€…æœºå™¨ç¿»è¯‘ä¹‹ç±»çš„ï¼Œå¯ä»¥è®©ä»–ä»¬å…±äº«ã€‚ Gradient Norm Clipping Rather than clipping each gradient independently, clipping the global norm of the gradient yields more significant improvements è¿™ç‚¹æˆ‘æ²¡çœ‹æ‡‚ã€‚ Classification practiceså…³äºCNN CNN filters:Combining filter sizes near the optimal filter size, e.g. (3,4,5) performs best (Kim, 2014; Kim et al., 2016). The optimal number of feature maps is in the range of 50-600 (Zhang &amp; Wallace, 2015) [59]. Aggregation function:1-max-pooling outperforms average-pooling and k-max pooling (Zhang &amp; Wallace, 2015). è¿™åœ¨æˆ‘ä¹‹å‰çš„å…³äºCNNæ–‡æœ¬åˆ†ç±»æŒ‡å—ä¸­æœ‰æ›´è¯¦å°½çš„åˆ†æã€‚ Conclusionè¿™æ˜¯ä¸€ç¯‡å¹²è´§æ»¡æ»¡çš„åšå®¢ï¼Œå®é™…ä¸Šæˆ‘è¿˜æ˜¯æœ‰è®¸å¤šåœ°æ–¹æ²¡æœ‰è¯»æ‡‚ï¼Œè¿™é€‚åˆå¤šçœ‹å‡ éï¼Œæ…¢æ…¢ç†è§£ã€‚]]></content>
      <tags>
        <tag>æŒ‡å—</tag>
        <tag>è°ƒå‚</tag>
        <tag>NLPğŸ¤–</tag>
        <tag>ç¬”è®°ğŸ“’</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†5]]></title>
    <url>%2F2018%2F08%2F26%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%865%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Paper]Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components åŸºæœ¬æ¡†æ¶å’ŒCBOWä¸€è‡´ï¼Œä¸»è¦è´¡çŒ®åœ¨äºé’ˆå¯¹ä¸­æ–‡è¯å‘é‡æ·»åŠ äº†åæ—ã€å­—çš„ç»„ä»¶ä½œä¸ºè®­ç»ƒä¿¡æ¯ã€‚ 2ï¸âƒ£[Paper]Highway Networks ä¸ºäº†è§£å†³ç¥ç»ç½‘ç»œæ·±åº¦è¿‡æ·±æ—¶å¯¼è‡´çš„åå‘ä¼ æ’­å›°éš¾çš„é—®é¢˜ã€‚å‰å‘ä¼ æ’­çš„å…¬å¼ï¼š y=H(x,W_H)è€Œè®ºæ–‡æ‰€åšçš„æ”¹è¿›ï¼š y=H(x,W_H) \cdot T(x,W_T)+ x \cdot C(x,W_C)å…¶ä¸­$T$æ˜¯transform gateï¼Œ$C$æ˜¯carry gateã€‚æ–¹ä¾¿èµ·è§ï¼Œå¯ä»¥å°† $C=1-T$ï¼Œæœ€ç»ˆæœ‰ï¼š y=H(x,W_H) \cdot T(x,W_T)+ x \cdot (1-T(x,W_T))å¯ä»¥çœ‹å‡ºæ€æƒ³å’ŒLSTMå¾ˆç±»ä¼¼ï¼Œéƒ½æ˜¯gateçš„æ€æƒ³ã€‚ 3ï¸âƒ£[è°ƒå‚æ–¹æ³•]åšå®¢ï¼šhttps://blog.goodaudience.com/how-to-make-your-model-happy-again-part-1-40d94a9ffb41 å­¦ä¹ ç‡ï¼š ä¸€æ¡åŸåˆ™ï¼šå½“validation losså¼€å§‹ä¸Šå‡æ—¶ï¼Œå‡å°‘å­¦ä¹ ç‡ã€‚ å¦‚ä½•å‡å°‘ï¼Ÿ æˆ–è€…ï¼š è®¾å®šä¸€å®šçš„epochä½œä¸ºä¸€ä¸ªstepsizeï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çº¿æ€§å¢åŠ å­¦ä¹ ç‡ï¼Œç„¶ååœ¨åˆ°è¾¾æœ€å¤§å€¼åå†çº¿æ€§å‡å°ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ–¹æ³•å¯ä»¥åœ¨ä¸€åŠçš„epochå†…è¾¾åˆ°ç›¸åŒçš„æ•ˆæœã€‚ batch sizeï¼š ç”±äºbatch sizeå’Œå­¦ä¹ ç‡çš„å¼ºç›¸å…³æ€§ï¼Œç›¸å…³è®ºæ–‡æå‡ºæé«˜batch sizeè€Œä¸æ˜¯é™ä½å­¦ä¹ ç‡çš„æ–¹æ³•æ¥æå‡æ¨¡å‹è¡¨ç°ã€‚ increasing the batch size during training, instead of decaying learning rate. â€” L. Smithhttps://arxiv.org/pdf/1711.00489.pdf ä¸€ä¸ªtrickï¼šä¿æŒå­¦ä¹ ç‡ä¸å˜ï¼Œæé«˜batch sizeï¼Œç›´åˆ°batch size~è®­ç»ƒé›†/10ï¼Œæ¥ä¸‹æ¥å†é‡‡ç”¨å­¦ä¹ ç‡ä¸‹é™çš„ç­–ç•¥ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Paper</tag>
        <tag>è°ƒå‚æ–¹æ³•</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•6]]></title>
    <url>%2F2018%2F08%2F26%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB6%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£å°†æ•°æ®æ•´ç†æˆbatch123456789101112131415161718192021def _iter_batch(paras,labels,batch_size,shuffle=True): ''' :param paras: :param labels: :param batch_size: :param shuffle: :return: ''' assert len(paras)==len(labels) paras_size=len(paras) if shuffle: indices=np.arange(paras_size) np.random.shuffle(indices) for start_idx in range(0,paras_size-batch_size+1,batch_size): if shuffle: excerpt=indices[start_idx:start_idx+batch_size] else: excerpt=slice(start_idx,start_idx+batch_size) yield paras[excerpt],labels[excerpt]]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯6]]></title>
    <url>%2F2018%2F08%2F26%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D6%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£ æˆä¸ºå…­ç»å¥[å”] æœç”«ã€å…¶äºŒã€‘ç‹æ¨å¢éª†å½“æ—¶ä½“ï¼Œè½»è–„ä¸ºæ–‡å“‚æœªä¼‘ã€‚å°”æ›¹èº«ä¸åä¿±ç­ï¼Œä¸åºŸæ±Ÿæ²³ä¸‡å¤æµã€‚ å“‚ï¼ˆshÄ›nï¼‰ï¼šè®¥ç¬‘ã€‚ http://m.xichuangzhu.com/work/57b9658c0a2b58005c95d2a7]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNNæ–‡æœ¬åˆ†ç±»ä»»åŠ¡æŒ‡å—]]></title>
    <url>%2F2018%2F08%2F25%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FCNN%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[æœ€è¿‘å› ä¸ºæ¯”èµ›çš„ç¼˜æ•…å¯¹æ–‡æœ¬åˆ†ç±»æœ‰ä¸€å®šçš„äº†è§£ã€‚å…¶ä¸­ä½¿ç”¨CNNæ–¹æ³•åšæƒ…æ„Ÿåˆ†æä»»åŠ¡å­˜åœ¨ç€è®¸å¤šä¼˜åŠ¿ã€‚è™½ç„¶æ¨¡å‹ç®€å•ï¼Œä½†å¦‚ä½•è®¾ç½®è¶…å‚æœ‰æ—¶å€™å¯¹ç»“æœæœ‰å¾ˆå¤§çš„å½±å“ã€‚æœ¬æ–‡è®°å½•äº†å…³äºCNNæ–‡æœ¬åˆ†ç±»çš„ä¸€äº›å­¦ä¹ å†ç¨‹å’ŒæŒ‡å—ï¼ŒåŸºæœ¬å‚è€ƒäº†è®ºæ–‡ã€‚ åšæ³•åŸºæœ¬ä¸Šç›®å‰è¾ƒä¸ºæµ…å±‚çš„CNNæ–‡æœ¬åˆ†ç±»çš„åšæ³•éƒ½æ˜¯å¦‚ä¸‹å›¾ï¼š å°†è¯å‘é‡å †ç§¯æˆä¸ºäºŒç»´çš„çŸ©é˜µï¼Œé€šè¿‡CNNçš„å·ç§¯å•å…ƒå¯¹çŸ©é˜µè¿›è¡Œå·ç§¯å¤„ç†ï¼ŒåŒæ—¶ä½¿ç”¨poolingï¼ˆé€šå¸¸æ˜¯1max-poolingï¼‰æ“ä½œï¼Œå°†ä¸ç­‰é•¿çš„å·ç§¯ç»“æœå˜ä¸ºç­‰é•¿ï¼Œå¯¹ä¸åŒçš„å·ç§¯å•å…ƒçš„ç»“æœè¿›è¡Œæ‹¼æ¥åç”Ÿæˆå•ä¸ªå‘é‡ï¼Œæœ€åå†é€šè¿‡çº¿æ€§å±‚è½¬åŒ–æˆç±»åˆ«æ¦‚ç‡åˆ†å¸ƒã€‚ å¦ä¸€å¼ å›¾ä¹Ÿè¯´æ˜äº†è¯¥æµç¨‹ã€‚ å»ºè®®ä¸æŒ‡å¯¼è¶…å‚åŠå…¶å¯¹ç»“æœçš„å½±å“æ¥ä¸‹æ¥çš„å†…å®¹å‚è€ƒäº†è®ºæ–‡A Sensitivity Analysis of (and Practitionersâ€™ Guide to) ConvolutionalNeural Networks for Sentence Classification CNNæ–‡æœ¬åˆ†ç±»çš„è¶…å‚ï¼š è¾“å…¥å‘é‡ å·ç§¯å¤§å° è¾“å‡ºé€šé“ï¼ˆfeature mapsï¼‰ æ¿€æ´»å‡½æ•° æ± åŒ–ç­–ç•¥ æ­£åˆ™åŒ– è¾“å…¥å‘é‡çš„å½±å“å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨word2vecå’ŒGloVeä¸åˆ†ä¼¯ä»²ï¼Œä½†å°†word2vecå’ŒGloVeç®€å•æ‹¼æ¥åœ¨ä¸€èµ·å¹¶ä¸èƒ½å¸¦æ¥æå‡ã€‚ unfortunately, simply concatenating these representations does necessarily seem helpful å½“å¥å­é•¿åº¦å¾ˆé•¿ï¼ˆdocument classificationï¼‰æ—¶ï¼Œä½¿ç”¨one-hotå¯èƒ½ä¼šæœ‰æ•ˆæœï¼Œä½†åœ¨å¥å­é•¿åº¦ä¸æ˜¯å¾ˆé•¿æ—¶ï¼Œæ•ˆæœä¸å¥½ã€‚ å»ºè®®å¯¹äºæ–°ä»»åŠ¡ï¼Œå¯ä»¥word2vecæˆ–GloVeæˆ–è€…å…¶ä»–è¯å‘é‡éƒ½è¯•ä¸€ä¸‹ï¼Œå¦‚æœå¥å­é•¿ï¼Œå¯ä»¥è¯•ç€ä½¿ç”¨one-hotã€‚ å·ç§¯å¤§å°ç”±äºå·ç§¯çš„é•¿åº¦æ˜¯å›ºå®šçš„ï¼Œä¹Ÿå°±æ˜¯è¯å‘é‡çš„é•¿åº¦ï¼Œå› æ­¤åªéœ€è®¨è®ºå®½åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œä¸åŒçš„æ•°æ®é›†ä¼šæœ‰ä¸åŒçš„æœ€ä½³å¤§å°ï¼Œä½†ä¼¼ä¹å¯¹äºé•¿åº¦è¶Šé•¿çš„å¥å­ï¼Œæœ€ä½³å¤§å°æœ‰è¶Šå¤§çš„è¶‹åŠ¿ã€‚ However, for datasets comprising longer sentences, such as CR (maximum sentence length is 105, whereas it ranges from 36-56 on the other sentiment datasets used here), the optimal region size may be larger. åŒæ—¶ï¼Œå½“å¢åŠ ä¸åŒå·ç§¯å¤§å°ä½œä¸ºç»„åˆæ—¶ï¼Œå¦‚æœç»„åˆçš„å·ç§¯æ ¸å¤§å°æ¥è¿‘äºæœ€ä½³å¤§å°ï¼ˆoptimal region sizeï¼‰ï¼Œæœ‰åŠ©äºç»“æœçš„æå‡ï¼›ç›¸åï¼Œå¦‚æœå·ç§¯æ ¸å¤§å°ç¦»æœ€ä½³å¤§å°å¾ˆè¿œæ—¶ï¼Œåè€Œä¼šäº§ç”Ÿè´Ÿé¢å½±å“ã€‚ å»ºè®®é¦–å…ˆè¯•ç€æ‰¾åˆ°æœ€ä¼˜çš„å·ç§¯æ ¸å¤§å°ï¼Œç„¶ååœ¨è¿™ä¸ªåŸºç¡€ä¸Šæ·»åŠ å’Œè¯¥å·ç§¯æ ¸å¤§å°ç±»ä¼¼çš„å·ç§¯æ ¸ã€‚ feature mapsä¹Ÿå°±æ˜¯è¾“å‡ºé€šé“ï¼ˆout channelï¼‰ï¼Œè¡¨æ˜è¯¥å·ç§¯æ ¸å¤§å°çš„å·ç§¯æ ¸æœ‰å¤šå°‘ä¸ªã€‚ å®éªŒè¡¨æ˜ï¼Œæœ€ä½³çš„feature mapså’Œæ•°æ®é›†ç›¸å…³ï¼Œä½†ä¸€èˆ¬ä¸è¶…è¿‡600ã€‚ it would seem that increasing the number of maps beyond 600 yields at best very marginal returns, and often hurts performance. å»ºè®®åœ¨600å†…æœç´¢æœ€ä¼˜ï¼Œå¦‚æœåœ¨600çš„è¾¹ç¼˜è¿˜æ²¡æœ‰æ˜æ˜¾çš„æ•ˆæœä¸‹é™ï¼Œé‚£ä¹ˆå¯ä»¥å°è¯•å¤§äº600çš„feature mapsã€‚ æ¿€æ´»å‡½æ•°å®éªŒç»“æœï¼š ç»“æœè¡¨æ˜ï¼Œtanhã€ReLUå’Œä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°æ•ˆæœè¾ƒå¥½ã€‚tanhçš„ä¼˜ç‚¹æ˜¯ä»¥0ä¸ºä¸­å¿ƒï¼ŒReLUèƒ½å¤ŸåŠ é€Ÿæ‹Ÿåˆï¼Œè‡³äºä¸ºä»€ä¹ˆä¸ä½¿ç”¨çš„æ•ˆæœä¼šå¥½ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ¨¡å‹è¾ƒä¸ºç®€å•ï¼š This indicates that on some datasets, a linear transformation is enough to capture thecorrelation between the word embedding and the output label. å»ºè®®ä½¿ç”¨tanhã€ReLUæˆ–è€…å¹²è„†ä¸ä½¿ç”¨ã€‚ä½†å¦‚æœæ¨¡å‹æ›´ä¸ºå¤æ‚ï¼Œæœ‰å¤šå±‚çš„ç»“æ„ï¼Œè¿˜æ˜¯éœ€è¦ä½¿ç”¨æ¿€æ´»å‡½æ•°çš„ã€‚ poolingç­–ç•¥æ‰€æœ‰çš„å®éªŒéƒ½è¡¨æ˜äº†ï¼Œ1-max poolingçš„æ•ˆæœæ¯”å…¶ä»–å¥½ï¼Œå¦‚k-max poolingã€‚åœ¨poolingè¿™ä¸€æ­¥å¯ä»¥ç›´æ¥é€‰æ‹©1-max poolingã€‚ This may be because the location of predictive contexts does not matter, and certain n-grams in the sentence can be more predictive on their own than the entire sentence considered jointly. æ­£åˆ™åŒ–ä¸»è¦æ˜¯dropoutå’Œl2 norm constraintã€‚dropoutå°±æ˜¯éšæœºå°†ä¸€äº›ç¥ç»å…ƒç½®ä¸º0ï¼Œl2 norm constraintæ˜¯å¯¹å‚æ•°çŸ©é˜µWè¿›è¡Œæ•´ä½“ç¼©æ”¾ï¼Œä½¿å…¶ä¸è¶…è¿‡ä¸€å®šé˜ˆå€¼ã€‚ï¼ˆä¸é€šå¸¸çš„l2 regularizationä¸åŒï¼Œæœ€æ—©å¯è¿½æº¯åˆ°Hintonçš„Improving neural networks by preventingco-adaptation of feature detectorsï¼‰ the l2 norm of a weight vector is linearly scaled to a constraint c when it exceeds this threshold, so a smaller c implies stronger regularization å®éªŒè¡¨æ˜ï¼Œdropoutèµ·çš„ä½œç”¨å¾ˆå°ï¼Œl2 normæ²¡æœ‰æå‡ç”šè‡³è¿˜ä¼šå¯¼è‡´ä¸‹é™ã€‚å¯èƒ½æ˜¯å› ä¸ºæ¨¡å‹å‚æ•°ä¸å¤šï¼Œå› æ­¤è¿‡æ‹Ÿåˆçš„å¯èƒ½æ€§è¾ƒä½ã€‚ å»ºè®®è®¾ç½®è¾ƒå°çš„dropoutå’Œè¾ƒå¤§çš„l2 normï¼Œå½“feature mapså¢å¤§æ—¶ï¼Œå¯ä»¥è¯•ç€è°ƒèŠ‚è¾ƒå¤§çš„dropoutä»¥é¿å…è¿‡æ‹Ÿåˆã€‚ å»ºè®®åŠç»“è®º åˆšå¼€å§‹çš„ä½¿ç”¨ä½¿ç”¨word2vecæˆ–è€…GloVeï¼Œå¦‚æœæ•°æ®é‡å¤Ÿå¤§ï¼Œå¯ä»¥å°è¯•one-hot çº¿æ€§æœç´¢æœ€ä½³çš„å·ç§¯æ ¸å¤§å°ï¼Œå¦‚æœå¥å­å¤Ÿé•¿ï¼Œé‚£ä¹ˆå¯ä»¥æ‰©å¤§æœç´¢èŒƒå›´ã€‚ä¸€æ—¦ç¡®å®šäº†æœ€ä½³å·ç§¯æ ¸å¤§å°ï¼Œå°è¯•åœ¨è¯¥å·ç§¯æ ¸å¤§å°çš„é™„è¿‘è¿›è¡Œç»„åˆï¼Œå¦‚æœ€ä½³å·ç§¯æ ¸å®½åº¦æ˜¯5ï¼Œé‚£ä¹ˆå°è¯•[3,4,5]æˆ–è€…[2,3,4,5]ç­‰ ä½¿ç”¨è¾ƒå°çš„dropoutå’Œè¾ƒå¤§çš„max norm constraintï¼Œç„¶ååœ¨[100,600]èŒƒå›´å†…æœç´¢feature mapsï¼Œå¦‚æœæœ€ä½³çš„feature mapsåœ¨600é™„è¿‘ï¼Œå¯ä»¥è¯•ç€é€‰æ‹©æ¯”600æ›´å¤§çš„èŒƒå›´ å°è¯•ä¸åŒçš„æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸tanhå’ŒReLUæ˜¯è¾ƒå¥½çš„ï¼Œä½†ä¹Ÿå¯ä»¥å°è¯•ä»€ä¹ˆéƒ½ä¸åŠ ã€‚ ä½¿ç”¨1-max poolingã€‚ å¦‚æœæ¨¡å‹å¤æ‚ï¼Œæ¯”å¦‚feature mapså¾ˆå¤§ï¼Œé‚£ä¹ˆå¯ä»¥å°è¯•æ›´ä¸ºä¸¥æ ¼çš„æ­£åˆ™åŒ–ï¼Œå¦‚æ›´å¤§çš„dropout rateå’Œè¾ƒå°çš„max norm constraintã€‚ ReferenceConvolutional Neural Networks for Sentence Classification A Sensitivity Analysis of (and Practitionersâ€™ Guide to) ConvolutionalNeural Networks for Sentence Classification]]></content>
      <tags>
        <tag>CNN</tag>
        <tag>æƒ…æ„Ÿåˆ†æ</tag>
        <tag>æŒ‡å—</tag>
        <tag>è°ƒå‚</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³äºPytorchä¸­çš„inplaceçš„æ“ä½œ]]></title>
    <url>%2F2018%2F08%2F20%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADin-place%E7%9A%84%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[æœ€è¿‘åœ¨å†™Hierarchical attention networkçš„æ—¶å€™é‡åˆ°äº†å¦‚ä¸‹çš„bugï¼š one of the variables needed for gradient computation has been modified by an inplace operation åœ¨æŸ¥é˜…äº†æ–‡æ¡£å’Œè¯·æ•™äº†å…¶ä»–äººä¹‹åï¼Œæœ€ç»ˆæ‰¾åˆ°äº†bugã€‚ 1234for i in range(seq_len): h_i = rnn_outputs[i] # batch,hidden*2 a_i = attn_weights[i].unsqueeze_(1) # take in-place opt may cause an error a_i = a_i.expand_as(h_i) # batch,hidden*2 è¿™æ˜¯æˆ‘åŸæ¥çš„é€»è¾‘ï¼Œæˆ‘åœ¨æ— æ„ä¸­åšäº†inplaceæ“ä½œï¼Œå¯¼è‡´äº†bugçš„å‘ç”Ÿã€‚æ­£ç¡®çš„åšæ³•åº”è¯¥æ˜¯è¿™æ ·çš„ï¼š 12345for i in range(seq_len): h_i = rnn_outputs[i] # batch,hidden*2 # a_i = attn_weights[i].unsqueeze_(1) # take in-place opt may cause an error a_i = attn_weights[i].unsqueeze(1) # batch,1 a_i = a_i.expand_as(h_i) # batch,hidden*2 å®é™…ä¸Šï¼Œåœ¨å®è·µè¿‡ç¨‹ä¸­åº”å½“å°½é‡é¿å…inplaceæ“ä½œï¼Œåœ¨å®˜æ–¹æ–‡æ¡£ä¸­ä¹Ÿæåˆ°äº†ï¼ˆå­˜ç–‘ï¼‰è¿™ç‚¹ï¼Œè™½ç„¶æä¾›äº†inplaceæ“ä½œï¼Œä½†å¹¶ä¸æ¨èä½¿ç”¨ã€‚ å…·ä½“çš„åŸå› æ˜¯ï¼Œåœ¨Pytorchæ„å»ºè®¡ç®—å›¾çš„è¿‡ç¨‹ä¸­ï¼Œä¼šè®°å½•æ¯ä¸ªèŠ‚ç‚¹æ˜¯æ€ä¹ˆæ¥çš„ï¼Œä½†inplaceä¼šç ´åè¿™ç§å…³ç³»ï¼Œä½¿å¾—åœ¨å›ä¼ çš„æ—¶å€™æ²¡æ³•æ­£å¸¸æ±‚å¯¼ã€‚ ç‰¹åˆ«åœ°ï¼Œæœ‰ä¸¤ç§æƒ…å†µä¸åº”è¯¥ä½¿ç”¨inplaceæ“ä½œï¼ˆæ‘˜è‡ªçŸ¥ä¹ï¼‰ï¼š å¯¹äºrequires_grad=Trueçš„å¶å­å¼ é‡(leaf tensor)ä¸èƒ½ä½¿ç”¨inplace operation å¯¹äºåœ¨æ±‚æ¢¯åº¦é˜¶æ®µéœ€è¦ç”¨åˆ°çš„å¼ é‡ä¸èƒ½ä½¿ç”¨inplace operation Reference:https://zhuanlan.zhihu.com/p/38475183]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>é‡åˆ°çš„é—®é¢˜</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ„¿ä¸­å›½é’å¹´éƒ½æ‘†è„±å†·æ°”]]></title>
    <url>%2F2018%2F08%2F19%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%84%BF%E4%B8%AD%E5%9B%BD%E9%9D%92%E5%B9%B4%E9%83%BD%E6%91%86%E8%84%B1%E5%86%B7%E6%B0%94%2F</url>
    <content type="text"><![CDATA[è¿‘æœŸçš„æ–°é—»å¸¸è®©äººæ„Ÿåˆ°æ„¤æ€’ä»¥è‡´ç»æœ›â€¦ æ„¿ä¸­å›½é’å¹´éƒ½æ‘†è„±å†·æ°”ï¼Œåªæ˜¯å‘ä¸Šèµ°ï¼Œä¸å¿…å¬è‡ªæš´è‡ªå¼ƒè€…æµçš„è¯ã€‚èƒ½åšäº‹çš„åšäº‹ï¼Œèƒ½å‘å£°çš„å‘å£°ã€‚æœ‰ä¸€åˆ†çƒ­ï¼Œå‘ä¸€åˆ†å…‰ã€‚å°±ä»¤è¤ç«ä¸€èˆ¬ï¼Œä¹Ÿå¯ä»¥åœ¨é»‘æš—é‡Œå‘ä¸€ç‚¹å…‰ï¼Œä¸å¿…ç­‰å€™ç‚¬ç«ã€‚ â€”é²è¿…ã€Šçƒ­é£ã€‹]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•5]]></title>
    <url>%2F2018%2F08%2F19%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB5%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£sklearnæ¨¡å‹çš„ä¿å­˜ä¸æ¢å¤123456789from sklearn import svmX = [[0, 0], [1, 1]]y = [0, 1]clf = svm.SVC()clf.fit(X, y) clf.fit(train_X,train_y)joblib.dump(clf, "train_model.m")clf = joblib.load("train_model.m")clf.predit(test_X) 2ï¸âƒ£Dictionaryç±»åœ¨æ„é€ å­—å…¸æ—¶éœ€è¦ç”¨åˆ°1234567891011121314151617181920212223242526class Dictionary(): def __init__(self): self.word2idx = &#123;&#125; self.idx2word = [] self.__vocab_size = 0 self.add_word('&lt;pad&gt;') self.add_word('&lt;UNK&gt;') def add_word(self, word): if word not in self.word2idx: self.idx2word.append(word) self.word2idx[word] = self.__vocab_size self.__vocab_size += 1 def __len__(self): return self.__vocab_size def get_index(self, word): if word in self.word2idx: return self.word2idx[word] else: return self.word2idx['&lt;UNK&gt;'] def get_word(self, idx): return self.idx2word[idx] 3ï¸âƒ£å¯¹dictæŒ‰å…ƒç´ æ’åºçš„ä¸‰ç§æ–¹æ³•12345678910111213d=&#123;'apple':10,'orange':20,'banana':5,'watermelon':1&#125;#æ³•1print(sorted(d.items(),key=lambda x:x[1])) #[('watermelon', 1), ('banana', 5), ('apple', 10), ('orange', 20)]#æ³•2from operator import itemgetterprint(sorted(d.items(),key=itemgetter(1))) #[('watermelon', 1), ('banana', 5), ('apple', 10), ('orange', 20)]#æ³•3print(sorted(d,key=d.get)) #['watermelon', 'banana', 'apple', 'orange'] æ²¡æœ‰valueäº† 4ï¸âƒ£åˆå¹¶dictçš„ä¸‰ç§æ–¹æ³•1234567891011121314151617&gt;&gt;&gt; d1=&#123;'a':1&#125;&gt;&gt;&gt; d2=&#123;'b':2&#125;#æ³•1&gt;&gt;&gt; d=&#123;**d1,**d2&#125;&gt;&gt;&gt; d&#123;'a': 1, 'b': 2&#125;#æ³•2&gt;&gt;&gt; dd=dict(d1.items()|d2.items())&gt;&gt;&gt; dd&#123;'a': 1, 'b': 2&#125;#æ³•3&gt;&gt;&gt; d1.update(d2)&gt;&gt;&gt; d1&#123;'a': 1, 'b': 2&#125; 5ï¸âƒ£æ‰¾åˆ°listæœ€å¤§æœ€å°å€¼çš„index12345678lst = [40, 10, 20, 30]def minIndex(lst): return min(range(len(lst)),key=lst.__getitem__)def maxIndex(lst): return max(range(len(lst)),key=lst.__getitem__) print(minIndex(lst))print(maxIndex(lst))]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³äºPytorchä¸­çš„Embedding padding]]></title>
    <url>%2F2018%2F08%2F19%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADEmbedding%E7%9A%84padding%2F</url>
    <content type="text"><![CDATA[åœ¨Pytorchä¸­ï¼Œnn.Embedding()ä»£è¡¨embeddingçŸ©é˜µï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªå‚æ•°padding_idxæŒ‡å®šç”¨ä»¥paddingçš„ç´¢å¼•ä½ç½®ã€‚æ‰€è°“paddingï¼Œå°±æ˜¯åœ¨å°†ä¸ç­‰é•¿çš„å¥å­ç»„æˆä¸€ä¸ªbatchæ—¶ï¼Œå¯¹é‚£äº›ç©ºç¼ºçš„ä½ç½®è¡¥0ï¼Œä»¥å½¢æˆä¸€ä¸ªç»Ÿä¸€çš„çŸ©é˜µã€‚ ç”¨æ³•ï¼š1self.embedding = nn.Embedding(vocab_size, embed_dim,padding_idx=0) #ä¹Ÿå¯ä»¥æ˜¯åˆ«çš„æ•°å€¼ åœ¨æ˜¾å¼è®¾å®špadding_idx=0åï¼Œåœ¨è‡ªå®šä¹‰çš„è¯å…¸å†…ä¹Ÿåº”å½“åœ¨ç›¸åº”ä½ç½®æ·»åŠ &lt;pad&gt;ä½œä¸ºä¸€ä¸ªè¯ã€‚å¦‚ï¼š 1234567class Dictionary(): def __init__(self): self.word2idx = &#123;&#125; self.idx2word = [] self.__vocab_size = 0 self.add_word('&lt;pad&gt;') # should add &lt;pad&gt; first self.add_word('&lt;UNK&gt;') é‚£ä¹ˆå¯¹äºpadding_idxï¼Œå†…éƒ¨æ˜¯å¦‚ä½•æ“ä½œçš„å‘¢ï¼Ÿ åœ¨æŸ¥çœ‹äº†Embeddingçš„æºç åï¼Œå‘ç°è®¾ç½®äº†padding_idxï¼Œç±»å†…éƒ¨ä¼šæœ‰å¦‚ä¸‹æ“ä½œï¼š 12345678910#-----Embedding __init__ å†…éƒ¨--------------if _weight is None: self.weight = Parameter(torch.Tensor(num_embeddings, embedding_dim)) self.reset_parameters() #---------reset_parameters()--------def reset_parameters(self): self.weight.data.normal_(0, 1) if self.padding_idx is not None: self.weight.data[self.padding_idx].fill_(0) ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“Embeddingæ˜¯éšæœºåˆå§‹åŒ–çš„çŸ©é˜µæ—¶ï¼Œä¼šå¯¹padding_idxæ‰€åœ¨çš„è¡Œè¿›è¡Œå¡«0ã€‚ä¿è¯äº†paddingè¡Œä¸ºçš„æ­£ç¡®æ€§ã€‚ é‚£ä¹ˆï¼Œè¿˜éœ€è¦ä¿è¯ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯åœ¨åå‘å›ä¼ çš„æ—¶å€™ï¼Œpadding_idxæ˜¯ä¸ä¼šæ›´æ–°çš„. åœ¨æŸ¥çœ‹äº†æºç åå‘ç°åœ¨Embeddingç±»å†…æœ‰å¦‚ä¸‹æ³¨é‡Šï¼š .. note:: With :attr:padding_idx set, the embedding vector at :attr:padding_idx is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from :class:~torch.nn.Embedding is always zero. å¹¶ä¸”åœ¨æŸ¥é˜…äº†å…¶ä»–èµ„æ–™åï¼Œå‘ç°è¯¥è¡Œç¡®å®ä¼šä¸æ›´æ–°ã€‚æœ‰æ„æ€çš„æ˜¯ï¼ŒæŸ¥é˜…æºç å¹¶æ²¡æœ‰æ‰¾åˆ°å¦‚ä½•ä½¿å…¶ä¸æ›´æ–°çš„æœºåˆ¶ï¼Œå› ä¸ºåœ¨F.embeddingå‡½æ•°ä¸­ï¼Œè¿”å›ï¼š 1return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) ä½†æˆ‘å¹¶ä¸èƒ½è·³è½¬åˆ°torch.embeddingä¸­ï¼Œå¤§æ¦‚æ˜¯å› ä¸ºè¿™éƒ¨åˆ†è¢«éšè—äº†å§ã€‚æˆ‘ä¹Ÿæ²¡æœ‰å†æ·±ç©¶ä¸‹å»ã€‚æˆ‘çŒœæµ‹æœ‰å¯èƒ½æ˜¯åœ¨autogradå†…éƒ¨æœ‰å¯¹è¯¥éƒ¨åˆ†è¿›è¡Œå•ç‹¬çš„å¤„ç†ï¼Œç”¨maskå±è”½è¿™éƒ¨åˆ†çš„æ›´æ–°ï¼›æˆ–è€…ä¸€ä¸ªæ›´ç®€å•çš„æ–¹æ³•ï¼Œå°±æ˜¯ä»»å…¶æ›´æ–°ï¼Œä½†æ¯ä¸€æ¬¡éƒ½resetï¼Œå°†ç¬¬ä¸€è¡Œæ‰‹åŠ¨è®¾ä¸ºå…¨0ã€‚ é™„è®°ï¼š å‡å¦‚è¯´æ²¡æœ‰æ˜¾å¼è®¾ç½®è¯¥è¡Œï¼Œæ˜¯å¦paddingå°±æ²¡æœ‰æ•ˆæœå‘¢ï¼Ÿæˆ‘è®¤ä¸ºæ˜¯çš„ã€‚ ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬éƒ½æ˜¯ä»¥0ä½œä¸ºpaddingçš„å¡«å……ï¼Œå¦‚ï¼š 12 44 22 67 85 12 13 534 31 0 87 23 0 0 0 æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªå¥å­ï¼Œå…¶ä¸­0ä½œä¸ºå¡«å……ã€‚ç„¶åå°†è¯¥çŸ©é˜µé€å…¥åˆ°embedding_lookupä¸­ï¼Œè·å¾—ä¸‰ç»´çš„tensorï¼Œé‚£ä¹ˆ0å¡«å……çš„éƒ¨åˆ†ï¼Œæ‰€è·å¾—çš„embeddingè¡¨ç¤ºåº”å½“æ˜¯è¦å…¨0ã€‚ å‡å¦‚ä¸æ˜¾å¼è®¾ç½®padding_idx=0ï¼Œå°±å¯èƒ½ä¼šå‡ºç°ä¸¤ä¸ªç»“æœï¼ˆä¸ªäººæ¨æµ‹)ï¼š â‘ æœ¬åº”è¯¥å…¨0çš„åœ°æ–¹ï¼Œè¢«è¯å…¸ä¸­ç¬¬ä¸€ä¸ªè¯çš„è¯å‘é‡è¡¨ç¤ºç»™æ›¿ä»£äº†ï¼Œå› ä¸ºå°†0ä½œä¸ºç´¢å¼•å»embeddingçŸ©é˜µè·å–åˆ°çš„è¯å‘é‡ï¼Œå°±æ˜¯ç¬¬ä¸€ä¸ªè¯çš„è¯å‘é‡ï¼Œè€Œè¯¥è¯å¹¶ä¸å…¨0ã€‚ â‘¡è¯å…¸çš„æœ€åä¸€ä¸ªè¯è¢«å…¨0è¦†ç›–ã€‚F.embeddingä¸­æœ‰å¦‚ä¸‹ç‰‡æ®µï¼š 12345678if padding_idx is not None: if padding_idx &gt; 0: assert padding_idx &lt; weight.size(0), 'Padding_idx must be within num_embeddings' elif padding_idx &lt; 0: assert padding_idx &gt;= -weight.size(0), 'Padding_idx must be within num_embeddings' padding_idx = weight.size(0) + padding_idxelif padding_idx is None: padding_idx = -1 ä¸Šé¢ç‰‡æ®µæ˜¾ç¤ºï¼Œpadding_idxè¢«è®¾ç½®ä¸º-1ï¼Œä¹Ÿå°±æ˜¯æœ€åä¸€ä¸ªå•è¯ã€‚åšå®Œè¿™æ­¥ç´§æ¥ç€å°±è¿”å›ï¼š 1return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) è¿˜æ˜¯ç”±äºtorch.embeddingæ— æ³•æŸ¥çœ‹çš„åŸå› ï¼Œæˆ‘ä¸çŸ¥é“å†…éƒ¨æ˜¯å¦‚ä½•å®ç°çš„ï¼Œä½†åº”è¯¥æ¥è¯´ï¼Œæœ€åä¸€ä¸ªè¯å°±æ˜¯è¢«è¦†ç›–äº†ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>Embedding</tag>
        <tag>padding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Tricks[è½¬]]]></title>
    <url>%2F2018%2F08%2F19%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPython%20Tricks%5B%E8%BD%AC%5D%2F</url>
    <content type="text"><![CDATA[åŸæ–‡åœ°å€:https://hackernoon.com/python-tricks-101-2836251922e0 æˆ‘è§‰å¾—è¿™ä¸ªä»‹ç»Pythonä¸€äº›tricksçš„æ–‡ç« å¾ˆå¥½ï¼Œèƒ½å¤Ÿæ›´åŠ ç†Ÿæ‚‰Pythonçš„ä¸€äº›éå¸¸æ–¹ä¾¿çš„ç”¨æ³•ã€‚ä»¥ä¸‹æ˜¯æˆ‘è§‰å¾—æœ‰ç”¨çš„å‡ ä¸ªç‚¹ã€‚ 1ï¸âƒ£Reverse a String/List [::-1]è§£é‡Šï¼š[:]è¡¨ç¤ºå–æ‰€æœ‰çš„å…ƒç´ ï¼Œ-1è¡¨ç¤ºæ­¥è¿›ã€‚[1:5:2]è¡¨ç¤ºçš„å°±æ˜¯ä»å…ƒç´ 1åˆ°å…ƒç´ 5ï¼Œæ¯2ä¸ªè·ç¦»å–ä¸€ä¸ªã€‚ 2ï¸âƒ£transpose 2d array zip()ç›¸å½“äºå‹ç¼©ï¼Œzip(*)ç›¸å½“äºè§£å‹ã€‚ 3ï¸âƒ£Chained function call éå¸¸ç®€æ´çš„å†™æ³•ã€‚ 4ï¸âƒ£Copy List ä¹‹å‰è°ˆè¿‡çš„Pythonçš„èµ‹å€¼ã€æµ…æ‹·è´ã€æ·±æ‹·è´ã€‚ 5ï¸âƒ£Dictionary get é¿å…äº†dictä¸å­˜åœ¨è¯¥å…ƒç´ çš„é—®é¢˜ã€‚ 6ï¸âƒ£âœ¨Sort Dictionary by Value å…¶ä¸­ç¬¬ä¸‰ç§è¿”å›çš„æ˜¯[â€˜watermelonâ€™, â€˜bananaâ€™, â€˜appleâ€™, â€˜orangeâ€™]ï¼Œæ²¡æœ‰valueäº†ã€‚ 7ï¸âƒ£Forâ€¦else æ³¨æ„åˆ°å¦‚æœforåœ¨ä¸­é€”breakäº†ï¼Œå°±ä¸ä¼šè¿›å…¥åˆ°elseäº†ï¼›åªæœ‰é¡ºåˆ©å¾ªç¯å®Œæ‰ä¼šè¿›å…¥åˆ°elseã€‚ 1234567891011121314151617&gt;&gt;&gt; a=[1,2,0]&gt;&gt;&gt; for e in a:... if e==0:... break... else:... print('hello')... #ä»€ä¹ˆéƒ½æ²¡æœ‰print&gt;&gt;&gt; for e in a:... print(e)... else:... print('hello')... 120hello 8ï¸âƒ£Merge dictâ€™s åˆå¹¶dictçš„æ–¹æ³•ã€‚ 9ï¸âƒ£Min and Max index in List]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Python</tag>
        <tag>Python tricks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†4]]></title>
    <url>%2F2018%2F08%2F19%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%864%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[æ¦‚ç‡æ ¡å‡†(Probability Calibration)]ä¸€ç§å¯¹æœºå™¨å­¦ä¹ ç®—æ³•è¾“å‡ºç»“æœçš„æ ¡å‡†ï¼Œé€šè¿‡å‡ ä¸ªå®éªŒå¯ä»¥å‘ç°ï¼Œæ¦‚ç‡æ ¡å‡†èƒ½å¤Ÿä¸€å®šç¨‹åº¦æé«˜è¡¨ç°ã€‚å‡ ä¸ªå‚è€ƒèµ„æ–™ï¼šç›´è§‚ç†è§£: http://www.bubuko.com/infodetail-2133893.htmlSVCçš„æ¦‚ç‡æ ¡å‡†åœ¨sklearnä¸Šçš„åº”ç”¨: https://blog.csdn.net/ericcchen/article/details/79337716âœ¨å®Œå…¨æ‰‹å†Œ: Calibration of Machine Learning Models 2ï¸âƒ£[Paper]Hierarchical Attention Networks for Document Classification äº®ç‚¹åœ¨ä½¿ç”¨å±‚æ¬¡çš„RNNç»“æ„ï¼Œä»¥åŠä½¿ç”¨äº†attentionæ–¹æ³•ã€‚ å‚è€ƒäº†å…¶ä»–äººçš„ä»£ç è‡ªå·±ä¹Ÿè¯•ç€å®ç°äº†ä¸€ä¸ªï¼ŒGitHubåœ°å€ï¼šhttps://github.com/linzehui/pytorch-hierarchical-attention-network 3ï¸âƒ£[XGBoost]kaggleç¥å™¨XGBoostï¼Œä¸€ç¯‡åŸç†çš„è¯¦ç»†ä»‹ç»ï¼šhttp://www.cnblogs.com/willnote/p/6801496.htmlè™½ç„¶è¿˜æ˜¯æœ‰å¥½äº›åœ°æ–¹æ²¡ææ‡‚ï¼Œæœ‰å¿…è¦ä»å¤´å­¦èµ·ã€‚ 4ï¸âƒ£[Python]å…³äºå‡½æ•°åˆ—è¡¨ä¸­å•æ˜Ÿå·(*)å’ŒåŒæ˜Ÿå·(**)å•æ˜Ÿå·ï¼š ä»£è¡¨æ¥æ”¶ä»»æ„å¤šä¸ªéå…³é”®å­—å‚æ•°ï¼Œå°†å…¶è½¬æ¢æˆå…ƒç»„ï¼š 1234def one(a,*b): """aæ˜¯ä¸€ä¸ªæ™®é€šä¼ å…¥å‚æ•°ï¼Œ*bæ˜¯ä¸€ä¸ªéå…³é”®å­—æ˜Ÿå·å‚æ•°""" print(b)one(1,2,3,4,5,6) #è¾“å‡ºï¼š(2, 3, 4, 5, 6) å¯¹ä¸€ä¸ªæ™®é€šå˜é‡ä½¿ç”¨å•æ˜Ÿå·ï¼Œè¡¨ç¤ºå¯¹è¯¥å˜é‡æ‹†åˆ†æˆå•ä¸ªå…ƒç´  1234def fun(a,b): print(a,b)l=[1,2]fun(*l) #è¾“å‡º 1,2 åŒæ˜Ÿå·ï¼š è·å¾—å­—å…¸å€¼ 1234def two(a=1,**b): """aæ˜¯ä¸€ä¸ªæ™®é€šå…³é”®å­—å‚æ•°ï¼Œ**bæ˜¯ä¸€ä¸ªå…³é”®å­—åŒæ˜Ÿå·å‚æ•°""" print(b)two(a=1,b=2,c=3,d=4,e=5,f=6) #è¾“å‡º&#123;'b': 2, 'c': 3, 'e': 5, 'f': 6, 'd': 4&#125; 5ï¸âƒ£[Pytorch]åœ¨Pytorchä¸­ï¼Œåªè¦ä¸€ä¸ªtensorçš„requires_gradæ˜¯trueï¼Œé‚£ä¹ˆä¸¤ä¸ªtensorçš„åŠ å‡ä¹˜é™¤åçš„ç»“æœçš„requires_gradä¹Ÿä¼šæ˜¯trueã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>Python</tag>
        <tag>Paper</tag>
        <tag>æ¦‚ç‡æ ¡å‡†</tag>
        <tag>Probability Calibration</tag>
        <tag>HAN</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯5]]></title>
    <url>%2F2018%2F08%2F19%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D5%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨å¤ªå¿™äº†ï¼Œæ²¡èƒŒä»€ä¹ˆè¯—è¯ï¼ŒåªèƒŒï¼ˆå¤ä¹ ï¼‰äº†éƒ¨åˆ†çš„ã€Šæ»•ç‹é˜åºã€‹ã€‚ 1ï¸âƒ£ æ»•ç‹é˜åºå—Ÿä¹ï¼æ—¶è¿ä¸é½ï¼Œå‘½é€”å¤šèˆ›ã€‚å†¯å”æ˜“è€ï¼Œæå¹¿éš¾å°ã€‚å±ˆè´¾è°Šäºé•¿æ²™ï¼Œéæ— åœ£ä¸»ï¼›çªœæ¢é¸¿äºæµ·æ›²ï¼Œå²‚ä¹æ˜æ—¶ï¼Ÿæ‰€èµ–å›å­è§æœºï¼Œè¾¾äººçŸ¥å‘½ã€‚è€å½“ç›Šå£®ï¼Œå®ç§»ç™½é¦–ä¹‹å¿ƒï¼Ÿç©·ä¸”ç›Šåšï¼Œä¸å é’äº‘ä¹‹å¿—ã€‚é…Œè´ªæ³‰è€Œè§‰çˆ½ï¼Œå¤„æ¶¸è¾™ä»¥çŠ¹æ¬¢ã€‚åŒ—æµ·è™½èµŠï¼Œæ‰¶æ‘‡å¯æ¥ï¼›ä¸œéš…å·²é€ï¼Œæ¡‘æ¦†éæ™šã€‚å­Ÿå°é«˜æ´ï¼Œç©ºé¦€æŠ¥å›½ä¹‹æƒ…ï¼›é˜®ç±çŒ–ç‹‚ï¼Œå²‚æ•ˆç©·é€”ä¹‹å“­ï¼ å‹ƒï¼Œä¸‰å°ºå¾®å‘½ï¼Œä¸€ä»‹ä¹¦ç”Ÿã€‚æ— è·¯è¯·ç¼¨ï¼Œç­‰ç»ˆå†›ä¹‹å¼±å† ï¼›æœ‰æ€€æŠ•ç¬”ï¼Œæ…•å®—æ…¤ä¹‹é•¿é£ã€‚èˆç°ªç¬äºç™¾é¾„ï¼Œå¥‰æ™¨æ˜äºä¸‡é‡Œã€‚éè°¢å®¶ä¹‹å®æ ‘ï¼Œæ¥å­Ÿæ°ä¹‹èŠ³é‚»ã€‚ä»–æ—¥è¶‹åº­ï¼Œå¨é™ªé²¤å¯¹ï¼›ä»Šå…¹æ§è¢‚ï¼Œå–œæ‰˜é¾™é—¨ã€‚æ¨æ„ä¸é€¢ï¼ŒæŠšå‡Œäº‘è€Œè‡ªæƒœï¼›é”ºæœŸæ—¢é‡ï¼Œå¥æµæ°´ä»¥ä½•æƒ­ï¼Ÿ æ³¨é‡Šï¼šå†¯å”ï¼šè¥¿æ±‰äººï¼Œæœ‰æ‰èƒ½å´ä¸€ç›´ä¸å—é‡ç”¨ã€‚æ±‰æ­¦å¸æ—¶é€‰æ±‚è´¤è‰¯ï¼Œæœ‰äººä¸¾èå†¯å”ï¼Œå¯æ˜¯ä»–å·²ä¹åå¤šå²ï¼Œéš¾å†åšå®˜äº†ã€‚æå¹¿ï¼šæ±‰æ­¦å¸æ—¶çš„åå°†ï¼Œå¤šå¹´æŠ—å‡»åŒˆå¥´ï¼Œå†›åŠŸå¾ˆå¤§ï¼Œå´ç»ˆèº«æ²¡æœ‰å°ä¾¯ã€‚ è´¾è°Šï¼šæ±‰æ–‡å¸æœ¬æƒ³ä»»è´¾è°Šä¸ºå…¬å¿ï¼Œä½†å› æœä¸­æƒè´µåå¯¹ï¼Œå°±ç–è¿œäº†è´¾è°Šï¼Œä»»ä»–ä¸ºé•¿æ²™ç‹å¤ªå‚…ã€‚æ¢é¸¿ï¼šä¸œæ±‰äººï¼Œå› ä½œè¯—è®½åˆºå›ç‹ï¼Œå¾—ç½ªäº†æ±‰ç« å¸ï¼Œè¢«è¿«é€ƒåˆ°é½é²ä¸€å¸¦èº²é¿ã€‚ é…Œï¼ˆzhuÃ³ï¼‰è´ªæ³‰è€Œè§‰çˆ½ï¼šå–ä¸‹è´ªæ³‰çš„æ°´ï¼Œä»è§‰å¾—å¿ƒå¢ƒæ¸…çˆ½ã€‚å¤ä»£ä¼ è¯´å¹¿å·æœ‰æ°´åè´ªæ³‰ï¼Œäººå–äº†è¿™é‡Œçš„æ°´å°±ä¼šå˜å¾—è´ªå©ªã€‚è¿™å¥æ˜¯è¯´æœ‰å¾·è¡Œçš„äººåœ¨æ±¡æµŠçš„ç¯å¢ƒä¸­ä¹Ÿèƒ½ä¿æŒçº¯æ­£ï¼Œä¸è¢«æ±¡æŸ“ã€‚å¤„æ¶¸è¾™ä»¥çŠ¹æ¬¢ï¼šå¤„åœ¨å¥„å¥„å¾…æ¯™çš„æ—¶å€™ï¼Œä»ç„¶ä¹è§‚å¼€æœ—ã€‚å¤„æ²³è¾™ï¼šåŸæŒ‡é²‹é±¼å¤„åœ¨å¹²æ¶¸çš„è½¦è¾™æ—¦ã€‚æ¯”å–»äººé™·å…¥å±æ€¥ä¹‹ä¸­ã€‚ å­Ÿå°ï¼šä¸œæ±‰äººï¼Œä¸ºå®˜æ¸…æ­£è´¤èƒ½ï¼Œä½†ä¸è¢«é‡ç”¨ï¼Œåæ¥å½’ç”°ã€‚é˜®ç±ï¼šä¸‰å›½é­è¯—äººï¼Œä»–æœ‰æ—¶ç‹¬è‡ªé©¾è½¦å‡ºè¡Œï¼Œåˆ°æ— è·¯å¤„ä¾¿æ¸å“­è€Œè¿”ï¼Œå€Ÿæ­¤å®£æ³„ä¸æ»¡äºç°å®çš„è‹¦é—·å¿ƒæƒ…ã€‚ ç»ˆå†›ï¼šã€Šæ±‰ä¹¦Â·ç»ˆå†›ä¼ ã€‹è®°è½½ï¼Œæ±‰æ­¦å¸æƒ³è®©å—è¶Šï¼ˆä»Šå¹¿ä¸œã€å¹¿è¥¿ä¸€å¸¦ï¼‰ç‹å½’é¡ºï¼Œæ´¾ç»ˆå†›å‰å¾€åŠè¯´ï¼Œç»ˆå†›è¯·æ±‚ç»™ä»–é•¿ç¼¨ï¼Œå¿…ç¼šä½å—è¶Šç‹ï¼Œå¸¦å›åˆ°çš‡å®«é—¨å‰ï¼ˆæ„æ€æ˜¯ä¸€å®šå®Œæˆä½¿å‘½ï¼‰ã€‚åæ¥ç”¨â€œè¯·ç¼¨â€æŒ‡æŠ•å†›æŠ¥å›½ã€‚ å®—æ‚«ï¼ˆquÃ¨ï¼‰ï¼šå—æœå®‹äººï¼Œå°‘å¹´æ—¶å¾ˆæœ‰æŠ±è´Ÿï¼Œè¯´â€œæ„¿ä¹˜é•¿é£ç ´ä¸‡é‡Œæµªâ€ã€‚ ç°ªï¼ˆzÄnï¼‰ç¬ï¼ˆhÃ¹ï¼‰ï¼šè¿™é‡Œä»£æŒ‡å®˜èŒã€‚æ™¨æ˜ï¼šæ™¨æ˜å®šçœï¼Œå‡ºè‡ª ã€Šç¤¼è®°Â·æ›²ç¤¼ä¸Šã€‹ï¼Œé‡Šä¹‰ä¸ºæ—§æ—¶ä¾å¥‰çˆ¶æ¯çš„æ—¥å¸¸ç¤¼èŠ‚ã€‚ éè°¢å®¶ä¹‹å®æ ‘ï¼Œæ¥å­Ÿæ°ä¹‹èŠ³é‚»ï¼šè‡ªå·±å¹¶ä¸æ˜¯åƒè°¢ç„é‚£æ ·å‡ºè‰²çš„äººæ‰ï¼Œå´èƒ½åœ¨ä»Šæ—¥çš„å®´ä¼šä¸Šç»“è¯†å„ä½åå£«ã€‚è°¢å®¶ä¹‹å®æ ‘ï¼šæŒ‡è°¢ç„ã€‚ã€Šæ™‹ä¹¦Â·è°¢ç„ä¼ ã€‹è®°è½½ï¼Œæ™‹æœè°¢å®‰æ›¾é—®å­ä¾„ä»¬ï¼šä¸ºä»€ä¹ˆäººä»¬æ€»å¸Œæœ›è‡ªå·±çš„å­å¼Ÿå¥½ï¼Ÿä¾„å­è°¢ç„å›ç­”ï¼šâ€œè­¬å¦‚èŠå…°ç‰æ ‘ï¼Œæ¬²ä½¿å…¶ç”Ÿäºåº­é˜¶è€³ã€‚â€åæ¥å°±ç§°è°¢ç„ä¸ºè°¢å®¶å®æ ‘ã€‚å­Ÿæ°ä¹‹èŠ³é‚»ï¼šè¿™é‡Œå€Ÿå­Ÿå­çš„æ¯äº²ä¸ºå¯»æ‰¾é‚»å±…è€Œä¸‰æ¬¡æ¬å®¶çš„æ•…äº‹ï¼Œæ¥æŒ‡èµ´å®´çš„å˜‰å®¾ã€‚ ä»–æ—¥è¶‹åº­ï¼Œå¨é™ªé²¤å¯¹ï¼šè¿‡äº›æ—¶å€™è‡ªå·±å°†åˆ°çˆ¶äº²é‚£é‡Œé™ªä¾å’Œè†å¬æ•™è¯²ã€‚è¶‹åº­ï¼šå¿«æ­¥èµ°è¿‡åº­é™¢ï¼Œè¿™æ˜¯è¡¨ç¤ºå¯¹é•¿è¾ˆçš„æ­æ•¬ã€‚å¨ï¼šæƒ­æ„§åœ°æ‰¿å—ï¼Œè¡¨ç¤ºè‡ªè°¦ã€‚é²¤å¯¹ï¼šå­”é²¤æ˜¯å­”å­çš„å„¿å­ï¼Œé²¤å¯¹æŒ‡æ¥å—çˆ¶äº²æ•™è¯²ã€‚äº‹è§ã€Šè®ºè¯­Â·å­£æ°ã€‹ï¼šï¼ˆå­”å­ï¼‰å°ç‹¬ç«‹ï¼Œï¼ˆå­”ï¼‰é²¤è¶‹è€Œè¿‡åº­ã€‚ï¼ˆå­ï¼‰æ›°ï¼šâ€œå­¦è¯—ä¹ï¼Ÿâ€å¯¹æ›°ï¼šâ€œæœªä¹Ÿã€‚â€â€œä¸å­¦è¯—ï¼Œæ— ä»¥è¨€ã€‚â€é²¤é€€è€Œå­¦è¯—ã€‚ä»–æ—¥ï¼Œåˆç‹¬ç«‹ï¼Œé²¤è¶‹è€Œè¿‡åº­ã€‚ï¼ˆå­ï¼‰æ›°ï¼šâ€œå­¦ç¤¼ä¹ï¼Ÿâ€å¯¹æ›°ï¼šâ€˜æœªä¹Ÿã€‚â€â€œä¸å­¦ç¤¼ï¼Œæ— ä»¥ç«‹ã€‚â€é²¤é€€è€Œå­¦ç¤¼ã€‚ æ§è¢‚ï¼ˆmÃ¨iï¼‰ï¼šä¸¾èµ·åŒè¢–ä½œæ–ï¼ŒæŒ‡è°’è§é˜å…¬ã€‚å–œæ‰˜é¾™é—¨ï¼šï¼ˆå—åˆ°é˜å…¬çš„æ¥å¾…ï¼‰ååˆ†é«˜å…´ï¼Œå¥½åƒç™»ä¸Šé¾™é—¨ä¸€æ ·ã€‚ æ¨æ„ï¼šå³èœ€äººæ¨å¾—æ„ï¼Œä»»æŒç®¡å¤©å­çŒçŠ¬çš„å®˜ï¼Œè¥¿æ±‰è¾èµ‹å®¶å¸é©¬ç›¸å¦‚æ˜¯ç”±ä»–æ¨èç»™æ±‰æ­¦å¸çš„ã€‚å‡Œäº‘ï¼šè¿™é‡ŒæŒ‡å¸é©¬ç›¸å¦‚çš„èµ‹ï¼Œã€Šå²è®°Â·å¸é©¬ç›¸å¦‚ä¼ ã€‹è¯´ï¼Œç›¸å¦‚çŒ®ã€Šå¤§äººèµ‹ã€‹ï¼Œâ€œå¤©å­å¤§æ‚¦ï¼Œé£˜é£˜æœ‰å‡Œäº‘ä¹‹æ°”ï¼Œä¼¼æ¸¸å¤©åœ°ä¹‹é—´â€ã€‚é’ŸæœŸï¼šå³é’Ÿå­æœŸã€‚]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pythonä¸­çš„æ‹·è´]]></title>
    <url>%2F2018%2F08%2F18%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPython%E4%B8%AD%E7%9A%84%E6%8B%B7%E8%B4%9D%2F</url>
    <content type="text"><![CDATA[Pythonçš„æ‹·è´å’ŒC/C++çš„å·®åˆ«å¾ˆå¤§ï¼Œå¾ˆç»å¸¸å°±å®¹æ˜“ææ··ï¼Œå› æ­¤è®°å½•ä¸€ä¸‹ã€‚ èµ‹å€¼ã€æ‹·è´ èµ‹å€¼ï¼šå®é™…ä¸Šå°±æ˜¯å¯¹è±¡çš„å¼•ç”¨ï¼Œæ²¡æœ‰å¼€è¾Ÿæ–°çš„å†…å­˜ç©ºé—´12lst=[1,2,3]l=lst æµ…æ‹·è´:åˆ›å»ºäº†æ–°å¯¹è±¡ï¼Œä½†æ˜¯å†…å®¹æ˜¯å¯¹åŸå¯¹è±¡çš„å¼•ç”¨ï¼Œæœ‰ä¸‰ç§å½¢å¼ åˆ‡ç‰‡ 12l=lst[:]l=[i for i in lst] å·¥å‚ 1l=list(lst) copy 12import copyl=copy.copy(lst) æ·±æ‹·è´:copyä¸­çš„deepcopyï¼Œç”Ÿæˆä¸€ä¸ªå…¨æ–°çš„å¯¹è±¡ï¼Œä¸åŸæ¥çš„å¯¹è±¡æ— å…³ 12import copyl=copy.deepcopy(lst) ä¾‹å­12345678910111213141516171819202122232425262728293031### å¼•ç”¨https://www.cnblogs.com/huangbiquan/p/7795152.html çš„ä¾‹å­###&gt;&gt;&gt; import copy&gt;&gt;&gt; a = [1,2,3,4,['a','b']] #å®šä¹‰ä¸€ä¸ªåˆ—è¡¨a&gt;&gt;&gt; b = a #èµ‹å€¼&gt;&gt;&gt; c = copy.copy(a) #æµ…æ‹·è´&gt;&gt;&gt; d = copy.deepcopy(a) #æ·±æ‹·è´&gt;&gt;&gt; a.append(5)&gt;&gt;&gt; print(a)[1, 2, 3, 4, ['a', 'b'], 5] #aæ·»åŠ ä¸€ä¸ªå…ƒç´ 5&gt;&gt;&gt; print(b) [1, 2, 3, 4, ['a', 'b'], 5] #bè·Ÿç€æ·»åŠ ä¸€ä¸ªå…ƒç´ 5 &gt;&gt;&gt; print(c)[1, 2, 3, 4, ['a', 'b']] #cä¿æŒä¸å˜&gt;&gt;&gt; print(d)[1, 2, 3, 4, ['a', 'b']] #dä¿æŒä¸å˜&gt;&gt;&gt; a[4].append('c') &gt;&gt;&gt; print(a)[1, 2, 3, 4, ['a', 'b', 'c'], 5] #aä¸­çš„list(å³a[4])æ·»åŠ ä¸€ä¸ªå…ƒç´ c&gt;&gt;&gt; print(b)[1, 2, 3, 4, ['a', 'b', 'c'], 5] #bè·Ÿç€æ·»åŠ ä¸€ä¸ªå…ƒç´ c&gt;&gt;&gt; print(c)[1, 2, 3, 4, ['a', 'b', 'c']] #cè·Ÿç€æ·»åŠ ä¸€ä¸ªå…ƒç´ c&gt;&gt;&gt; print(d)[1, 2, 3, 4, ['a', 'b']] #dä¿æŒä¸å˜#è¯´æ˜å¦‚ä¸‹ï¼š#1.å¤–å±‚æ·»åŠ å…ƒç´ æ—¶ï¼Œ æµ…æ‹·è´cä¸ä¼šéšåŸåˆ—è¡¨aå˜åŒ–è€Œå˜åŒ–ï¼›å†…å±‚listæ·»åŠ å…ƒç´ æ—¶ï¼Œæµ…æ‹·è´cæ‰ä¼šå˜åŒ–ã€‚#2.æ— è®ºåŸåˆ—è¡¨aå¦‚ä½•å˜åŒ–ï¼Œæ·±æ‹·è´déƒ½ä¿æŒä¸å˜ã€‚#3.èµ‹å€¼å¯¹è±¡éšç€åŸåˆ—è¡¨ä¸€èµ·å˜åŒ– Referencehttps://www.cnblogs.com/huangbiquan/p/7795152.htmlhttps://www.cnblogs.com/xueli/p/4952063.html]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Python</tag>
        <tag>æ‹·è´</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¦‚ä½•å°†ELMoè¯å‘é‡ç”¨äºä¸­æ–‡]]></title>
    <url>%2F2018%2F08%2F12%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%A6%82%E4%BD%95%E5%B0%86ELMo%E8%AF%8D%E5%90%91%E9%87%8F%E7%94%A8%E4%BA%8E%E4%B8%AD%E6%96%87%2F</url>
    <content type="text"><![CDATA[10.10æ›´æ–°ï¼šELMoå·²ç»ç”±å“ˆå·¥å¤§ç»„ç”¨PyTorché‡å†™äº†ï¼Œå¹¶ä¸”æä¾›äº†ä¸­æ–‡çš„é¢„è®­ç»ƒå¥½çš„language modelï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚ 2019.4.7æ›´æ–°ï¼šå¹´ä»£è¿‡äºä¹…è¿œï¼Œæœ¬äººäºç»†èŠ‚æ–¹é¢æ—©å·²è®°ä¸å¤§æ¸…æ¥šäº†ã€‚é‡åˆ°bugæˆ–é—®é¢˜çƒ¦è¯·è‡ªè¡ŒæŸ¥é˜…è§£å†³ï¼Œè¯·ä¸å¿…åœ¨è¯„è®ºåŒºæé—®æˆ–é‚®ä»¶æé—®ï¼Œä¸ä¼šå†å›å¤ã€‚ ELMoäºä»Šå¹´äºŒæœˆç”±AllenNLPæå‡ºï¼Œä¸word2vecæˆ–GloVeä¸åŒçš„æ˜¯å…¶åŠ¨æ€è¯å‘é‡çš„æ€æƒ³ï¼Œå…¶æœ¬è´¨å³é€šè¿‡è®­ç»ƒlanguage modelï¼Œå¯¹äºä¸€å¥è¯è¿›å…¥åˆ°language modelè·å¾—ä¸åŒçš„è¯å‘é‡ã€‚æ ¹æ®å®éªŒå¯å¾—ï¼Œä½¿ç”¨äº†Elmoè¯å‘é‡ä¹‹åï¼Œè®¸å¤šNLPä»»åŠ¡éƒ½æœ‰äº†å¤§å¹…çš„æé«˜ã€‚ è®ºæ–‡:Deep contextualized word representations AllenNLPä¸€å…±releaseäº†ä¸¤ä»½ELMoçš„ä»£ç ï¼Œä¸€ä»½æ˜¯Pytorchç‰ˆæœ¬çš„ï¼Œå¦ä¸€ä»½æ˜¯Tensorflowç‰ˆæœ¬çš„ã€‚Pytorchç‰ˆæœ¬çš„åªå¼€æ”¾äº†ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„è¯å‘é‡çš„æ¥å£ï¼Œä½†æ²¡æœ‰ç»™å‡ºè‡ªå·±è®­ç»ƒçš„æ¥å£ï¼Œå› æ­¤æ— æ³•ä½¿ç”¨åˆ°ä¸­æ–‡è¯­æ–™ä¸­ã€‚Tensorflowç‰ˆæœ¬æœ‰æä¾›è®­ç»ƒçš„ä»£ç ï¼Œå› æ­¤æœ¬æ–‡è®°å½•å¦‚ä½•å°†ELMoç”¨äºä¸­æ–‡è¯­æ–™ä¸­ï¼Œä½†æœ¬æ–‡åªè®°å½•ä½¿ç”¨åˆ°çš„éƒ¨åˆ†ï¼Œè€Œä¸ä¼šåˆ†æå…¨éƒ¨çš„ä»£ç ã€‚ éœ€æ±‚:ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„è¯å‘é‡ä½œä¸ºå¥å­è¡¨ç¤ºç›´æ¥ä¼ å…¥åˆ°RNNä¸­(ä¹Ÿå°±æ˜¯ä¸ä½¿ç”¨ä»£ç ä¸­é»˜è®¤çš„å…ˆè¿‡CNN)ï¼Œåœ¨è®­ç»ƒå®Œåï¼Œå°†æ¨¡å‹ä¿å­˜ï¼Œåœ¨éœ€è¦ç”¨çš„æ—¶å€™loadè¿›æ¥ï¼Œå¯¹äºä¸€ä¸ªç‰¹å®šçš„å¥å­ï¼Œé¦–å…ˆå°†å…¶è½¬æ¢æˆé¢„è®­ç»ƒçš„è¯å‘é‡ï¼Œä¼ å…¥language modelä¹‹åæœ€ç»ˆå¾—åˆ°ELMoè¯å‘é‡ã€‚ å‡†å¤‡å·¥ä½œ: å°†ä¸­æ–‡è¯­æ–™åˆ†è¯ è®­ç»ƒå¥½GloVeè¯å‘é‡æˆ–è€…word2vec ä¸‹è½½bilm-tfä»£ç  ç”Ÿæˆè¯è¡¨ vocab_file ï¼ˆè®­ç»ƒçš„æ—¶å€™è¦ç”¨åˆ°ï¼‰ optional:é˜…è¯»Readme optional:é€šè¯»bilm-tfçš„ä»£ç ï¼Œå¯¹ä»£ç ç»“æ„æœ‰ä¸€å®šçš„è®¤è¯† æ€è·¯: å°†é¢„è®­ç»ƒçš„è¯å‘é‡è¯»å…¥ ä¿®æ”¹bilm-tfä»£ç  optionéƒ¨åˆ† æ·»åŠ ç»™embedding weightèµ‹åˆå€¼ æ·»åŠ ä¿å­˜embedding weightçš„ä»£ç  å¼€å§‹è®­ç»ƒï¼Œè·å¾—checkpointå’Œoptionæ–‡ä»¶ è¿è¡Œè„šæœ¬ï¼Œè·å¾—language modelçš„weightæ–‡ä»¶ å°†embedding weightä¿å­˜ä¸ºhdf5æ–‡ä»¶å½¢å¼ è¿è¡Œè„šæœ¬ï¼Œå°†è¯­æ–™è½¬åŒ–æˆELMo embeddingã€‚ è®­ç»ƒGloVeæˆ–word2vecå¯å‚è§æˆ‘ä»¥å‰çš„åšå®¢æˆ–è€…ç½‘ä¸Šçš„æ•™ç¨‹ã€‚æ³¨æ„åˆ°ï¼Œå¦‚æœè¦ç”¨gensimå¯¼å…¥GloVeè®­å¥½çš„è¯å‘é‡ï¼Œéœ€è¦åœ¨å¼€å¤´æ·»åŠ num_word embedding_dimã€‚ å¦‚ï¼š è·å¾—vocabè¯è¡¨æ–‡ä»¶æ³¨æ„åˆ°ï¼Œè¯è¡¨æ–‡ä»¶çš„å¼€å¤´å¿…é¡»è¦æœ‰&lt;S&gt; &lt;/S&gt; &lt;UNK&gt;ï¼Œä¸”å¤§å°å†™æ•æ„Ÿã€‚å¹¶ä¸”åº”å½“æŒ‰ç…§å•è¯çš„è¯é¢‘é™åºæ’åˆ—ã€‚å¯ä»¥é€šè¿‡æ‰‹åŠ¨æ·»åŠ è¿™ä¸‰ä¸ªç‰¹æ®Šç¬¦å·ã€‚å¦‚ï¼š ä»£ç ï¼š1234567891011121314model=gensim.models.KeyedVectors.load_word2vec_format( fname='/home/zhlin/GloVe/vectors.txt',binary=False)words=model.vocabwith open('vocab.txt','w') as f: f.write('&lt;S&gt;') f.write('\n'ï¼‰ f.write('&lt;/S&gt;') f.write('\n') f.write('&lt;UNK&gt;') f.write('\n') # bilm-tf è¦æ±‚vocabæœ‰è¿™ä¸‰ä¸ªç¬¦å·ï¼Œå¹¶ä¸”åœ¨æœ€å‰é¢ for word in words: f.write(word) f.write('\n') ä¿®æ”¹bilm-tfä»£ç æ³¨æ„åˆ°ï¼Œåœ¨ä½¿ç”¨è¯¥ä»£ç ä¹‹å‰ï¼Œéœ€è¦å®‰è£…å¥½ç›¸åº”çš„ç¯å¢ƒã€‚ å¦‚æœä½¿ç”¨çš„æ˜¯condaä½œä¸ºé»˜è®¤çš„Pythonè§£é‡Šå™¨ï¼Œå¼ºçƒˆå»ºè®®ä½¿ç”¨condaå®‰è£…ï¼Œå¦åˆ™å¯èƒ½ä¼šå‡ºç°ä¸€äº›è«åçš„é”™è¯¯ã€‚123conda install tensorflow-gpu=1.4conda install h5pypython setup.py install #åº”åœ¨bilm-tfçš„æ–‡ä»¶å¤¹ä¸‹æ‰§è¡Œè¯¥æŒ‡ä»¤ ç„¶åå†è¿è¡Œæµ‹è¯•ä»£ç ï¼Œé€šè¿‡è¯´æ˜å®‰è£…æˆåŠŸã€‚ ä¿®æ”¹train_elmo.pybinæ–‡ä»¶å¤¹ä¸‹çš„train_elmo.pyæ˜¯ç¨‹åºçš„å…¥å£ã€‚ä¸»è¦ä¿®æ”¹çš„åœ°æ–¹ï¼š load_vocabçš„ç¬¬äºŒä¸ªå‚æ•°åº”è¯¥æ”¹ä¸ºNone n_gpus CUDA_VISIBLE_DEVICES æ ¹æ®è‡ªå·±éœ€æ±‚æ”¹ n_train_tokens å¯æ”¹å¯ä¸æ”¹ï¼Œå½±å“çš„æ˜¯è¾“å‡ºä¿¡æ¯ã€‚è¦æŸ¥çœ‹è‡ªå·±è¯­æ–™çš„è¡Œæ•°ï¼Œå¯ä»¥é€šè¿‡wc -l corpus.txt æŸ¥çœ‹ã€‚ optionçš„ä¿®æ”¹ï¼Œå°†char_cnnéƒ¨åˆ†éƒ½æ³¨é‡Šæ‰ï¼Œå…¶ä»–æ ¹æ®è‡ªå·±éœ€æ±‚ä¿®æ”¹ å¦‚ï¼š ä¿®æ”¹LanguageModelç±»ç”±äºæˆ‘éœ€è¦ä¼ å…¥é¢„è®­ç»ƒå¥½çš„GloVe embeddingï¼Œé‚£ä¹ˆè¿˜éœ€è¦ä¿®æ”¹embeddingéƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†åœ¨bilmæ–‡ä»¶å¤¹ä¸‹çš„training.pyï¼Œè¿›å…¥åˆ°LanguageModelç±»ä¸­_build_word_embeddingså‡½æ•°ä¸­ã€‚æ³¨æ„åˆ°ï¼Œç”±äºå‰ä¸‰ä¸ªæ˜¯&lt;S&gt; &lt;/S&gt; &lt;UNK&gt;ï¼Œè€Œè¿™ä¸‰ä¸ªå­—ç¬¦åœ¨GloVeé‡Œé¢æ˜¯æ²¡æœ‰çš„ï¼Œå› æ­¤è¿™ä¸‰ä¸ªå­—ç¬¦çš„embeddingåº”å½“åœ¨è®­ç»ƒçš„æ—¶å€™é€æ¸å­¦ä¹ åˆ°ï¼Œè€Œæ­£å› æ­¤ embedding_weightsçš„trainableåº”å½“è®¾ä¸ºTrue å¦‚: ä¿®æ”¹trainå‡½æ•°æ·»åŠ ä»£ç ï¼Œä½¿å¾—åœ¨trainå‡½æ•°çš„æœ€åä¿å­˜embeddingæ–‡ä»¶ã€‚ è®­ç»ƒå¹¶è·å¾—weightsæ–‡ä»¶è®­ç»ƒéœ€è¦è¯­æ–™æ–‡ä»¶corpus.txtï¼Œè¯è¡¨æ–‡ä»¶vocab.txtã€‚ è®­ç»ƒcdåˆ°bilm-tfæ–‡ä»¶å¤¹ä¸‹ï¼Œè¿è¡Œ12345export CUDA_VISIBLE_DEVICES=4nohup python -u bin/train_elmo.py \--train_prefix='/home/zhlin/bilm-tf/corpus.txt' \--vocab_file /home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab.txt \--save_dir /home/zhlin/bilm-tf/try &gt;bilm_out.txt 2&gt;&amp;1 &amp; æ ¹æ®å®é™…æƒ…å†µè®¾å®šä¸åŒçš„å€¼å’Œè·¯å¾„ã€‚ è¿è¡Œæƒ…å†µï¼š PS:è¿è¡Œè¿‡ç¨‹ä¸­å¯èƒ½ä¼šæœ‰warning: â€˜listâ€™ object has no attribute â€˜nameâ€™WARNING:tensorflow:Error encountered when serializing lstm_output_embeddings.Type is unsupported, or the types of the items donâ€™t match field type in CollectionDef. åº”è¯¥ä¸ç”¨æ‹…å¿ƒï¼Œè¿˜æ˜¯èƒ½å¤Ÿç»§ç»­è¿è¡Œçš„ï¼Œåé¢ä¹Ÿä¸å—å½±å“ã€‚ åœ¨ç­‰å¾…äº†ç›¸å½“é•¿çš„æ—¶é—´åï¼Œåœ¨save_diræ–‡ä»¶å¤¹å†…ç”Ÿæˆäº†å‡ ä¸ªæ–‡ä»¶ï¼Œå…¶ä¸­checkpointå’Œoptionsæ˜¯å…³é”®ï¼Œcheckpointèƒ½å¤Ÿè¿›ä¸€æ­¥ç”Ÿæˆlanguage modelçš„weightsæ–‡ä»¶ï¼Œè€Œoptionsè®°å½•language modelçš„å‚æ•°ã€‚ è·å¾—language modelçš„weightsæ¥ä¸‹æ¥è¿è¡Œbin/dump_weights.pyå°†checkpointè½¬æ¢æˆhdf5æ–‡ä»¶ã€‚ 123nohup python -u /home/zhlin/bilm-tf/bin/dump_weights.py \--save_dir /home/zhlin/bilm-tf/try \--outfile /home/zhlin/bilm-tf/try/weights.hdf5 &gt;outfile.txt 2&gt;&amp;1 &amp; å…¶ä¸­save_diræ˜¯checkpointå’Œoptionæ–‡ä»¶ä¿å­˜çš„åœ°å€ã€‚ æ¥ä¸‹æ¥ç­‰å¾…ç¨‹åºè¿è¡Œï¼š æœ€ç»ˆè·å¾—äº†æƒ³è¦çš„weightså’Œoptionï¼š å°†è¯­æ–™è½¬åŒ–æˆELMo embeddingç”±äºæˆ‘ä»¬æœ‰äº†vocab_fileã€ä¸vocab_fileä¸€ä¸€å¯¹åº”çš„embedding h5pyæ–‡ä»¶ã€ä»¥åŠlanguage modelçš„weights.hdf5å’Œoptions.jsonã€‚æ¥ä¸‹æ¥å‚è€ƒusage_token.pyå°†ä¸€å¥è¯è½¬åŒ–æˆELMo embeddingã€‚ å‚è€ƒä»£ç ï¼š12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import tensorflow as tfimport osfrom bilm import TokenBatcher, BidirectionalLanguageModel, weight_layers, \ dump_token_embeddings# Our small dataset.raw_context = [ 'è¿™ æ˜¯ æµ‹è¯• .', 'å¥½çš„ .']tokenized_context = [sentence.split() for sentence in raw_context]tokenized_question = [ ['è¿™', 'æ˜¯', 'ä»€ä¹ˆ'],]vocab_file='/home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab.txt'options_file='/home/zhlin/bilm-tf/try/options.json'weight_file='/home/zhlin/bilm-tf/try/weights.hdf5'token_embedding_file='/home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab_embedding.hdf5'## Now we can do inference.# Create a TokenBatcher to map text to token ids.batcher = TokenBatcher(vocab_file)# Input placeholders to the biLM.context_token_ids = tf.placeholder('int32', shape=(None, None))question_token_ids = tf.placeholder('int32', shape=(None, None))# Build the biLM graph.bilm = BidirectionalLanguageModel( options_file, weight_file, use_character_inputs=False, embedding_weight_file=token_embedding_file)# Get ops to compute the LM embeddings.context_embeddings_op = bilm(context_token_ids)question_embeddings_op = bilm(question_token_ids)elmo_context_input = weight_layers('input', context_embeddings_op, l2_coef=0.0)with tf.variable_scope('', reuse=True): # the reuse=True scope reuses weights from the context for the question elmo_question_input = weight_layers( 'input', question_embeddings_op, l2_coef=0.0 )elmo_context_output = weight_layers( 'output', context_embeddings_op, l2_coef=0.0)with tf.variable_scope('', reuse=True): # the reuse=True scope reuses weights from the context for the question elmo_question_output = weight_layers( 'output', question_embeddings_op, l2_coef=0.0 )with tf.Session() as sess: # It is necessary to initialize variables once before running inference. sess.run(tf.global_variables_initializer()) # Create batches of data. context_ids = batcher.batch_sentences(tokenized_context) question_ids = batcher.batch_sentences(tokenized_question) # Compute ELMo representations (here for the input only, for simplicity). elmo_context_input_, elmo_question_input_ = sess.run( [elmo_context_input['weighted_op'], elmo_question_input['weighted_op']], feed_dict=&#123;context_token_ids: context_ids, question_token_ids: question_ids&#125; )print(elmo_context_input_,elmo_context_input_) å¯ä»¥ä¿®æ”¹ä»£ç ä»¥é€‚åº”è‡ªå·±çš„éœ€æ±‚ã€‚ Referencehttps://github.com/allenai/bilm-tf]]></content>
      <tags>
        <tag>æ•™ç¨‹</tag>
        <tag>ELMo</tag>
        <tag>è¯å‘é‡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•4]]></title>
    <url>%2F2018%2F08%2F12%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB4%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨æ²¡æœ‰ä»€ä¹ˆä»£ç è¦è®°å½•çš„ã€‚ 1ï¸âƒ£sklearnä¹‹Pipelineä¾‹å­ç”¨æœºå™¨å­¦ä¹ è§£å†³é—®é¢˜çš„æµç¨‹ï¼š(å»æ‰éƒ¨åˆ†æ•°æ®ï¼‰â€”&gt; è·å–featureï¼ˆTf-idfç­‰ï¼‰ â€”&gt; ï¼ˆfeature selectionï¼Œchi2ã€äº’ä¿¡æ¯ç­‰ï¼‰ â€”&gt; ï¼ˆç¼©æ”¾/æ­£åˆ™åŒ–ï¼‰ â€”&gt; åˆ†ç±»å™¨ â€”&gt; GridSearch/RandomizedSearchè°ƒå‚ 123456789101112131415161718192021222324252627282930pipe=Pipeline([ #å»ºç«‹pipeline ('vect',TfidfVectorizer()), ('select',SelectKBest(chi2), ('norm',MaxAbsScaler()), ('svm',svm.LinearSVC())])parameters=&#123; 'vect__ngram_range':[(1,1),(1,2),(1,3),(2,3)], 'vect__max_df':[0.6,0.7,0.8,0.9], 'vect__min_df':[1,3,5,7,9], 'vect__norm':['l1','l2'], 'svm__penalty':['l1','l2'], 'svm__loss':['squared_hinge'], 'svm__dual':[False,True], 'svm__tol':[1e-5,1e-4], 'svm__C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1], 'svm__class_weight':[None,'balanced'], 'svm__max_iter':[1000,5000]&#125;grid_search_model=GridSearchCV(pipe,parameters,error_score=0,n_jobs=5)grid_search_model.fit(train[column],train['class'])for para_name in sorted(parameters.keys()): print(para_name,grid_search_model.best_params_[para_name])print("cv_result:")print(grid_search_model.cv_results_)]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†3]]></title>
    <url>%2F2018%2F08%2F12%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%863%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Python]åœ¨æœåŠ¡å™¨ä¸Šè·‘ä»£ç æ—¶ï¼Œå¦‚ python project/folder1/a.pyï¼Œå¦‚æœa.pyå¼•ç”¨äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„æ¨¡å—ä½†åˆä¸åœ¨folder1å†…ï¼Œæ­¤æ—¶interpreterå°±ä¼šæŠ¥é”™ï¼Œæç¤ºæ‰¾ä¸åˆ°è¯¥æ¨¡å—ã€‚è¿™æ˜¯å› ä¸ºè§£é‡Šå™¨é»˜è®¤åªä¼šåœ¨åŒä¸€ä¸ªfolderä¸‹æŸ¥æ‰¾ã€‚è§£å†³æ–¹æ¡ˆæ˜¯åœ¨è¿è¡Œå‰æ˜¾å¼æ·»åŠ æŸ¥æ‰¾èŒƒå›´ã€‚å¦‚ï¼š1export PYTHONPATH=/home/zhlin/bilm-tf:$PYTHONPATH é‚£ä¹ˆpythonè§£é‡Šå™¨å°±ä¼šåˆ°è¯¥ç›®å½•ä¸‹å»æ‰¾ã€‚ 2ï¸âƒ£[åº¦é‡æ ‡å‡†] å‡†ç¡®ç‡(accuracy): $ACC=\frac{TP+TN}{TP+TN+FP+FN}$ è¡¡é‡çš„æ˜¯åˆ†ç±»å™¨é¢„æµ‹å‡†ç¡®çš„æ¯”ä¾‹ å¬å›ç‡(recall): $Recall=\frac{TP}{TP+FN}$ æ­£ä¾‹ä¸­è¢«åˆ†å¯¹çš„æ¯”ä¾‹ï¼Œè¡¡é‡äº†åˆ†ç±»å™¨å¯¹æ­£ä¾‹çš„è¯†åˆ«èƒ½åŠ›ã€‚ ç²¾ç¡®ç‡(Precision): $P=\frac{TP}{TP+FP}$åº¦é‡äº†è¢«åˆ†ä¸ºæ­£ä¾‹çš„ç¤ºä¾‹ä¸­å®é™…ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ã€‚ F-Measure: $F=\frac{(\alpha^2 +1)P*R}{\alpha^2 (P+R)}$ å…¶ä¸­Pæ˜¯Precision,Ræ˜¯Recallã€‚ç»¼åˆè€ƒé‡äº†ä¸¤ç§åº¦é‡ã€‚ å½“$\alpha=1$æ—¶ï¼Œç§°ä¸ºF1å€¼ $F1=\frac{2PR}{P+R}$ 3ï¸âƒ£[è°ƒå‚æŠ€å·§]åœ¨googleå‘å¸ƒçš„ä¸€ä»½å…³äºtext-classificationçš„guideä¸­ï¼Œæåˆ°äº†å‡ ä¸ªè°ƒå‚çš„trickã€‚ åœ¨feature selectionæ­¥éª¤ä¸­ï¼Œå¡æ–¹æ£€éªŒchi2å’Œæ–¹å·®åˆ†æçš„Få€¼ f_classifçš„è¡¨ç°ç›¸å½“ï¼Œåœ¨å¤§çº¦é€‰æ‹©20kçš„featureæ—¶ï¼Œå‡†ç¡®ç‡è¾¾åˆ°é¡¶å³°ï¼Œå½“featureè¶Šå¤šï¼Œæ•ˆæœå¹¶æ²¡æœ‰æå‡ç”šè‡³ä¼šä¸‹é™ã€‚ åœ¨æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œä¼¼ä¹ä½¿ç”¨normalizationå¹¶æ²¡æœ‰å¤šå°‘ç”¨å¤„ï¼Œå»ºè®®è·³è¿‡ã€‚ Normalization converts all feature/sample values to small and similar values. This simplifies gradient descent convergence in learning algorithms. From what we have seen, normalization during data preprocessing does not seem to add much value in text classification problems; we recommend skipping this step. å®é™…ä¸Šæˆ‘ä¹Ÿæµ‹è¯•è¿‡ï¼Œå‘ç°ç¡®å®normalizationå¯¹äºå‡†ç¡®ç‡çš„æé«˜æ²¡ä»€ä¹ˆå¸®åŠ©ï¼Œç”šè‡³è¿˜æœ‰ä¸€ç‚¹ä¸‹é™ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Python</tag>
        <tag>åº¦é‡æ ‡å‡†</tag>
        <tag>è°ƒå‚æŠ€å·§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯4]]></title>
    <url>%2F2018%2F08%2F12%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D4%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£ çä¸Šç§‹å±…[å”] é©¬æˆ´çåŸé£é›¨å®šï¼Œæ™šè§é›è¡Œé¢‘ã€‚è½å¶ä»–ä¹¡æ ‘ï¼Œå¯’ç¯ç‹¬å¤œäººã€‚ç©ºå›­ç™½éœ²æ»´ï¼Œå­¤å£é‡åƒ§é‚»ã€‚å¯„å§éƒŠæ‰‰ä¹…ï¼Œä½•å¹´è‡´æ­¤èº«ã€‚ http://m.xichuangzhu.com/work/57b8a4080a2b58005c9108d9 2ï¸âƒ£ å”å¤šä»¤[å®‹] åˆ˜è¿‡èŠ¦å¶æ»¡æ±€æ´²ï¼Œå¯’æ²™å¸¦æµ…æµã€‚äºŒåå¹´é‡è¿‡å—æ¥¼ã€‚æŸ³ä¸‹ç³»èˆ¹çŠ¹æœªç¨³ï¼Œèƒ½å‡ æ—¥ï¼Œåˆä¸­ç§‹ã€‚é»„é¹¤æ–­çŸ¶å¤´ï¼Œæ•…äººä»Šåœ¨å¦ï¼Ÿæ—§æ±Ÿå±±æµ‘æ˜¯æ–°æ„ã€‚æ¬²ä¹°æ¡‚èŠ±åŒè½½é…’ï¼Œç»ˆä¸ä¼¼ã€å°‘å¹´æ¸¸ã€‚ http://m.xichuangzhu.com/work/57b922e7c4c9710055904842]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vimå¸¸ç”¨å¿«æ·é”®]]></title>
    <url>%2F2018%2F08%2F10%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2FVim%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[åœ¨æœåŠ¡å™¨ç»å¸¸è¦ç”¨åˆ°Vimï¼Œå› æ­¤è®°å½•å¸¸ç”¨çš„å¿«æ·é”®å¹¶ç†Ÿæ‚‰ä¹‹ã€‚ é€€å‡º:q é€€å‡º:wq å†™å…¥å¹¶é€€å‡º:q! é€€å‡ºå¹¶å¿½ç•¥æ‰€æœ‰æ›´æ”¹:e! æ”¾å¼ƒä¿®æ”¹å¹¶æ‰“å¼€åŸæ¥çš„æ–‡ä»¶ æ’å…¥i åœ¨å½“å‰ä½ç½®å‰æ’å…¥a åœ¨å½“å‰ä½ç½®åæ’å…¥ æ’¤é”€:u æ’¤é”€:U æ’¤é”€æ•´è¡Œæ“ä½œCtrl+r é‡åš åˆ é™¤:md åˆ é™¤ç¬¬mè¡Œnd åˆ é™¤å½“å‰è¡Œå¼€å§‹çš„nè¡Œ(ä¸€å…±n+1è¡Œ)dd åˆ é™¤å½“å‰è¡ŒD åˆ é™¤å½“å‰å­—ç¬¦è‡³è¡Œå°¾:m,nd åˆ é™¤ä»måˆ°nè¡Œçš„å†…å®¹ï¼Œå¦‚: :100,10000d:m,$d åˆ é™¤mè¡ŒåŠä»¥åæ‰€æœ‰çš„è¡Œ:10d ç§»åŠ¨:n è·³è½¬åˆ°è¡Œå· å¦‚ï¼Œ :100gg è·³åˆ°è¡Œé¦–G(shift+g)ç§»åŠ¨åˆ°æ–‡ä»¶å°¾ æœç´¢/text æœç´¢textï¼Œnæœç´¢ä¸‹ä¸€ä¸ªï¼ŒNæœç´¢ä¸Šä¸€ä¸ª?text åå‘æŸ¥æ‰¾:set ignorecase å¿½ç•¥å¤§å°å†™æŸ¥æ‰¾:set noignorecase ä¸å¿½ç•¥å¤§å°å†™æŸ¥æ‰¾*æˆ–# å¯¹å…‰æ ‡å¤„çš„å•è¯æœç´¢ å¤åˆ¶ç²˜è´´v ä»å½“å‰ä½ç½®å¼€å§‹ï¼Œå…‰æ ‡ç»è¿‡çš„åœ°æ–¹è¢«é€‰ä¸­ï¼Œå†æŒ‰ä¸€ä¸‹vç»“æŸ ç¯å¢ƒè®¾ç½®:set nu æ˜¾ç¤ºè¡Œå·:set nonu éšè—è¡Œå·:set hlsearch è®¾ç½®æœç´¢ç»“æœé«˜äº® Referencehttps://www.cnblogs.com/wangrx/p/5907013.htmlhttps://www.cnblogs.com/yangjig/p/6014198.html]]></content>
      <tags>
        <tag>æŠ€å·§</tag>
        <tag>æ‚ä¸ƒæ‚å…«</tag>
        <tag>å¿«æ·é”®</tag>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pycharmå¸¸ç”¨æŠ€å·§]]></title>
    <url>%2F2018%2F08%2F10%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2FPycharm%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[è®°å½•Pycharmçš„ä¸€äº›æŠ€å·§ï¼Œè®©Pycharmæ›´é¡ºæ‰‹ å¿«æ·é”®0ï¸âƒ£Double Shift ä¸‡èƒ½æœç´¢å¯ä»¥æœç´¢æ–‡ä»¶åã€ç±»åã€æ–¹æ³•åã€ç›®å½•åï¼ˆåœ¨å…³é”®å­—å‰é¢åŠ / ï¼‰ï¼Œå¹¶ä¸èƒ½ç”¨æ¥æœç´¢ä»»æ„å…³é”®å­— 1ï¸âƒ£ Command+F åœ¨é¡µé¢æœç´¢ 2ï¸âƒ£ Ctrl+Shift+F Find in Path åœ¨è·¯å¾„ä¸‹æœç´¢ 3ï¸âƒ£âœ¨Command+E å¿«é€ŸæŸ¥æ‰¾æ–‡ä»¶æ˜¾ç¤ºæœ€è¿‘æ‰“å¼€çš„æ–‡ä»¶ 4ï¸âƒ£ Shift+Enter ä»»æ„ä½ç½®æ¢è¡Œæ— è®ºå…‰æ ‡åœ¨ä½•å¤„éƒ½å¯ä»¥ç›´æ¥å¦èµ·ä¸€è¡Œ 5ï¸âƒ£ Option+Enter è‡ªåŠ¨å¯¼å…¥æ¨¡å—ï¼›ä¸‡èƒ½æç¤ºé”®è‡ªåŠ¨å¯¼å…¥å¦‚ä½•è®¾ç½®è§å°æŠ€å·§#0ï¸âƒ£ 6ï¸âƒ£ Ctrl+F10 è¿è¡Œæˆ‘å·²ç»æ·»åŠ äº†Ctrl+Rä½œä¸ºå¦ä¸€å¯¹è¿è¡Œå¿«æ·é”® 7ï¸âƒ£ Command+Shift+ +/- å±•å¼€/æ”¶ç¼©ä»£ç  8ï¸âƒ£ Option+F åœ¨Dashä¸­æœç´¢ 9ï¸âƒ£ Ctrl+J ä¸è·³è½¬æŸ¥çœ‹ä»£ç  å°æŠ€å·§0ï¸âƒ£ Pycharmè‡ªåŠ¨å¯¼å…¥æ¨¡å—https://blog.csdn.net/lantian_123/article/details/78094148 1ï¸âƒ£ âœ¨è¿œç¨‹éƒ¨ç½²å·¥ç¨‹ å¼ºçƒˆæ¨èä¸¤æ­¥èµ°ï¼šé…ç½®æœåŠ¡å™¨æ˜ å°„+é…ç½®æœåŠ¡å™¨è§£é‡Šå™¨ 2ï¸âƒ£è·³è½¬åå¦‚ä½•å›é€€å¼€å¯toolbarå³å¯https://segmentfault.com/a/1190000010205945 Referencehttps://foofish.net/pycharm-tips.htmlhttps://blog.csdn.net/lantian_123/article/details/78094148https://segmentfault.com/a/1190000010205945]]></content>
      <tags>
        <tag>æŠ€å·§</tag>
        <tag>Pycharm</tag>
        <tag>æ‚ä¸ƒæ‚å…«</tag>
        <tag>å¿«æ·é”®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ— é¢˜]]></title>
    <url>%2F2018%2F08%2F06%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%97%A0%E9%A2%98%2F</url>
    <content type="text"><![CDATA[äººè¿™è¾ˆå­ä¸€å…±ä¼šæ­»ä¸‰æ¬¡ã€‚ ç¬¬ä¸€æ¬¡æ˜¯ä½ çš„å¿ƒè„åœæ­¢è·³åŠ¨ï¼Œé‚£ä¹ˆä»ç”Ÿç‰©çš„è§’åº¦æ¥è¯´ï¼Œä½ æ­»äº†ï¼› ç¬¬äºŒæ¬¡æ˜¯åœ¨è‘¬ç¤¼ä¸Šï¼Œè®¤è¯†ä½ çš„äººéƒ½æ¥ç¥­å¥ ï¼Œé‚£ä¹ˆä½ åœ¨ç¤¾ä¼šå…³ç³»ä¸Šçš„äº‹å®å­˜åœ¨å°±æ­»äº†ï¼› ç¬¬ä¸‰æ¬¡æ˜¯åœ¨æœ€åä¸€ä¸ªè®°å¾—ä½ çš„äººæ­»åï¼Œé‚£ä½ å°±çœŸçš„æ­»äº†ã€‚]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[äººå¯ä»¥å‘å¾®å¦‚å°˜åœŸ,ä¸å¯æ‰­æ›²å¦‚è›†è™«]]></title>
    <url>%2F2018%2F08%2F05%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E4%BA%BA%E5%8F%AF%E4%BB%A5%E5%8D%91%E5%BE%AE%E5%A6%82%E5%B0%98%E5%9C%9F%2C%E4%B8%8D%E5%8F%AF%E6%89%AD%E6%9B%B2%E5%A6%82%E8%9B%86%E8%99%AB%2F</url>
    <content type="text"><![CDATA[å¦‚æœå¤©æ€»ä¹Ÿä¸äº®ï¼Œé‚£å°±æ‘¸é»‘è¿‡ç”Ÿæ´»; å¦‚æœå‘å‡ºå£°éŸ³æ˜¯å±é™©çš„ï¼Œé‚£å°±ä¿æŒæ²‰é»˜; å¦‚æœè‡ªè§‰æ— åŠ›å‘å…‰ï¼Œé‚£å°±åˆ«å»ç…§äº®åˆ«äººã€‚ ä½†æ˜¯â€”â€”ä¸è¦ä¹ æƒ¯äº†é»‘æš—å°±ä¸ºé»‘æš—è¾©æŠ¤; ä¸è¦ä¸ºè‡ªå·±çš„è‹Ÿä¸”è€Œå¾—æ„æ´‹æ´‹; ä¸è¦å˜²è®½é‚£äº›æ¯”è‡ªå·±æ›´å‹‡æ•¢ã€æ›´æœ‰çƒ­é‡çš„äººä»¬ã€‚ å¯ä»¥å‘å¾®å¦‚å°˜åœŸï¼Œä¸å¯æ‰­æ›²å¦‚è›†è™«ã€‚]]></content>
      <tags>
        <tag>ä½³å¥åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pythonæƒ¯ä¾‹[è½¬]]]></title>
    <url>%2F2018%2F08%2F05%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPython%E6%83%AF%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[fork from https://github.com/jackfrued/Python-100-Days/blob/master/Pythonæƒ¯ä¾‹.md Pythonæƒ¯ä¾‹â€œæƒ¯ä¾‹â€è¿™ä¸ªè¯æŒ‡çš„æ˜¯â€œä¹ æƒ¯çš„åšæ³•ï¼Œå¸¸è§„çš„åŠæ³•ï¼Œä¸€è´¯çš„åšæ³•â€ï¼Œä¸è¿™ä¸ªè¯å¯¹åº”çš„è‹±æ–‡å•è¯å«â€œidiomâ€ã€‚ç”±äºPythonè·Ÿå…¶ä»–å¾ˆå¤šç¼–ç¨‹è¯­è¨€åœ¨è¯­æ³•å’Œä½¿ç”¨ä¸Šè¿˜æ˜¯æœ‰æ¯”è¾ƒæ˜¾è‘—çš„å·®åˆ«ï¼Œå› æ­¤ä½œä¸ºä¸€ä¸ªPythonå¼€å‘è€…å¦‚æœä¸èƒ½æŒæ¡è¿™äº›æƒ¯ä¾‹ï¼Œå°±æ— æ³•å†™å‡ºâ€œPythonicâ€çš„ä»£ç ã€‚ä¸‹é¢æˆ‘ä»¬æ€»ç»“äº†ä¸€äº›åœ¨Pythonå¼€å‘ä¸­çš„æƒ¯ç”¨çš„ä»£ç ã€‚ è®©ä»£ç æ—¢å¯ä»¥è¢«å¯¼å…¥åˆå¯ä»¥è¢«æ‰§è¡Œã€‚ 1if __name__ == '__main__': ç”¨ä¸‹é¢çš„æ–¹å¼åˆ¤æ–­é€»è¾‘â€œçœŸâ€æˆ–â€œå‡â€ã€‚ 12if x:if not x: å¥½çš„ä»£ç ï¼š 12345name = 'jackfrued'fruits = ['apple', 'orange', 'grape']owners = &#123;'1001': 'éª†æ˜Š', '1002': 'ç‹å¤§é”¤'&#125;if name and fruits and owners: print('I love fruits!') ä¸å¥½çš„ä»£ç ï¼š 12345name = 'jackfrued'fruits = ['apple', 'orange', 'grape']owners = &#123;'1001': 'éª†æ˜Š', '1002': 'ç‹å¤§é”¤'&#125;if name != '' and len(fruits) &gt; 0 and owners != &#123;&#125;: print('I love fruits!') å–„äºä½¿ç”¨inè¿ç®—ç¬¦ã€‚ 12if x in items: # åŒ…å«for x in items: # è¿­ä»£ å¥½çš„ä»£ç ï¼š 123name = 'Hao LUO'if 'L' in name: print('The name has an L in it.') ä¸å¥½çš„ä»£ç ï¼š 123name = 'Hao LUO'if name.find('L') != -1: print('This name has an L in it!') ä¸ä½¿ç”¨ä¸´æ—¶å˜é‡äº¤æ¢ä¸¤ä¸ªå€¼ã€‚ 1a, b = b, a ç”¨åºåˆ—æ„å»ºå­—ç¬¦ä¸²ã€‚ å¥½çš„ä»£ç ï¼š 123chars = ['j', 'a', 'c', 'k', 'f', 'r', 'u', 'e', 'd']name = ''.join(chars)print(name) # jackfrued ä¸å¥½çš„ä»£ç ï¼š 12345chars = ['j', 'a', 'c', 'k', 'f', 'r', 'u', 'e', 'd']name = ''for char in chars: name += charprint(name) # jackfrued EAFPä¼˜äºLBYLã€‚ EAFP - Easier to Ask Forgiveness than Permission. LBYL - Look Before You Leap. å¥½çš„ä»£ç ï¼š 123456d = &#123;'x': '5'&#125;try: value = int(d['x']) print(value)except (KeyError, TypeError, ValueError): value = None ä¸å¥½çš„ä»£ç ï¼š 1234567d = &#123;'x': '5'&#125;if 'x' in d and isinstance(d['x'], str) \ and d['x'].isdigit(): value = int(d['x']) print(value)else: value = None ä½¿ç”¨enumerateè¿›è¡Œè¿­ä»£ã€‚ å¥½çš„ä»£ç ï¼š 123fruits = ['orange', 'grape', 'pitaya', 'blueberry']for index, fruit in enumerate(fruits): print(index, ':', fruit) ä¸å¥½çš„ä»£ç ï¼š 12345fruits = ['orange', 'grape', 'pitaya', 'blueberry']index = 0for fruit in fruits: print(index, ':', fruit) index += 1 ç”¨ç”Ÿæˆå¼ç”Ÿæˆåˆ—è¡¨ã€‚ å¥½çš„ä»£ç ï¼š 123data = [7, 20, 3, 15, 11]result = [num * 3 for num in data if num &gt; 10]print(result) # [60, 45, 33] ä¸å¥½çš„ä»£ç ï¼š 123456data = [7, 20, 3, 15, 11]result = []for i in data: if i &gt; 10: result.append(i * 3)print(result) # [60, 45, 33] ç”¨zipç»„åˆé”®å’Œå€¼æ¥åˆ›å»ºå­—å…¸ã€‚ å¥½çš„ä»£ç ï¼š 1234keys = ['1001', '1002', '1003']values = ['éª†æ˜Š', 'ç‹å¤§é”¤', 'ç™½å…ƒèŠ³']d = dict(zip(keys, values))print(d) ä¸å¥½çš„ä»£ç ï¼š 123456keys = ['1001', '1002', '1003']values = ['éª†æ˜Š', 'ç‹å¤§é”¤', 'ç™½å…ƒèŠ³']d = &#123;&#125;for i, key in enumerate(keys): d[key] = values[i]print(d) è¯´æ˜ï¼šè¿™ç¯‡æ–‡ç« çš„å†…å®¹æ¥è‡ªäºç½‘ç»œï¼Œæœ‰å…´è¶£çš„è¯»è€…å¯ä»¥é˜…è¯»åŸæ–‡ã€‚ æ³¨ï¼šè®¸å¤šåŸåˆ™æˆ‘è®¤ä¸ºéå¸¸æœ‰æ„ä¹‰ï¼Œèƒ½å¤Ÿæ‘†è„±C/C++çš„é£æ ¼ï¼ŒçœŸæ­£å†™å‡ºPythonicçš„ä»£ç ã€‚è®©æˆ‘æœ‰å¾ˆå¤§æ„Ÿè§¦çš„æ˜¯1ã€3ã€8ï¼Œèƒ½å¤Ÿå†™å‡ºéå¸¸ç®€æ´ä¼˜é›…çš„ä»£ç ã€‚åŒæ—¶6æˆ‘ä¹‹å‰ä»æ²¡æ³¨æ„è¿‡ï¼Œä¹ æƒ¯äº†C/C++é£æ ¼ä¹‹åæ€»æ˜¯ä¼šåœ¨æ‰§è¡Œä¹‹å‰è€ƒè™‘æ‰€æœ‰æƒ…å†µï¼Œä½†ç¡®å®ä¸å¤Ÿä¼˜é›…ï¼Œä»Šåå¯ä»¥å°è¯•EAFPé£æ ¼ï¼ˆä»€ä¹ˆæ˜¯EAFPï¼‰ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>æ‚ä¸ƒæ‚å…«</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¦‚ä½•è®­ç»ƒGloVeä¸­æ–‡è¯å‘é‡]]></title>
    <url>%2F2018%2F08%2F05%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83GloVe%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F%2F</url>
    <content type="text"><![CDATA[å‡†å¤‡è¯­æ–™å‡†å¤‡å¥½è‡ªå·±çš„è¯­æ–™ï¼Œä¿å­˜ä¸ºtxtï¼Œæ¯è¡Œä¸€ä¸ªå¥å­æˆ–ä¸€æ®µè¯ï¼Œæ³¨æ„è¦åˆ†å¥½è¯ã€‚ å‡†å¤‡æºç ä»GitHubä¸‹è½½ä»£ç ï¼Œhttps://github.com/stanfordnlp/GloVeå°†è¯­æ–™corpus.txtæ”¾å…¥åˆ°Gloveçš„ä¸»æ–‡ä»¶å¤¹ä¸‹ã€‚ ä¿®æ”¹bashæ‰“å¼€demo.shï¼Œä¿®æ”¹ç›¸åº”çš„å†…å®¹ å› ä¸ºdemoé»˜è®¤æ˜¯ä¸‹è½½ç½‘ä¸Šçš„è¯­æ–™æ¥è®­ç»ƒçš„ï¼Œå› æ­¤å¦‚æœè¦è®­ç»ƒè‡ªå·±çš„è¯­æ–™ï¼Œéœ€è¦æ³¨é‡Šæ‰ ä¿®æ”¹å‚æ•°è®¾ç½®ï¼Œå°†CORPUSè®¾ç½®æˆè¯­æ–™çš„åå­— æ‰§è¡Œbashæ–‡ä»¶è¿›å…¥åˆ°ä¸»æ–‡ä»¶å¤¹ä¸‹ make bash demo.sh æ³¨æ„ï¼Œå¦‚æœè®­ç»ƒæ•°æ®è¾ƒå¤§ï¼Œåˆ™è®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œé‚£ä¹ˆå»ºè®®ä½¿ç”¨nohupæ¥è¿è¡Œç¨‹åº 1nohup bash demo.sh &gt;output.txt 2&gt;&amp;1 &amp; åç­‰è®­ç»ƒï¼Œæœ€åä¼šå¾—åˆ°vectors.txt ä»¥åŠå…¶ä»–çš„ç›¸åº”çš„æ–‡ä»¶ã€‚å¦‚æœè¦ç”¨gensimçš„word2vec loadè¿›æ¥ï¼Œé‚£ä¹ˆéœ€è¦åœ¨vectors.txtçš„ç¬¬ä¸€è¡ŒåŠ ä¸Švacob_size vector_sizeï¼Œç¬¬ä¸€ä¸ªæ•°æŒ‡æ˜ä¸€å…±æœ‰å¤šå°‘ä¸ªå‘é‡ï¼Œç¬¬äºŒä¸ªæ•°æŒ‡æ˜æ¯ä¸ªå‘é‡æœ‰å¤šå°‘ç»´ã€‚ å‚è€ƒhttps://www.cnblogs.com/echo-cheng/p/8561171.html]]></content>
      <tags>
        <tag>GloVe</tag>
        <tag>æ•™ç¨‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†2]]></title>
    <url>%2F2018%2F08%2F05%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%862%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Pytorch]é¿å…å†™å‡ºï¼š 1x = Variable(torch.zeros(...), requires_grad=True).cuda() è€Œæ˜¯åº”è¯¥è¦ï¼š 1x = Variable(torch.zeros(...).cuda(), requires_grad=True) Reference:https://discuss.pytorch.org/t/variable-grad-is-always-none-when-extending-autograd/12187 2ï¸âƒ£[Tf-idf]æœ¬å‘¨å› ä¸ºæ¯”èµ›çš„åŸå› äº†è§£äº†ä¸€ä¸‹å„ç§æ–‡æœ¬å»ºæ¨¡çš„æ–¹æ³•ã€‚Tf-idfèƒ½å¤Ÿå–å¾—ä¸é”™çš„æˆç»©ï¼Œä½†æœ‰ä¸€å®šçš„ç¼ºé™·ã€‚ TF-IDFç”¨äºå‘é‡ç©ºé—´æ¨¡å‹ï¼Œè¿›è¡Œæ–‡æ¡£ç›¸ä¼¼åº¦è®¡ç®—æ˜¯ç›¸å½“æœ‰æ•ˆçš„ã€‚ä½†åœ¨æ–‡æœ¬åˆ†ç±»ä¸­å•çº¯ä½¿ç”¨TF-IDFæ¥åˆ¤æ–­ä¸€ä¸ªç‰¹å¾æ˜¯å¦æœ‰åŒºåˆ†åº¦æ˜¯ä¸å¤Ÿçš„ã€‚ å®ƒä»…ä»…ç»¼åˆè€ƒè™‘äº†è¯¥è¯åœ¨æ–‡æ¡£ä¸­çš„é‡è¦ç¨‹åº¦å’Œæ–‡æ¡£åŒºåˆ†åº¦ã€‚ å®ƒæ²¡æœ‰è€ƒè™‘ç‰¹å¾è¯åœ¨ç±»é—´çš„åˆ†å¸ƒã€‚ç‰¹å¾é€‰æ‹©æ‰€é€‰æ‹©çš„ç‰¹å¾åº”è¯¥åœ¨æŸç±»å‡ºç°å¤šï¼Œè€Œå…¶å®ƒç±»å‡ºç°å°‘ï¼Œå³è€ƒå¯Ÿå„ç±»çš„æ–‡æ¡£é¢‘ç‡çš„å·®å¼‚ã€‚å¦‚æœä¸€ä¸ªç‰¹å¾è¯ï¼Œåœ¨å„ä¸ªç±»é—´åˆ†å¸ƒæ¯”è¾ƒå‡åŒ€ï¼Œè¿™æ ·çš„è¯å¯¹åˆ†ç±»åŸºæœ¬æ²¡æœ‰è´¡çŒ®ï¼›ä½†æ˜¯å¦‚æœä¸€ä¸ªç‰¹å¾è¯æ¯”è¾ƒé›†ä¸­çš„åˆ†å¸ƒåœ¨æŸä¸ªç±»ä¸­ï¼Œè€Œåœ¨å…¶å®ƒç±»ä¸­å‡ ä¹ä¸å‡ºç°ï¼Œè¿™æ ·çš„è¯å´èƒ½å¤Ÿå¾ˆå¥½ä»£è¡¨è¿™ä¸ªç±»çš„ç‰¹å¾ï¼Œè€ŒTF-IDFä¸èƒ½åŒºåˆ†è¿™ä¸¤ç§æƒ…å†µã€‚ å®ƒæ²¡æœ‰è€ƒè™‘ç‰¹å¾è¯åœ¨ç±»å†…éƒ¨æ–‡æ¡£ä¸­çš„åˆ†å¸ƒæƒ…å†µã€‚åœ¨ç±»å†…éƒ¨çš„æ–‡æ¡£ä¸­ï¼Œå¦‚æœç‰¹å¾è¯å‡åŒ€åˆ†å¸ƒåœ¨å…¶ä¸­ï¼Œåˆ™è¿™ä¸ªç‰¹å¾è¯èƒ½å¤Ÿå¾ˆå¥½çš„ä»£è¡¨è¿™ä¸ªç±»çš„ç‰¹å¾ï¼Œå¦‚æœåªåœ¨å‡ ç¯‡æ–‡æ¡£ä¸­å‡ºç°ï¼Œè€Œåœ¨æ­¤ç±»çš„å…¶å®ƒæ–‡æ¡£ä¸­ä¸å‡ºç°ï¼Œæ˜¾ç„¶è¿™æ ·çš„ç‰¹å¾è¯ä¸èƒ½å¤Ÿä»£è¡¨è¿™ä¸ªç±»çš„ç‰¹å¾ã€‚ Reference:https://blog.csdn.net/mmc2015/article/details/46771791 3ï¸âƒ£[å¡æ–¹æ£€éªŒCHI]åœ¨æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œç”¨äºé€‰æ‹©æœ€ç›¸å…³çš„ç‰¹å¾ã€‚ Reference:https://blog.csdn.net/blockheadls/article/details/49977361 4ï¸âƒ£[æ–‡æœ¬åˆ†ç±»]å„ç§æ–‡æœ¬åˆ†ç±»æ–¹æ³•çš„ç®€å•ä»‹ç»ã€‚ Reference:https://github.com/wangjiang0624/Note/blob/master/MachineLearning/æ–‡æœ¬åˆ†ç±».md 5ï¸âƒ£[Python]collectionsçš„ä¸¤ä¸ªæœ‰ç”¨çš„ç±» named_tupleï¼šå¿«é€Ÿå»ºç«‹ä¸€ä¸ªç±»ï¼Œä½¿å¾—å¯ä»¥ä½¿ç”¨å±æ€§æ¥è®¿é—®è€Œéç´¢å¼•ï¼Œæé«˜äº†ä»£ç å¯è¯»æ€§ 12345from collections import namedtuplePoint = namedtuple('Point',['x','y'])p = Point(1,2)print(p.x) # 1print(p.y) # 2 Counterï¼šç»Ÿè®¡å­—ç¬¦å‡ºç°çš„æ¬¡æ•° 1234567from collections import Countercount = Counter([...]).most_commom() #ä¼šæŒ‰ç…§å‡ºç°çš„æ¬¡æ•°æ’åºï¼Œé€šå¸¸å¯ç”¨äºæ„å»ºè¯å…¸for c in count: # cæ˜¯ä¸€ä¸ªtupleï¼Œc[0]æ˜¯è¯ï¼Œc[1]æ˜¯é¢‘ç‡ if c[1]&gt;= threshold: vocab.add_word(c[0]) else: break Counterç”¨æ³•ï¼šhttps://blog.csdn.net/u014755493/article/details/69812244 6ï¸âƒ£[nohup]æœ¬å‘¨åœ¨æœåŠ¡å™¨ä¸Šè·‘ä»£ç çš„æ—¶å€™é‡åˆ°ä¸€ä¸ªé—®é¢˜ï¼Œä½¿ç”¨nohupæ‰§è¡Œpythonç¨‹åºæ—¶ï¼Œå‘ç°è¾“å‡ºæ–‡ä»¶æ²¡æœ‰æ˜¾ç¤ºã€‚ä»¥ä¸ºæ˜¯ä»£ç çš„é—®é¢˜ï¼Œä½†ç»è¿‡æ’æŸ¥å¹¶éæ˜¯ä»£ç çš„é—®é¢˜ã€‚é€šè¿‡æŸ¥é˜…èµ„æ–™ï¼Œå‘ç°é—®é¢˜æ‰€åœ¨ï¼šå› ä¸ºpythonè¾“å‡ºæœ‰ç¼“å†²ï¼Œå¯¼è‡´outputä¸èƒ½é©¬ä¸Šçœ‹åˆ°è¾“å‡ºã€‚å®é™…ä¸Šï¼Œåœ¨ç­‰å¾…äº†ä¸€æ®µæ—¶é—´åï¼Œè¾“å‡ºæ–‡ä»¶ç»ˆäºæ˜¾ç¤ºå‡ºæ¥äº†ã€‚ è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨pythonçš„å‚æ•° -u ä½¿å¾—pythonä¸å¯ç”¨ç¼“å†²ã€‚ 1nohup python -u test.py &gt; nohup.out 2&gt;&amp;1 &amp; Reference:https://blog.csdn.net/sunlylorn/article/details/19127107 7ï¸âƒ£[hexoé…ç½®] mathjaxé…ç½®: https://www.jianshu.com/p/7ab21c7f0674 é…ç½®åŸŸå:https://www.zhihu.com/question/31377141 é…ç½®sitemap:http://www.yuan-ji.me/Hexo-ä¼˜åŒ–ï¼šæäº¤sitemapåŠè§£å†³ç™¾åº¦çˆ¬è™«æŠ“å–-GitHub-Pages-é—®é¢˜/ 8ï¸âƒ£[Paper]Learning Chinese Word Representations From Glyphs Of Characters ä½¿ç”¨å›¾åƒçš„å·ç§¯æ¥ç”Ÿæˆè¯å‘é‡:]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>Paper</tag>
        <tag>Tf-idf</tag>
        <tag>æ–‡æœ¬åˆ†ç±»</tag>
        <tag>hexo</tag>
        <tag>nohup</tag>
        <tag>CHI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•3]]></title>
    <url>%2F2018%2F08%2F05%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB3%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨åªæœ‰ç®€å•çš„ä»£ç ã€‚ 1ï¸âƒ£ä½¿ç”¨gensimè®­ç»ƒword2vec12345678910111213141516#encoding=utf-8from gensim.models import word2vecsentences=word2vec.Text8Corpus(u'åˆ†è¯åçš„çˆ½è‚¤æ°´è¯„è®º.txt') #sentence:[ [ a b ],[c d]... ]model=word2vec.Word2Vec(sentences, size=50) #size:dim y2=model.similarity(u"å¥½", u"è¿˜è¡Œ") #è®¡ç®—ç›¸ä¼¼åº¦print(y2)for i in model.most_similar(u"æ»‹æ¶¦"): print i[0],i[1] #ä¿å­˜model.save('/model/word2vec_model')new_model=gensim.models.Word2Vec.load('/model/word2vec_model') 2ï¸âƒ£ä½¿ç”¨Counterå»ºç«‹è¯è¡¨123456789def build_dict(dataset,min_freq=5): dictionary=Dictionary() count=Counter(flat(dataset)).most_common() for c in count: if c[1]&gt;=min_freq: dictionary.add_word(c[0]) else: break return dictionary]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯3]]></title>
    <url>%2F2018%2F08%2F05%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D3%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨èƒŒçš„éƒ½æ˜¯æ¯”è¾ƒç®€å•çš„ã€‚ 1ï¸âƒ£ æŠŠé…’é—®æœˆ[å”] æç™½é’å¤©æœ‰æœˆæ¥å‡ æ—¶ï¼Ÿæˆ‘ä»Šåœæ¯ä¸€é—®ä¹‹ã€‚äººæ”€æ˜æœˆä¸å¯å¾—ï¼Œæœˆè¡Œå´ä¸äººç›¸éšã€‚çšå¦‚é£é•œä¸´ä¸¹é˜™ï¼Œç»¿çƒŸç­å°½æ¸…è¾‰å‘ã€‚ä½†è§å®µä»æµ·ä¸Šæ¥ï¼Œå®çŸ¥æ™“å‘äº‘é—´æ²¡ã€‚ç™½å…”æ£è¯ç§‹å¤æ˜¥ï¼Œå«¦å¨¥å­¤æ –ä¸è°é‚»ï¼Ÿä»Šäººä¸è§å¤æ—¶æœˆï¼Œä»Šæœˆæ›¾ç»ç…§å¤äººã€‚å¤äººä»Šäººè‹¥æµæ°´ï¼Œå…±çœ‹æ˜æœˆçš†å¦‚æ­¤ã€‚å”¯æ„¿å½“æ­Œå¯¹é…’æ—¶ï¼Œæœˆå…‰é•¿ç…§é‡‘æ¨½é‡Œã€‚ http://m.xichuangzhu.com/work/57b8f6f4165abd0054bf8c13 2ï¸âƒ£ é‡‘ç¼•è¡£[å”] æœç§‹å¨˜åŠå›è«æƒœé‡‘ç¼•è¡£ï¼ŒåŠå›æƒœå–å°‘å¹´æ—¶ã€‚èŠ±å¼€å ªæŠ˜ç›´é¡»æŠ˜ï¼Œè«å¾…æ— èŠ±ç©ºæŠ˜æã€‚ http://m.xichuangzhu.com/work/57b92bdca633bd00665eb99e 3ï¸âƒ£ åŒ—é’è[å”] æå•†éšæ®‹é˜³è¥¿å…¥å´¦ï¼ŒèŒ…å±‹è®¿å­¤åƒ§ã€‚è½å¶äººä½•åœ¨ï¼Œå¯’äº‘è·¯å‡ å±‚ã€‚ç‹¬æ•²åˆå¤œç£¬ï¼Œé—²å€šä¸€æè—¤ã€‚ä¸–ç•Œå¾®å°˜é‡Œï¼Œå¾å®çˆ±ä¸æ†ã€‚ å´¦ï¼ˆyÄnï¼‰ï¼šå³â€œå´¦åµ«ï¼ˆzÄ«ï¼‰â€ï¼Œå±±åï¼Œåœ¨ç”˜è‚ƒã€‚å¤æ—¶å¸¸ç”¨æ¥æŒ‡å¤ªé˜³è½å±±çš„åœ°æ–¹ã€‚ç£¬ï¼ˆqÃ¬ngï¼‰ï¼šå¤ä»£æ‰“å‡»ä¹å™¨ï¼Œå½¢çŠ¶åƒæ›²å°ºï¼Œç”¨ç‰ã€çŸ³åˆ¶æˆï¼Œå¯æ‚¬æŒ‚ã€‚ http://m.xichuangzhu.com/work/57b8ee240a2b58005c91c99e 4ï¸âƒ£ å¤æ—¥ç»å¥[å®‹] ææ¸…ç…§ç”Ÿå½“ä½œäººæ°ï¼Œæ­»äº¦ä¸ºé¬¼é›„ã€‚è‡³ä»Šæ€é¡¹ç¾½ï¼Œä¸è‚¯è¿‡æ±Ÿä¸œã€‚ http://m.xichuangzhu.com/work/57b911dac4c97100558fb30e 5ï¸âƒ£ é›¨éœ–é“ƒ[å®‹] æŸ³æ°¸å¯’è‰å‡„åˆ‡ï¼Œå¯¹é•¿äº­æ™šï¼Œéª¤é›¨åˆæ­‡ã€‚éƒ½é—¨å¸é¥®æ— ç»ªï¼Œç•™æ‹å¤„ï¼Œå…°èˆŸå‚¬å‘ã€‚æ‰§æ‰‹ç›¸çœ‹æ³ªçœ¼ï¼Œç«Ÿæ— è¯­å‡å™ã€‚å¿µå»å»ï¼Œåƒé‡ŒçƒŸæ³¢ï¼Œæš®éœ­æ²‰æ²‰æ¥šå¤©é˜”ã€‚å¤šæƒ…è‡ªå¤ä¼¤ç¦»åˆ«ï¼Œæ›´é‚£å ªã€å†·è½æ¸…ç§‹èŠ‚ã€‚ä»Šå®µé…’é†’ä½•å¤„ï¼Ÿæ¨æŸ³å²¸ï¼Œæ™“é£æ®‹æœˆã€‚æ­¤å»ç»å¹´ï¼Œåº”æ˜¯è‰¯è¾°å¥½æ™¯è™šè®¾ã€‚ä¾¿çºµæœ‰åƒç§é£æƒ…ï¼Œæ›´ä¸ä½•äººè¯´ï¼Ÿ http://m.xichuangzhu.com/work/57ad742f5bbb500062bc7c9c]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³äºPytorchä¸­gradçš„ç†è§£]]></title>
    <url>%2F2018%2F08%2F03%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADgrad%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[äº‹æƒ…èµ·æºäºæˆ‘å†™äº†ä¸€ä¸ªCNNç”¨äºæ–‡æœ¬åˆ†ç±»ï¼Œä½†lossä¸€ç›´æ²¡é™ï¼Œå› æ­¤æˆ‘å°è¯•print(loss.grad)çš„gradï¼Œå‘ç°ç¥å¥‡çš„æ˜¯loss gradæ˜¾ç¤ºä¸ºNoneï¼Œæ¥ç€å°è¯•print(y_pred.grad)ï¼ŒåŒæ ·æ˜¯Noneï¼Œä½†å†print losså’Œy_predçš„requires_gradå‘ç°æ˜¯æ­£å¸¸çš„Trueã€‚ åœ¨æŸ¥é˜…äº†èµ„æ–™ï¼Œä»¥åŠé—®äº†å­¦é•¿ä¹‹åå‘ç°åŸæ¥å¹¶ä¸æ˜¯bugï¼Œè€Œæ˜¯å› ä¸ºï¼ŒPytorché»˜è®¤ä¸ä¼šä¿å­˜ä¸­é—´èŠ‚ç‚¹(intermediate variable)çš„gradï¼Œæ­¤ä¸¾æ˜¯ä¸ºäº†èŠ‚çœå†…å­˜ã€‚ By default, gradients are only retained for leaf variables. non-leaf variablesâ€™ gradients are not retained to be inspected later. This was done by design, to save memory. https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94 å®é™…ä¸Šå¯ä»¥é€šè¿‡retain_grad()æˆ–è€…hookæ¥æŸ¥çœ‹ä¸­é—´èŠ‚ç‚¹çš„gradã€‚ æˆ‘åé¢å°è¯•printäº†å¶å­èŠ‚ç‚¹ï¼Œå¦‚ print(CNN_model.fc.weight.grad)ï¼Œæœ€ç»ˆè·å¾—äº†æ­£ç¡®çš„gradã€‚ psï¼šæ‰€è°“ä¸­é—´èŠ‚ç‚¹ï¼Œæ˜¯ç”±å…¶ä»–èŠ‚ç‚¹è®¡ç®—æ‰€å¾—çš„tensorï¼Œè€Œå¶å­èŠ‚ç‚¹åˆ™æ˜¯è‡ªå·±å®šä¹‰å‡ºæ¥çš„ã€‚ æœ€åæˆ‘å‘ç°ï¼ŒåŸæ¥lossä¸€ç›´æ²¡é™çš„åŸå› æ˜¯å› ä¸ºæˆ‘å®šä¹‰çš„CNNè¿‡äºå¤æ‚ï¼Œå¹¶ä¸”æ•°æ®é›†åå°ï¼Œæ— æ³•å¿«é€Ÿæ”¶æ•›å¯¼è‡´çš„ã€‚]]></content>
      <tags>
        <tag>é‡åˆ°çš„é—®é¢˜</tag>
        <tag>Pytorch</tag>
        <tag>grad</tag>
        <tag>bug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linuxå¸¸ç”¨å‘½ä»¤]]></title>
    <url>%2F2018%2F08%2F03%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[å‘½ä»¤è®°å½•è‡ªå·±å¸¸ç”¨çš„å‘½ä»¤ã€‚ 1ï¸âƒ£lsï¼šæ˜¾å¼å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶å’Œç›®å½• -a åŒ…æ‹¬éšè—æ–‡ä»¶ -h å°†æ–‡ä»¶çš„å®¹é‡ä»¥æ˜“è¯»æ–¹å¼åˆ—å‡ºï¼ˆé…åˆ-sä½¿ç”¨ï¼‰ -s ä»¥å—æ•°å½¢å¼æ˜¾ç¤ºæ¯ä¸ªæ–‡ä»¶åˆ†é…çš„å°ºå¯¸ -l ä»¥è¾ƒé•¿æ ¼å¼åˆ—å‡ºä¿¡æ¯ï¼Œå¯ä»¥ç›´æ¥å†™æˆ ll 2ï¸âƒ£cd åˆ°è¾¾æŒ‡å®šåœ°å€ 3ï¸âƒ£kill æ€æ­»ç¨‹åº -l ä¿¡æ¯ç¼–å·ã€‚å½“l=9æ—¶ï¼Œæ— æ¡ä»¶ç»ˆæ­¢ï¼Œå…¶ä»–ä¿¡å·å¯èƒ½å¿½ç•¥ killall -u æ€æ­»è¯¥ç”¨æˆ·å…¨éƒ¨è¿›ç¨‹ 4ï¸âƒ£ps æŠ¥å‘Šå½“å‰ç³»ç»Ÿçš„è¿›ç¨‹çŠ¶æ€ -a æ‰€æœ‰ -p æŒ‡å®šç¨‹åº -u æŒ‡å®šç”¨æˆ· -x åˆ—å‡ºè¯¥ç”¨æˆ·çš„è¿›ç¨‹çš„è¯¦ç»†ä¿¡æ¯(æˆ‘çš„ç†è§£åº”è¯¥æ˜¯) å¦‚ï¼š 5ï¸âƒ£htop æ¯”topæ›´ä¼˜ï¼Œäº¤äº’æ›´å¥½ï¼ŒåŒæ—¶å¯ä»¥ç›´è§‚çœ‹åˆ°èµ„æºå ç”¨æƒ…å†µåŸºæœ¬å‘½ä»¤ä¸topä¸€è‡´ 6ï¸âƒ£topï¼šåŠ¨æ€æŸ¥çœ‹ç³»ç»Ÿè¿è¡ŒçŠ¶æ€ -u æŒ‡å®šç”¨æˆ·å -p æŒ‡å®šè¿›ç¨‹ 7ï¸âƒ£nvidia-smi æŸ¥çœ‹æ˜¾å¡çŠ¶æ€watch nvidia-smi å®æ—¶æŸ¥çœ‹æ˜¾å¡çŠ¶æ€ï¼Œå®šæ—¶åˆ·æ–° 8ï¸âƒ£tail æ˜¾ç¤ºæŒ‡å®šæ–‡ä»¶çš„æœ«å°¾è‹¥å¹²è¡Œ -f æ˜¾ç¤ºæ–‡ä»¶æœ€æ–°è¿½åŠ çš„å†…å®¹ -n æ˜¾ç¤ºæ–‡ä»¶å°¾éƒ¨nè¡Œå†…å®¹ -c æ˜¾ç¤ºæ–‡ä»¶å°¾éƒ¨æœ€åcä¸ªå­—ç¬¦ å¦‚ï¼š 123tail file æ˜¾ç¤ºæœ€å10è¡Œtail -n +20 file æ˜¾ç¤ºä»ç¬¬20è¡Œè‡³æœ«å°¾tail -c 10 file æ˜¾ç¤ºæ–‡ä»¶fileçš„æœ€å10ä¸ªå­—ç¬¦ ------- 9ï¸âƒ£echo ç”¨äºæ‰“å°æŒ‡å®šçš„å­—ç¬¦ä¸² ğŸ”Ÿwhich ç”¨äºæŸ¥æ‰¾å¹¶æ˜¾ç¤ºç»™å®šå‘½ä»¤çš„ç»å¯¹è·¯å¾„ï¼ŒwhichæŒ‡ä»¤ä¼šåœ¨ç¯å¢ƒå˜é‡$PATHè®¾ç½®çš„ç›®å½•é‡ŒæŸ¥æ‰¾ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶ã€‚ä½¿ç”¨whichå‘½ä»¤ï¼Œå¯ä»¥çœ‹åˆ°æŸä¸ªç³»ç»Ÿå‘½ä»¤æ˜¯å¦å­˜åœ¨ï¼Œä»¥åŠæ‰§è¡Œçš„æ˜¯å“ªä¸ªä½ç½®çš„å‘½ä»¤ã€‚å¦‚ï¼š 1ï¸âƒ£1ï¸âƒ£nohup å°†ç¨‹åºä»¥å¿½ç•¥æŒ‚èµ·ä¿¡å·çš„æ–¹å¼è¿è¡Œï¼Œç»å¸¸ç”¨äºåœ¨æœåŠ¡å™¨è·‘ä»£ç å¦‚ï¼š1nohup python xxx.py &gt;output.txt 2&gt;&amp;1 &amp; å³ï¼Œå°†è¾“å‡ºé‡å®šå‘åˆ°output.txt ï¼›æœ€åä¸€ä¸ª&amp;è¡¨ç¤ºåå°æŒ‚èµ· 1ï¸âƒ£2ï¸âƒ£cp å¤åˆ¶æ–‡ä»¶ cp [æ–‡ä»¶] [ç›®æ ‡æ–‡ä»¶å¤¹] -r é€’å½’å¤åˆ¶ï¼Œç”¨äºç›®å½•çš„å¤åˆ¶ 1ï¸âƒ£3ï¸âƒ£mv ç§»åŠ¨æ–‡ä»¶ã€ç›®å½•æˆ–æ›´å mv [æ–‡ä»¶/æ–‡ä»¶å¤¹] [æ–‡ä»¶å¤¹] -f å¼ºåˆ¶ï¼Œå½“ç›®æ ‡æ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥è¦†ç›– -i ä¼šè¯¢é—® 1ï¸âƒ£4ï¸âƒ£rm åˆ é™¤æ–‡ä»¶æˆ–ç›®å½• -f å¼ºåˆ¶åˆ é™¤ -r é€’å½’åˆ é™¤ï¼Œç”¨äºç›®å½•åˆ é™¤ 1ï¸âƒ£5ï¸âƒ£file ç”¨äºåˆ¤æ–­æ–‡ä»¶çš„åŸºæœ¬æ•°æ®å¦‚ï¼š 1ï¸âƒ£6ï¸âƒ£tar å¯¹æ–‡ä»¶æ‰“åŒ…/å‹ç¼© -t æŸ¥çœ‹æ‰“åŒ…æ–‡ä»¶çš„å†…å®¹å«æœ‰å“ªäº›æ–‡ä»¶å -x è§£å‹ç¼© -c æ–°å»ºæ‰“åŒ…æ–‡ä»¶ -C æŒ‡å®šå‹ç¼©/è§£å‹ç›®å½• -v è§£å‹/å‹ç¼©è¿‡ç¨‹ä¸­å°†å¤„ç†çš„æ–‡ä»¶åæ˜¾ç¤ºå‡ºæ¥å¸¸ç”¨çš„ï¼š 123å‹ç¼©ï¼štar -jcv -f filename.tar.bz2 è¦è¢«å¤„ç†çš„æ–‡ä»¶æˆ–ç›®å½•åç§°æŸ¥è¯¢ï¼štar -jtv -f filename.tar.bz2è§£å‹ï¼štar -jxv -f filename.tar.bz2 -C æ¬²è§£å‹ç¼©çš„ç›®å½• 1ï¸âƒ£7ï¸âƒ£wc word count ç»Ÿè®¡æ–‡ä»¶å†…å®¹ä¿¡æ¯ï¼Œå¦‚è¡Œæ•°ã€å­—ç¬¦æ•° -l æ˜¾ç¤ºæ–‡ä»¶è¡Œæ•° -c æ˜¾ç¤ºå­—èŠ‚æ•° -m æ˜¾ç¤ºå­—ç¬¦æ•° -w æ˜¾ç¤ºå­—æ•° å­—è¢«å®šä¹‰ä¸ºç”±ç©ºç™½ã€è·³æ ¼ã€æ¢è¡Œå­—ç¬¦åˆ†éš”çš„å­—ç¬¦ä¸² -L æ˜¾ç¤ºæœ€é•¿è¡Œçš„é•¿åº¦ ä¸åŠ å‚æ•°ï¼Œæ‰€æœ‰çš„éƒ½æ˜¾ç¤ºï¼Œä¾æ¬¡æ˜¯è¡Œæ•°ã€å•è¯æ•°ã€å­—èŠ‚æ•°ã€æ–‡ä»¶å 1ï¸âƒ£8ï¸âƒ£df æ˜¾ç¤ºç£ç›˜ç›¸å…³ä¿¡æ¯ -h ä»¥å¯è¯»æ€§è¾ƒé«˜çš„æ–¹å¼æ˜¾ç¤ºä¿¡æ¯ 1ï¸âƒ£9ï¸âƒ£scp æœåŠ¡å™¨ä¹‹é—´çš„æ–‡ä»¶å¤åˆ¶ å¦‚: 1scp -r /test1 zhlin@123.12.1.12:/home/zhlin âœ¨å¿«æ·é”®Ctrl+a è·³åˆ°è¡Œé¦–Ctrl+c é€€å‡ºå½“å‰è¿›ç¨‹Ctrl+e è·³åˆ°é¡µå°¾Ctrl+k åˆ é™¤å½“å‰å…‰æ ‡åé¢çš„æ–‡å­—Ctrl+l æ¸…å±ï¼Œç­‰ä»·äºclearCtrl+r æœç´¢ä¹‹å‰æ‰“è¿‡çš„å‘½ä»¤Ctrl+u åˆ é™¤å½“å‰å…‰æ ‡å‰é¢çš„æ–‡å­—âœ¨Ctrl+å·¦å³é”® å•è¯ä¹‹é—´è·³è½¬ åœ¨Macä¸Šå¯ä»¥ä½¿ç”¨option+å·¦å³é”®Ctrl+y è¿›è¡Œæ¢å¤åˆ é™¤Ctrl+z å°†å½“å‰è¿›ç¨‹è½¬åˆ°åå°ï¼Œä½¿ç”¨fgæ¢å¤ Referencehttps://blog.csdn.net/leo_618/article/details/53003111 â€”â€”â€”-æŒç»­æ›´æ–°â€”â€”â€”-]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>æŠ€å·§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä¸€ä¸ªå…³äºyieldçš„é‡æ–°è®¤è¯†]]></title>
    <url>%2F2018%2F07%2F31%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E4%B8%80%E4%B8%AA%E5%85%B3%E4%BA%8Eyield%E7%9A%84%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86%2F</url>
    <content type="text"><![CDATA[ä»Šå¤©é‡åˆ°äº†ä¸€ä¸ªç¥å¥‡çš„â€bugâ€ï¼Œè®©æˆ‘å¯¹yieldçš„ç†è§£æ›´æ·±ä¸€æ­¥ã€‚ è¿™æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œæˆ‘æœ¬æ¥æ‰“ç®—è¯•ç€printä¸€ä¸‹lineå†…éƒ¨çš„æ ¼å¼å’Œå†…å®¹ã€‚ è¿™æ˜¯è°ƒç”¨çš„ä¸»å‡½æ•°ï¼š ç»“æœè·‘å‡ºçš„ç»“æœæ˜¯ï¼š ï¼Ÿï¼Ÿï¼Ÿ æˆ‘å°è¯•åœ¨å‡½æ•°çš„å¼€å¤´æ·»åŠ printï¼š ç»“æœä»ç„¶æ²¡æœ‰ä»»ä½•çš„è¾“å‡ºã€‚ æˆ‘è¯•ç€åœ¨mainå‡½æ•°æ·»åŠ printï¼š ç»“æœï¼š ä¹Ÿå°±æ˜¯è¯´ï¼Œæ ¹æœ¬æ²¡æœ‰è¿›å…¥åˆ°get_dataset_from_txtå‡½æ•°å•Šã€‚ æˆ‘ä»¥ä¸ºæ˜¯pycharmçš„é—®é¢˜è¿˜é‡å¯äº†ä¸€éï¼Œç„¶è€Œå¹¶æ²¡æœ‰ä»»ä½•ä½œç”¨ã€‚é—®äº†å…¶ä»–äººï¼Œä»–ä»¬ä¹Ÿè§‰å¾—å¾ˆç¥å¥‡ã€‚æœ€åä¸€ä¸ªåŒå­¦çœ‹äº†ä¸€ä¸‹å‡½æ•°ï¼Œå‘ç°äº†é—®é¢˜æ‰€åœ¨ï¼šyield æˆ‘çªç„¶æƒ³èµ·æ¥ï¼Œyieldè¿”å›çš„æ˜¯ä¸€ä¸ªgeneratorï¼Œåªæœ‰åœ¨å¯¹generatorè¿›è¡Œéå†æ—¶ï¼Œæ‰ä¼šå¼€å§‹è¿è¡Œâ€¦ äºæ˜¯ï¼Œæˆ‘è¯•ç€è¿™ä¹ˆå†™ï¼Œè¯•ç€å¯¹generatoréå†ï¼š è™½ç„¶æŠ¥é”™äº†ï¼Œä½†å‡½æ•°ç»ˆäºæ˜¯è¿›å»äº†â€¦ ç»“è®ºï¼šæœ‰yieldçš„å‡½æ•°ä¼šè¿”å›ä¸€ä¸ªgeneratorï¼Œå½“å¯¹å…¶è¿›è¡Œéå†æ—¶ï¼Œå‡½æ•°æ‰ä¼šå¼€å§‹è¿è¡Œã€‚]]></content>
      <tags>
        <tag>é‡åˆ°çš„é—®é¢˜</tag>
        <tag>Python</tag>
        <tag>yield</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•2]]></title>
    <url>%2F2018%2F07%2F29%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB2%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨ä¸»è¦çœ‹äº†AllenNLP/ELMOçš„ä»£ç ï¼Œä½†å¹¶æ²¡æœ‰æ‰¾åˆ°å¾ˆå¤šå¯å¤ç”¨çš„ä»£ç ã€‚æœ¬å‘¨ä¹Ÿæ²¡æœ‰æ¯”è¾ƒæœ‰æ„ä¹‰çš„ä»£ç ã€‚ 1ï¸âƒ£get_time_diffè·å–å·²ä½¿ç”¨çš„æ—¶é—´123456789import timefrom datetime import timedeltastart_time=time.time()def get_time_dif(start_time): """è·å–å·²ä½¿ç”¨æ—¶é—´""" end_time = time.time() time_dif = end_time - start_time return timedelta(seconds=int(round(time_dif))) 2ï¸âƒ£parserä½¿ç”¨12345678parser = argparse.ArgumentParser()parser.add_argument('--save_dir', help='Location of checkpoint files')parser.add_argument('--vocab_file', help='Vocabulary file')parser.add_argument('--train_prefix', help='Prefix for train files')args = parser.parse_args() main(args) #ä½¿ç”¨]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨ç¢ç‰‡çŸ¥è¯†1]]></title>
    <url>%2F2018%2F07%2F29%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%861%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£[Python]assertç”¨æ³•ï¼š assert expressionç­‰ä»·äºif not expression: raise AssertionError 2ï¸âƒ£[Pytorch]Pytorch viewï¼šåˆ›å»ºä¸€ä¸ªæ–°çš„tensorï¼Œä½†ä»–ä»¬çš„dataæ˜¯å…±äº«çš„ã€‚ 3ï¸âƒ£[Pytorch]åœ¨Pytorchä¸­ï¼Œembeddingçš„indexæ˜¯ä¸èƒ½requires_grad=Trueçš„ï¼Œå¦åˆ™ä¼šå‡ºé”™ã€‚https://github.com/pytorch/pytorch/issues/7021 ä¹‹å‰çœ‹è¿‡ä¸€ä»½ä»£ç ï¼Œè®¾ç½®volatile=falseä½†æ²¡æœ‰å‡ºé”™ï¼Œæ˜¯å› ä¸ºåœ¨Pytorch0.4ä¹‹åvolatileå·²ç»è¢«å¼ƒç”¨äº†ï¼Œå› æ­¤volatile=falseä¸èµ·ä½œç”¨ï¼Œè€Œé»˜è®¤requires_grad=false 4ï¸âƒ£[Pytorch]åœ¨Pytorchä¸­ï¼Œnn.Linear(self.hidden_dim,self.vocab_size)çš„ç»´åº¦æ˜¯vocab_sizehidden_dimï¼Œä¹‹å‰å±…ç„¶æ²¡æœ‰æ³¨æ„åˆ°è¿™ä¸ªé—®é¢˜ã€‚å› ä¸ºnn.Linearçš„*ç¬¬ä¸€ä¸ªå‚æ•°è¡¨ç¤ºè¾“å…¥ç»´åº¦ï¼Œç¬¬äºŒä¸ªå‚æ•°è¡¨ç¤ºè¾“å‡ºç»´åº¦ 5ï¸âƒ£[Pytorch]Pytorchä¸­ï¼Œä½¿ç”¨viewä¸€èˆ¬æ¥è¯´å¿…é¡»è¦ç”¨ .contiguous()ã€‚ä¹Ÿå³ï¼š 1batch.view(batch_size, -1).t().contiguous() contiguous()çš„å®˜æ–¹è§£é‡Šï¼šhttps://discuss.pytorch.org/t/runtimeerror-input-is-not-contiguous/930 It means that your tensor is not a single block of memory, but a block with holes. view can be only used with contiguous tensors, so if you need to use it here, just call .contiguous() before. ä¹Ÿå°±æ˜¯è¯´ï¼Œcontiguousä¼šå°†æ•°æ®å­˜åˆ°ä¸€ä¸ªè¿ç»­çš„ç©ºé—´å†…ï¼ˆblockï¼‰ã€‚ 6ï¸âƒ£[Pytorch]è°ƒç”¨Cross_entropyæ—¶ï¼ŒPytorchä¼šå¸®åŠ©ä½ åŠ logå’Œsoftmaxã€‚ 7ï¸âƒ£[Paper]Sliced_RNN å°†RNNåˆ†å—ä»¥æé«˜å¹¶è¡Œæ€§ï¼Œç”šè‡³æ¯å±‚çš„RNNéƒ½å¯ä»¥ä¸ä¸€æ ·ï¼Œè¾¾åˆ°æŠ½å–ä¸åŒç¨‹åº¦çš„æŠ½è±¡è¯­ä¹‰ä¿¡æ¯çš„ç›®çš„ã€‚å®éªŒè¯æ˜ï¼Œåœ¨ä¸åŒä»»åŠ¡ä¸Šéƒ½æœ‰ä¸€å®šçš„æå‡ï¼Œä½†é€Ÿåº¦çš„æå‡å¾ˆå¤§ã€‚ 8ï¸âƒ£[Tf-idf]è®¡ç®—è¯è¯­å¯¹äºå¥å­çš„é‡è¦ç¨‹åº¦ https://zh.wikipedia.org/wiki/Tf-idf tfæ˜¯è¯é¢‘ï¼Œidfæ˜¯é€†å‘æ–‡ä»¶é¢‘ç‡ã€‚ä¹Ÿå³å¦‚æœè¯åœ¨è¯¥å¥å‡ºç°çš„æ¬¡æ•°è¶Šå¤šï¼Œåœ¨æ‰€æœ‰æ–‡æœ¬çš„å‡ºç°æ¬¡æ•°è¶Šå°‘ï¼Œåˆ™è¯å¯¹äºå¥å­çš„é‡è¦ç¨‹åº¦è¶Šé«˜ã€‚ 9ï¸âƒ£[Numpy]åœ¨Numpyä¸­ï¼Œä¸€ä¸ªåˆ—è¡¨è™½ç„¶æ˜¯æ¨ªç€è¡¨ç¤ºçš„ï¼Œä½†å®ƒæ˜¯åˆ—å‘é‡ã€‚æˆ‘ä¹‹å‰å±…ç„¶æ²¡æœ‰æ³¨æ„åˆ°è¿™ä¸ªé—®é¢˜ã€‚]]></content>
      <tags>
        <tag>ç¢ç‰‡çŸ¥è¯†</tag>
        <tag>Pytorch</tag>
        <tag>Python</tag>
        <tag>Paper</tag>
        <tag>Tf-idf</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Macé…ç½®å¤æ—¦æœ‰çº¿ç½‘]]></title>
    <url>%2F2018%2F07%2F29%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2FMac%E9%85%8D%E7%BD%AE%E5%A4%8D%E6%97%A6%E6%9C%89%E7%BA%BF%E7%BD%91%2F</url>
    <content type="text"><![CDATA[é…ç½®ipã€å­ç½‘æ©ç ã€DNSã€è·¯ç”±å™¨æœ‰çº¿ä¼¼ä¹ä¸æ”¯æŒDHCPï¼Œå› æ­¤åªå¥½è‡ªå·±è®¾ç½®ã€‚é¦–å…ˆè¿æ¥ä¸Šæœ‰çº¿ï¼Œå°†é…ç½®iPv4é€‰ä¸ºæ‰‹åŠ¨ã€‚é—®å®éªŒå®¤çš„å­¦é•¿å…·ä½“çš„ipåœ°å€ã€å­ç½‘æ©ç ã€è·¯ç”±å™¨ã€DNSæœåŠ¡å™¨ã€‚å…¶ä¸­ipåœ°å€æœ€åä¸‰ä½è¦è‡ªå·±è®¾å®šï¼Œåªè¦ä¸å’Œå…¶ä»–äººå†²çªå°±å¥½ã€‚ æ‰‹åŠ¨è®¤è¯åˆ°è®¤è¯å¹³å°ï¼Œä¸‹è½½Macå®¢æˆ·ç«¯ï¼Œå…¶å®å°±æ˜¯ä¸€ä¸ª.shæ–‡ä»¶ï¼šhttp://10.108.255.249/srun_portal_pc.php?ac_id=1&amp;&amp;phone=1 ç„¶åï¼Œæ‰“å¼€æ–‡ä»¶é…ç½®ç”¨æˆ·åå¯†ç ï¼Œæ³¨æ„åˆ°ç­‰å·åé¢è¦æœ‰åŒå¼•å·ï¼š ä¿å­˜å¹¶æ”¾å…¥ç»ˆç«¯è¿è¡Œï¼Œæ¥ä¸‹æ¥å°±å¯ä»¥ä½¿ç”¨æœ‰çº¿ç½‘äº†ã€‚ å…¶ä»–ä¼¼ä¹ï¼Œæ¯æ¬¡é‡æ–°è¿æ¥éƒ½è¦è¿™æ ·é…ç½®ï¼Œæˆ‘æ²¡æœ‰è¯•è¿‡ä¸æ¸…æ¥šï¼›æœ‰çº¿ç½‘å¥½åƒä¹Ÿæ²¡æœ‰æ¯”æ— çº¿ç½‘å¿«å¤šå°‘ï¼Œä½†åº”è¯¥ä¼šç¨³å®šä¸€äº›ã€‚]]></content>
      <tags>
        <tag>ç½‘ç»œ</tag>
        <tag>é…ç½®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sshå¿«é€Ÿç™»å½•é…ç½®]]></title>
    <url>%2F2018%2F07%2F29%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2Fssh%E5%BF%AB%E9%80%9F%E7%99%BB%E5%BD%95%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Motivationåˆ†é…äº†æœåŠ¡å™¨ä¹‹åï¼Œæ¯æ¬¡è¦sshè¿›å…¥éƒ½å¾ˆéº»çƒ¦ï¼šssh user_name@ip_address ç„¶åè¿˜è¦è¾“å…¥å¯†ç ã€‚ ç‰¹åˆ«æ˜¯å¦‚æœåˆ†é…äº†å¤šä¸ªæœåŠ¡å™¨ï¼Œé‚£æœ‰æ—¶å€™è¿˜å®¹æ˜“å¿˜è®°ipåœ°å€ã€‚å› æ­¤å¦‚æœèƒ½å¤Ÿä¸€æ¡å‘½ä»¤å°±è¿›å…¥æœåŠ¡å™¨èƒ½å¤Ÿå‡å°‘éº»çƒ¦ã€‚ä¸»è¦æœ‰ä¸‰ç‚¹ï¼š åˆ›å»ºrsa key ä¸Šä¼ public keyåˆ°æœåŠ¡å™¨ è®¾ç½®alias é…ç½®åˆ›å»ºrsa keyåœ¨ç»ˆç«¯è¾“å…¥å‘½ä»¤ï¼š 1ssh-keygen -t rsa å½“ç„¶å¦‚æœä»¥å‰æœ‰åˆ›å»ºè¿‡çš„å¯ä»¥ä¸ç”¨ã€‚ ç»“æœï¼š ä¸Šä¼ public keyåˆ°æœåŠ¡å™¨ä½¿ç”¨å‘½ä»¤ï¼š1ssh-copy-id -i ~/.ssh/id_rsa.pub user@127.0.0.1 è¾“å…¥å¯†ç å³å¯ ç»“æœï¼š è®¾ç½®aliaså®Œæˆä»¥ä¸Šæ­¥éª¤å°±å¯ä»¥ä¸è¾“å…¥å¯†ç ç™»å½•ï¼Œä½†è¿˜æ˜¯éœ€è¦è¾“å…¥ipåœ°å€å’Œç”¨æˆ·åï¼Œä¸ºäº†æ›´ç®€åŒ–æ“ä½œï¼Œç»™å‘½ä»¤èµ·ä¸ªåˆ«åã€‚éœ€è¦é…ç½® .bash_profileæ–‡ä»¶ã€‚è¾“å…¥å‘½ä»¤: 1vim ~/.bash_profile åœ¨æ–‡ä»¶åé¢æ·»åŠ ä»¥ä¸‹æ–‡å­—ï¼š 123# alias alias sshÃ—Ã—Ã—=&quot;ssh user_name@ip_address&quot;alias sshÃ—Ã—Ã—=&quot;ssh user_name@ip_address&quot; å…¶ä¸­ Ã—Ã—Ã—æ˜¯ä½ è‡ªå·±èµ·çš„åå­—ï¼Œå¯ä»¥æ˜¯æœåŠ¡å™¨çš„åå­—ï¼Œuser_nameå’Œip_addressæ˜¯è‡ªå·±æœåŠ¡å™¨çš„ç”¨æˆ·åå’Œåœ°å€ã€‚ä¿å­˜æ›´æ”¹é€€å‡ºã€‚ ç„¶åè¿˜è¦ä½¿å…¶ç”Ÿæ•ˆ: 1source ~/.bash_profile è¿™æ ·ï¼Œè¾“å…¥åˆ«åï¼Œå°±å¯ä»¥ç›´æ¥ç™»å½•äº†ï¼š å‚è€ƒhttps://www.jianshu.com/p/66d658c7cb9e]]></content>
      <tags>
        <tag>é…ç½®</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å…³äºå›°æƒ‘åº¦]]></title>
    <url>%2F2018%2F07%2F29%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8E%E5%9B%B0%E6%83%91%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[å‰å‡ å¤©åœ¨å†™æ–°æ‰‹ä»»åŠ¡task3çš„æ—¶å€™ï¼Œå‚è€ƒäº†Pytorchå®˜æ–¹exampleçš„word language modelï¼Œå®˜æ–¹exampleåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—å›°æƒ‘åº¦æ˜¯è¿™æ ·çš„ï¼š 1math.exp(cur_loss) å…¶ä¸­ï¼Œcur_lossè¡¨ç¤ºäº¤å‰ç†µçš„lossï¼Œå³ $-P(\hat{x})logP(x)$ï¼Œ$\hat{x}$è¡¨ç¤ºground truthã€‚ ç„¶è€Œï¼Œåœ¨æŸ¥é˜…äº†å›°æƒ‘åº¦ç›¸å…³èµ„æ–™åï¼Œæˆ‘å‘ç°ï¼Œå›°æƒ‘åº¦çš„å®šä¹‰æ˜¯è¿™æ ·çš„ï¼š \begin{aligned} PP(S)= &{P(w_{1}w_{2}...w_{N})}^{-\frac{1}{N}} \\ = &\sqrt[N]{\frac{1}{p(w_1 w_2 ... w_N)}} \\ = & \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} } \end{aligned}è¿™æ˜¯å¦ä¸€ç§å½¢å¼: \begin{aligned} Perplexity (W)=& 2^{H(W)} \\ = & {P(w_{1}w_{2}...w_{N})}^{-\frac{1}{N}} \\ = & \sqrt[N]{\frac{1}{p(w_1 w_2 ... w_N)}} \\ = & \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} } \end{aligned}å¯ä»¥çœ‹åˆ°ï¼ŒäºŒè€…æœ¬è´¨æ˜¯ä¸€æ ·çš„ã€‚ é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆåœ¨ä»£ç ä¸­ä»¥eä¸ºåº•å»è®¡ç®—å›°æƒ‘åº¦ï¼Œè€Œä¸æ˜¯2å‘¢? å®é™…ä¸Šï¼Œæ˜¯å› ä¸ºåœ¨ä¸Šè¿°å…¬å¼ä¸­ï¼Œlogæ˜¯ä»¥2ä¸ºåº•çš„ï¼Œä½†åœ¨Pytorchä¸­ï¼Œlogé»˜è®¤æ˜¯ä»¥eä¸ºåº•çš„ã€‚å› æ­¤åœ¨ä»£ç ä¸­ï¼Œéœ€è¦ç”¨eä½œä¸ºæŒ‡æ•°çš„åº•æ¥è¿˜åŸæˆå›°æƒ‘åº¦çš„åŸæœ¬å½¢å¼ï¼š \begin{aligned} \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} } \end{aligned}æœ€åè¿™æ˜¯perplexityçš„æ•°å­¦æ¨å¯¼ï¼šhttps://www.zhihu.com/question/58482430]]></content>
      <tags>
        <tag>å›°æƒ‘åº¦</tag>
        <tag>perplexity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯2]]></title>
    <url>%2F2018%2F07%2F29%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D2%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨çš„è¯—è¯æœ‰ä¸¤ç¯‡æ˜¯å·²ç»èƒŒè¿‡çš„ï¼Œæƒå½“æ˜¯å¤ä¹ äº†ä¸€éã€‚ 1ï¸âƒ£ ä¸‹ç»ˆå—å±±è¿‡æ–›æ–¯å±±äººå®¿ç½®é…’[å”] æç™½æš®ä»ç¢§å±±ä¸‹ï¼Œå±±æœˆéšäººå½’ã€‚å´é¡¾æ‰€æ¥å¾„ï¼Œè‹è‹æ¨ªç¿ å¾®ã€‚ç›¸æºåŠç”°å®¶ï¼Œç«¥ç¨šå¼€è†æ‰‰ã€‚ç»¿ç«¹å…¥å¹½å¾„ï¼Œé’èæ‹‚è¡Œè¡£ã€‚æ¬¢è¨€å¾—æ‰€æ†©ï¼Œç¾é…’èŠå…±æŒ¥ã€‚é•¿æ­ŒåŸæ¾é£ï¼Œæ›²å°½æ²³æ˜Ÿç¨€ã€‚æˆ‘é†‰å›å¤ä¹ï¼Œé™¶ç„¶å…±å¿˜æœºã€‚ http://m.xichuangzhu.com/work/57b900307db2a20054269a2a 2ï¸âƒ£ é€¢å…¥äº¬ä½¿[å”] å²‘å‚æ•…å›­ä¸œæœ›è·¯æ¼«æ¼«ï¼ŒåŒè¢–é¾™é’Ÿæ³ªä¸ä¹¾ã€‚é©¬ä¸Šç›¸é€¢æ— çº¸ç¬”ï¼Œå‡­å›ä¼ è¯­æŠ¥å¹³å®‰ã€‚ http://m.xichuangzhu.com/work/57b92218df0eea006335f923 3ï¸âƒ£ å¿µå¥´å¨‡Â·èµ¤å£æ€€å¤[å®‹] è‹è½¼å¤§æ±Ÿä¸œå»ï¼Œæµªæ·˜å°½ã€åƒå¤é£æµäººç‰©ã€‚æ•…å’è¥¿è¾¹ï¼Œäººé“æ˜¯ã€ä¸‰å›½å‘¨éƒèµ¤å£ã€‚ä¹±çŸ³ç©¿ç©ºï¼ŒæƒŠæ¶›æ‹å²¸ï¼Œå·èµ·åƒå †é›ªã€‚æ±Ÿå±±å¦‚ç”»ï¼Œä¸€æ—¶å¤šå°‘è±ªæ°ã€‚é¥æƒ³å…¬ç‘¾å½“å¹´ï¼Œå°ä¹”åˆå«äº†ï¼Œé›„å§¿è‹±å‘ã€‚ç¾½æ‰‡çº¶å·¾ï¼Œè°ˆç¬‘é—´ï¼Œæ¨¯æ©¹ç°é£çƒŸç­ã€‚æ•…å›½ç¥æ¸¸ï¼Œå¤šæƒ…åº”ç¬‘æˆ‘ï¼Œæ—©ç”Ÿåå‘ã€‚äººç”Ÿå¦‚æ¢¦ï¼Œä¸€å°Šè¿˜é…¹æ±Ÿæœˆã€‚ http://m.xichuangzhu.com/work/57b8bda2df0eea006333ecd2]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä»£ç ç‰‡æ®µè®°å½•1]]></title>
    <url>%2F2018%2F07%2F23%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB1%2F</url>
    <content type="text"><![CDATA[1ï¸âƒ£ get_batchæ³¨æ„åˆ°shuffleçš„æ ‡å‡†åšæ³• 123456789101112def get_batch(self,data,batch_size=32,is_shuffle): N=len(data) #è·å¾—æ•°æ®çš„é•¿åº¦ if is_shuffle is True: r=random.Random() r.seed() r.shuffle(data) #å¦‚æœis_shuffleä¸ºçœŸåˆ™æ‰“ä¹± #å¼€å§‹è·å¾—batchï¼Œä½¿ç”¨[ for in ] batch=[data[k:k+batch_size] for k in range(0,N,batch_size)] if N%batch_size!=0: #å¤„ç†ä¸æ•´é™¤é—®é¢˜ï¼Œå¦‚æœæœ‰æ˜¾å¼è¦æ±‚ä¸¢æ‰åˆ™ä¸éœ€è¦å¤„ç†ï¼Œè¿™é‡Œé»˜è®¤å¤„ç† remainder=N-N%batch_size #å‰©ä¸‹çš„éƒ¨åˆ† batch.append(data[temp:N]) return batch 2ï¸âƒ£ä½¿ç”¨gensimå°†GloVeè¯»å…¥å®é™…ä¸Šè¿™ä»½ä»£ç æœ‰ç‚¹é—®é¢˜ï¼Œåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œå‘ç°gloveæ–‡ä»¶éœ€è¦æ”¾åœ¨gensimçš„æ–‡ä»¶å¤¹ä¸‹æ‰èƒ½è¢«è¯»åˆ°(7.20 updated,åº”è¯¥ä½¿ç”¨ç»å¯¹åœ°å€)ï¼Œå¹¶ä¸å¥½ã€‚ æ•™ç¨‹åœ°å€ï¼šgensim: scripts.glove2word2vec â€“ Convert glove format to word2vec123456789101112131415161718#1. ä½¿ç”¨gensimè¯»å…¥word2vecmodel = gensim.models.KeyedVectors.load_word2vec_format( fname='GoogleNews-vectors-negative300-SLIM.bin', binary=True)words = model.vocab #è·å¾—è¯è¡¨vector= model[word] #wordæ˜¯wordsé‡Œé¢çš„å…ƒç´ #2. ä½¿ç”¨gensimè¯»å…¥glovefrom gensim.models import KeyedVectorsfrom gensim.test.utils import datapath, get_tmpfilefrom gensim.scripts.glove2word2vec import glove2word2vecglove_file=datapath('glove.txt') #æœ€å¥½ä½¿ç”¨ç»å¯¹åœ°å€tmp_file=get_tmpfile('word2vec.txt')glove2word2vec(glove_file,tmp_file)model=KeyedVectors.load_word2vec_format(tmp_file)#æ¥ä¸‹æ¥ä½¿ç”¨çš„æ–¹æ³•æ˜¯ä¸€æ ·çš„ 3ï¸âƒ£data_splitæ–¹æ³•1234567891011121314151617181920def data_split(seed=1, proportion=0.7): data = list(iter_corpus()) ids = list(range(len(data))) N = int(len(ids) * proportion) # number of training data rng = random.Random(seed) rng.shuffle(ids) test_ids = set(ids[N:]) train_data = [] test_data = [] for x in data: if x[1] in test_ids: # x[1]: sentence id test_data.append(x) else: train_data.append(x) return train_data, test_data 4ï¸âƒ£å¯¹stringé¢„å¤„ç†123456789101112131415161718def clean_str(string): string = re.sub(r"[^A-Za-z0-9()!?\'\`]", "", string) string = re.sub(r"\'s", " \'s", string) string = re.sub(r"\'m", " \'m", string) string = re.sub(r"\'ve", " \'ve", string) string = re.sub(r"n\'t", " n\'t", string) string = re.sub(r"\'re", " \'re", string) string = re.sub(r"\'d", " \'d", string) string = re.sub(r"\'ll", " \'ll", string) string = re.sub(r",", " , ", string) string = re.sub(r"!", " ! ", string) string = re.sub(r"\(", " \( ", string) string = re.sub(r"\)", " \) ", string) string = re.sub(r"\?", " \? ", string) string = re.sub(r"\s&#123;2,&#125;", " ", string) string = re.sub(r"\@.*?[\s\n]", "", string) string = re.sub(r"https*://.+[\s]", "", string) return string.strip().lower() 5ï¸âƒ£collate_fn(batchï¼‰é‡å†™collate_fnç»„å»ºmini-batchï¼Œåœ¨NLPä¸­å¸¸ç”¨ï¼Œå¥å­çš„ä¸ç­‰é•¿æ€§123456789101112131415161718192021222324252627def collate_fn(batch): # rewrite collate_fn to form a mini-batch lengths = np.array([len(data['sentence']) for data in batch]) sorted_index = np.argsort(-lengths) lengths = lengths[sorted_index] # descend order max_length = lengths[0] batch_size = len(batch) sentence_tensor = torch.LongTensor(batch_size, int(max_length)).zero_() for i, index in enumerate(sorted_index): sentence_tensor[i][:lengths[i]] = torch.LongTensor(batch[index]['sentence'][:max_length]) sentiments = torch.autograd.Variable(torch.LongTensor([batch[i]['sentiment'] for i in sorted_index])) if config.use_cuda: packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(Variable(sentence_tensor.t()).cuda(), lengths) #remember to transpose sentiments = sentiments.cuda() else: packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(Variable(sentence_tensor.t()),lengths) # remember to transpose return &#123;'sentence': packed_sequences, 'sentiment': sentiments&#125;## é‡å†™collate_fn(batch)ä»¥ç”¨äºdataloaderä½¿ç”¨ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼štrain_dataloader=DataLoader(train_data,batch_size=32,shuffle=True,collate_fn=collate_fn)â€‹## å…¶ä¸­ï¼Œtrain_dataloaderå¯å¾ªç¯éå†â€‹â€‹ã€‚for data in train_dataloader: ... 6ï¸âƒ£ä½¿ç”¨yieldè·å¾—æ•°æ®çš„generatoryieldçš„ç”¨æ³•123456789101112131415def get_dataset(txt_file): # return generator with open(txt_file,'r') as f: for line in f: if len(line.strip())==0: continue sentence=list(line.strip())+['&lt;eos&gt;'] yield sentence #åœ¨ä½¿ç”¨çš„æ—¶å€™ï¼šdataset=get_dataset(txt_file)for d in dataset: pass#å¦‚æœéœ€è¦è¿˜å¯ä»¥æ”¹æˆlistå½¢å¼dataset=list(get_dataset(txt_file)) 7ï¸âƒ£åŠ¨æ€åˆ›å»ºRNNå®ä¾‹æ ¹æ®rnn_typeåŠ¨æ€åˆ›å»ºå¯¹è±¡å®ä¾‹ï¼Œä½¿ç”¨äº†getattr123# rnn in ['GRU','LSTM','RNN']self.rnn = getattr(nn, self.rnn_type)(self.embedding_dim, self.hidden_dim, self.num_layers, dropout=self.dropout)]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>ä»£ç ç‰‡æ®µ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¯å‘¨è¯—è¯1]]></title>
    <url>%2F2018%2F07%2F23%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D1%2F</url>
    <content type="text"><![CDATA[æœ¬å‘¨èƒŒäº†å››ç¯‡ã€‚ 1ï¸âƒ£ ä¸´æ±Ÿä»™Â·å¤œå½’ä¸´çš‹[å®‹] è‹è½¼å¤œé¥®ä¸œå¡é†’å¤é†‰ï¼Œå½’æ¥å½·å½¿ä¸‰æ›´ã€‚å®¶ç«¥é¼»æ¯å·²é›·é¸£ï¼Œæ•²é—¨éƒ½ä¸åº”ï¼Œå€šæ–å¬æ±Ÿå£°ã€‚é•¿æ¨æ­¤èº«éæˆ‘æœ‰ï¼Œä½•æ—¶å¿˜å´è¥è¥ï¼Ÿå¤œé˜‘é£é™ç¸ çº¹å¹³ï¼Œå°èˆŸä»æ­¤é€ï¼Œæ±Ÿæµ·å¯„é¦€ç”Ÿã€‚ ç¸ ï¼ˆhÃºï¼‰çº¹çš‹ï¼ˆgaoï¼‰http://m.xichuangzhu.com/work/57ae79400a2b580063150e39 2ï¸âƒ£ è¶æ‹èŠ±Â·é˜…å°½å¤©æ¶¯ç¦»åˆ«è‹¦[æ¸…] ç‹å›½ç»´é˜…å°½å¤©æ¶¯ç¦»åˆ«è‹¦ã€‚ä¸é“å½’æ¥ï¼Œé›¶è½èŠ±å¦‚è®¸ã€‚èŠ±åº•ç›¸çœ‹æ— ä¸€è¯­ï¼Œç»¿çª—æ˜¥ä¸å¤©ä¿±è«ã€‚å¾…æŠŠç›¸æ€ç¯ä¸‹è¯‰ã€‚ä¸€ç¼•æ–°æ¬¢ï¼Œæ—§æ¨åƒåƒç¼•ã€‚æœ€æ˜¯äººé—´ç•™ä¸ä½ï¼Œæœ±é¢œè¾é•œèŠ±è¾æ ‘ã€‚ http://m.xichuangzhu.com/work/57b8ef70128fe10054c91d17 3ï¸âƒ£ é€å‹äºº[å”] æç™½é’å±±æ¨ªåŒ—éƒ­ï¼Œç™½æ°´ç»•ä¸œåŸã€‚æ­¤åœ°ä¸€ä¸ºåˆ«ï¼Œå­¤è“¬ä¸‡é‡Œå¾ã€‚æµ®äº‘æ¸¸å­æ„ï¼Œè½æ—¥æ•…äººæƒ…ã€‚æŒ¥æ‰‹è‡ªå…¹å»ï¼Œè§è§ç­é©¬é¸£ã€‚ http://m.xichuangzhu.com/work/57b8facfd342d3005ac6ffb4 4ï¸âƒ£ é»„é¹¤æ¥¼é€å­Ÿæµ©ç„¶ä¹‹å¹¿é™µ[å”] æç™½æ•…äººè¥¿è¾é»„é¹¤æ¥¼ï¼ŒçƒŸèŠ±ä¸‰æœˆä¸‹æ‰¬å·ã€‚å­¤å¸†è¿œå½±ç¢§ç©ºå°½ï¼Œå”¯è§é•¿æ±Ÿå¤©é™…æµã€‚ http://m.xichuangzhu.com/work/57b8f306128fe10054c92fb8]]></content>
      <tags>
        <tag>è¯—è¯</tag>
        <tag>è¯—è¯åˆ†äº«</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å®‰è£…condaé”™è¯¯]]></title>
    <url>%2F2018%2F07%2F23%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2F%E5%AE%89%E8%A3%85conda%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…condaçš„æ—¶å€™ï¼Œä¸€å¼€å§‹ä½¿ç”¨äº†pipå®‰è£…pip install condaåœ¨å®‰è£…å¥½condaä¹‹åæƒ³è¦ä½¿ç”¨condaå‘½ä»¤ï¼Œå‡ºç°ï¼š ERROR: The install method you used for condaâ€”probably either pip install conda or easy_install condaâ€”is not compatible with using conda as an application. If your intention is to install conda as a standalone application, currently supported install methods include the Anaconda installer and the miniconda installer. You can download the miniconda installer from https://conda.io/miniconda.html. ç„¶ååˆ°å®˜ç½‘ä¸‹è½½.shæ–‡ä»¶å¹¶bashå®‰è£…ï¼Œä»ç„¶æ²¡æœ‰è§£å†³è¯¥é—®é¢˜ï¼›æ¥ç€å°è¯•pip uninstall condaï¼Œå‡ºç° æœ€ååœ¨æŸ¥é˜…äº†ç½‘ä¸Šä¹‹åï¼Œä½¿ç”¨ which condaæ‰¾åˆ°condaçš„åœ°å€ï¼Œå¹¶åˆ é™¤rm Ã—Ã—Ã— æœ€åé‡æ–°bashå®‰è£…å³å¯ã€‚]]></content>
      <tags>
        <tag>æ‚ä¸ƒæ‚å…«</tag>
        <tag>conda</tag>
        <tag>é‡åˆ°çš„é—®é¢˜</tag>
      </tags>
  </entry>
</search>
