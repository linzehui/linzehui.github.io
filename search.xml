<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[每周论文15]]></title>
    <url>%2F2019%2F03%2F31%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8715%2F</url>
    <content type="text"><![CDATA[本周论文: Selective Kernel Networks Attentional pooling for action recognition 1️⃣[Selective Kernel Networks]通过对不同kernel size的feature map之间进行信息筛选获得更为鲁棒的表示，能够对不同的感受野进行整合，实现动态调整感受野。其思路还挺有意思的。 Introduction将该模型与视觉神经的理论结合在一起，也即，对于人类而言，在看不同尺寸不同远近的物体时，视觉皮层神经元感受野大小是会根据刺激来进行调节的，但一般而言在CNN中卷积核的大小是固定的。该模型正是从这一现象中获得灵感。 整个模型一共分为三个步骤：split，fuse，select split生成多个不同kernel size的feature map，也即对应不同的感受野大小；fuse将不同feature map结合起来，获得一个全局的综合的向量表示；select根据不同的weight选择不同感受野的feature map。 以上图为例。 SK-Net第一步split给定输入$\mathbf{X} \in \mathbb{R}^{H^{\prime} \times W^{\prime} \times C^{\prime}}$，通过不同的kernel size的CNN的卷积获得不同的feature map，上图是$3\times 3$与$5\times 5$的卷积核。卷积可以是传统的convolution卷积，也可以是空洞卷积（dilated convolution），或者深度卷积（depthwise convolution）。则有：$\widetilde{\mathcal{F}} : \mathbf{X} \rightarrow \widetilde{\mathbf{U}} \in \mathbb{R}^{H \times W \times C}$ 与 $\widehat{\mathcal{F}} : \mathbf{X} \rightarrow \widehat{\mathbf{U}} \in \mathbb{R}^{H \times W \times C}$，其中$\widetilde{\mathcal{F}},\widehat{\mathcal{F}}$是卷积变换。 第二步fuse直接将不同的feature map结合起来以获得全局信息，用以之后的动态调整。这里采用简单的求和以及global average pooling以获得channel-wise的信息$\mathbf{s} \in \mathbb{R}^{C}$： \mathbf{U}=\widetilde{\mathbf{U}}+\widehat{\mathbf{U}} \\ s_{c}=\mathcal{F}_{g p}\left(\mathbf{U}_{c}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} \mathbf{U}_{c}(i, j)在获得$\mathbf{s}$后再通过MLP获得$\mathbf{z}$： \mathbf{z}=\mathcal{F}_{f c}(\mathbf{s})=\delta(\mathcal{B}(\mathbf{W} \mathbf{s}))其中$\mathcal{B}$是batch normalization；$\delta$是Relu。 第三步select使用soft attention去选择不同kernel size的feature map并结合在一起。也即： a_{c}=\frac{e^{\mathbf{A}_{c} \mathbf{z}}}{e^{\mathbf{A}_{c} \mathbf{z}}+e^{\mathbf{B}_{c} \mathbf{z}}}, b_{c}=\frac{e^{\mathbf{B}_{c} \mathbf{z}}}{e^{\mathbf{A}_{c} \mathbf{z}}+e^{\mathbf{B}_{c} \mathbf{z}}}其中$\mathbf{A}_{c}$是对应$\widetilde{\mathbf{U}}$第$c$个channel的参数，$\mathbf{B}_{c}$是对应$\widehat{\mathbf{U}}$第$c$个channel的参数。$\mathbf{A}, \mathbf{B} \in \mathbb{R}^{C \times d}$，那么$a_{c},b_{c}$就对应不同feature map的weight。 因此，最终的feature map $\mathbf{V}$： \mathbf{V}_{c}=a_{c} \cdot \tilde{\mathbf{U}}_{c}+b_{c} \cdot \widehat{\mathbf{U}}_{c}, \quad a_{c}+b_{c}=1 \\ \mathbf{V}=\left[\mathbf{V}_{1}, \mathbf{V}_{2}, \dots, \mathbf{V}_{C}\right], \mathbf{V}_{c} \in \mathbb{R}^{H \times W}对比&amp;思考与SE-NetSE-Net是通过不同channel之间的交互，使得channel获得全局的感受野，使用的是对channel的放缩（详见上一篇论文笔记）；而SK-Net是不同的感受野之间的同一channel在通过全局信息的指导下以soft-attention的形式加权平均，这就和论文中提到的人类视觉对不同物体进行动态调整感受野的思路一致。 与dynamic convolution在论文[PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS]中，研究人员提出动态感受野的convolution，通过利用当前词预测一个卷积窗口，增加了模型的灵活性，并在机器翻译上取得了很好的结果。 虽然目的与本篇论文一致，但思路是完全不同的。一个是通过预测；另一个是在全局信息的指导下进行加权。在我的理解看来，或许本篇论文的思路更加合理一些，第一，在有了全局信息的指导下能够更好的进行加权，而通过预测，似乎有些盲目，可能需要更多的数据才能学得更好；第二，dynamic convolution论文中也提到了，如果不使用如深度可分离卷积等轻量级卷积方法，dynamic convolution不大现实（A dynamic version of standard convolutions would be impractical for current GPUs due to their large memory requirements），而SK-Net则不会有这个问题。 其他思考从另一个角度去思考，SK-Net通过人工定义好的几种不同大小的卷积，相当于在模型中引入更强的先验（inductive bias），也即假设了数据不会超过这几种大小的卷积的处理范围，这或许比不引入先验，完全靠数据去学某种特定pattern的dynamic convolution对小数据集更友好，因此可以不需要更多的数据来使得模型表现良好。类似的理解可以在CNN/RNN与Transformer的对比中看见，因为CNN/RNN引入了较强的local bias，因此对于小数据集更友好，但同时其上限或许不如Transformer高；而Transformer一开始就是全局感受野，使得需要更多数据来帮助模型学到某种特定pattern（如某种local bias），但当数据充足时，Transformer的上限更高，近期非常火的pretrained model GPT/GPT-2.0/Bert似乎也印证了这点。 2️⃣[Attentional pooling for action recognition]提出一种基于attention的pooling策略，采用低秩近似的方法，使得模型能够在计算量不增加很多的情况下达到更好的效果。可以将该方法理解成对二阶pooling的低秩近似。 方法一阶pooling记$X \in R^{n \times f}$为被pooling的层，其中n为空间位置的个数，如$16\times 16$，$f$为channel个数。标准的sum/max pooling将该矩阵缩减为$R^{f \times 1}$，然后使用全连接的权重$\mathbf{w} \in R^{f \times 1}$获得一个分类的分数。这里假设的是二分类，但可以很容易推广为多分类。 上述操作形式化可以写成： \operatorname{score}_{p o o l}(X)=\mathbf{1}^{T} X \mathbf{w}, \quad \text { where } \quad X \in R^{n \times f}, \mathbf{1} \in R^{n \times 1}, \mathbf{w} \in R^{f \times 1}其中$\mathbf{1}$为全1向量，$\mathbf{x}=\mathbf{1}^{T} X \in R^{1 \times f}$就是通过sum pooling后的feature。 二阶pooling构建二阶feature $X^{T} X \in R^{f \times f}$，在获得二阶feature后，通常或向量化该矩阵，再送入全连接以做分类。也即我们会学习一个$f\times f$的全连接权重向量。若保持二阶feature与对应的全连接权重向量的形式为矩阵，矩阵相乘，其中的迹实际上就是这两个向量化后的矩阵所做内积获得的分数。形式化可以写成： \text {score}_{order2}(X)=\operatorname{Tr}\left(X^{T} X W^{T}\right), \quad \text { where } \quad X \in R^{n \times f}, W \in R^{f \times f}这可以用迹的定义去证明：示意图 低秩二阶pooling现尝试使用低秩去近似该二阶pooling，也即对$W$近似，将$W$写成两个向量的乘积，也即： W=\mathbf{a b}^{T} \text { where } \mathbf{a}, \mathbf{b} \in R^{f \times 1}将上式代入二阶pooling，可获得： \begin{aligned} \text {score}_{\text {attention}}(X) &=\operatorname{Tr}\left(X^{T} X \mathbf{b a}^{T}\right), \quad \text { where } \quad X \in R^{n \times f}, \mathbf{a}, \mathbf{b} \in R^{f \times 1} \\ &=\operatorname{Tr}\left(\mathbf{a}^{T} X^{T} X \mathbf{b}\right) \\ &=\mathbf{a}^{T} X^{T} X \mathbf{b} \\ &=\mathbf{a}^{T}\left(X^{T}(X \mathbf{b})\right) \end{aligned}第二行使用的是迹的定理：$\operatorname{Tr}(A B C)=\operatorname{Tr}(C A B)$第三行使用的是标量的迹等于标量本身。最后一行表明整个流程：给定一个feature map $X$，首先计算一个对所有空间位置的attentional map：$\mathbf{h}= {X \mathbf{b} \in R^{n \times 1}}$；然后根据该attentional map计算加权平均的feature：$\mathbf{x}=X^{T} \mathbf{h} \in R^{f \times 1}$。该feature再通过线性层获得最终的分数。 实际上上式还有其他理解的角度： \begin{aligned} \text {score}_{\text {attention}}(X) &=\left((X \mathbf{a})^{T} X\right) \mathbf{b} \\ &=(X \mathbf{a})^{T}(X \mathbf{b}) \end{aligned}第一行表明attentional map也可以通过$X \mathbf{a} \in R^{n \times 1}$来计算，$\mathbf{b}$来做classifier。第二行表明，该式子本质上是对称的，可以看成两个attentional heapmap的内积。 下图是整个流程： Top-down attention现将二分类推广为多分类： \text {score}_{order2}(X, k)=\operatorname{Tr}\left(X^{T} X W_{k}^{T}\right), \quad \text { where } \quad X \in R^{n \times f}, W_{k} \in R^{f \times f}也即将$W$替换成类相关的参数，仿照上面的推导，可以推出每个类都有特定的$\boldsymbol{a}_{k}$与$\boldsymbol{b}_{k}$。 但在这里通过固定其中一个参数为与类无关的参数，也即$\boldsymbol{b}_{k}=\boldsymbol{b}$。实际上就等价于一个是类相关的top-down attention；另一个是类无关的bottom-up attention。一个获得类特定的特征；另一个获得全局通用的特征。 因此最终低秩attention model为： \text {score}_{attention}(X, k)=\mathbf{t}_{k}^{T} \mathbf{h}, \quad \text { where } \quad \mathbf{t}_{k}=X \mathbf{a}_{k}, \mathbf{h}=X \mathbf{b}Average-pooling Revisited当然在给定了上述一系列的推导，我们对average-pooling重新进行形式化： \text {score}_{top-down}(X, k)=\mathbf{1}^{T} X \mathbf{w}_{k}=\mathbf{1}^{T} \mathbf{t}_{k} \quad \text { where } \quad \mathbf{t}_{k}=X \mathbf{w}_{k}将$\mathbf{w}$替换成类相关的$\mathbf{w}_{k}$，实际上就是将二分类推广为多分类。但该形式赋予了average-pooling新的理解。 当然，我们还可以将rank-1推广为rank-k，实验证明对于大数据集使用大的秩会更好。 对比与Self-attentive的联系论文[A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING]就提出了利用可学习的head对feature进行attention加权平均的方法，并且将一个head推广到多个head。实际上在$\mathbf{h}= {X \mathbf{b} \in R^{n \times 1}}$我们就可以看出，$\mathbf{b}$在这里扮演的角色就是self-attentive的head的角色。对于秩为1的近似，就是head为1的情况，若将秩为1推广为秩为k，也即等价于在Self-attentive中多个head的情况。 本文巧妙的地方在于head有两个作用，一种是top-down的head，获得的是类相关的feature；另一个是bottom-up的feature，获得的是通用的feature。并且本文通过巧妙的数学推导来获得新的解释，本来仅仅是二阶feature过一个全连接，但通过公式推导赋予了attention的解释，这点让人眼前一亮。]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>每周论文阅读</tag>
        <tag>second-order</tag>
        <tag>pooling</tag>
        <tag>SK-Net</tag>
        <tag>attentional pooling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词21]]></title>
    <url>%2F2019%2F03%2F31%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D21%2F</url>
    <content type="text"><![CDATA[1️⃣醉落魄 · 席上呈元素[宋] 苏轼分携如昨，人生到处萍飘泊。偶然相聚还离索。多病多愁，须信从来错。尊前一笑休辞却，天涯同是伤沦落。故山犹负平生约。西望峨嵋，长羡归飞鹤。 http://lib.xcz.im/work/57c467a86be3ff0058452840 2️⃣戏为六绝句[唐] 杜甫【其一】庾信文章老更成，凌云健笔意纵横。今人嗤点流传赋，不觉前贤畏后生。 【其三】纵使卢王操翰墨，劣于汉魏近风骚。龙文虎脊皆君驭，历块过都见尔曹。 过都历块 (guò dōu lì kuài)解释：越过都市，经过山阜。意指纵横驰骋，施展才能。]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周论文14]]></title>
    <url>%2F2019%2F03%2F24%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8714%2F</url>
    <content type="text"><![CDATA[本周论文: Is Second-order Information Helpful for Large-scale Visual Recognition? The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classification 1️⃣[Is Second-order Information Helpful for Large-scale Visual Recognition?]通过协方差的方法获得图像的二阶信息。参考了https://zhuanlan.zhihu.com/p/46864160 深度分类网络主要分为两个部分：特征提取和分类器。无论是VGG还是GoogleNet，后来的Resnet、Densenet，在连接分类器之前，一般都连接了一个Pooling层。但pooling只获得了feature的一阶信息，对于细分类问题中类间差异不显著，一阶信息可能有一些不适用，因此我们可以通过一阶信息获得二阶信息，从而获取更有价值的信息。 本文通过获取特征协方差的方法，以达到该目的。 输入:$\mathbf{X} \in \mathbb{R}^{d \times N}$ 则协方差矩阵为$\mathbf{X} \mapsto \mathbf{P}, \quad \mathbf{P}=\mathbf{X} \overline{\mathbf{I}} \mathbf{X}^{T}$，其中$\overline{\mathbf{I}}=\frac{1}{N}\left(\mathbf{I}-\frac{1}{N} \mathbf{1} \mathbf{1}^{T}\right)$, $\mathbf{I}$是单位阵，$\mathbf{1}$是全1的向量。 协方差矩阵是半正定矩阵，因此可写成$\mathbf{P} \mapsto(\mathbf{U}, \mathbf{\Lambda}), \quad \mathbf{P}=\mathbf{U} \mathbf{\Lambda} \mathbf{U}^{T}$，其中$\boldsymbol{\Lambda}=\operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{d}\right)$，$\mathbf{U}=\left[\mathbf{u}_{1}, \dots, \mathbf{u}_{d}\right]$，$\mathbf{U}$是对应的特征向量。 最终可获得$\mathbf{Q}$矩阵：$(\mathbf{U}, \boldsymbol{\Lambda}) \mapsto \mathbf{Q}, \mathbf{Q} \triangleq \mathbf{P}^{\alpha}=\mathbf{U F}(\mathbf{\Lambda}) \mathbf{U}^{T}$，其中$\alpha$是一个正实数，$\mathbf{F}(\boldsymbol{\Lambda})=\operatorname{diag}\left(f\left(\lambda_{1}\right), \ldots, f\left(\lambda_{d}\right)\right)$，其中$f\left(\lambda_{i}\right)=\lambda_{i}^{\alpha}$，是特征值的幂，如果要做归一化，那么可以有： f\left(\lambda_{i}\right)=\left\{\begin{array}{cc}{\lambda_{i}^{\alpha} / \lambda_{1}^{\alpha}} & {\text { for MPN+M }-\ell_{2}} \\ {\lambda_{i}^{\alpha} /\left(\sum_{k} \lambda_{k}^{2 \alpha}\right)^{\frac{1}{2}}} & {\text { for MPN+M-Fro }}\end{array}\right.之所以取幂，是为了解决在协方差估计中小样本高维度的问题，以resnet为例，最后得到的feature为7X7X512，也就是49个512维的feature，这样估计出来的协方差矩阵是不靠谱的，而通过幂这个操作，可以解决这一问题。通过实验可以发现，当幂次为0.5也就是平方根操作时，效果最优。（似乎类似的有word2vec的平滑） （虽然这篇有些看不大懂，但一个启发就是，可以通过协方差的方式进行特征之间的交互） 2️⃣[The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classification]提出使用卷积出来后的feature经过pooling作为最后的图像特征表示而不是全连接后的特征表示。 Motivation：只使用最后一层fc的特征有一个缺点，就是丢失位置信息，而convolution layer包含了丰富的空间信息。在pooling完后每个local区域都能获得一个特征，并拼接起来作为最后的表示。 prerequisite:①首先有一个预训练好的模型②有两层一样$H\times W$的convolution。论文以AlexNet作为例子 假设卷积后的feature map是$H × W × D$，那么可以理解成，我们将图片分为$H × W$的区域，每个区域的特征用$D$维表示。我们称每个$D$维特征一个spatial unit。当使用全连接时，这部分的空间信息就丢失了，并且无法还原。 本文提出，将每个区域提取出一个特征，然后拼起来组成一整张图的特征，如下图，每个长条（也即$1\times 1\times channel$）作为一个特征： 如何判断区域？一种方法是首先检测出多个区域，每个区域对应一种object part，然后对于落入该区域的特征进行pooling，给定D种human-specified object parts，那么可以获得D个feature且拼在一起。 \mathbf{P}_{k}^{t}=\sum_{i=1} \mathbf{x}_{i} I_{i, k}具体而言，$\mathbf{x}_{i}$是特征，$I_{i, k}$是二元的indicator，表明$\mathbf{x}_{i}$是否落入该区域，每个$I$实际上定义了一个池化通道。当然，这里可以进一步将indicator从二元扩展为权重。 但在实现的过程中，并没有human-specified的区域。这里我们就借助下一层的卷积作为indicator。 By doing so, D t+1 pooling channels are created for the local features extracted from the tth convolutional layer 这也就被称为cross-convolutional-layer pooling。 如何做？ the filter of a convolutional layer works as a part detector and its feature map serves a similar role as the part region indicator map. 具体而言，有： \begin{array}{l}{\mathbf{P}^{t}=\left[\mathbf{P}_{1}^{t}, \mathbf{P}_{2}^{t}, \cdots, \mathbf{P}_{k}^{t}, \cdots, \mathbf{P}_{D_{t+1}}^{t}\right]} \\ {\text { where, } \mathbf{P}_{k}^{t}=\sum_{i=1}^{N_{t}} \mathbf{x}_{i}^{t} a_{i, k}^{t+1}}\end{array}$\mathbf{P}^{t}$表示第t层convolution在卷积过后做cross-pooling后的特征集合，也即我们要获得的表示，该表示通过$D_{t+1}$次pooling后的结果拼接而成。$D_{t+1}$具体来说，就是第t+1层的卷积的channel维数。假设$\mathbf{a}_{i}^{t+1} \in \mathbb{R}^{D_{t+1}}$是第t+1层convolution的第i个空间单位（spatial unit）的feature vector，其中$a_{i, k}^{t+1}$是该向量的一个值，该值就作为pooling的权重。 上述有些绕口且难懂，直接看例子： 即，第t+1层convolution的channel维度为多少，则pooling后的特征个数即为多少。因为第t层与第t+1层的$H\times W$是一致的，那么可以用t+1层的每个slice去对第t层的convolution进行加权。 为什么这样是合理的？因为第t+1层的convolution提取了$D_{t+1}$个特征，使用的是$m\times n$的kernel size，如果$x$是被$m\times n$的某个kernel提取了，那么很自然的，$x$就是对应该kernel提取出来的feature的一个spatial unit。说白了就是第t层与第t+1层的空间对应。]]></content>
      <tags>
        <tag>CNN</tag>
        <tag>Paper</tag>
        <tag>每周论文阅读</tag>
        <tag>second-order</tag>
        <tag>pooling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词20]]></title>
    <url>%2F2019%2F03%2F23%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D20%2F</url>
    <content type="text"><![CDATA[1️⃣杂诗七首（其四）[三国] 曹植南国有佳人，容华若桃李。朝游江北岸，夕宿潇湘沚。时俗薄朱颜，谁为发皓齿？俯仰岁将暮，荣耀难久恃。 http://lib.xcz.im/work/58ad4c78ac502e007e9f6f9f 2️⃣梦江南[唐] 温庭筠千万恨，恨极在天涯。山月不知心里事，水风空落眼前花，摇曳碧云斜。 http://lib.xcz.im/work/57b8d0c77db2a2005425c856]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周论文13]]></title>
    <url>%2F2019%2F03%2F17%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8713%2F</url>
    <content type="text"><![CDATA[1️⃣[Depthwise Separable Convolutions for Neural Machine Translation]将depthwise separable convolution 深度可分离卷积 用于翻译任务，并在此基础上对depthwise separable进行更进一步的参数量优化，也即super-separable。（其实我觉得并没有啥创新性的感觉） 首先介绍什么是depthwise separable convolution，实际上就是一个depthwise+pointwise。 \operatorname{Conv}(W, y)_{(i, j)}=\sum_{k, l, m}^{K, L, M} W_{(k, l, m)} \cdot y_{(i+k, j+l, m)}\operatorname{PointwiseConv}(W, y)_{(i, j)}=\sum_{m}^{M} W_{m} \cdot y_{(i, j, m)}\text {DepthwiseConv}(W, y)_{(i, j)}=\sum_{k, l}^{K, L} W_{(k, l)} \odot y_{(i+k, j+l)}\operatorname{SepConv}\left(W_{p}, W_{d}, y\right)_{(i, j)}=\text {PointwiseConv}_{(i, j)}\left(W_{p}, \text { DepthwiseConv }_{(i, j)}\left(W_{d}, y\right)\right)几种convolution的参数量对比：其中k是kernel size，c是channel，g是group。 g-Sub-separable是指将channel分为几个group，每个group进行常规的convolution操作；g-Super-separable，也即本文中提出的convolution，同样是将channel分为几个group，然后对每个group进行depthwise-separable的卷积。 2️⃣[Squeeze-and-Excitation Networks]提出一种新型的网络，能够通过建模channel之间的关系，使得每个channel能够获得全局的信息，进而提高模型的能力。 分为两步：第一步是获得一个全局的表示，第二步是根据全局信息更新每个channel的信息。 符号输入：$ \mathbf{X} \in \mathbb{R}^{H^{\prime} \times W^{\prime} \times C^{\prime}} $经过特征提取后（如Convolution)：$\mathbf{U} \in \mathbb{R}^{H \times W \times C}$，也即：$\mathbf{U}=\mathbf{F}_{t r}(\mathbf{X})$将$\mathbf{U}$写成：$\mathbf{U}=\left[\mathbf{u}_{1}, \mathbf{u}_{2}, \dots, \mathbf{u}_{C}\right]$$\mathbf{V}$ 是可学习的卷积核参数： $\mathbf{V}=\left[\mathbf{v}_{1}, \mathbf{v}_{2}, \ldots, \mathbf{v}_{C}\right]$ 则上述卷积变换可写成：$\mathbf{u}_{c}=\mathbf{v}_{c} \ast \mathbf{X}=\sum_{s=1}^{C^{\prime}} \mathbf{v}_{c}^{s} \ast \mathbf{x}^{s}$ Squeeze: Global Information Embedding第一步，将所有的特征进行整合得到全局的特征： z_{c}=\mathbf{F}_{s q}\left(\mathbf{u}_{c}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} u_{c}(i, j)论文提取全局特征的方法直接用简单的global average pooling。那么$\mathbf{z} \in \mathbb{R}^{C}$的每一维就代表每一维的channel。 Excitation: Adaptive Recalibration与attention不同的是，论文希望能够同时强调不同多个channel的重要（而不是one-hot的形式），因此使用一个简单的门控制机制，采用sigmoid激活函数：（这里的想法挺有意思，相对attention的softmax似乎确实会更好的样子） \mathbf{s}=\mathbf{F}_{ex}(\mathbf{z}, \mathbf{W})=\sigma(g(\mathbf{z}, \mathbf{W}))=\sigma\left(\mathbf{W}_{2} \delta\left(\mathbf{W}_{1} \mathbf{z}\right)\right)为了减少参数这里的MLP采用了bottleneck的形式。亦即：${\mathbf{W}_{1} \in \mathbb{R}^{\frac{C}{r} \times C}}$ $ {\mathbf{W}_{2} \in \mathbb{R}^{C \times \frac{C}{r}}}$$r$是reduction ratio。 贴上作者的思路： To make use of the information aggregated in the squeeze operation, we follow it with a second operation which aims to fully capture channel-wise dependencies. To fulfill this objective, the function must meet two criteria: first, it must be ﬂexible (in particular, it must be capable of learning a nonlinear interaction between channels) and second, it must learn a non-mutually-exclusive relationship since we would like to ensure that multiple channels are allowed to be emphasised (rather than enforcing a one-hot activation). To meet these criteria, we opt to employ a simple gating mechanism with a sigmoid activation. 最后对每个channel进行放缩，获得新的表示： \widetilde{\mathbf{x}}_{c}=\mathbf{F}_{\text {scale}}\left(\mathbf{u}_{c}, s_{c}\right)=s_{c} \cdot \mathbf{u}_{c} 3️⃣[Non-local Neural Networks]提出一种新的结构，与上一篇类似，希望模型的每个位置都能感知到其他位置，从而捕获长程依赖，拥有全局信息。 Non-local Network定义non-local网络： \mathbf{y}_{i}=\frac{1}{\mathcal{C}(\mathbf{x})} \sum_{\forall j} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) g\left(\mathbf{x}_{j}\right)其中$\mathcal{C}$是归一化函数；$f$是第$i$个位置与第$j$个位置的交互函数；$g$计算第$j$个位置的表示。 $g$的具体形式一个线性函数：$g\left(\mathbf{x}_{j}\right)=W_{g} \mathbf{x}_{j}$在实现的时候是一个$1\times1$或 $1\times1\times1$的convolution。 $f$的具体形式①Gaussian$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=e^{\mathbf{x}_{i}^{T} \mathbf{x}_{j}}$则归一化定义为$\mathcal{C}(\mathbf{x})=\sum_{\forall j} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)$ 。 ②Embedded Gaussian$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=e^{\theta\left(\mathbf{x}_{i}\right)^{T} \phi\left(\mathbf{x}_{j}\right)}$其中：$\theta\left(\mathbf{x}_{i}\right)=W_{\theta} \mathbf{x}_{i} $, $ \phi\left(\mathbf{x}_{j}\right)=W_{\phi} \mathbf{x}_{j}$归一化：$\mathcal{C}(\mathbf{x})=\sum_{\forall j} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)$ 可以看到self-attention是Embedded Gaussian的一种形式。虽然有这样的关系，但作者在实验中发现softmax并不是必要的。 ③Dot product$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\theta\left(\mathbf{x}_{i}\right)^{T} \phi\left(\mathbf{x}_{j}\right)$归一化：$\mathcal{C}(\mathbf{x})=N$ ④Concatenation$f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\operatorname{ReLU}\left(\mathbf{w}_{f}^{T}\left[\theta\left(\mathbf{x}_{i}\right), \phi\left(\mathbf{x}_{j}\right)\right]\right)$$\mathcal{C}(\mathbf{x})=N$ 有了上面的non-local的介绍，可以直接将其用于residual network。$\mathbf{z}_{i}=W_{z} \mathbf{y}_{i}+\mathbf{x}_{i}$$y$则是non-local network的输出。 Non-local block的策略/tricks①设置$W_g$,$W_θ$,$W_ϕ$的channel的数目为x的channel数目的一半，这样就形成了一个bottleneck，能够减少一半的计算量。Wz再重新放大到x的channel数目，保证输入输出维度一致。 ②在$\frac{1}{\mathcal{C}(\hat{\mathbf{x}})} \sum_{\forall j} f\left(\mathbf{x}_{i}, \hat{\mathbf{x}}_{j}\right) g\left(\hat{\mathbf{x}}_{j}\right)$使用下采样，如max-pooling，减少计算量。 4️⃣[Bilinear CNN Models for Fine-grained Visual Recognition]提出一种双线性模型，由两个特征提取器组成，他们的输出做外积，最终获得图像描述特征。 Motivation(?不确定是不是这样)：对于细粒度物体的分类，先对局部定位，再提取特征。两个特征提取器一个是提取location，另一个提取特征。 为什么用外积？ outer product captures pairwise correlations between the feature channels 有意思的是作者将该模型和人脑视觉处理的两个假设联系在一起(stream hypothesis)：here are two main pathways, or “streams”. The ventral stream (or, “what pathway”) is involved with object identiﬁcation and recognition. The dorsal stream (or, “where pathway”) is involved with processing the object’s spatial location relative to the viewer.不过看看就好，并没有什么道理。 对于一个分类的双线性模型而言，其一般形式是一个四元组：$\mathcal{B}=\left(f_{A}, f_{B}, \mathcal{P}, \mathcal{C}\right)$。其中$f$是特征函数，$\mathcal{P}$是pooling函数，$\mathcal{C}$是分类函数。具体而言，$f$是一个映射，${f : \mathcal{L} \times \mathcal{I} \rightarrow} {R^ {c\times D}} $。也即将一个image和一个location L 映射成feature。（We refer to locations generally which can include position and scale 其实这里不是很懂location的意思） 将feature a和feature b结合在一起：$\text { bilinear }\left(l, \mathcal{I}, f_{A}, f_{B}\right)=f_{A}(l, \mathcal{I})^{T} f_{B}(l, \mathcal{I})$ pooling有好几种，可以直接加起来，或者使用max-pooling。这里使用直接加起来的方式，可以理解为，这些特征是无序(orderless)的叠加。 在获得输出后再做一些操作/trick能够提升表现：$\begin{array}{l}{\mathbf{y} \leftarrow \operatorname{sign}(\mathbf{x}) \sqrt{|\mathbf{x}|}} \\ {\mathbf{z} \leftarrow \mathbf{y} /|\mathbf{y}|_{2}}\end{array}$ 讨论：①But do the networks specialize into roles of localization (“where”) and appearance modeling (“what”) when initialized asymmetrically and ﬁne-tuned?通过可视化发现，并没有明确的功能分开。Both these networks tend to activate strongly on highly speciﬁc semantic parts ②bilinear的好处还可以扩展成trilinear，添加更多的信息。]]></content>
      <tags>
        <tag>CNN</tag>
        <tag>Paper</tag>
        <tag>每周论文阅读</tag>
        <tag>NMT</tag>
        <tag>SE-Net</tag>
        <tag>Non-local</tag>
        <tag>Bilinear</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词19]]></title>
    <url>%2F2019%2F03%2F17%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D19%2F</url>
    <content type="text"><![CDATA[1️⃣送灵澈上人[唐] 刘长卿苍苍竹林寺，杳杳钟声晚。荷笠带斜阳，青山独归远。 荷（hè）笠：背着斗笠。 http://lib.xcz.im/work/57b90887128fe10054c9c750 2️⃣苏幕遮 · 怀旧[宋] 范仲淹碧云天，黄叶地，秋色连波，波上寒烟翠。山映斜阳天接水，芳草无情，更在斜阳外。黯乡魂，追旅思。夜夜除非，好梦留人睡。明月楼高休独倚，酒入愁肠，化作相思泪。 http://lib.xcz.im/work/57b8ee4a128fe10054c91757]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周论文12]]></title>
    <url>%2F2019%2F03%2F10%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8712%2F</url>
    <content type="text"><![CDATA[1️⃣[PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS]Facebook研究人员提出的两种基于卷积的方法尝试替代self-attention在transformer中的作用，拥有更少的参数以及更快的速度，并且能够达到很好的效果。 Lightweight convolution背景：depthwise convolution每个channel独立进行卷积，注意到放到NLP任务上channel是指embedding的每一维。 O_{i, c}=\text{DepthwiseConv}\left(X, W_{c, :}, i, c\right)=\sum_{j=1}^{k} W_{c, j} \cdot X_{\left(i+j-\left\lceil\frac{k+1}{2}\right]\right), c}因此Lightweight convolution的计算方法为： \operatorname{LightConv}\left(X, W_{\left\lceil\frac{c H}{d}\right\rceil,:}, i, c\right)=\text { DepthwiseConv}\left(X, \text{softmax}(W_{\left\lceil\frac{c H}{d}\right\rceil,:}), i, c\right)每一层都有固定的window size，这和self-attention不同，self-attention是所有的context都进行交互。 Weight sharing 注意到这里讲每d/H个channel的参数进行绑定，进一步减少参数。 Softmax-normalization 对channel一维进行softmax，相当于归一化每个词的每一维的的重要性（比self-attention更精细）。实验证明，如果没有softmax没办法收敛。 因此总体的架构为：input—&gt;linear —&gt; GLU(gated linear unit) —&gt; lightconv/dynamicConv —&gt; linear Dynamic convolution与lightweight convolution相似，但加了一个动态的kernel size。 \text { DynamicConv}( X , i , c ) = \operatorname{LightConv}\left(X, f\left(X_{i}\right)_{h,:}, i, c\right)这里的kernel size简单使用线性映射：$f : \mathbb { R } ^ { d } \rightarrow \mathbb { R } ^ { H \times k }$如：$f\left(X_{i}\right)=\sum_{c=1}^{d} W_{h, j, c}^{Q} X_{i, c}$ 2️⃣[Joint Embedding of Words and Labels for Text Classiﬁcation]提出一种机制将label作为embedding与词一同训练，同时引入label和word的attention机制，在分类上获得效果。 上图中，C是label embedding，维度为$P\times K$ ; V是句子所有词的embedding矩阵，维度为$P\times L$。$\mathbf{G}$的计算公式为： \mathbf{G}=\left(\mathbf{C}^{\top} \mathbf{V}\right) \oslash \hat{\mathbf{G}}$\oslash$表示element-wise相除。$\hat{\mathbf{G}}$表示l2 norm，也即： \hat{g}_{k l}=\left\|\boldsymbol{c}_{k}\right\|\left\|\boldsymbol{v}_{l}\right\|因此公式的本质即在计算label与每个词的cos距离。 在获得了$\mathbf{G}$后，为了获得更高的的表示，如phrase，将一个一个block取出，并过线性层： \boldsymbol{u}_{l}=\operatorname{ReLU}\left(\mathbf{G}_{l-r : l+r} \mathbf{W}_{1}+\boldsymbol{b}_{1}\right)接着对每个$\boldsymbol{u}_{l}$取最大值： m_{l}=\textbf{max-pooling}\left(\boldsymbol{u}_{l}\right)此时的$\mathbf{m}$是一个长度为L的向量。最终对m做softmax获得一个分数的分布： \boldsymbol{\beta}=\operatorname{SoftMax}(\boldsymbol{m})将该分数和每个词做加权求和，获得最终的向量表示： \boldsymbol{z}=\sum_{l} \beta_{l} \boldsymbol{v}_{l}思考：将label与embedding放在一起训练这个思路不错。但整合的方式是否过于简单粗暴了？特别是phrase的提取和随后的max-pooling的可解释性并不强的样子。]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>attention</tag>
        <tag>Convolution</tag>
        <tag>每周论文阅读</tag>
        <tag>embedding</tag>
        <tag>text classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识18]]></title>
    <url>%2F2019%2F03%2F10%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8618%2F</url>
    <content type="text"><![CDATA[1️⃣[Depthwise seperable convolution]Depthwise seperable convolution = depthwise + pointwise先每个卷积核独立对一个feature map进行卷积，再通过一个$1\times 1 \times n$的卷积核对feature map进行整合。 https://blog.csdn.net/tintinetmilou/article/details/81607721 2️⃣[如何寻找较好的lr]一种启发式的方法： Over an epoch begin your SGD with a very low learning rate (like 10−8) but change it (by multiplying it by a certain factor for instance) at each mini-batch until it reaches a very high value (like 1 or 10). Record the loss each time at each iteration and once you’re finished, plot those losses against the learning rate. https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Convolution</tag>
        <tag>learning rate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词18]]></title>
    <url>%2F2019%2F03%2F09%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D18%2F</url>
    <content type="text"><![CDATA[1️⃣西江月 · 遣兴[宋] 辛弃疾醉里且贪欢笑，要愁那得工夫。近来始觉古人书，信著全无是处。昨夜松边醉倒，问松「我醉何如」。只疑松动要来扶，以手推松曰「去」！ http://lib.xcz.im/work/57b935bcd342d3005ac8e63f 2️⃣蝶恋花[宋] 晏殊槛菊愁烟兰泣露，罗幕轻寒，燕子双飞去。明月不谙离恨苦，斜光到晓穿朱户。昨夜西风凋碧树，独上高楼，望尽天涯路。欲寄彩笺兼尺素，山长水阔知何处？ http://lib.xcz.im/work/57b318dd1532bc00618ffaff]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用fairseq复现Transformer NMT]]></title>
    <url>%2F2019%2F01%2F28%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8fairseq%E5%A4%8D%E7%8E%B0Transformer%20NMT%2F</url>
    <content type="text"><![CDATA[基于Transformer的NMT虽然结果好，但超参非常难调，只要有一两个参数和论文不一样，就有可能得到和论文相去甚远的结果。fairseq是现有比较完善的seq2seq库，由于是大公司出品，因此也写得较为完善，不论是代码还是文档。 本文讨论如何使用fairseq复现基于Transformer的翻译任务，也即复现Vaswani, et al. 的论文结果。本文尽量不讨论实现细节，只讨论如何复现出结果。 fairseq项目地址：https://github.com/pytorch/fairseq 使用教程在这里我们参考的是18年的文章Scaling Neural Machine Translation，同样是基于Transformer的NMT。同时，我们使用WMT16 EN-DE而不是Vaswani, et al.论文中的WMT14 EN-DE。二者只在一个文件（commoncrawl）上有区别，其他是一样的，由于WMT16 EN-DE有预处理好的数据，为了方便起见，我们就使用该份数据（下文也有预处理WMT14数据的方法） 准备工作 安装fairseq，在Readme内有 阅读Readme（optional） 阅读doc（optional） 数据预处理Step1数据预处理主要是下载多个文件并合并—&gt;清理/tokenize数据—&gt;将数据分为train、valid—&gt;bpe(bype pair encoding)。fairseq提供了一整套处理流程的脚本，在examples/translation/prepare-wmt14en2de.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139#!/bin/bash# Adapted from https://github.com/facebookresearch/MIXER/blob/master/prepareData.shecho 'Cloning Moses github repository (for tokenization scripts)...'git clone https://github.com/moses-smt/mosesdecoder.gitecho 'Cloning Subword NMT repository (for BPE pre-processing)...'git clone https://github.com/rsennrich/subword-nmt.gitSCRIPTS=mosesdecoder/scriptsTOKENIZER=$SCRIPTS/tokenizer/tokenizer.perlCLEAN=$SCRIPTS/training/clean-corpus-n.perlNORM_PUNC=$SCRIPTS/tokenizer/normalize-punctuation.perlREM_NON_PRINT_CHAR=$SCRIPTS/tokenizer/remove-non-printing-char.perlBPEROOT=subword-nmtBPE_TOKENS=40000URLS=( "http://statmt.org/wmt13/training-parallel-europarl-v7.tgz" "http://statmt.org/wmt13/training-parallel-commoncrawl.tgz" "http://data.statmt.org/wmt17/translation-task/training-parallel-nc-v12.tgz" "http://data.statmt.org/wmt17/translation-task/dev.tgz" "http://statmt.org/wmt14/test-full.tgz")FILES=( "training-parallel-europarl-v7.tgz" "training-parallel-commoncrawl.tgz" "training-parallel-nc-v12.tgz" "dev.tgz" "test-full.tgz")CORPORA=( "training/europarl-v7.de-en" "commoncrawl.de-en" "training/news-commentary-v12.de-en")# This will make the dataset compatible to the one used in "Convolutional Sequence to Sequence Learning"# https://arxiv.org/abs/1705.03122if [ "$1" == "--icml17" ]; then URLS[2]="http://statmt.org/wmt14/training-parallel-nc-v9.tgz" FILES[2]="training-parallel-nc-v9.tgz" CORPORA[2]="training/news-commentary-v9.de-en"fiif [ ! -d "$SCRIPTS" ]; then echo "Please set SCRIPTS variable correctly to point to Moses scripts." exitfisrc=entgt=delang=en-deprep=wmt14_en_detmp=$prep/tmporig=origdev=dev/newstest2013mkdir -p $orig $tmp $prepcd $origfor ((i=0;i&lt;$&#123;#URLS[@]&#125;;++i)); do file=$&#123;FILES[i]&#125; if [ -f $file ]; then echo "$file already exists, skipping download" else url=$&#123;URLS[i]&#125; wget "$url" if [ -f $file ]; then echo "$url successfully downloaded." else echo "$url not successfully downloaded." exit -1 fi if [ $&#123;file: -4&#125; == ".tgz" ]; then tar zxvf $file elif [ $&#123;file: -4&#125; == ".tar" ]; then tar xvf $file fi fidonecd ..echo "pre-processing train data..."for l in $src $tgt; do rm $tmp/train.tags.$lang.tok.$l for f in "$&#123;CORPORA[@]&#125;"; do cat $orig/$f.$l | \ perl $NORM_PUNC $l | \ perl $REM_NON_PRINT_CHAR | \ perl $TOKENIZER -threads 8 -a -l $l &gt;&gt; $tmp/train.tags.$lang.tok.$l donedoneecho "pre-processing test data..."for l in $src $tgt; do if [ "$l" == "$src" ]; then t="src" else t="ref" fi grep '&lt;seg id' $orig/test-full/newstest2014-deen-$t.$l.sgm | \ sed -e 's/&lt;seg id="[0-9]*"&gt;\s*//g' | \ sed -e 's/\s*&lt;\/seg&gt;\s*//g' | \ sed -e "s/\’/\'/g" | \ perl $TOKENIZER -threads 8 -a -l $l &gt; $tmp/test.$l echo ""doneecho "splitting train and valid..."for l in $src $tgt; do awk '&#123;if (NR%100 == 0) print $0; &#125;' $tmp/train.tags.$lang.tok.$l &gt; $tmp/valid.$l awk '&#123;if (NR%100 != 0) print $0; &#125;' $tmp/train.tags.$lang.tok.$l &gt; $tmp/train.$ldoneTRAIN=$tmp/train.de-enBPE_CODE=$prep/coderm -f $TRAINfor l in $src $tgt; do cat $tmp/train.$l &gt;&gt; $TRAINdoneecho "learn_bpe.py on $&#123;TRAIN&#125;..."python $BPEROOT/learn_bpe.py -s $BPE_TOKENS &lt; $TRAIN &gt; $BPE_CODEfor L in $src $tgt; do for f in train.$L valid.$L test.$L; do echo "apply_bpe.py to $&#123;f&#125;..." python $BPEROOT/apply_bpe.py -c $BPE_CODE &lt; $tmp/$f &gt; $tmp/bpe.$f donedoneperl $CLEAN -ratio 1.5 $tmp/bpe.train $src $tgt $prep/train 1 250perl $CLEAN -ratio 1.5 $tmp/bpe.valid $src $tgt $prep/valid 1 250for L in $src $tgt; do cp $tmp/bpe.test.$L $prep/test.$Ldone 如果希望使用预处理好的数据，则可以使用WMT16 EN-DE，地址为：https://drive.google.com/uc?export=download&amp;id=0B_bZck-ksdkpM25jRUN2X2UxMm8并解压。 Step2接下来对数据进行二值化(binarize): 12345678910TEXT=wmt16_en_de_bpe32kmkdir $TEXTtar -xzvf wmt16_en_de.tar.gz -C $TEXT # 解压文件python preprocess.py --source-lang en --target-lang de \ --trainpref $TEXT/train.tok.clean.bpe.32000 \ --validpref $TEXT/newstest2013.tok.bpe.32000 \ --testpref $TEXT/newstest2014.tok.bpe.32000 \ --destdir data-bin/wmt16_en_de_bpe32k \ --nwordssrc 32768 --nwordstgt 32768 \ --joined-dictionary 到这里，麻烦的预处理就结束了。 训练cd到fairseq目录下，执行以下命令： 1CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node 8 train.py data-bin/wmt16_en_de_bpe32k \ --arch transformer_wmt_en_de --share-all-embeddings \ --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \ --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 \ --lr 0.0007 --min-lr 1e-09 \ --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --weight-decay 0.0\ --max-tokens 4096 --save-dir checkpoints/en-de-base\ --no-progress-bar --log-format json --log-interval 50\ --save-interval-updates 1000 --keep-interval-updates 20 注意到该设置与原论文不大一致。但已证实该设置可以复现论文结果。 如果没有这么多卡，那么可以设置update freq以模拟8卡行为。如： 1234567891011CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node 4 \train.py data-bin/wmt16_en_de_bpe32k \ --arch transformer_wmt_en_de --share-all-embeddings \--optimizer adam --adam-betas '(0.9, 0.98)' \--clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 4000 \--lr 0.0007 --min-lr 1e-09 --criterion label_smoothed_cross_entropy \--label-smoothing 0.1 --weight-decay 0.0 --max-tokens 4096 \--save-dir checkpoints/en-de-16-base \ --no-progress-bar --log-format json --log-interval 50 --save-interval-updates 1000 \--keep-interval-updates 20 --update-freq 2 |tee exp2.log 4张卡则设update freq=2，2张卡则设update freq=4，以此类推。 大概在100个epoch内能够收敛，也即在475000个step。8张1080Ti在大概两天能够训练完成，4张1080Ti大概4天训练完成。 开始训练… 最后则会获得checkpoint： 测试测试分为几个阶段：首先将几个checkpoint进行平均，实验表明，进行平均能够有一定的提升；其次，使用平均后的模型对test集的句子进行翻译；最终将生成的句子和正确的句子计算bleu值。 average checkpoint在测试阶段，论文在Transformer-base中对最后五个checkpoint进行平均，也即对权值进行平均： 123python scripts/average_checkpoints.py \--inputs checkpoints/en-de-base/ \--num-epoch-checkpoints 5 --output averaged_model.pt 最终获得averaged_model.pt，我们将用该文件进行测试。 generate我们采用和论文一致的超参： 1234CUDA_VISIBLE_DEVICES=0 python generate.py \data-bin/wmt16_en_de_bpe32k/ --path /some_checkpoint \--remove-bpe --beam 4 --batch-size 64 --lenpen 0.6 \--max-len-a 1 --max-len-b 50|tee generate.out 其中lenpen是生成句子的长度惩罚系数；max-len-a和max-len-b指的是每个句子的最长长度限制，也即：假设源句子长度为x，则目标句子的长度应小于ax+b 。 最终我们翻译好的句子以及相对应的详细信息都在generate.out里面。我们需要提取源语言句子和目标语言句子，以方便后面的计算。因此： 123grep ^T generate.out | cut -f2- | perl -ple 's&#123;(\S)-(\S)&#125;&#123;$1 ##AT##-##AT## $2&#125;g' &gt; generate.refgrep ^H generate.out |cut -f3- | perl -ple 's&#123;(\S)-(\S)&#125;&#123;$1 ##AT##-##AT## $2&#125;g' &gt; generate.sys 分别运行这两个bash命令，我们则获得了generate.ref和generate.sys，分别是目标和源语言的句子。 注意到这里有一个非常重要的小trick，也即split compound。因为一些历史原因（我也不知道为啥，tensor2tensor里面的脚本有提到），该trick已经在上面的脚本命令体现出来了。实践证明，使用该trick能够提高bleu值 0.5个点以上。 score我们此时就可以计算bleu值了，fairseq提供了该脚本： 1python score.py --sys generate.sys --ref generate.ref 大功告成！我们终于复现出结果了。作为参考：根据我的实验，只使用checkpoint中最好的一个checkpoint，在经过了上述的流程后，可以得到27.30的结果。 其他根据我的需求，我还需要详细记录中间结果，并打印在tensorboard上方便可视化，如： fairseq并没有提供这种功能，因此需要自己修改部分源代码。只需要修改train.py源文件即可。 ①在开头加summary writer 注意到每次实验都需要修改实验的名字。 ②修改train函数在训练过程中，添加记录的代码： 在epoch结束，添加记录的代码： 对validate的使用进行修改（添加了is_epoch）： train函数全部代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273def train(args, trainer, task, epoch_itr): """Train the model for one epoch.""" # Update parameters every N batches if epoch_itr.epoch &lt;= len(args.update_freq): update_freq = args.update_freq[epoch_itr.epoch - 1] else: update_freq = args.update_freq[-1] # Initialize data iterator itr = epoch_itr.next_epoch_itr(fix_batches_to_gpus=args.fix_batches_to_gpus) itr = iterators.GroupedIterator(itr, update_freq) progress = progress_bar.build_progress_bar( args, itr, epoch_itr.epoch, no_progress_bar='simple', ) extra_meters = collections.defaultdict(lambda: AverageMeter()) first_valid = args.valid_subset.split(',')[0] max_update = args.max_update or math.inf for i, samples in enumerate(progress, start=epoch_itr.iterations_in_epoch): log_output = trainer.train_step(samples) if log_output is None: continue # log mid-epoch stats stats = get_training_stats(trainer) num_updates = stats['num_updates'] # print(type(num_updates)) # print(type(stats['loss'])) summary_writer.add_scalar('Training/training_loss_update', float(stats['loss']), num_updates) summary_writer.add_scalar('Training/training_nll_loss_update', float(stats['nll_loss']), num_updates) summary_writer.add_scalar('Training/training_ppl_update', float(stats['ppl']), num_updates) # ------record training metrics --- # for k, v in log_output.items(): if k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size']: continue # these are already logged above if 'loss' in k: extra_meters[k].update(v, log_output['sample_size']) else: extra_meters[k].update(v) stats[k] = extra_meters[k].avg progress.log(stats) # ignore the first mini-batch in words-per-second calculation if i == 0: trainer.get_meter('wps').reset() num_updates = trainer.get_num_updates() if args.save_interval_updates &gt; 0 and num_updates % args.save_interval_updates == 0 and num_updates &gt; 0: valid_losses = validate(args, trainer, task, epoch_itr, [first_valid], is_epoch=False) save_checkpoint(args, trainer, epoch_itr, valid_losses[0]) if num_updates &gt;= max_update: break # log end-of-epoch stats stats = get_training_stats(trainer) # ------record training metrics --- # summary_writer.add_scalar('Training/training_loss_epoch', float(stats['loss']), epoch_itr.epoch) summary_writer.add_scalar('Training/training_nll_loss_epoch', float(stats['nll_loss']), epoch_itr.epoch) summary_writer.add_scalar('Training/training_ppl_epoch', float(stats['ppl']), epoch_itr.epoch) for k, meter in extra_meters.items(): stats[k] = meter.avg progress.print(stats) # reset training meters for k in [ 'train_loss', 'train_nll_loss', 'wps', 'ups', 'wpb', 'bsz', 'gnorm', 'clip', ]: meter = trainer.get_meter(k) if meter is not None: meter.reset() ③修改validate函数添加了一个参数is_epoch： 添加记录的代码： validate全部代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263def validate(args, trainer, task, epoch_itr, subsets, is_epoch=True): """Evaluate the model on the validation set(s) and return the losses.""" valid_losses = [] for subset in subsets: # Initialize data iterator itr = task.get_batch_iterator( dataset=task.dataset(subset), max_tokens=args.max_tokens, max_sentences=args.max_sentences_valid, max_positions=utils.resolve_max_positions( task.max_positions(), trainer.get_model().max_positions(), ), ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test, required_batch_size_multiple=8, seed=args.seed, num_shards=args.distributed_world_size, shard_id=args.distributed_rank, num_workers=args.num_workers, ).next_epoch_itr(shuffle=False) progress = progress_bar.build_progress_bar( args, itr, epoch_itr.epoch, prefix='valid on \'&#123;&#125;\' subset'.format(subset), no_progress_bar='simple' ) # reset validation loss meters for k in ['valid_loss', 'valid_nll_loss']: meter = trainer.get_meter(k) if meter is not None: meter.reset() extra_meters = collections.defaultdict(lambda: AverageMeter()) for sample in progress: log_output = trainer.valid_step(sample) for k, v in log_output.items(): if k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size']: continue extra_meters[k].update(v) # log validation stats stats = get_valid_stats(trainer) # ------record validate metrics --- # if is_epoch: # every epoch summary_writer.add_scalar('Validation/valid_loss_epoch', float(stats['valid_loss']), epoch_itr.epoch) summary_writer.add_scalar('Validation/valid_nll_loss_epoch', float(stats['valid_nll_loss']), epoch_itr.epoch) summary_writer.add_scalar('Validation/valid_ppl_epoch', float(stats['valid_ppl']), epoch_itr.epoch) else: # every n update num_updates = stats['num_updates'] summary_writer.add_scalar('Validation/valid_loss_update', float(stats['valid_loss']), num_updates / args.save_interval_updates) summary_writer.add_scalar('Validation/valid_nll_loss_update', float(stats['valid_nll_loss']), num_updates / args.save_interval_updates) summary_writer.add_scalar('Validation/valid_ppl_update', float(stats['valid_ppl']), num_updates / args.save_interval_updates) for k, meter in extra_meters.items(): stats[k] = meter.avg progress.print(stats) valid_losses.append(stats['valid_loss']) return valid_losses ReferenceReplicating results from “Scaling Neural Machine Translation” How to reproduce the result of WMT14 en-de on transformer BASE model?]]></content>
      <tags>
        <tag>教程</tag>
        <tag>Transformer</tag>
        <tag>NMT</tag>
        <tag>fairseq</tag>
        <tag>机器翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录15]]></title>
    <url>%2F2019%2F01%2F06%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9515%2F</url>
    <content type="text"><![CDATA[1️⃣[flatten multi-dimentional list]对多层嵌套的list进行展平。 12345678910# 递归def flatten(nestedList): def aux(listOrItem): if isinstance(listOrItem, list): for elem in listOrItem: for item in aux(elem): yield item else: yield listOrItem return list(aux(nestedList)) 2️⃣[sorted index]使用内置方法获得排好序的index 123sorted_index=[i[0] for i in sorted(enumerate(sent_length), key=lambda x:x[1], reverse=self.reverse)]]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周论文11]]></title>
    <url>%2F2019%2F01%2F06%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8711%2F</url>
    <content type="text"><![CDATA[1️⃣[Multi-Head Attention with Disagreement Regularization]EMNLP的短文。 鼓励transformer中head与head之间的差异。 加了三种正则化方法：①on subspace ②on attention position ③on output 没什么亮点。 2️⃣[Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting]经典论文。dropout方法很简单，但如何想到，其背后的intuition，以及一些现象很有启发意义。仅罗列一些intuition/motivation以及现象： 网络复杂关系学到很多噪声，导致overfitting 最好的regularization方法是对所有的parameter setting的结果进行average。这就是贝叶斯方法， dropout是对该方法进行近似，论文也提到了model combination dropout能够减少unit之间复杂的co-adaptation，能够更鲁棒，也就是说，不需要依赖其他unit去纠正自己的错误。each hidden unit in a neural network trained with dropout must learn to work with a randomly chosen sample of other units. This should make each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes dropout的特性：sparsity。标准的网络在训练过程中会固化其他unit的错误，导致复杂的co-adaptation，但这种复杂的adaptation会导致泛化性的降低，因为对于未见到的数据这种复杂的adaptation是没用的。因此dropout的网络中每个unit都要学会自己纠正自己的错误，因此每个unit能够独立学到数据的一部分特性。dropout会导致稀疏化，每次都只会有一小部分的activation高。使用dropout配合高的学习率比较好，因为dropout可能会导致gradient之间互相cancel，同时也可以使用高的momentum。]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>attention</tag>
        <tag>dropout</tag>
        <tag>每周论文阅读</tag>
        <tag>regularization</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Pytorch中index_copy_及其思考]]></title>
    <url>%2F2018%2F12%2F31%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADindex_copy_%E5%8F%8A%E5%85%B6%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[前几日因为in-place操作的问题，debug了好几天，最终才发现问题。 12output,_=pad_packed_sequence(output,batch_first=True)output=output.index_copy(0,torch.tensor(sorted_index),output) 因为Pytorch中pack_sequence需要将batch按长度排列，我在过完GRU后需要将其顺序还原，在这边sorted_index即是记录原来index映射。 然而我在写的时候，参考的是官方的example： 123456789&gt;&gt;&gt; x = torch.zeros(5, 3)&gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)&gt;&gt;&gt; index = torch.tensor([0, 4, 2])&gt;&gt;&gt; x.index_copy_(0, index, t)tensor([[ 1., 2., 3.], [ 0., 0., 0.], [ 7., 8., 9.], [ 0., 0., 0.], [ 4., 5., 6.]]) 因此我也不假思索地写：12output,_=pad_packed_sequence(output,batch_first=True)output=output.index_copy_(0,torch.tensor(sorted_index),output) 就因为多了一个_，导致逻辑和我想象中的不一样。 一个简单的例子展示为什么这么是错的： 1234567891011import torchx=torch.Tensor([21,42,45,59])print(x) # tensor([21., 42., 45., 59.])index=torch.tensor([1,2,0,3])x=x.index_copy_(0,index,x)print(x) # tensor([21., 21., 21., 59.]) 由于是in-place操作，第一步，将index=0的数值（也即21）复制到index=1的地方，此时变成[21,21,45,59]；接着将index=1的数值复制到index=2的位置上，注意到之前已经是in-place操作，因此此时取的不是想象中的42，而是已经被替换的21。后面的也是如此。 正确的做法只需要去掉in-place即可。 已经好几次遇到in-place的问题了，在每次做in-place操作时，都要警惕。应尽可能避免in-place操作。实际上Pytorch官方也不建议使用in-place操作。]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>遇到的问题</tag>
        <tag>Pytorch</tag>
        <tag>index_coopy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录14]]></title>
    <url>%2F2018%2F12%2F29%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9514%2F</url>
    <content type="text"><![CDATA[1️⃣[shuffle list]shuffle list可以使用random的shuffle函数，亦即： 12l=[1,2,3,4]shuffle(l) # in place operation 而想要shuffle两个对应list，也即等长且一一对应的list，则可以： 123456789101112131415# borrow from stackoverflowimport randomdef shuffle(a,b): assert len(a) == len(b) start_state = random.getstate() random.shuffle(a) random.setstate(start_state) random.shuffle(b)a = [1,2,3,4,5,6,7,8,9]b = [11,12,13,14,15,16,17,18,19]shuffle(a,b)print(a) # [9, 7, 3, 1, 2, 5, 4, 8, 6]print(b) # [19, 17, 13, 11, 12, 15, 14, 18, 16] 2️⃣[inverse tensor]Pytorch目前还不支持步进为负的情况，因此不能使用类似Python的l[::-1]的方法reverse tensor。一种解决方案： 1234567import torchinv_idx = torch.arange(tensor.size(0)-1, -1, -1).long()# or equivalently torch.range(tensor.size(0)-1, 0, -1).long()inv_tensor = tensor.index_select(0, inv_idx)# or equivalentlyinv_tensor = tensor[inv_idx] 3️⃣[GRU initialization]12345def _gru_init(self): # use orthogonal seems better nn.init.orthogonal_(self.word_RNN.weight_ih_l0.data) #没有data不行，会报leaf variable in-place错误，可能weight_ih_l0不是parameter nn.init.orthogonal_(self.word_RNN.weight_hh_l0.data) self.word_RNN.bias_ih_l0.data.zero_() self.word_RNN.bias_hh_l0.data.zero_() 4️⃣[sort counter]需求：统计document的句子个数的分布，并按照长度顺序排列。 123n_sents=[len(sentences) for sentences in documents]n_lengths=Counter(n_sents)n_lengths=sorted(n_lengths.items())]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识17]]></title>
    <url>%2F2018%2F12%2F29%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8617%2F</url>
    <content type="text"><![CDATA[1️⃣[Pytorch]在有RNN的代码中，如果出现 Cuda Error : RuntimeError: CUDNN_STATUS_EXECUTION_FAILED 那么可能的出错原因是没有将init state放入cuda中。 Reference: https://discuss.pytorch.org/t/cuda-error-runtimeerror-cudnn-status-execution-failed/17625 2️⃣[Pytorch]clone() → TensorReturns a copy of the self tensor. The copy has the same size and data type as self.Unlike copy_(), this function is recorded in the computation graph. Gradients propagating to the cloned tensor will propagate to the original tensor. 如果需要另一个相同的tensor做其他计算，则使用clone()而不是copy_() 123forward_vec=sent_vec# backward_vec=sent_vec wrongbackward_vec=sent_vec.clone() 当然也不能直接赋值，因为赋的只是指针，改变backward_vec也会改变原来的值。 3️⃣[Python]Python中==和is的区别：is表示是否是同一个object；而==表示是否是同一个值。 123456str='GRU'str == 'GRU' # Truestr is 'GRU' # Truestr=str.upper()str == 'GRU' # Falsestr is 'GRU' # True 4️⃣[RNN]在RNN的初始化中，使用正交初始化会比其他方法好一些（待对比实验测验）。Reference: https://smerity.com/articles/2016/orthogonal_init.html 5️⃣[Pytorch]在提供预训练embedding作为初始化时，正确做法： 1234if pretrained_matrix is not None: pretrained_matrix=torch.from_numpy(pretrained_matrix).type(torch.FloatTensor) self.embedding.weight= nn.Parameter(pretrained_matrix, requires_grad=True) 必须要有.type(torch.FloatTensor)，否则会出错：CuDNN error: CUDNN_STATUS_EXECUTION_FAILED 6️⃣[Pytorch]Pytorch中，将初始hidden state作为可学习参数实践：https://discuss.pytorch.org/t/solved-train-initial-hidden-state-of-rnns/2589/9https://discuss.pytorch.org/t/learn-initial-hidden-state-h0-for-rnn/10013/7]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Python</tag>
        <tag>Pytorch</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Day with Google]]></title>
    <url>%2F2018%2F12%2F23%2F%E8%A7%81%E9%97%BB%26%E6%83%B3%E6%B3%95%2FA%20Day%20with%20Google%2F</url>
    <content type="text"><![CDATA[本周一乡下人终于再次进城了🙈 本次的目的是来参观Google。 高楼林立： Here We are: 咕果是什么鬼？ 宣讲： 不得不感慨食堂真好🦆，还有专门吃面的食堂。而且还都不用钱🙉，对比张江的食堂🙉： 溜了溜了：]]></content>
      <tags>
        <tag>Google</tag>
        <tag>活动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周论文10]]></title>
    <url>%2F2018%2F12%2F23%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%8710%2F</url>
    <content type="text"><![CDATA[1️⃣[Regularization of Neural Networks using DropConnect]在dropout的基础上提出dropconnect。与dropout不同的是，dropconnect对weight进行drop而不是对layer进行drop。 创新之处在于inference的时候和dropout不同。 训练 inference 在inference的时候通过高斯采样的方法去模拟训练时的伯努利分布。intuition：本文对dropout在inference简单对unit进行缩放进行反思，认为这在数学上并不合理，因此提出用高斯分布去采样。 2️⃣[Attentive Pooling Networks]提出attentive pooling机制，用以answer selection。（什么是answer selection：给定一个问题，给定多个答案候选，要从答案选项中选择正确的答案。） 传统answer selection：首先将词转化成词向量，接着通过bi-LSTM或CNN获得一个矩阵表示，接下来对Q和A分别进行max-pooling获得固定表示，最后通过cos距离判断答案是否是正确答案，从答案候选中选择分数最高的。 但这样的问题在于Q和A之间没有交互。 本文利用attention作为Q和A的交互。 获得Q和A矩阵的方式是一致的。接下来，首先计算一个G矩阵，通过双线性attention公式获得： G所代表的意义是Q和A的每个词之间的对齐：对于第i行来说，代表Q的第i个词和A中所有词的一个分数；对于第j列来说，代表第j个词和Q中所有词的分数。 接下来对G的行和列分别进行max-pooling操作： 此步代表选择与某词关系最重要的词。 接下来对g分别进行softmax，再分别进行点积以获得最终向量表示： 同样，最终使用cos距离计算相似度。 3️⃣[Improved Regularization of Convolutional Neural Networks with Cutout]是从数据增强和dropout的角度： dropout in convolutional layers simply acts to increase robustness to noisy inputs, rather than having the same model averaging effect that is observed in fully-connected layers 某个输入被移去，所有后面相关的的feature map都被移去： In this sense, cutout is much closer to data augmentation than dropout, as it is not creating noise, but instead generating images that appear novel to the network 其实只是将输入随机drop掉一块。]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>每周论文阅读</tag>
        <tag>DropConnect</tag>
        <tag>Cutout</tag>
        <tag>Attentive Pooling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录13]]></title>
    <url>%2F2018%2F12%2F23%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9513%2F</url>
    <content type="text"><![CDATA[1️⃣[flatten list]对二维list进行展开。 12345678list2d = [[1,2,3],[4,5,6], [7], [8,9]]# ①flatten = [l for list in list2d for l in list]# ②import itertoolsmerged = list(itertools.chain(*list2d))# ormerged = list(itertools.chain.from_iterable(list2d))]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识16]]></title>
    <url>%2F2018%2F12%2F23%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8616%2F</url>
    <content type="text"><![CDATA[1️⃣[Softmax]在使用softmax的时候，要非常注意softmax的行为。应尽量控制softmax前元素的规模，否则容易出现one-hot的情况，导致训练困难。 同时，对全-inf做softmax是未定义的，因此也会出现问题： 2️⃣[slice]在对tensor或array操作时，如果需要取某维的slice： 1234import torcha = torch.rand(2,5)a[:,1:3] # 取第1列到第2列的slicea[:][1:3] # wrong，获得的是第1行到第2行的slice 原因是，a[:][1:3]是先做a[:]操作，获得了全部元素，然后再做[1:3]操作，也即获得第1行到第2行的元素。]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Python</tag>
        <tag>Softmax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录12]]></title>
    <url>%2F2018%2F12%2F16%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9512%2F</url>
    <content type="text"><![CDATA[1️⃣[CUDA time]正确测试代码在cuda运行时间。需要加上torch.cuda.synchronize()。 12345678910111213141516171819202122232425262728293031323334import torchimport timea = torch.randint(high=1000, size=(20, 200, 256)).double().cuda()b = torch.randint(high=1000, size=(20, 200, 256)).double().cuda()torch.cuda.synchronize()start = time.time()M = torch.bmm(a, b.transpose(1, 2))torch.cuda.synchronize()end = time.time()print("bmm", end - start)print("max_mem", torch.cuda.max_memory_allocated())torch.cuda.synchronize()start = time.time()local_a = a.unsqueeze(2)local_b = b.unsqueeze(1)N = (local_a*local_b).sum(-1)torch.cuda.synchronize()end = time.time()print("element-wise", end - start)print("max_mem", torch.cuda.max_memory_allocated())print("output difference (should be 0)", (N - M).abs().max())print("In single precision this can fail because of the size of the tensors.")print("Using double should always work")]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识15]]></title>
    <url>%2F2018%2F12%2F16%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8615%2F</url>
    <content type="text"><![CDATA[1️⃣[Pytorch]在0.41的pytorch中，bernoulli的速度会比随机sample的速度慢很多；在1.0中修复了该bug，但速度上还是随机sample快一点点。 1234567# Pytorch0.41Bernoulli 0.430371046066sample 0.24411702156# Pytorch1.0Bernoulli 0.256921529sample 0.25317035184 123# 以下二者等价mask = Bernoulli(gamma).sample(x.size()) # slowmask = (torch.rand_like(x)&lt;gamma).float() # faster Reference:https://github.com/pytorch/pytorch/issues/6940]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周论文9]]></title>
    <url>%2F2018%2F12%2F16%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%879%2F</url>
    <content type="text"><![CDATA[1️⃣[Sentence-State LSTM for Text Representation]提出一种新型的encode句子的方法。有点类似gather-distribute的想法。 每个时间步t所有的h一起更新。更新方式是与其左右的点进行交互，同时与一个global representation进行交互。这样即考虑了local的信息也考虑了global的信息。每次更新都增加了信息交互，从3gram到5gram再到7gram… 具体来说：①如何求$h_i$ 从公式可以看出，对于一个特定的$h_i$，同时考虑左右两点，以及global信息$g$，以及输入$x$。 ②如何求g 通过average同时考虑所有的词，同时考虑自己上一个状态。]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>每周论文阅读</tag>
        <tag>Encode</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的+=操作]]></title>
    <url>%2F2018%2F12%2F09%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPython%E4%B8%AD%E7%9A%84%2B%3D%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前几日在写一段Pytorch代码时，又一次遇到了in-place操作的问题。 1output+=pos # pos是不可更新的tensor，output是可更新的tensor 程序报错：“one of the variables needed for gradient computation has been modified by an inplace operation”。 无意中将代码改成output=output+pos，程序就不会报错了。 在查阅了相关资料后，将我的思考整理下来。 在Python中，i=i+1和i+=1是不同的，如果被操作数没有部署 ’iadd‘方法，则i=i+1和i+=1是等价的，’+=‘并不会产生in-place操作；当被操作数有部署该方法且正确部署，则是会产生in-place操作的。当没有in-place操作时，i=i+1表示对i重分配，也即i指向了另一个空间而不是原来的空间。 所以，这样的例子就能解释清楚了： 12345678910import numpy as npA = np.arange(12).reshape(4,3)for a in A: a = a + 1# A并没有被改变B = np.arange(12).reshape(4,3)for b in B: b += 1# B被改变了 在Pytorch中，也有部署’iadd()‘操作，所以对于output+=pos，output内部的值被改变了，也即在计算图中引入了环，在反向求导时则会出错。 因此，在Pytorch中，应当避免in-place的操作。 Reference:https://stackoverflow.com/questions/41446833/what-is-the-difference-between-i-i-1-and-i-1-in-a-for-loop]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>遇到的问题</tag>
        <tag>Python</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周论文8]]></title>
    <url>%2F2018%2F12%2F09%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%878%2F</url>
    <content type="text"><![CDATA[1️⃣[DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding]提出了两种attention机制，即 multi-dimentional attention和directional self-attention，在此基础上提出有向自注意力网络（directional self-attention network) Multi-dimensional Attention与传统的方法不同的是，对于每个词对，attention出来的不是标量而是向量。 计算公式： $f$的维度与$q$相同，每一维代表的是$x_i$在该维对$q$的重要性。也即feature-wise的attention。因此对于$q$而言，其获得的加权求和向量为： 使用feature-wise的attention能够解决一次多义的问题，因为能够计算每一维的重要性，在不同的context下有不同的重要性。 将其应用于self-attention中，有两种变体：①token2token 因此x在交互完有： ②source2token 也即$x_i$没有和其他元素有交互。可用作获得sentence encoding： Directional Self-Attention使用mask达到有向性这一目的：通过mask矩阵将位置/方向编码进attention，解决时序丢失问题。首先将x过一层获得新的h表示： 接着使用token2token求attention，这里为了减少参数作了一定改动，将W换成c，tanh替换σ。 $\textbf{1}$是全1的向量。M就是mask矩阵，代表i与j是否连通，Mask矩阵有： 也即： 首先mask掉自己，第二：分别mask掉forward和backward，类似biLSTM，只和前面或后面的交互。 Directional Self-Attention Network在上述两个方法的基础上，此时已获得了上下文相关的$s_i$，再引入fusion gate： 整个流程： 将前向和反向的表示拼接起来，获得最终的表示$[u^{fw};u^{bw}]$： 对于所获得的每一个表示，通过source2token，获得最终的句子表示。 这一点论文也提到了，非常类似bi-LSTM。 2️⃣[Targeted Dropout]一种网络剪枝方法，想法简单易实现。简单说，在每次更新时对最不重要的weight或者unit进行随机dropout。 Targeted DropoutDropout给定输入X，权重W，输出Y M为dropout的mask矩阵。unit dropout： weight dropout： 也即drop掉的是layer之间的connection。 Magnitude-based pruning剪枝通常对权重最小的进行剪枝，也即保留topk个最大的权重。 Unit pruning：直接剪掉的是一整列，也即一个unit Weight pruning：对W的每个元素进行剪枝。注意是对每行的topk进行保留 可以理解成对一个unit来说，保留最高的k个connection。 方法结合dropout和剪枝。主要思想：首先选择N-k最不重要的element，由于我们希望这些low-value的元素有机会在训练过程中变得重要，因此我们对这些element进行随机dropout。 引入targeting proportion γ和drop probability α，亦即：选择最低的γ|θ|个weight，再根据α进行dropout。这样做的结果是：减少重要的子网络对不重要的子网络的依赖。 附录①dropout的intuition：减少unit之间的相互适应。when dropout is applied to a unit, the remaining network can no longer depend on that unit’s contribution to the function and must learn to propagate that unit’s information through a more reliable channel。也可以理解成：使得unit之间的交互信息达到最大，在失去某个unit的时候影响不会那么大。 ②targeted dropout intuition：the important subnetwork is completely separated from the unimportant one。假设一个网络由两个不相交的子网络组成，每个都能输出正确的结果，总的网络是这两个网络的平均。我们通过对不重要的子网络进行dropout（也即往子网络里加noise，会破坏该子网络的输出，由于重要的子网络已经能够输出正确的结果，因此为了减少损失，我们需要减少不重要网络的输出到0，也即kill掉该子网络，并且加强这两个网络的分离。（为什么不直接舍弃呢？因为是在训练过程中，有可能会有变化）这个解释还是没完全懂。 3️⃣[A2-Nets: Double Attention Networks]发表于NIPS2018，个人认为很有启发。提出一种新的attention机制，基于“收集-分发”的思想，能够让CNN获得更大的感受野。 MotivationCNN本身主要是捕获局部特征与关系，但对于长距离之间的关系只能通过堆叠多几层才能实现。但这样需要更高的计算量，且容易过拟合；同时，远处的特征实际上是来自好几层的延迟，导致推理的困难。 通过将feature收集起来，然后分发下去，使得feature之间有交互，让CNN获得更大的感受野，能够捕获长距离的特征。 方法 也即： X是所有输入，$v_i是$local feature。 The First Attention Step: Feature Gathering对于两个feature map A,B，有： 其中： 如果A、B都来自同一个X，将B归一化softmax，就类似transformer的attention。其中上式的最右边是外积的形式。 我们将G拆分成向量形式：同时将B重写成行向量形式，则有： 则会有： 上式让我们有一个新的理解角度：G实际上就是 a bag of visual primitives。每个$g_i$是所有local feature加权求和，其中$b_i$是求和的weight。 因此我们对B做softmax，保证权重为1： The Second Attention Step: Feature Distribution在获得了全局的feature G后，现在根据local feature去获取全局feature的部分，这通过一个权重控制，也即$v_i$（local feature)的每一维作为权重。可以不将local feature $v_i$归一化，但归一化能更好地converge。 The Double Attention Block最终得到double attention block： 整个流程： 所以其实是有三个convolution layer。 上式还可以写成：数学上等价，但计算上差很多。第一个式子会有更低的复杂度。 思考虽然用了attention，但这里和Transformer还是有非常大的区别的。Transformer每个元素都和其他元素有交互，通过直接的计算得到权重。而这边的权重由feature本身来决定。并没有直接的交互。]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>attention</tag>
        <tag>每周论文阅读</tag>
        <tag>DiSAN</tag>
        <tag>Dropout</tag>
        <tag>Targeted Dropout</tag>
        <tag>double attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识14]]></title>
    <url>%2F2018%2F12%2F09%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8614%2F</url>
    <content type="text"><![CDATA[1️⃣[Pytorch]Pytorch的tensor和Tensor是有区别的： 123import torcha = torch.tensor(2) # 是标量，size为[]b = torch.Tensor(2) # 是向量，size为[2]]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词17]]></title>
    <url>%2F2018%2F12%2F09%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D17%2F</url>
    <content type="text"><![CDATA[1️⃣虞美人[宋] 叶梦得落花已作风前舞，又送黄昏雨。晓来庭院半残红，惟有游丝，千丈袅晴空。殷勤花下同携手，更尽杯中酒。美人不用敛蛾眉，我亦多情，无奈酒阑时。]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识13]]></title>
    <url>%2F2018%2F12%2F02%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8613%2F</url>
    <content type="text"><![CDATA[1️⃣[attention]所有attention的总结：Attention? Attention! 2️⃣[Pytorch]①torch.no_grad能够显著减少内存使用，model.eval不能。因为eval不会关闭历史追踪。 model.eval() will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval model instead of training mode.torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script). Reference:Does model.eval() &amp; with torch.set_grad_enabled(is_train) have the same effect for grad history? ‘model.eval()’ vs ‘with torch.no_grad()’ ②torch.full(…) returns a tensor filled with value.]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Pytorch</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录11]]></title>
    <url>%2F2018%2F12%2F02%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9511%2F</url>
    <content type="text"><![CDATA[①需求：对于两个向量$a$、$b$，$a,b \in R^d$，定义一种减法，有： a-b=M其中$M \in R^{d\times d}$，$M_{ij}=a_i-b_j$ 在代码中实际的维度： 12a=torch.rand(batch_size,sequence_len,dim)b=torch.rand(batch_size,sequence_len,dim) 方法①：for循环 123456M=torch.zeros(bz,seq_len,seq_len)for b_i in range(bz): for i in range(seq_len): for j in range(seq_len): M_ij=torch.norm(a[b_i][i]-b[b_i][j]) M[b][i][j]=M_ij 方法②：矩阵运算 123a=a.unsqueeze(2) # bz,seq_len,1,dimb=b.unsqueeze(1) # bz,1,seq_lens,dimM=torch.norm(a-b,dim=-1) # will broadcast ②需求，生成一个mask矩阵，每一行有一段连续的位置填充1，其中每一行填充1的开始位置和结束位置都不同。具体来说，先生成一个中心位置center，则开始位置为center-window；结束位置为center+window。其中开始位置和结束位置不能越界，也即不小于0和大于行的总长度。如： 思路：①先生成n行每行对应的随机中心位置，然后再获得左和右边界 12345678centers=torch.randint(low=0,high=query_len,size=(query_len,),dtype=torch.long)left=centers-self.windowleft=torch.max(left,torch.LongTensor([0])).unsqueeze(1) # query_len,1right=centers+self.windowright=torch.min(right,torch.LongTensor([query_len-1])).unsqueeze(1) # query_len,1 ②生成一个每行都用[0,n-1]填充的矩阵，[0,n-1]表示的是该元素的index，亦即： 1range_matrix=torch.range(0,query_len-1,dtype=torch.long).unsqueeze(0).expand(query_len,-1) # query_len,query_len ③利用&lt;=和&gt;=获得一个左边界和右边界矩阵，左边界矩阵表示在该左边界的左边都是填充的1；右边界矩阵表示在该右边界右边都是填充的1。再进行异或操作。 1234range_matrix=torch.range(0,query_len-1,dtype=torch.long).unsqueeze(0).expand(query_len,-1) # query_len,query_lenleft_matrix=range_matrix&lt;=leftright_matrix=range_matrix&lt;=rightfinal_matrix=left_matrix^right_matrix]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周论文7]]></title>
    <url>%2F2018%2F12%2F02%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%877%2F</url>
    <content type="text"><![CDATA[1️⃣[Convolutional Self-Attention Network]对self-attention进行改进，引入CNN的local-bias，也即对query的邻近词进行attention而不是所有词；将self-attention扩展到2D，也即让不同的head之间也有attention交互。 Motivation1️⃣the normalization in Softmax may inhibits the attention to neighboring information 也即邻居的信息更重要，要加强邻居的重要性 2️⃣features can be better captured by modeling dependencies across different channels 对于不同的channel/head也增加他们之间的交互。 方法 对于1D的convolution：选取中心词周围一个window： 对于2D的convolution，则有： 在具体实践中，只对前三层添加local bias，这是因为modeling locality在底层更有效，对于高层应该捕获更远的信息。 2️⃣[Modeling Localness for Self-Attention Networks]和上文一样，引入local bias对self-attention进行改进，从而提升了翻译表现。和上文是同一作者，发在EMNLP上。 Motivation1️⃣self-attention存在的问题：虽然能够增加长程关注，但因此会导致注意力的分散，对邻居的信号会忽略。实践证明，对local bias建模在self-attention有提升。 2️⃣从直觉上来说，在翻译模型中，当目标词i与源语言词j有对齐关系时，我们希望词i能同时对词j周围的词进行对齐，使得能够捕获上下文信息，如phrase的信息。 方法在原来的公式上添加G：也即： G是一个alignment position matrix（对齐位置矩阵），元素ij代表目标词i与源语言词j之间的紧密程度。我们每次根据目标词i预测一个源语言的中心词，则$G_{ij}$则为： $P_i$就是对于目标词j而言源语言的中心词。 $\sigma$ 手动设定，通常是$\frac{D}{2}$，D代表窗口大小。 也即最终我们需要计算的是，中心词$P_i$和窗口$D$。 计算$P_i$利用对应的目标词i的query即可：$p_i$是一个实数。 计算window size①固定窗口，将其作为一个超参。 ②Layer-Speciﬁc Window将该层所有的key平均，计算出一个共享的window size： ③Query-Speciﬁc Window每个query都有自己的window size 实验分析与结论①将model locality用于低层效果会更好，这是因为低层对相邻建模，而越高层越关注更远的词。 ②将model locality放在encoder和encoder-decoder部分会更好（transformer有三个地方可以放） 因为decoder本身就倾向关注临近的词，如果继续让其关注临近的词，那么就难以进行长程建模。 ③越高层，window size（scope）越大。 也即，在底层更倾向于捕获邻近词的语义；而高层倾向捕获长程依赖。但这不包括第一层，第一层是embedding，还没有上下文信息，因此倾向于捕获全局信息。 3️⃣[Effective Approaches to Attention-based Neural Machine Translation]提出两种attention机制的翻译模型，global和local。 本文与原版的翻译模型略有不同： c是context，h是decode的隐层。 global attention 计算attention分数： score有多种选择： 注意到该模型与第一个提出attention based的模型不同之处：$h_t -&gt; a_t -&gt; c_t -&gt; \tilde{h_t}$原版是：$h_{t-1} -&gt; a_t -&gt; c_t -&gt; h_t$ local attention 由于global attention计算代价高，且对于长句效果不好，我们可以选择一部分来做attention。首先生成一个对齐位置$p_t$，再选择一个窗口$[p_t - D,p_t + D]$，其中D是超参。 如何获得$p_t$?①直接假设$p_t=t$，也即source和target的位置大致一一对应。 ②做预测：其中S是source的句子长度。 接着，以$p_t$为中心，添加一个高斯分布。最终attention计算公式： 其中align和上面一致： 也就是说，将位置信息也考虑进来。 Input-feeding Approachmotivation：在下一次的alignment（也就是计算attention）之前，应当知道之前的alignment情况，所以应当作为输入信息传进下一层： 注意这里和Bahdanau的不同。Bahdanau是直接用上下文去构造隐层。这里提出的模型相对更为通用，也可以被应用于非attention的模型中（也就是每次将encoder的最后一层作为输入在每个time step都输入） 4️⃣[Towards Linear Time Neural Machine Translation with Capsule Networks]思想：利用capsule提前生成source sentence的固定长度的表示，在decode的时候直接使用，而不需要attention，以达到线性时间NMT的目的。 Motivation：attention-based的NMT时间复杂度为$|S|\times |T|$，而本文希望能够将NMT减少到线性时间。而传统不加attention的NMT通常使用LSTM最后一层隐层作为源语言的encode信息传入decode，但这样的信息并不能很好地代表整个句子，因此本文使用capsule作为提取source sentence信息的方法，利用capsule生成固定长度表示，直接传入decode端，以达到线性时间的目的。 问题定义对于embedding：希望能够转换成固定长度的表示C： 我们首先通过一个双向的LSTM： 一种简单的获取C的方法：其中$h_1$和$h_L$有互补关系。 本文使用capsule提取更丰富的信息。 在decode阶段，由于拥有固定表示，那么就不需要attention： 总体架构： Aggregation layers with Capsule Networks实际上就是dynamic routing那一套，对信息进行提取（论文公式有误就不贴图了） 算法： 最终获得了： 5️⃣[DropBlock: A regularization method for convolutional networks]重读了一遍。介绍一种新型的dropout，可用于卷积层提高表现。通过大量的实验得出许多有意义的结论。本文发表于NIPS2018。 Motivation由于卷积层的feature相互之间有联系，即使使用了dropout，信息也能够根据周围的feature传到下一层。因此使用dropblock，一次将一个方块内的都drop掉。 算法 其中有两个超参：①block_size表示块的大小；γ表示有多少个unit要drop掉，等价传统的dropout的p。当block_size=1时等价dropout；当block size=整个feature map，等价于spatial dropout。 在实践中，通过以下公式计算γ： (why? 通过计算期望的方式将传统dropout的keep_prob与当前的γ联系起来，得到一个等式，整理即可获得上式） 在实验中，还可以逐渐减小keep_prob使得更加鲁棒性。 实验&amp;结论①效果:dropout&lt; spatial dropout &lt; dropblock ②dropblock能有效去掉semantic information ③dropblock是一个更加强的regularization ④使用dropblock的模型，能够学习更多的区域，而不是只专注于一个区域 对于resnet，直接将dropblock应用于添加完skip connection后的feature能够有更高的表现。 6️⃣[Contextual String Embeddings for Sequence Labeling]提出一种建立在character基础上的新型的上下文embedding(contextualized embedding）。用于sequence labeling。本文发表于coling2018。 方法整体架构： 首先将character作为基本单位，过一个双向LSTM，进行language model的建模。 如何提取一个词的词向量：提取前向LSTM中该词的最后一个character的后一个hidden state，以及后向LSTM中第一个词的前一个hidden state， 如上图所示。最终拼起来即可：因此该词不仅与词内部的character相关，还跟其周围的context有关。 sequence labeling我不感兴趣，该部分没看。 Discussion相比word level的language model，character-level独立于tokenization和fixed vocabulary，模型更容易被训练，因为词表小且训练时间短。]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>attention</tag>
        <tag>capsule</tag>
        <tag>每周论文阅读</tag>
        <tag>self-attention</tag>
        <tag>NMT</tag>
        <tag>locality modeling</tag>
        <tag>dropblock</tag>
        <tag>contextualized embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词16]]></title>
    <url>%2F2018%2F12%2F01%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D16%2F</url>
    <content type="text"><![CDATA[1️⃣菩萨蛮[五代十国] 李煜人生愁恨何能免，销魂独我情何限！故国梦重归，觉来双泪垂。髙楼谁与上？长记秋晴望。往事已成空，还如一梦中。 觉(jue)来：醒来。 2️⃣南乡子 · 和杨元素，时移守密州[宋] 苏轼东武望馀杭，云海天涯两杳茫。何日功成名遂了，还乡，醉笑陪公三万场。不用诉离觞，痛饮从来别有肠。今夜送归灯火冷，河塘，堕泪羊公却姓杨。]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无题]]></title>
    <url>%2F2018%2F12%2F01%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E4%BA%BA%E4%B8%8D%E5%8F%AF%E8%83%BD%E7%BB%8F%E5%8E%86%E4%B8%96%E7%95%8C%E4%B8%8A%E6%89%80%E6%9C%89%E7%83%AD%E9%97%B9%2F</url>
    <content type="text"><![CDATA[人不可能经历世界上所有热闹，但可以用眼睛看，用心感受，用胸怀扩张。]]></content>
      <tags>
        <tag>佳句分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识12]]></title>
    <url>%2F2018%2F11%2F19%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8612%2F</url>
    <content type="text"><![CDATA[1️⃣[Transformer]对Transformer新理解： 可以将Transformer理解成一张全连接图，其中每个节点与其他节点的关系通过attention权重表现。图关系是序列关系或者树关系的一般化。 为什么要有multi-head？不仅仅是论文的解释，或许还可以理解成，对一个向量的不同部分（如第1维到20维，第21维到40维等）施以不同的attention权重，如果不使用multi-head，那么对于一个query，就只会有一个权重，而不同的维度有不同的重要性。 2️⃣[attention&amp;capsule]attention是收信息，query从value按权重获取信息，其中所有value的权重和是1。capsule是发信息，对于$l-1$层的一个capsule来说，在传入到$l$层的k个capsule的信息，其权重和为1。]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Transformer</tag>
        <tag>attention</tag>
        <tag>capsule</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周论文6]]></title>
    <url>%2F2018%2F11%2F19%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%876%2F</url>
    <content type="text"><![CDATA[1️⃣[A STRUCTURED SELF ATTENTIVE SENTENCE EMBEDDING]介绍了一种生成sentence embedding的方法。与其他sentence embedding不同的地方在于，生成的是一个矩阵而不是一个向量。通过矩阵的形式，能够关注不同部分的语义表示，类似于Transformer的multi-head。 Contribution: 将sentence embedding扩展为矩阵形式，能够获得更多的信息。 引入正则化，使得sentence matrix具有更丰富的多样性。 方法双向LSTM+self-attention。 双向的LSTM获得上下文的表示： 因此可以获得attention权重向量： 其中$H:n\times2u,W_{s1}:d_a\times2u ,w_{s2}:d_a$ ，$d_a$是超参。 现将向量$w_{s2}$扩展为矩阵，亦即有Multi-hop attention： $W_{s2}$维度为$r\times d_a$，$r$代表了head的个数。 因此最终的sentence embedding矩阵为： 正则化为了让A尽可能有多样性（因为如果都是相似的，那么则会有冗余性），引入如下的正则化： 原因：对于不同的head $a^i$与$a^j$，$A A^T$有： 如果$a^i$与$a^j$很相似那么就会接近于1，如果非常不相似(no overlay)则会接近于0。因此整个式子就是:希望对角线部分接近于0（因为减了单位阵），这就相当于尽可能focus小部分的词；同时其他部分尽可能接近于0，也即不同的head之间没有overlap。 如何使用文章提到，在做分类的时候可以直接将矩阵M展开，过全连接层即可。 2️⃣[Attention-over-Attention Neural Networks for Reading Comprehension]在完形填空任务(Cloze-style Reading Comprehension)上提出一种新的attention，即nested-attention。 任务描述三元组 $ D,Q,A $，document，question，answer。其中answer一般是document的一个词。 方法本文提出的attention机制，是通过一个新的attention去指示另一个attention的重要程度。 首先通过一层共享的embedding层，将document和query都encode成word embedding，然后通过双向的GRU，将隐层拼接起来成为新的表示。 接着获得pair-wise matching matrix： 其中$h$代表上述提到的拼接起来的表示，$M(i,j)$代表了document的词$i$和question的词$j$之间的匹配程度。 接着对column做softmax：其代表的意义即query-to-document attention，亦即对于一个query内的词，document的每个词与其匹配的权重。 接下来，对row进行softmax操作：代表的是给定一个document的词，query的哪个词更为重要。 接下来我们将β平均起来，获得一个向量：这个向量仍有attention的性质，即所有元素加和为1。代表的是从平均来看，query词的重要性。 最后，我们对α和β做点积以获得attended document-level attention： 其中$s$的维度是$D\times 1$。s代表的意义即“a weighted sum of each individual document-level attention α(t) when looking at query word at time t”，也就是说，对α进行加权，代表query word的平均重要程度。 最终在做完型填空的预测时： 个人觉得这种attention-over-attention的想法还是挺有创新的。]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>attention</tag>
        <tag>每周论文阅读</tag>
        <tag>sentence embedding</tag>
        <tag>nested attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络优化与正则化总结]]></title>
    <url>%2F2018%2F11%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[大量参考自《神经网络与深度学习》 优化算法对于标准的SGD，常见的改进算法从两个方面进行：学习率衰减&amp;梯度方向优化。记$g_t$为t时刻的导数： 学习率衰减AdaGrad算法通过计算历次的梯度平方累计值进行学习率衰减。$G_t$是累计值： 更新值则为： 缺点：随着迭代次数的增加学习率递减。在经过一定次数的迭代依然没有找到最优点时，由于这时的学习率已经非常小，很难再继续找到最优点。 RMSprop算法对AdaGrad的改进，唯一的区别在于$G_t$的计算，将历史信息和当前信息进行线性加权，使得学习率可以动态改变而不是单调递减： β为衰减率，通常取0.9。也即历史信息占主导。 AdaDelta算法同样是对AdaGrad的改进。每次计算： 也即历史更新差和上一时刻的更新差的加权（RMSprop是历史梯度和当前梯度）。 最终更新差值为： 其中$G_t$计算方法和RMSprop一致。 梯度方向优化利用历史的梯度（方向）调整当前时刻的梯度。 动量（Momentum）法动量法（Momentum Method）是用之前积累动量来替代真正的梯度。每次迭代的梯度可以看作是加速度。 也即上一时刻的更新差值和当前梯度共同决定当前的更新差值。$ρ$为动量因子，通常为0.9。也即动量占了主导。 当某个参数在最近一段时间内的梯度方向不一致时，其真实的参数更新幅度变小；相反，当在最近一段时间内的梯度方向都一致时，其真实的参数更新幅度变大，起到加速作用。一般而言，在迭代初期，梯度方法都比较一致，动量法会起到加速作用，可以更快地到达最优点。在迭代后期，梯度方法会取决不一致，在收敛值附近震荡，动量法会起到减速作用，增加稳定性。 Nesterov加速梯度动量法的改进版本。 前面提到的动量法，是上一步的更新方向$\Delta \theta_{t-1}$与当前梯度$-g_t$的加和。因此可以理解成，先根据$∆θ_{t−1}$更新一次得到参数θ，再用$g_t$进行更新。亦即：上式的第二步中，$g_t$是在$ \theta_{t-1}$上的梯度。我们将该步改为在$\theta_{t}$的梯度。因此，有： 和动量法相比，相当于提前走了一步。 Adam&amp;NadamAdam一方面计算梯度平方的加权，同时还计算梯度的加权：通常$β_1=0.9$，$β_2=0.99$也即历史信息占了主导。 在初期$M_t$与$G_t$会比真实均值和方差要小（想象$M_0=0$，$G_0=0$时）。因此对其进行修正，即：因此最终有： 同理有Nadam。 Adam = Momentum + RMSpropNadam = Nesterov + RMSprop 梯度截断 gradient clipping分为按值截断与按模截断。 参数初始化初始值选取很关键。假设全部初始化为0，则后续更新导致所有的激活值相同，也即对称权重现象。 原则：不能过大，否则激活值会变得饱和，如sigmoid；不能过小，否则经过多层信号会逐渐消失，并且导致sigmoid丢失非线性的能力（在0附近基本近似线性）。如果一个神经元的输入连接很多，它的每个输入连接上的权重就应该小一些，这是为了避免输出过大。 Gaussian分布初始化同时考虑输入输出，可以按 $N(0,\sqrt{\frac{2}{n_{in} + n_{out}}})$ 高斯分布来初始化。 均匀分布初始化在$[-r,r]$区间均匀分布初始化，其中r可以按照神经元数量自适应调整。 Xavier初始化方法自动计算超参r。r的公式为：其中$n^l$代表第$l$层的神经元个数。 为什么是这个式子（推导见参考资料）：综合考虑了①输入输出的方差要一致；②反向传播中误差信号的方差不被放大或缩小。 归一化将数据分布归一化，使得分布保持稳定。假设数据有四维(N,C,H,W)。N代表batch；C代表channel；H,W代表height和width。 Batch Normalization沿着通道进行归一化，亦即每个通道都有自己的均值和方差。其中缩放平移变量是可学习的。 缺点：①对batch size敏感，batch size太小则方差均值不足以代表数据分布②对于不等长的输入如RNN来说，每一个timestep都需要保存不同的特征。 Layer Normalization对一个输入进行正则化，亦即每个输入都有自己的方差、均值。这样不依赖于batch大小和输入sequence的深度。 对RNN效果比较明显，但CNN中不如BN Instance Normalization对HW进行归一化 Group Normalization将channel分为多个group，每个group内做归一化 Reference《神经网络与深度学习》https://blog.csdn.net/liuxiao214/article/details/81037416]]></content>
      <tags>
        <tag>深度学习🤖</tag>
        <tag>优化算法</tag>
        <tag>参数初始化</tag>
        <tag>Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录10]]></title>
    <url>%2F2018%2F11%2F11%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%9510%2F</url>
    <content type="text"><![CDATA[1️⃣[get_sinusoid_encoding_table]Transformer绝对位置。 12345678910111213141516def get_sinusoid_encoding_table(n_position, d_hid, padding_idx=None): def cal_angle(position, hid_idx): return position / np.power(10000, 2 * (hid_idx // 2) / d_hid) def get_posi_angle_vec(position): return [cal_angle(position, hid_j) for hid_j in range(d_hid)] sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)]) sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) sinusoid_table[:, 0::2] = np.cos(sinusoid_table[:, 0::2]) if padding_idx is not None: sinusoid_table[padding_idx] = 0. return torch.FloatTensor(sinusoid_table) # n_position,embed_dim]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识11]]></title>
    <url>%2F2018%2F11%2F11%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8611%2F</url>
    <content type="text"><![CDATA[1️⃣[Optimizer]https://zhuanlan.zhihu.com/p/32262540https://zhuanlan.zhihu.com/p/32338983 Adam等自适应学习率算法对于稀疏数据具有优势，且收敛速度很快；但精调参数的SGD（+Momentum）往往能够取得更好的最终结果。 建议：前期用Adam，享受Adam快速收敛的优势；后期切换到SGD，慢慢寻找最优解。什么时候从Adam切换到SGD？当SGD的相应学习率的移动平均值基本不变的时候。 2️⃣[Pytorch]LongTensor除以浮点数，会对除数进行取整，再做除法。 3️⃣[Pytorch]使用Pytorch的DataParallel 1234567891011121314device = torch.device('cuda:' + str( config.CUDA_VISIBLE_DEVICES[0]) if config.use_cuda else 'cpu') # 指定第一个设备model = ClassifyModel( vocab_size=len(vocab), max_seq_len=config.max_sent_len, embed_dim=config.embed_dim, n_layers=config.n_layers, n_head=config.n_head, d_k=config.d_k, d_v=config.d_v, d_model=config.d_model, d_inner=config.d_inner_hid, n_label=config.n_label, dropout=config.dropout).to(device)model = DataParallel(model, device_ids=config.CUDA_VISIBLE_DEVICES) # 显式定义device_ids 注意到：device_ids的起始编号要与之前定义的device中的“cuda:0”相一致，不然会报错。 如果不显式在代码中的DataParallel指定设备，那么需要在命令行内指定。如果是在命令行里面运行的，且device不是从0开始，应当显式设置GPU_id，否则会出错‘AssertionError: Invalid device id’，正确的命令： 1CUDA_VISIBLE_DEVICES=4,5 python -u classify_main.py --gpu_id 0,1]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Pytorch</tag>
        <tag>Optimizer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于sparse gradient]]></title>
    <url>%2F2018%2F11%2F11%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8Esparse%20gradient%2F</url>
    <content type="text"><![CDATA[前几天在看AllenAI在EMNLP的ppt时，有一页写道： 为什么会出现这种情况？ Embedding是一个很大的矩阵，每一次其实都只有一个小部分进行了更新，对于一些词来说，出现的频率不高，或者说，其实大部分的词在一个loop/epoch中，被更新的次数是较少的。但是，注意到一般的optimizer算法，是以matrix为单位进行更新的，也就是每一次都是$W^{t+1}=W^{t}-\eta \frac{\partial L}{\partial{W}}$ 而Adam算法： 动量占了主导。但这样，每次batch更新，那些没被更新的词（也即gradient=0）的动量仍然会被衰减，所以这样当到这个词更新的时候，他的动量已经被衰减完了，所以更新的gradient就很小。 解决方案： ①在PyTorch中，Embedding的API：torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False, _weight=None) 其中sparse (bool, optional) – if True, gradient w.r.t. weight matrix will be a sparse tensor. 将sparse设为True即可。 ②针对sparse矩阵，使用不同的optimizer，如torch.optim.SparseAdam： Implements lazy version of Adam algorithm suitable for sparse tensors.In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.]]></content>
      <tags>
        <tag>sparse gradient</tag>
        <tag>代码实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周论文5]]></title>
    <url>%2F2018%2F11%2F10%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%875%2F</url>
    <content type="text"><![CDATA[1️⃣[Neural Turing Machine]通过模仿冯诺依曼机，引入外部内存(externel memory)。 和普通神经网络一样，与外界交互，获得一个输入，产生一个输出。但不同的是，内部还有一个memory进行读写。假设memory是一个N × M的矩阵，N是内存的位置数量。 读写memory①读其中读的时候对各内存位置线性加权。w是归一化权重。 ②写$e_t$是擦除向量（erase vector） $a_t$是加和向量(add vector) 具体如何获得权重就不说了。 Controller network中间的controller network可以是一个普通的feed forward或者RNN。 在实际中NTM用得并不多。 2️⃣[Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling]ELMo的精简版，通过即插即用的方法来压缩语言模型，对特定任务剪枝不同的层，使得能够减少inference的时间。这篇的idea挺有创新的，但似乎有些trivial的感觉。 RNN and Dense Connectivity每一层的输出都会传到所有层作为输入，因此对于L层的输入： 这样我们就能够随意地去掉任意中间层了。同时一些语言信息也分散到各个层，即使去掉某些层也没有关系。 则最终的output为： 最终作projection到正常维度（在每层都会这么做，将输入降维到正常维度再输入）： 再做一个softmax： 由于 $h^{※}$ 用于softmax，所以可能和target word，也即下一个词比较相似，因此可能没有很多的上下文信息。 所以最终我们使用$h_t$，以及反向的$h_t^r$，再过一层线性层获得最终的embedding（和ELMo有些不同，ELMo是直接拼起来）： Layer Selection我们在每层的output都加一个权重系数。 我们希望在target task上用的时候，部分z能够变成0，达到layer selection的效果，加快inference的速度。 亦即： 一种理想的方法是L0正则化： 但由于没办法求导，因此，采用L1正则化：但使用L1正则化有一定的风险，因为如果让所有z都远离1，那么会影响performance。 引入新的正则化方法$R_2 =\delta(|z|_0&gt;\lambda_1) |z|_1$亦即，只有在非零z的个数大于某个阈值时，才能有正则化效果，保证非零的个数。’it can be “turned-off” after achieving a satisfying sparsity’. 进一步引入$R_3=\delta(|z|_0&gt;\lambda_1) |z|_1 + |z(1-z)|_1$其中第二项为了鼓励z向0或1走。 Layer-wise Dropout随机删除部分layer，这些layer的输出不会传入之后的层，但仍然会参与最后的representation计算。 这种dropout会让perplexity更高，但对生成更好的representation有帮助。 3️⃣[Constituency Parsing with a Self-Attentive Encoder]其中的positional encoding我比较感兴趣。原版的positional encoding是直接和embedding相加的。亦即：那么在selt-attention时，有：这样会有交叉项：该项没有什么意义，且可能会带来过拟合。 因此在这边将positional encoding和embedding拼起来，亦即： 并且，在进入multi-head时的线性层也做改变： 这样在相乘的时候就不会有交叉项了。 实验证明，该方法有一定的提升。 4️⃣[DropBlock: A regularization method for convolutional networks]大致翻了一下。Motivation:在CNN中，dropout对convolutional layer的作用不大，一般都只用在全连接层。作者推测，因为每个feature map都有一个感受野范围，仅仅对单个像素进行dropout并不能降低feature map学习的特征范围，亦即网络仍可以通过该位置的相邻位置元素去学习对应的语义信息，也就不会促使网络去学习更加鲁棒的特征。 因此作者的做法是，dropout一整块位置。 5️⃣[Accelerating Neural Transformer via an Average Attention Network]提出了AAN(average attention network)，对transformer翻译模型的decode部分进行改进，加速了过程。 由于Transformer在decode阶段需要用到前面所有的y，也即自回归(auto-regressive)的性质，所以无法并行： 过程给定y： 首先将他们加起来，过一层全连接：这也相当于就是让所有的y有相同的权重，此时g就是上下文相关的表示。 接下来添加一个gating：控制了从过去保存多少信息和获取多少新的信息。 和Transformer原版论文一样，添加一个residual connection： 如图整个过程： 总结：AAN=average layer+gating layer 加速①考虑到加和操作是序列化的，只能一个一个来，不能并行，在这里使用一个mask的trick，使得在训练时也能够并行： ②在inference时的加速： 这样Transformer就能够类似RNN，只考虑前一个的state，而不是前面所有的state。 最终的模型：]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>Transformer</tag>
        <tag>dropout</tag>
        <tag>每周论文阅读</tag>
        <tag>self-attention</tag>
        <tag>NTM</tag>
        <tag>ELMo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词15]]></title>
    <url>%2F2018%2F11%2F10%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D15%2F</url>
    <content type="text"><![CDATA[1️⃣蜀相[唐] 杜甫丞相祠堂何处寻，锦官城外柏森森。映阶碧草自春色，隔叶黄鹂空好音。三顾频烦天下计，两朝开济老臣心。出师未捷身先死，长使英雄泪满襟。 http://m.xichuangzhu.com/work/57b93410a633bd00665efd4a]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周论文4]]></title>
    <url>%2F2018%2F11%2F04%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%874%2F</url>
    <content type="text"><![CDATA[1️⃣[Character-Level Language Modeling with Deeper Self-Attention]将transformer用于character-level的语言模型中，通过添加多个loss来提高其表现以及加快拟合速度，同时加深transformer的层数，极大提升表现，12层的transformer layer能达到SOTA，而64层则有更多的提升。 普通RNN用于character-level language model：将句子按character为单位组成多个batch，每个batch预测最后一个词，然后将该batch的隐状态传入下一个batch。也即“truncated backpropagation through time” (TBTT)。 如果用在Transformer，如下图，我们只预测$t_4$。 本文的一大贡献是多加了三种loss，并且有些loss的权值会随着训练的过程而逐渐减小，每个loss都会自己的schedule。这些loss加快了拟合速度，同时也提升了表现。 LossMultiple Positions对于batch内而言，每个时间步t都要预测下一个词。 Intermediate Layer Losses要求中间层也做出预测： 在这里，越底层的layer其loss权值越低。 Multiple Targets每一个position，不仅仅要预测下一个词，还要预测下几个词，预测下一个词和预测下几个词的分类器是独立的。 Positional embedding每一层的都添加一个不共享的可学习的positional embedding。 2️⃣[Self-Attention with Relative Position Representations]提出使用相对位置替代Transformer的绝对位置信息，并在NMT上有一定的提升。 分解：在原先的self-attention中，输出为： 其中： 现在我们考虑添加相对位置，其中相对位置信息在各层都是共享的： $a_{ij}^K$的具体形式：上式为了降低复杂度，不考虑长于k的相对位置信息。 考虑到transformer的并行性，为了并行性，我们考虑如下式子：其中，第一项和原来的Transformer一致；第二项，通过reshape可以达到并行的效果，然后两项直接加起来。 实验证明，使用相对位置效果是有一定的提升的，而同时使用绝对位置和相对位置并没有提升。 3️⃣[WEIGHTED TRANSFORMER NETWORK FOR MACHINE TRANSLATION]这篇被ICLR拒了，但有审稿人打了9分的高分。 对Transformer进行改进，拥有更好的效果和更小的计算代价。 传统的Transformer： Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})Vhead_i=Attention(QW_i^Q,KW_i^K,VW_i^V)MultiHead(Q,K,V)=Concat_i (head_i)W^OFFN(x)=max(0,xW_1+b_1)W_2 + b_2在本文中，先对head进行升维并乘以权重，过了FNN后，再乘以另一个权重。其中权重$\alpha$ $ \kappa$为可学习参数： head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)\overline{head_i}=head_i W^{O_i} \times \kappa_iBranchedAttention(Q,K,V)=\sum_{i=1}^{M} \alpha_i FFN(\overline{head}_i)其中要求权重之和为1。即$\sum_{i=1}^{M}\alpha_i=1$,$\sum_{i=1}^{M}\kappa_i=1$。 文中对$\kappa$和$\alpha$作了解释。 κ can be interpreted as a learned concatenation weight and α as the learned addition weight 通过实验，发现该模型会有更好的正则化特性。同时效果也有一定提升，收敛速度更快： 4️⃣[You May Not Need Attention]粗略地过了一遍，一些细节没有弄明白。 提出一种将encoder-decoder融合起来的模型，也即eager translation model，不需要attention，能够实现即时的翻译，也即读入一个词就能翻译一个词，同时不需要记录encoder的所有输出，因此需要很少的内存。 分为三步：①pre-processing进行预处理，使得源句子和目标句子满足eager feasible for every aligned pair of words $(s_i , t_j ), i ≤ j$。 首先通过现成的工具进行对齐操作(alignment)，然后对于那些不符合eager feasible的有具体算法（没认真看）进行补padding。如图 我们还可以在target sentence的开头添加b个padding，使得模型能够在开始预测之前获取更多的source sentence的词。 ②模型两层的LSTM，输入是上一次的y和当前的x拼接起来直接传进去。 ③post processing在最终结果之前，将padding去掉。 在inference（也即beam search）时，还有几个操作/trick： Padding limit Source padding injection SPI 实验表明，eager model在长的句子表现超过传统带attention的NMT，而长句子的建模正是attention-based 的模型的一大挑战；而在短句子上就不如attention-based的NMT。]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>Transformer</tag>
        <tag>每周论文阅读</tag>
        <tag>Language Modeling</tag>
        <tag>self-attention</tag>
        <tag>relative position</tag>
        <tag>positional encoding</tag>
        <tag>NMT</tag>
        <tag>eager translation model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词14]]></title>
    <url>%2F2018%2F11%2F04%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D14%2F</url>
    <content type="text"><![CDATA[1️⃣鹤冲天[宋] 柳永黄金榜上，偶失龙头望。明代暂遗贤，如何向？未遂风云便，争不恣游狂荡。何须论得丧？才子词人，自是白衣卿相。烟花巷陌，依约丹靑屛障。幸有意中人，堪寻访。且恁偎红倚翠，风流事，平生畅。靑春都一饷。忍把浮名，换了浅斟低唱！ 恣（zì）：放纵，随心所欲。恁（nèn）：如此。 http://m.xichuangzhu.com/work/57aeff68a633bd0057f7d406]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录9]]></title>
    <url>%2F2018%2F11%2F04%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%959%2F</url>
    <content type="text"><![CDATA[1️⃣[collate_fn]将不等长句子组合成batch。 1234567891011121314151617def collate_fn(insts): ''' Pad the instance to the max seq length in batch ''' max_len = max(len(inst) for inst in insts) batch_seq = np.array([ inst + [Constants.PAD] * (max_len - len(inst)) for inst in insts]) batch_pos = np.array([ [pos_i + 1 if w_i != Constants.PAD else 0 for pos_i, w_i in enumerate(inst)] for inst in batch_seq]) # 位置信息 batch_seq = torch.LongTensor(batch_seq) batch_pos = torch.LongTensor(batch_pos) return batch_seq, batch_pos]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[“莱斯杯”挑战赛有感]]></title>
    <url>%2F2018%2F10%2F30%2F%E8%A7%81%E9%97%BB%26%E6%83%B3%E6%B3%95%2F%E2%80%9C%E8%8E%B1%E6%96%AF%E6%9D%AF%E2%80%9D%E6%8C%91%E6%88%98%E8%B5%9B%E6%9C%89%E6%84%9F%2F</url>
    <content type="text"><![CDATA[历时三个月的“莱斯杯”全国第一届“军事智能·机器阅读”挑战赛终于落下帷幕，前几日（10.26-10.28）有幸在南京青旅宾馆参与决赛，体验多多，收获满满，心中亦有一些感想。 一个是南京总带给我一种回家的感觉，对南京的事物总有亲切感。第一次来南京是一年半前，也是来参加比赛。周五晚上的夜游秦淮，让我感受到许久未曾感受到的烟火气息。 第二个是此次主办方提供的食宿令人惊喜。一开始听到青旅宾馆，我已经做好了艰苦奋战的准备了，然而酒店是星级酒店的，吃方面直接到楼下的自助。可以看出主办方此次确实用心在举办这次比赛。 第三点是关于比赛的，关于比赛的整个历程我还是颇有感触。我们是以第9名的成绩挺进决赛，其实在后期比赛中，我们都有所懈怠了，几乎没有花时间在这上面，10月初发布决赛的数据集，而我们在10月20日才得知这一事情，此时离决赛只剩一周时间。因此我们确实准备不足。当然我们也没有预料到我们的决赛成绩会这么靠前，否则我们肯定会更加充分去准备。这确实是我们的失误。 我们在比赛过程中，一直尝试在使用ELMo，这正是我负责的部分。一开始使用官方TensorFlow的代码，费了九牛二虎之力我才跑通代码，但因为队长使用的是pytorch，而二者在cuda版本上不兼容，因此在初赛我们没有使用ELMo。而在最后几天，我尝试使用哈工大的pytorch训练代码，但因为inference速度实在太慢，我们最终还是弃用了这个方案。而在决赛现场，我们发现也确实是因为速度和资源的原因，大家都没有使用ELMo，除了一组。该组正是凭借了ELMo弯道超车从第7升到了第一，拿走了20万大奖。这也是我们非常遗憾的一个地方，我们在遇到困难时没有尝试解决，而是直接弃用，最终没有取得更好的成绩。 此次我们的成绩排名第4(三等奖)，是有一定的进步的，但有一点遗憾的是，我们仅差0.18百分点，就能超过第三名拿到5万的奖金了。后面我们分析了一下，还是因为我们对比赛懈怠的态度，其他组都对数据进行了分析并有针对性的改进，而我们并没有做这一步。 Anyway，第一次组队参加比赛就有收获，增长了见识，从交流中也获得了许多。这个比赛之后，就得好好看paper了。 __(:з」∠)_]]></content>
      <tags>
        <tag>有感</tag>
        <tag>莱斯杯</tag>
        <tag>比赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词13]]></title>
    <url>%2F2018%2F10%2F29%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D13%2F</url>
    <content type="text"><![CDATA[1️⃣行路难三首[唐] 李白【其一】金樽清酒斗十千，玉盘珍羞直万钱。停杯投箸不能食，拔剑四顾心茫然。欲渡黄河冰塞川，将登太行雪满山。闲来垂钓碧溪上，忽复乘舟梦日边。行路难，行路难，多歧路，今安在？长风破浪会有时，直挂云帆济沧海！ 注释：「闲来垂钓碧溪上，忽复乘舟梦日边。」句：暗用典故：姜太公吕尚曾在渭水的磻溪上钓鱼，得遇周文王，助周灭商；伊尹曾梦见自己乘船从日月旁边经过，后被商汤聘请，助商灭夏。这两句表示诗人自己对从政仍有所期待。碧，一作「坐」。 2️⃣登科后[唐] 孟郊昔日龌龊不足夸，今朝放荡思无涯。春风得意马蹄疾，一日看尽长安花。 注释：龌龊（wò chuò）：原意是肮脏，这里指不如意的处境。不足夸：不值得提起。放荡（dàng）：自由自在，不受约束。思无涯：兴致高涨。 http://m.xichuangzhu.com/work/57add198a633bd0057eefa8a]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周论文3]]></title>
    <url>%2F2018%2F10%2F29%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%873%2F</url>
    <content type="text"><![CDATA[1️⃣[A Neural Probabilistic Language Model]第一篇使用神经网络获得词向量的paper。 通过对language model建模，将词映射到低维表示，在训练过程中同时训练语言模型以及每个词的词向量。 将中心词的前n个拼接起来 $x=(C(w_{t-1},C(w_{t-2}),…,C(w_{t-n+1}))$将$x$送入神经网络中获得$y=b+Wx+Utanh(d+Hx)$，最后做一个softmax即可。 2️⃣[Adaptive Computation Time for Recurrent Neural Networks]一种允许RNN动态堆叠层数的算法。 Motivation证据证明，RNN的堆叠层数多，效果会有提升。但是，对于不同的任务，要求不同的计算复杂度。我们需要先验来决定特定任务的计算复杂度。当然我们可以粗暴地直接堆叠深层的网络。ACT(Adaptive Computation Time)能够动态决定每个输入t所需的计算次数。 方法将RNN每一步的输出过一个网络+sigmoid层，获得一个概率分布，也即什么时候应当停止不再继续往上堆叠，直到概率加和为1。同时为了尽可能抑制层数的无限增长，在loss添加一项惩罚。 模型对于普通的RNN： s是隐藏层；y是输出。 对于ACT的RNN，有： 上标n是指的t时刻的层数；其中： $δ$是flat，指示x是第几次输入。 引入新的网络，输入时隐状态，输出是一个概率分布： 那么每一层的概率是： 其中$R(t)$是在每一层概率求和超过1时的剩余概率（为了保证概率和为1，可以试着举一个例子来证明） ε是为了解决第一次输出时就超过1-ε的情况，ε一般取很小。 最终，加权求和，作为最终的结果，传入下一个时间步： 普通RNN与ACT的RNN对比： 损失函数为了防止模型层数无限增长，添加一项惩罚项以抑制。 记每一步的惩罚项为： 总的惩罚项则为： Loss function则为： 因为N(t)是不可导的，我们在实际过程中只去最小化R(t) （我觉得不甚合理，一种解读是如果我们不断最小化R(t)直到变成0，那么相当于N(t)少了一层，接着R(t)就会变得很大，然后又继续最小化R(t)…） 3️⃣[Universal Transformers]提出一种新型通用的transformer。 MotivationTransformer的问题：RNN的归纳偏置(inductive bias)在一些任务上很重要，也即RNN的循环学习的过程；Transformer在一些问题上表现不好，可能是归纳偏置的原因。 Notably, however, the Transformer foregoes the RNN’s inductive bias towards learning iterative or recursive transformations.Our experiments indicate that this inductive bias may be crucial for several algorithmic and language understanding tasks of varying complexity: in contrast to models such as the Neural Turing Machine [13], the Neural GPU [17] or Stack RNNs [16], the Transformer does not generalize well to input lengths not encountered during training. 因此在Transformer内引入归纳偏置 特点 每一层的权重是共享的，也即multi-head上的权重以及transition function在每一层是一致的。这一点和RNN、CNN一致。 动态层数（ACT mechanism ）：对于每个词都会有不同的循环次数；也即有些词需要更多的refine；而有些词不需要。和固定层数的transformer相比，会有更好的通用性。 模型总体架构 过程： 和普通Transformer不同的地方在于： 加了一层Transition层，Transition可以是depth-wise separable convolution（是什么？）或者全连接层。 每层都添加了position embedding；以及timestep embedding，用以指示层数。 ACT由于一个句子中间，有些词比其他词更难学会，需要更多计算量，但堆叠太多层会大大增加计算量，为了节省计算量，我们可以引入ACT来动态分配计算量。 ACT原来用于RNN，在Transformer中，当halting unit指示词t应当停止时，直接讲该词的状态复制到下一个time step，直到所有的词都停止。]]></content>
      <tags>
        <tag>Embedding</tag>
        <tag>Paper</tag>
        <tag>Transformer</tag>
        <tag>每周论文阅读</tag>
        <tag>ACT</tag>
        <tag>Language Modeling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词12]]></title>
    <url>%2F2018%2F10%2F21%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D12%2F</url>
    <content type="text"><![CDATA[1️⃣望海潮[宋] 柳永东南形胜，三吴都会，钱塘自古繁华。烟柳画桥，风帘翠幕，参差十万人家。云树绕堤沙，怒涛卷霜雪，天堑无涯。市列珠玑，户盈罗绮，竞豪奢。重湖叠巘清嘉，有三秋桂子，十里荷花。羌管弄晴，菱歌泛夜，嬉嬉钓叟莲娃。千骑拥高牙，乘醉听箫鼓，吟赏烟霞。异日图将好景，归去凤池夸。 叠巘（yǎn）：层层叠叠的山峦。 http://m.xichuangzhu.com/work/57b318228ac247005f2223db]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录8]]></title>
    <url>%2F2018%2F10%2F21%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%958%2F</url>
    <content type="text"><![CDATA[1️⃣[batchify]快速将数据分成batch。 12345678def batchify(data, bsz): # Work out how cleanly we can divide the dataset into bsz parts. nbatch = data.size(0) // bsz # Trim off any extra elements that wouldn't cleanly fit (remainders). data = data.narrow(0, 0, nbatch * bsz) # Evenly divide the data across the bsz batches. data = data.view(bsz, -1).t().contiguous() return data.to(device)]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRML第四章 分类的线性模型]]></title>
    <url>%2F2018%2F10%2F21%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FPRML%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%88%86%E7%B1%BB%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[判别函数 —-未完—-]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周论文2]]></title>
    <url>%2F2018%2F10%2F20%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%872%2F</url>
    <content type="text"><![CDATA[1️⃣[An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling]本文贡献：提出一种新的模型TCN（Temporal Convolutional Networks）进行language model建模。 Dilated convolution每一层的感受野都可以是不同的，也即，同样的kernel size，高层的可以跳着看。 每层的d逐渐增大（也即跳的步数），一般按指数增大。（我觉得这样很有道理，如果每一层的d都是一样的，那capture到的信息就会有重复，能看到的视野也不如逐渐增大的多） Residual block 这边的residual block比较复杂；一个值得主意的细节是，因为感受野的不同，上层的感受野总是比下层的大很多，因此不应该直接将下层的加到上层，而是可以使用一个1*1的convolution对下层的x进行卷积，这就类似scale对输入进行放缩。 2️⃣[Dissecting Contextual Word Embeddings： Architecture and Representation]一篇分析的文章。ELMo作者的又一篇文章。 对比三种不同的建模方式（LSTM/GCNN/Transformer）获得的词向量，以及在不同任务上的表现；以及不同层获得的不同信息…获得了不同的结论。 ①biLM 专注于word morphology词的形态；底层的LM关注local syntax；而高层的LM关注semantic content； ②不同的任务会有不同的正则化s的倾向。 3️⃣[Transformer-XL: Language modeling with longer-term dependency]利用Transformer进行language model，与普通的Transformer建模不同的是，Transformer-XL添加了历史信息，能够显著提升表现。这篇还在ICLR2019审稿中。 贡献：本文提出了能够进行长程依赖的基于Transformer的语言模型 Transformer-XL；引入相对位置的positional encoding。 结构原先的transformer language model是将句子分为一个一个segment。segment之间是没有联系的。（为什么不直接按原版的Transformer那样所有的词都相互做self-attention？因为考虑到效率问题，句子长度可能会很长） 训练阶段： 而在测试阶段，每次向右滑动一格：这样每一个时间步都要重新计算一遍，历史信息没有利用到。显然速度很慢。 在Transformer引入recurrence，也即引入历史信息。基于这样的想法，提出的新模型Transformer-XL。在结构上同样分为每个segment，但在每个阶段都接收上一个（甚至上L个）历史信息。 训练阶段： 而在测试阶段，同样分为segment，但因为接收了历史信息，不需要每次滑动一格也能获得大量信息。 具体来说：SG代表stop gradient，和该阶段的hidden state进行拼接。 RELATIVE POSITIONAL ENCODINGS如果我们使用了absolute positional encodings（也即原版的positional encodings）那么会出现这种情况 在同一层之间的前一个segment和后一个segment使用了同样的绝对位置信息，对于当前segment的高层，对于同一个位置i，无法区分该位置信息是来自当前segment的还是上一个segment的（因为都是同样的绝对位置）。 因此我们引入相对位置信息R，其中第i行代表相对距离i的encoding。 具体来说： 首先我们在传统的计算$query_i$和$key_j$的attention分数时，可以拆解成： （因为query=(embedding E +positional embedding U），key也一样，将式子拆开就能获得上述式子) 我们将该式子进行修改： 第一，将出现了absolute positional embedding $U$的地方，统统改成$R_{i-j}$，也即在b和d项。其中这里的R和原版的Transformer的位置计算公式相同。 第二，在c项中，使用一个$u$替代了$U_i W_q$，这一项原本的意义在于，$query_i$的positional encoding对$key_j$的embedding进行attention，也就是说，该项表现了$query_i$位置对哪些$key_j$的内容有兴趣，作者认为query不管在哪个位置上都是一样的，也就是说query的位置信息应当没影响，所以统统替换成一个可学习的$u$。基于类似的理由d项换成了$v$。 第三，将$W_k$细分成了两个$W_{k,E}$和$W_{k,R}$。这是根据query是Embedding还是positional encoding来区分的。for producing the content-based key vectors and location-based key vectors respectively 每一项现在都有了不同的意义： Under the new parameterization, each term has an intuitive meaning: term (a) represents contentbased addressing, term (b) captures a content-dependent positional bias, term (c) governs a global content bias, and (d) encodes a global positional bias. 最后总结一下整个结构： 与原版Transformer不同的是，Transformer-XL在每一层都添加了位置信息。 4️⃣[Trellis Networks for Sequence Modeling]一种结合RNN和CNN的语言建模方式。 最小的单元结构： 也即： 接下来再处理非线性： 因为每层都要输入x，且W是共享的，所以我们可以提前计算好这一项，后面直接用即可。 最终在实现的时候是： 总体框架： 与TCN（temporal convolution network）不同之处：①filter weight不仅在time step之间共享，在不同层之间也共享；②在每一层都添加了输入 优点：共享了W，显著减少了参数；‘Weight tying can be viewed as a form of regularization that can stabilize training’ 我们还可以扩展该网络，引入gate： 5️⃣[Towards Decoding as Continuous Optimisation in Neural Machine Translation]一篇很有意思的paper。用于NMT decode的inference阶段。这篇有一定的难度，以下只是我的理解。 思想Motivation：NMT中的decode inference阶段，通常都是从左到右的，这样有个缺点，就是整体的target之间的依赖是没有被充分利用到的，比如说生成的词的右边是没有用到的。那么我们为什么不直接全部生成呢？然后不断更新。也就是说我们将离散（discrete）的decode过程变成一个连续的过程（continuous optimization）。 假设我们已经训练好模型，给定一个句子，我们要翻译成目标句子，且假设我们已知要生成的句子长度是l，那么我们有：我们要找到一个最优的序列$y$，使得$-log$最小。 等价于：其中$\widetilde{y}_i$是one-hot。其实这里就是假设有这么一个ground truth，但实际上是没有的。 我们将$\widetilde{y}_i$是one-hot这个条件放宽一些，变成是一个概率单纯型（其实就是所有元素加起来是1，且都大于等于0）。 那么就变成了： 这个改变的本质是： 就是说原来one-hot的$\widetilde{y}_i$生成后丢到下一个时间步，取了一个词向量，接着计算。现在是一个概率分布$\hat{y}_i$丢进来，就相当于取了多个词向量的加权求和。 在利用下述的更新算法更新完$\hat{y}_i$之后，对于每个时间步t，我们找$\hat{y}_i$中元素最大的值对应的词作为生成的词。 有两种方法Exponentiated Gradient 和 SGD。实际上方法倒在其次了，主要是前面所述的continuous optimization这种思想。 算法Exponentiated Gradient具体见论文 SGD因为我们要保证单纯形的约束不变，因此我们引入一个r，然后做一个softmax 应用这种连续decode可以用在哪？ Bidirectional Ensemble可以很方便地进行双向的生成： 而在传统的方法中没办法（很难）做到 Bilingual Ensemble我们希望源语言到目标语言和目标到源语言都生成得好 问题$\hat{y}_i$的初始化很重要，一不小心就会陷入local minima；生成的速度慢 6️⃣[Universal Language Model Fine-tuning for Text Classiﬁcation]和ELMo、OpenAI GPT一样，都是预训练语言模型，迁移到其他任务上（这里是分类任务）。可以在非常小的数据集上有很好的效果。 贡献： 迁移学习模型ULMFiT 提出几种trick：discriminative ﬁne-tuning, slanted triangular learning rates,gradual unfreezing ，最大保证知识的保留。 模型 三部曲： 通用语言模型预训练 目标任务的语言模型fine-tuning 目标任务的分类fine-tuning trickDiscriminative ﬁne-tuningMotivation：不同层有不同的信息；应当fine-tune 不同程度，也即使用不同的learning rate。 作者发现上一层的学习率是下一层的2.6倍时效果比较好。 Slanted triangular learning rates (STLR) 具体公式： Gradual unfreezing从顶层到底层，一步一步unfreeze，也即从上到下fine-tune。这是因为最上一层有最少的general knowledge。]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>每周论文阅读</tag>
        <tag>TCN</tag>
        <tag>Transformer-XL</tag>
        <tag>Trellis Networks</tag>
        <tag>continuous decoding</tag>
        <tag>ULMFiT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周论文1]]></title>
    <url>%2F2018%2F10%2F14%2F%E8%AE%BA%E6%96%87%2F%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%871%2F</url>
    <content type="text"><![CDATA[1️⃣[Learned in Translation: Contextualized Word Vectors]CoVe是第一个引入动态词向量的模型。Motivation：翻译模型能够保存最多的信息，因为如果保存信息不够多，decoder接收到的信息不足，翻译效果就不会好。（但实际上，我个人认为，decoder的表现还和language model有关，如果decoder是一个好的language model，也有可能翻译出不错的结果） 做法：使用传统NMT的encoder-decoder的做法翻译模型，只是将(bi)LSTM所得到的隐层状态表示取出来和embedding拼接起来，作为一个词的表示： w=[GloVe(w); CoVe(w)] 2️⃣[Language Modeling with Gated Convolutional Networks]使用CNN对语言模型进行建模，提高并行性。 贡献：使用了CNN进行language model建模；提出了简化版的gate机制应用在CNN中。 做法： 实际上就是一个输入两个filter，卷积出来的做一个gate的操作$H_0 = A⊗σ(B)$，控制流向下一层的数据。 一个小细节是，为了不让language model看到下一个词，每一层在开始卷积的时候会在左边添加kernel_size-1个padding。 扩展：因为CNN的并行性高，可以使用CNN来对language model建模替代ELMo，同样可以获得动态词向量。这个想法已经由提出ELMo的团队做出来并进行对比了。论文：Dissecting Contextual Word Embeddings: Architecture and Representation 目前正在复现该论文 。 3️⃣[Attention is All you need]非常经典的论文。提出了Transformer。为了读BERT重温了一遍。 4️⃣[Improving Language Understanding by Generative Pre-Training]BERT就是follow这篇文章的工作。使用Transformer预训练一个language model进行迁移学习。 训练过程分为两步：①使用未标记数据训练language model；②使用有标记数据进行fine-tune Motivation：ELMo是训练好language model，然后获得动态词向量再用到其他任务上，这样就会多了很多参数。和ELMo不同的是，这里使用一个Transformer模型解决多种任务（利用迁移学习）。 贡献：使用Transformer进行language model建模；尝试利用language model进行迁移学习而不是另一种思路（ELMo）只提取词向量。 ①无监督学习language model 具体到Transformer就是： ②监督学习（fine-tune）根据输入预测标签 具体就是： 将两个任务一起训练，则有： 对于不同任务，对输入进行一定的改动以适应Transformer结构： 5️⃣[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]刷爆各榜单的一篇神文。使用Transformer预训练一个language model进行迁移学习。 Motivation：之前的language model只能根据前面的词来预测下一个（即使ELMo是双向的LSTM，也是分别训练一个前向和一个后向的），限制了双向的context；因此提出了双向的language model。 做法：模型分为两个部分：①masked LM：因为使用了两边的context，而language model的目的是预测下一个词，这样模型会提前看到下一个词，为了解决该问题，训练的时候讲部分词mask掉，最终只预测被mask掉的词。 ②Next Sentence Prediction：随机50%生成两个句子是有上下句关系的，50%两个句子是没有关系的，然后做分类；具体来说是拿第一个词[CLS]（这是手动添加的）的表示，过一个softmax层得到。 联合训练这两个任务。 接下来是通过具体的任务进行fine-tune。一个模型解决多种问题： 本文贡献：使用Transformer进行双向的language model建模。论文提到的一些细节/tricks非常值得讨论，比如对token embedding添加了许多信息，非常简单粗暴。]]></content>
      <tags>
        <tag>Paper</tag>
        <tag>Transformer</tag>
        <tag>每周论文阅读</tag>
        <tag>CoVe</tag>
        <tag>GCNN</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 18:Deep Reinforcement Learning]]></title>
    <url>%2F2018%2F10%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2018%3A%20Deep%20Reinforcement%20Learning%2F</url>
    <content type="text"><![CDATA[记号： $a$是action，$s$即外部状态state，$\pi_{\theta}(s)$也即从$s$映射到$a$的函数；$r$是reward，每采取一个动作，会有一个reward，则总的reward为 R_\theta = \sum_{t=1}^{T} r_t我们使用神经网络来拟合$\pi$，一个eposide $\tau$是一个流程下来的的所有state、action和reward的集合。 \tau = \{s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T \}如果我们使用相同的actor运行n次，则每个$\tau$会有一定的概率被采样到，采样概率记为$P(\tau|\theta)$，则我们可以通过采样的方式来对期望reward进行估计： \overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta) ≈ \frac{1}{N} \sum_{n=1}^{N} R(\tau^n)那么我们接下来的目标就是最大化期望reward，其中期望reward是： \overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta)我们同样使用梯度上升：其中与$θ$相关的是$P$，则可以写成： \nabla \overline{R}_\theta = \sum_\tau R(\tau) \nabla P(\tau|\theta)= \sum_\tau R(\tau) P(\tau|\theta) \frac{\nabla P(\tau|\theta)}{P(\tau|\theta)}由于$\dfrac {d\log \left( f\left( x\right) \right) }{dx}=\dfrac {1}{f\left( x\right) }\dfrac {df(x)}{dx}$，则前式可写成： \nabla \overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta) \nabla log P(\tau | \theta) ≈ \frac{1}{N} \sum_{n=1}^{N} R(\tau^n) log P(\tau ^n| \theta)如何求梯度？由于： P(\tau | \theta)=p(s_1)p(a_1|s_1,\theta)p(r_1,s_2|s_1,a_1)p(a_2|s_2,\theta)p(r_2,s_3|s_2,a_2)... \\=p(s_1)\prod_{t=1}^{T}p(a_t|s_t,\theta)p(r_t , s_{t+1}| s_t,a_t)实际上，其中与梯度相关的只有中间项$p(a_t|s_t,\theta)$，该项也即$π$函数，从state到action的映射。取log并求导，有： \nabla log P(\tau | \theta)= \sum_{t=1}^{T} \nabla log p(a_t|s_t,\theta)代回，因此最终$\overline{R}_\theta$的梯度为： \nabla \overline{R}_\theta = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^n) \nabla log p(a_{t}^n | s_t^n,\theta)注意到该式子告诉我们，应考虑整体的reward而不应该只考虑每一步的reward；并且取log的原因可以理解成是对action取归一化，因为： \frac{\nabla p(a_t^n | s_t^n,\theta)}{p(a_t^n | s_t^n,\theta)}也就是说对于那些出现次数较多的action，要衡量他们对reward的真正影响，应当对他们归一化。 为了让那些出现可能性较低的action不会因为没被sample到而在更新后被降低他们的概率，可以添加一个baseline，只有超过$b$的reward才会增加他们出现的概率。 \nabla \overline{R}_\theta = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} (R(\tau^n)-b) \nabla log p(a_{t}^n | s_t^n,\theta)]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>李宏毅机器学习课程</tag>
        <tag>Deep Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 17:Ensemble]]></title>
    <url>%2F2018%2F10%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2017%3A%20Ensemble%2F</url>
    <content type="text"><![CDATA[Bagging对于复杂模型，往往variance会大，通过对多个模型的平均，能够减小variance： bagging的思想是多次有放回地采样N’个点（通常N’=N），然后对采样的几个数据集分别训练一个模型 测试的时候再对几个模型进行平均或投票 Boosting基本思想是对几个弱分类器线性加权，得到强分类器。分类器按先后顺序训练，每次训练完，对新模型分类错误的数据进行调高权重，而正确的数据则降低权重。 可以保证：只要分类器的错误率小于50%，在boosting后能够有100%的正确率（在训练集）。 证明过程略。 Ensemble: Stacking基本思想：使用训练数据训练多个初级分类器，将初级分类器的输出作为次级分类器的输入，获得最终的输出。我们应当使用不同的训练数据来训练次级分类器]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>李宏毅机器学习课程</tag>
        <tag>Ensemble</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈mask矩阵]]></title>
    <url>%2F2018%2F10%2F12%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%B5%85%E8%B0%88mask%E7%9F%A9%E9%98%B5%2F</url>
    <content type="text"><![CDATA[个人目前对mask矩阵的一点理解。 是什么mask矩阵是什么？是一个由0和1组成的矩阵。一个例子是，在自然语言处理(NLP)中，句子的长度是不等长的，但因为我们经常将句子组成mini-batch用以训练，因此那些长度较短的句子都会在句尾进行填充0，也即padding的操作。一个mask矩阵即用以指示哪些是真正的数据，哪些是padding。如：图片来源：Theano：LSTM源码解析 其中mask矩阵中1代表真实数据；0代表padding数据。 为什么为什么要使用mask矩阵？使用mask矩阵是为了让那些被mask掉的tensor不会被更新。考虑一个tensor T的size(a,b)，同样大小的mask矩阵M，相乘后，在反向回传的时候在T对应mask为0的地方，0的梯度仍为0。因此不会被更新。 怎么做接下来介绍几种（可能不全）使用mask的场景。 对输入进行mask考虑NLP中常见的句子不等长的情况。设我们的输入的batch I:(batch_size,max_seqlen)，我们在过一层Embedding层之前，在过了一层Embedding层，则有 E:(batch_size,max_seqlen,embed_dim)，如果我们希望Embedding是更新的(比如我们的Embedding是随机初始化的，那当然Embedding需要更新)，但我们又不希望padding更新。一种方法即令E与M相乘。其中M是mask矩阵(batch_size,max_seqlen,1) (1是因为要broadcast），这样在Embedding更新梯度时，因为mask矩阵的关系，padding位置上的梯度就是0。当然在Pytorch中还可以直接显式地写：1self.embedding = nn.Embedding(vocab_size, embed_dim,padding_idx=0) 而此时应当将padding显式添加到词典的第一个。 对模型中间进行mask一个很经典的场景就是dropout。对于参数矩阵W:(h,w)，同样大小的mask矩阵M，在前向传播时令W’=W*M，则在反向传播时，M中为0的部分不被更新。当然，我们可以直接调用PyTorch中的包nn.Dropout() 123m = nn.Dropout(p=0.2)input = torch.randn(20, 16)output = m(input) 对loss进行mask考虑NLP中的language model，每个词都需要预测下一个词，在一个batch中句子总是有长有短，对于一个短句，此时在计算loss的时候，会出现这样的场景：&lt;pad&gt;词要预测下一个&lt;pad&gt;词。举个例子：三个句子[a,b,c,d],[e,f,g],[h,i]，在组成batch后，会变成X： a b c d e f g &lt;pad&gt; h i &lt;pad&gt; &lt;pad&gt; Y： b c d &lt;pad&gt; f g &lt;eos&gt; &lt;pad&gt; i &lt;eos&gt; &lt;pad&gt; &lt;pad&gt; X是输入，Y是预测。那么从第三行可以看出，&lt;pad&gt;在预测下一个&lt;pad&gt;。这显然是有问题的。一种解决方案就是使用mask矩阵，在loss的计算时，将那些本不应该计算的mask掉，使得其loss为0，这样就不会反向回传了。具体实践：在PyTorch中，以CrossEntropy为例： 12class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=’elementwise_mean’ 如果reduction=None则会返回一个与输入同样大小的矩阵。在与mask矩阵相乘后，再对新矩阵进行mean操作。在PyTorch实践上还可以可以这么写： 123masked_outputs = torch.masked_select(dec_outputs, mask)masked_targets = torch.masked_select(targets, mask)loss = my_criterion(masked_outputs, masked_targets) 另一种更为简单的解决方案是，直接在CrossEntropy中设ignore_index=0，这样，在计算loss的时候，发现target=0时，会自动不对其进行loss的计算。其本质和mask矩阵是一致的。 总结mask矩阵可以用在任何地方，只要希望与之相乘的tensor相对应的地方不更新就可以进行mask操作。]]></content>
      <tags>
        <tag>代码实践</tag>
        <tag>mask矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度炼丹tricks合集]]></title>
    <url>%2F2018%2F10%2F07%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%B7%B1%E5%BA%A6%E7%82%BC%E4%B8%B9tricks%E5%90%88%E9%9B%86%2F</url>
    <content type="text"><![CDATA[—-Deprecated—- 调参技巧数据增强预处理1️⃣zero-center[9]将数据中心化 初始化1️⃣Xavier initialization[7]方法适用[9]于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n) 2️⃣He initialization[8]方法适用[9]于ReLU：scale = np.sqrt(6/n) 3️⃣Batch normalization[10]4️⃣RNN/LSTM init hidden stateHinton[3]提到将RNN/LSTM的初始hidden state设置为可学习的weight 训练技巧1️⃣Gradient Clipping[5,6]2️⃣learning rate原则：当validation loss开始上升时，减少学习率。[1]Time/Drop-based/Cyclical Learning Rate 3️⃣batch size[2]中详细论述了增加batch size而不是减小learning rate能够提升模型表现。保持学习率不变，提高batch size，直到batch size~训练集/10，接下来再采用学习率下降的策略。 Reference[1]How to make your model happy again — part 1 [2]Don’t Decay the Learning Rate, Increase the Batch Size [3]CSC2535 2013: Advanced Machine Learning Lecture 10 Recurrent neural networks [4]https://zhuanlan.zhihu.com/p/25110150 [5]On the difficulty of training Recurrent Neural Networks [6]Language Modeling with Gated Convolutional Networks [7]Understanding the difficulty of training deep feedforward neural networks [8]Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification [9]知乎：你有哪些deep learning（rnn、cnn）调参的经验？ [10]Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift]]></content>
      <tags>
        <tag>调参</tag>
        <tag>tricks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词11]]></title>
    <url>%2F2018%2F10%2F07%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D11%2F</url>
    <content type="text"><![CDATA[1️⃣赋得古原草送别[唐] 白居易离离原上草，一岁一枯荣。野火烧不尽，春风吹又生。远芳侵古道，晴翠接荒城。又送王孙去，萋萋满别情。 萋萋（qī）：形容草木长得茂盛的样子。 http://m.xichuangzhu.com/work/57b8a5371532bc005b99da51]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRML第三章 回归的线性模型]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FPRML%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%9B%9E%E5%BD%92%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[线性基函数模型 偏置-⽅差分解 贝叶斯线性回归 贝叶斯模型⽐较 证据近似]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 16:SVM]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2016%3A%20SVM%2F</url>
    <content type="text"><![CDATA[Hinge Loss+kernel method = SVM Hinge LossSVM与logistic regression的区别即在于loss function的不同，logistic是cross entropy，而SVM是hinge loss 也即如果分类间隔大于1，则 $L(m_i)=max(0,1−m_i(w))$，则损失为0。因此SVM更具鲁棒性，因为对离群点不敏感。 对于linear SVM： 定义函数 $f(x)=\sum_i w_i x_i +b=w^T x$ 定义损失函数 $L(f)=\sum_n l(f(x^n),\hat{y}^n)+\lambda ||w||_2$，其中$l(f(x^n),\hat{y}^n)=max(0,1-\hat{y}^n f(x))$ 梯度下降求解（省略了正则化） \frac{\partial{l(f(x^n),\hat{y}^n})}{\partial{w_i}}= \frac{\partial{l(f(x^n),\hat{y}^n})}{\partial{f(x^n)}} \frac{\partial{f(x^n)}}{\partial{w_i}} x_i^n 而 f(x^n)=w^T \cdot x^n \frac{\partial{max(0,1-\hat{y}^n f(x^n)})}{\partial{f(x^n)}}= \left\{ \begin{array}{**lr**} -\hat{y}^n & if \hat{y}^n f(x^n)]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>李宏毅机器学习课程</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 15:Transfer Learning]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2015%3A%20Transfer%20Learning%2F</url>
    <content type="text"><![CDATA[Model Fine-tuning假设我们有很多的source data $(x^s,y^s )$，与任务相关的target data $(x^t,y^t )$ 很少。我们利用source data训练一个模型，然后用target data来fine tune模型。 conservative training 我们可以用source data训练好的模型的weight作为新的模型的weight，然后设定一些限制，比如source data作为输入的output应和target data作为输入的output尽量相似，或者参数尽量相似等。 layer transfer也就是新模型有几层是直接copy旧模型的，只训练其它层。注意到不同任务所应copy的层是不同的，语音任务最后几层效果好，图像识别前面几层效果好 Multitask Learning不同任务之间共享相同的中间层，如： 还有一种progressive neural networks：首先训练好第一个任务的模型，然后在训练第二个模型的时候将第一个模型的隐层加入到第二个模型的隐层中；训练第三个模型则将第二个和第一个模型的隐层加入到第三个模型的隐层中，以此类推 Domain-adversarial trainingsource data是有标签的，而target data是无标签的，都属于同一个任务，但数据是mismatch的，如： 因为NN的隐层可以理解成是在抽取图像的特征，我们希望能够在训练NN的过程中去掉source data的一些domain specific的特性，这样就可以用在target data上了。因此我们在feature exactor后面连接两个模块： 一方面我们希望抽取的特征能够使得分类器正确地分类，另一方面我们希望这些特征能够让domain classifier能够无法识别特征是从哪些data抽取得到的，这样得到的特征就是被去掉domain specific特征的。 具体训练： Zero-shot Learningsource data有标签，target data无标签，但任务不同，如： Representing each class by its attributes一种方法是将每一个类都用特征表示，但特征要足够丰富： 在训练的时候，输入是图片，输出则是这些特征：这样在将target data放入训练好的NN后也会得到一个这样的attribute，查表即可找到最相似的特征对应的类。 Attribute embedding如果特征维度太高，也可以将特征压缩成一个向量表示，这样在训练的时候，输出则是这样的向量特征，输入target data，输出向量特征，找到最近的特征对应的类即可 Attribute embedding + word embedding如果没有attribute数据，利用word embedding也可以达到不错的效果。在zero-shot learning中，光是让相同类的f和g相似是不够的，还应该让不同的f和g尽量远。 f^∗,g^∗=arg min_{(f,g)}⁡∑_nmax(0,k−f(x^n )\cdot g(y^n )+max_{(m≠n)} ⁡f(x^m )\cdot g(x^m ) )]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>李宏毅机器学习课程</tag>
        <tag>Transfer Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 14:Unsupervised Learning:Generation]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2014%3A%20Unsupervised%20Learning%3A%20Generation%2F</url>
    <content type="text"><![CDATA[Component-by-component对于图像来说，每次生成一个pixel：PixelRNN VAE架构： 其中e是噪声，σ是方差，目标是最小化reconstruction error，以及一个限制。该限制的目的即防止σ=0，m是正则化项。 中间的推导以及为什么是这样的架构我还不是很懂，之后再更新。实际上可以这么理解，有几个要点： 首先我们是基于这么一个假设：中间的code应当是服从正态分布的，而encoder的作用即在于拟合该正态分布的均值与方差的对数（因为方差应当恒为正，但神经网络的输出可能有正有负） 如果生成出来的code不符合正态分布，会有一个惩罚项，也就是上图的constraint（可以通过KL散度推导获得） 按理说，应当是在生成了均值和方差后，定义好该正态分布，然后再从中采样，但是这样没办法回传更新梯度，因此这里使用重参数技巧(Reparameterization Trick)，也即从$N(\mu,\sigma^2)$中采样$Z$，相当于从$N(0,I)$中采样$\varepsilon$，然后让$Z=\mu + \varepsilon \times \mu$ Reference:https://www.sohu.com/a/226209674_500659 VAE的主要问题在于，网络只试图去记住见过的图像，但没法真正去生成没见过的图像。 Generative Adversarial Network (GAN)GAN包含一个discriminator和一个generator，generator试图生成能够骗过discriminator的样本，而generator试图能够将generator生成的样本和真实的样本区分。 之后会有详细的介绍。]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>李宏毅机器学习课程</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Generation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 13:Unsupervised Learning:Auto-encoder]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2013%3A%20Unsupervised%20Learning%3A%20Auto-encoder%2F</url>
    <content type="text"><![CDATA[Auto-encoder由一个encoder和一个decoder组成，encoder负责将输入转成一个向量表示（维度通常小于输入），decoder负责将这段向量表示恢复成原来的输入。那么中间的code就可以作为输入的一个低维表示： Auto-encoder for CNN Unpooling有两种方法，一种在pooling的时候记录最大值的位置，在unpooling时在相对位置填充最大值，其他位置填充0；另一种不记录最大值位置，直接在pooling区域全部填充最大值。 Deconvolution其实本质就是convolution。 这是convolution: 我们期待的convolution： 实际上就等价在两边做padding，然后直接convolution： Auto-encoder的用处可以预训练每一层的DNN： 同理其它层也是一样，每次fix住其他层然后做Auto-encoder。那么在bp的时候只需要fine-tune就行。]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>李宏毅机器学习课程</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Auto-encoder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 12:Unsupervised Learning:Neighbor Embedding]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2012%3A%20Unsupervised%20Learning%3A%20Neighbor%20Embedding%2F</url>
    <content type="text"><![CDATA[Locally Linear Embedding (LLE)一种降维方法思想：假设每个点可以由其周围的点来表示 我们需要找到这样的$w_{ij}$，使得： ∑_i‖x^i−∑_j w_{ij} x^j ‖_2这样在降维的时候，我们仍然保持x之间的这样的关系: Laplacian Eigenmaps一种降维方法基本思想：如果$x^1$与$x^2$在高维空间中相近，则降维后也应该接近： S=1/2 ∑_{i,j} w_{i,j} (z^i−z^j )^2其中： 如果将z全设为0，显然S最小，因此我们需要给z一个限制：z应当充满空间，也即假如z是M维，那么$\{z^1,z^2…,z^N\}$的秩应该等于M T-distributed Stochastic Neighbor Embedding (t-SNE)也是一种降维方法前面提到的方法有一个问题：同一类的点确实聚在一起，但不同类的点并没有尽量分开 t-SNE的主要思想：将数据点映射到概率分布，我们希望降维前和降维后，数据分布的概率应当尽可能一致。t-SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。t-SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。 如何做？在高维空间中，我们定义： P(x^j |x^i )=\frac{S(x^i,x^j )}{∑_{k≠i}S(x^i,x^k )}其中S表示i与j之间的相似度。 在低维空间中，同样有： Q(z^j |z^i )=\frac{S′(z^i,z^j )}{∑_{k≠i}S′(z^i,z^k )}使用KL散度去计算两个分布之间的差异： L=∑_i KL(P(∗|x^i )||Q(∗|z^i )) =∑_i∑_j P(x^j |x^i )\frac{log P(x^j |x^i )}{Q(z^j |z^i )}t-SNE中，高维空间和低维空间计算相似度的公式不大一样： S(x^i,x^j )=exp(−‖x^i−x^j ‖_2 )S′(z^i,z^j )=\frac{1}{(1+‖z^i−z^j ‖_2)}两个公式的图示： 也即低维空间会拉长距离，使得距离远的点尽可能被拉开。 t-SNE的问题在于：t-SNE无法对新的数据点进行降维。]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>李宏毅机器学习课程</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Neighbor Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 11:Unsupervised Learning:Linear Dimension Reduction]]></title>
    <url>%2F2018%2F10%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2011%3A%20Unsupervised%20Learning%3A%20Linear%20Dimension%20Reduction%2F</url>
    <content type="text"><![CDATA[ClusteringK-means算法步骤： 迭代更新使得最后聚类中心收敛。但事先需要定好有多少类。 Hierarchical Agglomerative Clustering (HAC)自下而上，每次选两个最近的聚为一类，直到所有的都分成一类最后选择一个阈值划分，如蓝色绿色和红色的线 Dimension Reduction找到一个映射，使得x能够映射到低维z Principle Component Analysis (PCA)目的是找到一个维度，使得投影得到的variance最大，也即最大程度保留数据的差异性。 形式化可以写成（一维情形）： Var(z_1 )=\frac{1}{N} ∑_{z_1}(z_1−\overline{z_1} )^2其中： ‖w^1 ‖_2=1z_1=w^1 \cdot x$\overline{z_1}$表示z的均值 假如我们要投影到多维，其他维度也有同样的目标。其中每个维度之间都应该是相互正交的。 如何做？找到$ \frac{1}{N}∑(x−\overline{x} ) (x−\overline{x})^T$的前k个最大的特征值对应的特征向量，组合起来即是我们要找的$W$ 证明—-Warning of Math—-目的：$Var(z_1 )=\frac{1}{N} ∑_{z_1}(z_1−\overline{z_1} )^2 $其中 $\overline{z_1} =\frac{1}{N} ∑{z_1} = \frac{1}{N} ∑ w^1 \cdot x=w^1\cdot \overline{x}$ 推导：改变符号 $S=Cov(x)$ 利用拉格朗日乘数法，有：$Sw^1=αw^1$等式两边各左乘$(w^1)^T$，有：$(w^1 )^T Sw^1=α(w^1 )^T w^1=α$ 也即，$α$是$S$的特征值，选择最大的特征值，就能够最大化我们的目标。 同理，我们要找$w^2$，最大化$(w^2 )^T Sw^2$，其中有：$(w^2 )^T w^2=1$$(w^2 )^T w^1=0$ （与第一维正交） 因此利用拉格朗日乘数法： g(w^2 )= (w^2 )^T Sw^2−α((w^2 )^T w^2−1)−β((w^2 )^T w^1−0)最终得到，w2对应第二大的特征值的特征向量。 以此类推，其他维也同理。—-End of Math—- PCA的其他实际上最终得到的z，每一维之间的协方差都为0 证明如下： PCA也可以用SVD来做： U中保存了K个特征向量。 从另一种角度理解PCA，也可以认为PCA是一种autoencoder： PCA的问题PCA是无监督学习，如果有标签，则无法按照类别来进行正确降维，如： 第二就是PCA是线性变换，对于一些需要非线性变换的无能为力 Matrix Factorization定义：矩阵分解，就是将一个矩阵D分解为U和V的乘积，即对于一个特定的规模为m*n的矩阵D，估计出规模分别为m*k和n*k的矩阵U和V，使得$UV^T$的值尽可能逼近矩阵D。常用于推荐系统。 思想：假如有一个矩阵： 假设横轴和纵轴每一维都有一个向量代表该维，矩阵的每个元素就是横轴和纵轴对应维的点积。我们的目的是尽可能减小： L=\sum_{(i,j)} (r^i \cdot r^j -n_{ij})^2其中$r_i$ $r_j$就是向量表示，$n_{ij}$就是矩阵的内容。 可以使用SVD求解上式： 实际上，考虑每一行或列本身的特性，我们对Loss进行扩展： Minimizing \ \ L=\sum_{(i,j)} (r^i \cdot r^j +b_i+b_j-n_{ij})^2使用SGD可以求解。]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>李宏毅机器学习课程</tag>
        <tag>Unsupervised Learning</tag>
        <tag>Linear Dimension Reduction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度消失与梯度爆炸的推导]]></title>
    <url>%2F2018%2F10%2F07%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[记RNN中每一步的损失为$E_t$，则损失对$h_{t-1}$的权重$W$的导数有： \frac{\partial{E_t}}{\partial{W}}=\sum_{k=1}^{t} \frac{\partial{E_t}}{\partial{y_t}} \frac{\partial{y_t}}{\partial{h_t}} \frac{\partial{h_t}}{\partial{h_k}} \frac{\partial{h_k}}{\partial{W}}其中$\frac{\partial{h_t}}{\partial{h_k}}$使用链式法则有： \frac{\partial{h_t}}{\partial{h_k}} = \prod_{j=k+1}^{t} \frac{\partial{h_j}}{\partial{h_{j-1}}} = \prod_{j=k+1}^{t} W^T \times diag[f^{\prime}(h_{j-1})]其中$\frac{\partial{h_j}}{\partial{h_{j-1}}}$ 是雅克比矩阵。对其取模(norm)，有： \rVert \frac{\partial{h_j}}{\partial{h_{j-1}}}\rVert ≤ \rVert W^T \rVert \rVert diag[f^{\prime}(h_{j-1})] \rVert ≤ \beta_W \beta_h当$f$为sigmoid时，$f^{\prime}(h_{j-1})$最大值为1。 最终我们有： \rVert \frac{\partial{h_t}}{\partial{h_{k}}}\rVert ≤ \rVert \prod_{j=k+1}^{t} \frac{\partial{h_j}}{\partial{h_{j-1}}} \rVert ≤ (\beta_W \beta_h)^{t-k}从上式可以看出，当t-k足够大时，如果$(\beta_W \beta_h)$小于1则$(\beta_W \beta_h)^{t-k}$则会变得非常小，相反，若$(\beta_W \beta_h)$大于1则$(\beta_W \beta_h)^{t-k}$则会变得非常大。 在计算机中，当梯度值很大时，会造成上溢(NaN)，也即梯度爆炸问题，当梯度值很小时，会变成0，也即梯度消失。注意到，t-k的损失实际上评估的是一个较远的词对当前t的贡献，梯度消失也即意味着对当前的贡献消失。 Reference:CS224d: Deep Learning for NLP Lecture4]]></content>
      <tags>
        <tag>梯度消失</tag>
        <tag>梯度爆炸</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识10]]></title>
    <url>%2F2018%2F10%2F07%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%8610%2F</url>
    <content type="text"><![CDATA[1️⃣[正态分布]高维正态分布是从一维发展而来的： https://www.zhihu.com/question/36339816 2️⃣[RNN]from https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf 通常而言，我们都会将RNN的initial state设为全0，但在Hinton的slide中提到，我们可以将初始状态作为可学习的变量，和我们在学习权重矩阵一样。]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>正态分布</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRML第二章 概率分布]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FPRML%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[二元变量 多项式分布 高斯分布 指数族分布 非参数优化]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 10:Semi-supervised learning]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%2010%3A%20Semi-supervised%2F</url>
    <content type="text"><![CDATA[什么是semi-supervised learning 给定数据${(x^r,\hat{y}^r)}_{r=1}^{R},{(x_u)}_{u=R}^{R+U}$，其中未标记数据远远多于标记数据 $U&gt;&gt;R$ 为什么半监督学习有用？因为未标记数据的分布可能能够给我们一些信息。 生成模型的半监督学习给定两类$C_1$、$C_2$，要求得到后验概率分布 P(C_1 |x)=\frac{P(x|C_1 )P(C_1 )}{(P(x|C_1 )P(C_1 )+P(x|C_2 )P(C_2 ) )}其中联合概率分布服从高斯分布。未标记数据此时的作用即帮我们重新估计$P(C_1),P(C_2),\mu,\Sigma$ 如何做?先初始化$P(C_1),P(C_2),\mu,\Sigma$，通常可以先用有标记数据进行估计 计算每个未标记数据的后验概率分布 以该概率分布更新模型不断重复直至拟合 原因：当我们在做监督学习时，使用最大似然求解： logL(θ)=∑_{x^r,\hat{y}^r} logP_θ (x^r |\hat{y}^r )加上了未标记数据后，同样也要做最大似然： logL(θ)=∑_{(x^r,\hat{y}^r)} logP_θ (x^r |\hat{y}^r )+∑_{x^u} logP_θ (x^u)Low-density Separation假设不同类别之间有一条明显的分界线，也即存在一个区域，其密度比其他区域小 Self-training如何做? 先用有标签数据训练一个模型$f$； 利用模型对未标记数据进行标记，这些标签称为伪标签（pseudo-label） 将部分有伪标签的数据放入有标签数据中，重新训练重复直到拟合 这种方式和生成模型的区别：该方法使用的是hard label而生成模型使用的是soft label Entropy-based Regularization将未标记数据充当正则化的效果，我们希望模型预测标签的概率较为集中，也即熵应该尽可能小。也就是说，未标记数据使得分类边界尽可能划在低密度区域。 Smoothness Assumption假设：位于稠密数据区域的两个距离很近的样例的类标签相似，通过high density path连接。 x1与x2之间较为稠密，因此x2与x1比x2与x3更为接近。 如何知道x1与x2通过high density path连接？ 基于图的方法： 定义xi与xj之间的相似度$s(x^i,x^j)$ 添加边，有两种选择 k nearest neighbor e-neighborhood 边之间的权重通过相似度来衡量。如： $s(x^i,x^j )=exp(−γ‖x^i−x^j‖^2)$ 该方法本质即利用有标签数据去影响未标记数据，通过图的传播。但一个问题是如果数据不够多，就可能没办法传播。如： 在建立好图后，如何使用? 定义图的平滑程度，$y$表示标签。$S$越小表示越平滑。S=1/2∑_{i,j} w_{i,j} (y^i−y^j )^2=y^T Lyy=[⋯y^i⋯y^j⋯]^TL=D−W D是邻接矩阵，第ij个元素即xi与xj之间的weight，W是对角矩阵，ii个元素是D的第i行的加和；L称为Graph Laplacian 我们最终在计算Loss的时候要加上这项正则项L=∑_{x^r}C(y^r,\hat{y}^r ) +λS]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>Semi-supervised learning</tag>
        <tag>李宏毅机器学习课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 7:Tips for DL]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%207%3A%20Tips%20for%20DL%2F</url>
    <content type="text"><![CDATA[大纲 new activation function梯度消失问题：由于sigmoid会将值压缩，所以在反向传播时，越到后面值越小。 所以后层的更新会比前层的更新更快，导致前层还没converge，后层就根据前层的数据（random）达到converge了 ReLU能够快速计算，且能够解决梯度消失问题。 因为会有部分neuron的值是0，所以相当于每次训练一个瘦长的神经网络。 ReLU的变体 Maxout首先将几个neuron归为一组，然后每次前向传播时取最大的作为输出。 实际上ReLU是maxout的一种特殊形式： 更一般的，有： 因为w和b的变化，所以该activation function实际上就是一个learnable activation function 这样一个learnable activation function有这样的特点： Activation function in maxout network can be any piecewise linear convex functionHow many pieces depending on how many elements in a group 如： maxout应如何训练？ 实际上就是一个普通的瘦长network，常规训练即可。 Adaptive learning rate在adagrad中: 越到后面learning rate越来越小，但实际上在dl里面，error surface是非常复杂的，越来越小的learning rate可能不适用于dl。如： RMSprop$σ^t$是历史信息，也就是说$σ^t$参考了过去的梯度和当前的梯度获得一个新的放缩大小 Momentum引入惯性作为参考，也即参考了上一次梯度的方向。引入惯性后，可能有机会越过local minimum。普通的gradient descent:每次朝着梯度的反方向走。 Momentum: 考虑了上一步走的方向。 具体算法： Adam结合了RMSprop和Momentum，也即综合考虑了历史信息决定当前步长；考虑了上一步的方向决定当前走的方向。具体算法： Early Stopping就是在validation set的loss不再减小时停止 RegularizationL2正则化其中因此更新公式为： 也即每次以$1-\eta \lambda$对w进行放缩，使w更接近0正则化在DL中也称为weight decay L1正则化 则更新公式为： 也即每次以$ηλsgn(w)$ 使w往0靠（sgn表示符号函数） 可以看出，L1每次都加减相同的值，而L2按比例进行缩放。因此L1更为稀疏(sparse)。 Dropout训练的时候每一层采样p%的神经元设为0，让其不工作 实际上就是每个batch改变了网络结构，使得网络更细长 测试的时候所有的weight都乘以1-p% 从ensemble的角度看待dropout：在训练的时候训练一堆不同结构的network，最多有$2^N$种组合，N为neuron个数，可以称为终极的ensemble方法了。而在测试的时候对这些不同的网络进行平均。]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>李宏毅机器学习课程</tag>
        <tag>Tips for DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[采样浅析]]></title>
    <url>%2F2018%2F09%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2F%E9%87%87%E6%A0%B7%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[总结在NLP中的采样方法（持续更新）。 采样方法1️⃣逆变换采样(Inverse Sampling)目的：已知任意概率分布的累积分布函数时，用于从该分布中生成随机样本。 —-什么是累积分布函数(CDF)—-是概率密度函数(PDF)的积分，定义： F_X(x)=P(X≤x)=\int_{-∞}^{x}f_X(t)dt—-END—- 想象我们知道高斯分布的概率密度函数，我们应该如何采样？本质上我们只能对均匀分布进行直接采样（高斯分布有算法可以生成采样，但无法一般化）。对于这种连续的随机变量，我们只能通过间接的方法进行采样。 逆变换采样即是通过累积分布函数的反函数来采样。因为累积分布函数的值域为$[0,1]$，因此我们通过在$[0,1]$上进行采样，再映射到原分布。例子:映射关系如图： 2️⃣重要性采样(Importance Sampling)目的：已知某个分布$P$，希望能估计$f(x)$的期望。亦即： E[f(x)]=\int_{x}f(x)p(x)dx≈\frac{1}{n}\sum_{i=1}^{n}f(x_i)其中$x\sim p$。假设$p(x)$的分布复杂或样本不好生成，另一分布$q(x)$方便生成样本。因此我们引入$q(x)$对原先分布进行估计。 E[f(x)]=\int_{x}f(x)p(x)dx=\int_{x}f(x)\frac{p(x)}{q(x)}q(x)dx≈\frac{1}{n}\sum_{i=1}^{n}f(x_i)\frac{p(x_i)}{q(x_i)}其中，$x \sim q$。$w(x)=\frac{p(x)}{q(x)}$称为Importance Weight 根据上式，实际上就是每次采样的加权求和。 Reference逆变换采样https://zh.wikipedia.org/wiki/%E9%80%86%E5%8F%98%E6%8D%A2%E9%87%87%E6%A0%B7 重要性采样https://www.youtube.com/watch?v=S3LAOZxGcnk ——持续更新——]]></content>
      <tags>
        <tag>采样</tag>
        <tag>sampling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识9]]></title>
    <url>%2F2018%2F09%2F30%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%869%2F</url>
    <content type="text"><![CDATA[1️⃣[Pytorch]Pytorch中保存checkpoint是一个dict形式，可以保存任意多个模型到一个checkpoint中。1234567import torch#savetorch.save(&#123; 'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': loss, ... &#125;, PATH)#loadmodel = TheModelClass(*args, **kwargs) optimizer = TheOptimizerClass(*args, **kwargs)checkpoint = torch.load(PATH) model.load_state_dict(checkpoint['model_state_dict']) optimizer.load_state_dict(checkpoint['optimizer_state_dict']) epoch = checkpoint['epoch'] loss = checkpoint['loss']model.eval() # - or - model.train() 2️⃣[Pytorch]Pytorch可以load部分模型，也就是只load进来部分我们需要的层，这在transfer learning中用到。123torch.save(modelA.state_dict(), PATH)modelB = TheModelBClass(*args, **kwargs) modelB.load_state_dict(torch.load(PATH), strict=False)]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[no title]]></title>
    <url>%2F2018%2F09%2F30%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%9C%89%E5%8A%9B%E9%87%8F%E7%9A%84%E6%96%87%E5%AD%97%2F</url>
    <content type="text"><![CDATA[每当我遇到自己不敢直视的困难时，我就会闭上双眼，想象自己是一个80岁的老人，为人生中曾放弃和逃避过的无数困难而懊悔不已，我会对自己说，能再年轻一次该有多好，然后我睁开眼睛：砰！我又年轻一次了！]]></content>
      <tags>
        <tag>佳句分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词10]]></title>
    <url>%2F2018%2F09%2F30%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D10%2F</url>
    <content type="text"><![CDATA[1️⃣次北固山下[唐] 王湾客路青山外，行舟绿水前。潮平两岸阔，风正一帆悬。海日生残夜，江春入旧年。乡书何处达，归雁洛阳边。 次：旅途中暂时停宿，这里是停泊的意思。 http://m.xichuangzhu.com/work/57b95de92e958a005fa8919e 2️⃣将赴吴兴登乐游原[唐] 杜牧清时有味是无能，闲爱孤云静爱僧。欲把一麾江海去，乐游原上望昭陵。 无能：无所作为。 http://m.xichuangzhu.com/work/57b99db9165abd005a6da742]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PRML第一章 绪论]]></title>
    <url>%2F2018%2F09%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FPRML%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E7%BB%AA%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[记录PRML学习过程。笔记共享链接：https://1drv.ms/u/s!Apsp2510NHF6rIRjMclFB16v7B0FWg 概率论 决策论 信息论]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 6:Backpropagation]]></title>
    <url>%2F2018%2F09%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%206%3A%20Backpropagation%2F</url>
    <content type="text"><![CDATA[Chain Rule基本公式 forward pass和backward pass可以将backpropagation分为两步 forward pass在前向传播的时候提前计算/保存好，因为该梯度很简单 比如z对w1的梯度就是x1，就是和w1相连的项 backward pass回传的时候逐层相乘下去，类似动态规划，获得了后一层的梯度才能求出前一层的梯度。 总结 先前向，提前算出最邻近的梯度，直到output layer，计算完该梯度，再不断回传逐层相乘获得output对各层的梯度。 代码实现例子relu实现forward pass和backward pass1234567891011121314151617181920212223242526272829303132import torchclass MyReLU(torch.autograd.Function): """ We can implement our own custom autograd Functions by subclassing torch.autograd.Function and implementing the forward and backward passes which operate on Tensors. """ @staticmethod def forward(ctx, input): """ In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. """ ctx.save_for_backward(input) #为了之后的backward计算 return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): """ In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. """ input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input &lt; 0] = 0 return grad_input]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>李宏毅机器学习课程</tag>
        <tag>Backpropagation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 5:Classification:Logistic Regression]]></title>
    <url>%2F2018%2F09%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%205%20Classification%3A%20Logistic%20Regression%2F</url>
    <content type="text"><![CDATA[logistic regression如何做？step1: 定义function set step2: 更新使用最大似然更新 L(w,b)=f_{w,b}(x^1 )f_{w,b}(x^2 )(1−f_{w,b} (x^3 ))⋯f_{w,b} (x^N )找到w，b使得L最大 对似然函数取负对数，则有： 将式子的每个元素写成伯努利分布形式： 上式就是cross-entropy损失函数。 求导该式子可得：更新公式：可以看出上式很直观：和答案差距越大，更新步伐越大。 同时发现上式和linear regression的更新公式是一致的。 为什么不像linear regression那样设loss为square？假设我们使用square loss，则求导得到的梯度：上式可以看出，当接近target时，梯度小；远离target时，梯度也小。难以达到全局最小 下图是cross entropy和square error的图像示意： 如图，square loss难以到达全局最小。 生成式模型与判别式模型的区别生成式对联合概率分布进行建模，再通过贝叶斯定理获得后验概率；而判别式模型直接对后验概率建模。二者所定义的function set是一致的，但同一组数据可能会得到不同的w和b。 二者优劣对比： 数据量多时，一般来说判别式模型会更好。因为判别式模型没有先验假设，完全依赖于数据。但如果数据有噪声，容易受影响。 生成式模型是有一定的假设的，当假设错误，会影响分类效果。 正因为有一定的先验假设，当数据量很少时，可能效果会不错；对于噪声更具有鲁棒性。 先验可以从其他数据源获得来帮助特定任务，如语音识别问题。 logistic的局限本质仍是一个线性分类器，没办法分类非线性的数据。如何解决该问题?将logistic regression model拼接起来，前面的model对数据进行feature transformation，然后再对新的feature进行分类。 logistic与deep learning的联系：如果将logistic regression的一个单元称为neuron，拼起来就是neural network了！！！]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>李宏毅机器学习课程</tag>
        <tag>Classification</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识8]]></title>
    <url>%2F2018%2F09%2F23%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%868%2F</url>
    <content type="text"><![CDATA[1️⃣[Pytorch]torch.max()有两种不同写法。torch.max(input) → Tensor 返回其中最大的元素torch.max(input, dim, keepdim=False, out=None) → (Tensor, LongTensor) 返回该维度上最大值，以及对应的index 2️⃣[Pytorch]将模型同时部署到多张卡上训练，本质就是将一个batch的数据split，送到各个model，然后合并结果。 123456model = nn.DataParallel(model)device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")model.to(device)for data in rand_loader: input = data.to(device) output = model(input) 3️⃣[求导]标量、向量、矩阵之间的求导有两种布局，即分子布局和分母布局。分子布局和分母布局只差一个转置。我的记法：在求导过程中，假设分母为m*n，分子为 k*n，则导数矩阵应该为 k*m 。一些特殊的如标量对矩阵求导等除外。具体直接查表：https://en.m.wikipedia.org/wiki/Matrix_calculus 按位计算求导：假设一个函数$f(x)$的输入是标量$x$。对于一组K个标量$x_1,··· ,x_K$，我们可以通过$f(x)$得到另外一组K个标量$z_1,··· ,z_K$，$z_k = f(x_k),∀k = 1,··· ,K$其中，$f(x)$是按位运算的，即$[f(x)]_i = f(x_i)$其导数是一个对角矩阵： Reference：https://en.m.wikipedia.org/wiki/Matrix_calculushttps://blog.csdn.net/uncle_gy/article/details/78879131https://nndl.github.io/chap-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.pdf]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Pytorch</tag>
        <tag>求导</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录7]]></title>
    <url>%2F2018%2F09%2F23%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E8%AE%B0%E5%BD%957%2F</url>
    <content type="text"><![CDATA[1️⃣softmax的numpy实现123def softmax(x,axis=0): """Compute softmax values for each sets of scores in x.""" return np.exp(x) / np.sum(np.exp(x), axis=axis) 2️⃣numpy 手动求导relu123456789101112131415161718192021222324252627282930313233343536import numpy as np# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random input and output datax = np.random.randn(N, D_in)y = np.random.randn(N, D_out)# Randomly initialize weightsw1 = np.random.randn(D_in, H)w2 = np.random.randn(H, D_out)learning_rate = 1e-6for t in range(500): # Forward pass: compute predicted y h = x.dot(w1) h_relu = np.maximum(h, 0) y_pred = h_relu.dot(w2) # Compute and print loss loss = np.square(y_pred - y).sum() print(t, loss) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.T.dot(grad_y_pred) grad_h_relu = grad_y_pred.dot(w2.T) grad_h = grad_h_relu.copy() grad_h[h &lt; 0] = 0 grad_w1 = x.T.dot(grad_h) # Update weights w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 3️⃣Pytorch实现relu1234567891011121314151617181920212223242526272829303132import torchclass MyReLU(torch.autograd.Function): """ We can implement our own custom autograd Functions by subclassing torch.autograd.Function and implementing the forward and backward passes which operate on Tensors. """ @staticmethod def forward(ctx, input): """ In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. """ ctx.save_for_backward(input) #为了之后的backward计算 return input.clamp(min=0) @staticmethod def backward(ctx, grad_output): """ In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. """ input, = ctx.saved_tensors grad_input = grad_output.clone() grad_input[input &lt; 0] = 0 return grad_input 4️⃣Pytorch在多张卡上部署123456model = nn.DataParallel(model)device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")model.to(device)for data in rand_loader: input = data.to(device) output = model(input)]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[听达观杯现场答辩有感]]></title>
    <url>%2F2018%2F09%2F19%2F%E8%A7%81%E9%97%BB%26%E6%83%B3%E6%B3%95%2F%E5%90%AC%E8%BE%BE%E8%A7%82%E6%9D%AF%E7%8E%B0%E5%9C%BA%E7%AD%94%E8%BE%A9%E6%9C%89%E6%84%9F%2F</url>
    <content type="text"><![CDATA[前几日（周日）去了达观杯答辩现场听了前10名做了报告，有了一些感想，但一直没有抽出时间写一下自己的感想（懒）。 自己大概花了十来天做了一下比赛，实际上也就是一个文本分类的比赛，因为没有比赛经验的缘故，走了很多弯路。不过也学到了一些东西。 现记录前十名的一些idea/trick： 数据增强 因为给的句子长度很长，因此在做截断的时候后面的就没法训练到了，可以将文本倒序作为新的数据训练模型。可以充分利用到数据 将数据打乱、随机删除，实际上就是对一个句子的词进行sample再组合 打乱词序以增加数据量 使用pseudo labeling，但有的队伍使用这个做出效果了，但有的没有 特征工程 假设开头中间结尾的信息对分类有帮助，因此截取该部分信息做训练 改进baseline的tfidf的特征工程方法，使用基于熵的词权重计算 降维，留下最重要的特征。先用卡方分布降到20万，再用SVD降到8000 将word2vec和GloVe拼接起来作为deep learning模型的输入 将文章分段，每段取前20后20拼起来 模型融合 所有队伍都无一例外使用了模型融合，stacking或者简单的投票 DL+ML —&gt; lgbm model —&gt; voting 深度模型+传统模型，在深度模型最后一层加入传统模型的信息/feature 后向选择剔除冗余模型 DL&amp;其他 HAN，选择10个attention vector 对易错类增加权重，通过改变损失函数来增加权重 CNN, [1,2,3,4,5,6]*600 提出新的模型（第一名） 其实除了一些trick，我还是有些失望的，因为都是用模型融合堆出来的，这也让我对比赛失去了一些兴趣。虽然能理解现在的比赛都是这样的，但感觉实在太暴力了。当然，其中还是有一些亮点的，有一支队伍立意很高，从理解业务的角度出发而不是堆模型，也取得了很好的效果；还有一个使用了最新论文中的特征工程改进方法，令我耳目一新；以及第一名在比赛过程中提出来三个新的模型。 Anyway，我目前还是太菜了，还是安心搞科研吧。_(:з」∠)]]></content>
      <tags>
        <tag>有感</tag>
        <tag>比赛</tag>
        <tag>达观杯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识7]]></title>
    <url>%2F2018%2F09%2F16%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%867%2F</url>
    <content type="text"><![CDATA[1️⃣[Pytorch]只有一个元素的tensor，可用.item()来获取元素 tensor &lt;—&gt; numpy 相互转化会共享内部数据，因此改变其中一个会改变另一个 可用使用 .to 来移动到设备 .detech() detach it from the computation history, and to prevent future computation from being tracked. 将其从计算图中分离，变为叶子节点，并且requires_grad=False Function 记录了这个tensor是怎么来的，所有的tensor都有，除非是用户自定义的： 2️⃣[协方差]关于协方差的理解，x与y关于某个自变量的变化程度，即度量了x与y之间的联系。https://www.zhihu.com/question/20852004]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Pytorch</tag>
        <tag>协方差</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 4:Classification:Probabilistic Generative Model]]></title>
    <url>%2F2018%2F09%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%204%20Classification%20%20Probabilistic%20Generative%20Model%2F</url>
    <content type="text"><![CDATA[为什么不使用regression来分类？1️⃣如果使用regression的思想来分类，会对离边界较远的点进行惩罚： 2️⃣如果多分类使用regression，如class 1, class 2, class 3；则隐式地假设了class 1 和 class 2较为接近，如果没有这种接近关系，则分类会不正确。 问题描述与定义 当P大于0.5则是C1类，反之是C2类先验P(C1)和P(C2)都好计算，计算C1占总的比例即可因此，我们需要计算的就是p(x|C) 这一想法，本质是得到了生成式模型： 原理概述现假设训练数据点的分布服从高斯分布：（显然可以自己设任何分布）即数据从高斯分布采样得到： 根据最大似然估计，可以获得每个类别的μ和Σ： 得到了参数后，即可代入得到P(C|x) ： 刚刚假设$Σ$对于不同类别不同，现我们令不同类别共享相同$Σ$：（因为协方差代表的是不同feature之间的联系，可以认为是和类别无关的） $Σ$的计算公式是加权求和： 在使用了相同的协方差矩阵后，边界就是线性的（后面会提到为什么是这样）： 总结： 三步走，定义function set，计算μ和协方差矩阵，得到best function： 注意到，如果我们认为，不同feature之间没有关系，每个feature符合特定的高斯分布，则该分类器则是朴素贝叶斯分类器： 分类与logistics regression现推导，该分类问题与logistics regression之间的联系：即： 假设数据服从高斯分布，共享$Σ$ 推导①总框架： 令 则有： ②z的进一步推导与简化： 将z展开： 而第一部分有： 第一部分相除，有： 再进行展开，有： 最终z的公式为： 由于共享协方差矩阵，则可以消去部分，得到： 替换成w和b： ③最终，将z带回到原式： 所以我们不需要再估计N1,N2,μ和Σ，直接计算w和b即可。也因此，分界线是线性的。 全过程：]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>李宏毅机器学习课程</tag>
        <tag>Classification</tag>
        <tag>Probabilistic Generative Model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 3:Gradient Descent]]></title>
    <url>%2F2018%2F09%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%203%20Gradient%20Descent%2F</url>
    <content type="text"><![CDATA[Gradient Descent tipstip 1：Adaptive Learning RatesAdagrad基本思想 其中σ是之前所有的梯度的平方根 化简形式： 为什么要怎么做？考虑一个开口向上的二次函数 也即，最好的步长是一次导除以二次导，但二次导计算量大，因此使用近似的方式：对一次导作多次的sample。下图显示，如果二次导小，那么多次sample获得的一次导也小，反之则大，也就是说，一次导在某种程度上可以反映二次导的大小，所以直接用一次导近似，可以减少计算量。 tip 2：feature scaling 能够改变loss的分布，上图1中w2对loss的影响较大，则较陡峭，参数更新就较困难，需要adaptive learning rate；如果进行feature scaling，能够更好达到local optimal Gradient Descent Theory另一种角度看gradient descent： 基本思想：我们希望每一次都在当前点附近找到一个最小的点，即在一个范围内： 应该如何找到该最小点？ 我们知道，泰勒级数的形式： 当x接近x0时，会有如下近似： 推广到多元泰勒级数则有： 那么，如前所述，x接近x0，对于图中，即圆圈足够小时： 简化符号： 所以可以简写成： 由于s,u,v都是常数，在圆圈范围内寻找最小值对应的参数可以简化成： 再度简化，可以表达成： 在图中可以画为两个向量的点积 显然，当反方向时，最小： 也即： 最终完整的式子： 因此，当learning rate不够小时，是不满足泰勒级数近似的。]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>李宏毅机器学习课程</tag>
        <tag>Gradient Descent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lecture 2:Bias and Variance]]></title>
    <url>%2F2018%2F09%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%2FLecture%202%20Bias%20and%20Variance%2F</url>
    <content type="text"><![CDATA[如何理解bias&amp;variancebias是function space中心离optimal model的差距，variance是某次实验所得模型离function space中心的距离。 比如说，简单地模型的function space小，随机性小，因此variance小，但也因为function space小，表示能力有限，因此bias大。 如图：该图中蓝色圈代表模型所能表达的范围。 如何解决variance大的问题①更多的data②regularization：强迫function更平滑，因此减小variance，但因为调整了function space，可能会增加bias。]]></content>
      <tags>
        <tag>机器学习🤖</tag>
        <tag>李宏毅机器学习课程</tag>
        <tag>bias&amp;variance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词9]]></title>
    <url>%2F2018%2F09%2F16%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D9%2F</url>
    <content type="text"><![CDATA[白雪歌送武判官归京[唐] 岑参北风卷地白草折，胡天八月即飞雪。忽如一夜春风来，千树万树梨花开。散入珠帘湿罗幕，狐裘不暖锦衾薄。将军角弓不得控，都护铁衣冷难着。瀚海阑干百丈冰，愁云惨淡万里凝。中军置酒饮归客，胡琴琵琶与羌笛。纷纷暮雪下辕门，风掣红旗冻不翻。轮台东门送君去，去时雪满天山路。山回路转不见君，雪上空留马行处。 http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290 绝命诗谭嗣同望门投止思张俭，忍死须臾待杜根。我自横刀向天笑，去留肝胆两昆仑！]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch backward()浅析]]></title>
    <url>%2F2018%2F09%2F16%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPytorch%20backward()%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[最近在看pytorch文档的时候，看到backward内有一个参数gradient，在经过查阅了相关资料和进行了实验后，对backward有了更深的认识。 backward1️⃣如果调用backward的是一个标量，如：loss.backward()则gradient不需要手动传入，会自动求导。例子:$a=[x_1,x_2],b=\frac{x_1+x_2}{2}$则b对a求导，有：$\dfrac {\partial b}{\partial x_{1}}=\frac{1}{2}，\dfrac {\partial b}{\partial x_{2}}=\frac{1}{2}$ 123456import torcha=torch.Tensor([2,3])a.requires_grad=Trueb=torch.mean(a) #tensor(2.5000, grad_fn=&lt;MeanBackward1&gt;)b.backward()a.grad #tensor([0.5000, 0.5000]) gradient此时只是在缩放原grad的大小，也即不指定gradient和gradient=1是等价的 当然，也可以指定gradient，其中指定gradient的shape必须和b的维度相同123gradient=torch.tensor(10.0)b.backward(gradient)a.grad #tensor([5., 5.]) 2️⃣如果调用backward的是一个向量例子：$a=[x_1,x_2],b=[b_1,b_2]$, 其中 $b_1=x_1+x_2,b_2=x_1*x_2$b对a求导，有：$\dfrac {\partial b_1}{\partial x_{1}}=1,\dfrac {\partial b_1}{\partial x_{2}}=1$ $\dfrac {\partial b_2}{\partial x_{1}}=x_2,\dfrac {\partial b_2}{\partial x_{2}}=x_1$ 在backward的时候则必须指定gradient。 1234567891011121314151617181920import torcha=torch.FloatTensor([2,3])a.requires_grad=Trueb=torch.zeros(2)b[0]=a[0]+a[1]b[1]=a[0]*a[1] # b=tensor([5., 6.], grad_fn=&lt;CopySlices&gt;)gradient=torch.tensor([1.0,0.0])b.backward(gradient,retain_graph=True)a.grad #tensor([1., 1.])，说明是对b_1进行求导a.grad.zero_() #将梯度清空，否则会叠加#-------------- #gradient=torch.tensor([0.0,1.0])b.backward(gradient,retain_graph=True)a.grad # tensor([3., 2.])，说明对b_2进行求导a.grad.zero_()# ------------- #gradient=torch.tensor([1.0,1.0])b.backward(gradient,retain_graph=True)a.grad # tensor([4., 3.])，即b_1,b_2的导数的叠加a.grad.zero_() 注意到b.backward()时需要retain_graph设为True，否则在计算完后会自动释放计算图的内存，这样就没法进行二次反向传播了。 Referencehttps://www.pytorchtutorial.com/pytorch-backward/]]></content>
      <tags>
        <tag>Pytorch</tag>
        <tag>backward</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词8]]></title>
    <url>%2F2018%2F09%2F09%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D8%2F</url>
    <content type="text"><![CDATA[望月怀远[唐] 张九龄海上生明月，天涯共此时。情人怨遥夜，竟夕起相思。灭烛怜光满，披衣觉露滋。不堪盈手赠，还寝梦佳期。 遥夜，长夜。 http://m.xichuangzhu.com/work/57aca120a341310060e2a09f 无题萨镇冰五十七载犹如梦，举国沦亡缘汉城。龙游浅水勿自弃，终有扬眉吐气天。 1951年，中国人民志愿军在抗美援朝战争第三次战役后打进了汉城，萨镇冰得知此事，回想起57年前的甲午悲歌，当即作诗一首。 白雪歌送武判官归京[唐] 岑参北风卷地白草折，胡天八月即飞雪。忽如一夜春风来，千树万树梨花开。散入珠帘湿罗幕，狐裘不暖锦衾薄。将军角弓不得控，都护铁衣冷难着。瀚海阑干百丈冰，愁云惨淡万里凝。中军置酒饮归客，胡琴琵琶与羌笛。纷纷暮雪下辕门，风掣红旗冻不翻。轮台东门送君去，去时雪满天山路。山回路转不见君，雪上空留马行处。 http://m.xichuangzhu.com/work/57b903e3d342d3005ac74290]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词7]]></title>
    <url>%2F2018%2F09%2F02%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D7%2F</url>
    <content type="text"><![CDATA[滕王阁序遥襟甫畅，逸兴遄飞。爽籁发而清风生，纤歌凝而白云遏。睢园绿竹，气凌彭泽之樽；邺水朱华，光照临川之笔。四美具，二难并。穷睇眄于中天，极娱游于暇日。天高地迥，觉宇宙之无穷；兴尽悲来，识盈虚之有数。望长安于日下，目吴会于云间。地势极而南溟深，天柱高而北辰远。关山难越，谁悲失路之人；萍水相逢，尽是他乡之客。怀帝阍而不见，奉宣室以何年？ 注释：遥襟甫畅，逸兴遄（chuán）飞：登高望远的胸怀顿时舒畅，飘欲脱俗的兴致油然而生。 爽籁（lài）发而清风生，纤歌凝而白云遏：宴会上，排箫响起，好像清风拂来；柔美的歌声缭绕不散，遏止了白云飞动。爽：形容籁的发音清脆。籁：排箫，一种由多根竹管编排而成的管乐器。 睢（suī）园绿竹，气凌彭泽之樽：今日的宴会，好比当年睢园竹林的聚会，在座的文人雅士，豪爽善饮的气概超过了陶渊明。睢园：西汉梁孝王在睢水旁修建的竹园，他常和一些文人在此饮酒赋诗。 邺（yè）水朱华，光照临川之笔：这是借诗人曹植、谢灵运来比拟参加宴会的文人。邺：今河北临漳，是曹魏兴起的地方。曹植曾在这里作过《公宴诗》，诗中有“朱华冒绿池”的句子。临川之笔：指谢灵运，他曾任临川（今属江西）内史。 四美：指良辰、美景、赏心、乐事。 二难：贤主、嘉宾。 地势极而南溟深，天柱高而北辰远：地势偏远，南海深邃；天柱高耸，北极星远悬。 帝阍（hūn）：原指天帝的守门者。这里指皇帝的宫门。 奉宣室以何年：什么时候才能像贾谊那样去侍奉君王呢]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识6]]></title>
    <url>%2F2018%2F09%2F02%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%866%2F</url>
    <content type="text"><![CDATA[1️⃣[dropout]dropout形式:RNN的形式有多种： recurrent dropoutRNN: $h_t=f(W_h ⊙ [x_t,h_{t-1}]+b_h)$加上dropout的RNN：$h_t=f(W_h ⊙ [x_t,d(h_{t-1})]+b_h)$，其中$d(\cdot)$为dropout函数同理：LSTM:$c_t=f_t ⊙c_{t-1} + i_t ⊙ d(g_t)$GRU:$h_t=(1-z_t)⊙c_{t-1}+z_t⊙d(g_t)$ 垂直连接的dropoutdropout的作用即是否允许L层某个LSTM单元的隐状态信息流入L+1层对应单元。 Reference:https://blog.csdn.net/falianghuang/article/details/72910161 2️⃣[Pytorch]pack_padded_sequence用于RNN中，将padding矩阵压缩:这样就可以实现在RNN传输过程中短句提前结束。 pad_packed_sequence是pack_padded_sequence的逆运算。]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Pytorch</tag>
        <tag>dropout</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我没有说话]]></title>
    <url>%2F2018%2F08%2F29%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%88%91%E6%B2%A1%E6%9C%89%E8%AF%B4%E8%AF%9D%2F</url>
    <content type="text"><![CDATA[《我没有说话》 纳粹杀共产党时，我没有出声——因为我不是共产党员；接着他们迫害犹太人，我没有出声——因为我不是犹太人；然后他们杀工会成员，我没有出声——因为我不是工会成员；后来他们迫害天主教徒，我没有出声——因为我是新教徒；最后当他们开始对付我的时候，已经没有人能站出来为我发声了]]></content>
      <tags>
        <tag>佳句分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning NLP best practices笔记]]></title>
    <url>%2F2018%2F08%2F26%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FDeep%20Learning%20NLP%20best%20practices%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[博客地址：http://ruder.io/deep-learning-nlp-best-practices/index.html个人觉得这篇文章写得很好，有许多实践得到的经验，通过这篇可以避免走一些弯路。 PracticesWord Embedding The optimal dimensionality of word embeddings is mostly task-dependent: a smaller dimensionality works better for more syntactic tasks such as named entity recognition or part-of-speech (POS) tagging, while a larger dimensionality is more useful for more semantic tasks such as sentiment analysis. 对于偏向语法的，使用维度低一些的词向量；而对于偏向语义内容的，使用维度大一些的词向量，如情感分析。 LSTM Depth performance improvements of making the model deeper than 2 layers are minimal LSTM深度最好不要超过两层。 Optimization It is often thought that Adam clearly outperforms vanilla stochastic gradient descent (SGD). However, while it converges much faster than SGD, it has been observed that SGD with learning rate annealing slightly outperforms Adam. Recent work furthermore shows that SGD with properly tuned momentum outperforms Adam . Adam可以更早拟合，而SGD效果可能会更好一些。 可以采用优化策略，比如说使用Adam训练直到拟合，然后将学习率减半，并重新导入之前训练好的最好的模型。这样Adam能够忘记之前的信息并重新开始训练。 Denkowski &amp; Neubig (2017) show that Adam with 2 restarts and learning rate annealing is faster and performs better than SGD with annealing Ensembling Combining multiple models into an ensemble by averaging their predictions is a proven strategy to improve model performance. Ensembling很重要的一点是需要保证多样性： Ensembling is an important way to ensure that results are still reliable if the diversity of the evaluated models increases (Denkowski &amp; Neubig, 2017). While ensembling different checkpoints of a model has been shown to be effective (Jean et al., 2015; Sennrich et al., 2016) [51, 52], it comes at the cost of model diversity. Cyclical learning rates can help to mitigate this effect LSTM tricks 在initial state中我们常常使用全0向量，实际上可以将其作为参数学习。 Instead of fixing the initial state, we can learn it like any other parameter, which can improve performance 将input和output embedding的参数共享，如果是做language model或者机器翻译之类的，可以让他们共享。 Gradient Norm Clipping Rather than clipping each gradient independently, clipping the global norm of the gradient yields more significant improvements 这点我没看懂。 Classification practices关于CNN CNN filters:Combining filter sizes near the optimal filter size, e.g. (3,4,5) performs best (Kim, 2014; Kim et al., 2016). The optimal number of feature maps is in the range of 50-600 (Zhang &amp; Wallace, 2015) [59]. Aggregation function:1-max-pooling outperforms average-pooling and k-max pooling (Zhang &amp; Wallace, 2015). 这在我之前的关于CNN文本分类指南中有更详尽的分析。 Conclusion这是一篇干货满满的博客，实际上我还是有许多地方没有读懂，这适合多看几遍，慢慢理解。]]></content>
      <tags>
        <tag>指南</tag>
        <tag>调参</tag>
        <tag>NLP🤖</tag>
        <tag>笔记📒</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识5]]></title>
    <url>%2F2018%2F08%2F26%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%865%2F</url>
    <content type="text"><![CDATA[1️⃣[Paper]Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components 基本框架和CBOW一致，主要贡献在于针对中文词向量添加了偏旁、字的组件作为训练信息。 2️⃣[Paper]Highway Networks 为了解决神经网络深度过深时导致的反向传播困难的问题。前向传播的公式： y=H(x,W_H)而论文所做的改进： y=H(x,W_H) \cdot T(x,W_T)+ x \cdot C(x,W_C)其中$T$是transform gate，$C$是carry gate。方便起见，可以将 $C=1-T$，最终有： y=H(x,W_H) \cdot T(x,W_T)+ x \cdot (1-T(x,W_T))可以看出思想和LSTM很类似，都是gate的思想。 3️⃣[调参方法]博客：https://blog.goodaudience.com/how-to-make-your-model-happy-again-part-1-40d94a9ffb41 学习率： 一条原则：当validation loss开始上升时，减少学习率。 如何减少？ 或者： 设定一定的epoch作为一个stepsize，在训练过程中线性增加学习率，然后在到达最大值后再线性减小。实验表明，使用该方法可以在一半的epoch内达到相同的效果。 batch size： 由于batch size和学习率的强相关性，相关论文提出提高batch size而不是降低学习率的方法来提升模型表现。 increasing the batch size during training, instead of decaying learning rate. — L. Smithhttps://arxiv.org/pdf/1711.00489.pdf 一个trick：保持学习率不变，提高batch size，直到batch size~训练集/10，接下来再采用学习率下降的策略。]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Paper</tag>
        <tag>调参方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录6]]></title>
    <url>%2F2018%2F08%2F26%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB6%2F</url>
    <content type="text"><![CDATA[1️⃣将数据整理成batch123456789101112131415161718192021def _iter_batch(paras,labels,batch_size,shuffle=True): ''' :param paras: :param labels: :param batch_size: :param shuffle: :return: ''' assert len(paras)==len(labels) paras_size=len(paras) if shuffle: indices=np.arange(paras_size) np.random.shuffle(indices) for start_idx in range(0,paras_size-batch_size+1,batch_size): if shuffle: excerpt=indices[start_idx:start_idx+batch_size] else: excerpt=slice(start_idx,start_idx+batch_size) yield paras[excerpt],labels[excerpt]]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词6]]></title>
    <url>%2F2018%2F08%2F26%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D6%2F</url>
    <content type="text"><![CDATA[1️⃣ 戏为六绝句[唐] 杜甫【其二】王杨卢骆当时体，轻薄为文哂未休。尔曹身与名俱灭，不废江河万古流。 哂（shěn）：讥笑。 http://m.xichuangzhu.com/work/57b9658c0a2b58005c95d2a7]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN文本分类任务指南]]></title>
    <url>%2F2018%2F08%2F25%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FCNN%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[最近因为比赛的缘故对文本分类有一定的了解。其中使用CNN方法做情感分析任务存在着许多优势。虽然模型简单，但如何设置超参有时候对结果有很大的影响。本文记录了关于CNN文本分类的一些学习历程和指南，基本参考了论文。 做法基本上目前较为浅层的CNN文本分类的做法都是如下图： 将词向量堆积成为二维的矩阵，通过CNN的卷积单元对矩阵进行卷积处理，同时使用pooling（通常是1max-pooling）操作，将不等长的卷积结果变为等长，对不同的卷积单元的结果进行拼接后生成单个向量，最后再通过线性层转化成类别概率分布。 另一张图也说明了该流程。 建议与指导超参及其对结果的影响接下来的内容参考了论文A Sensitivity Analysis of (and Practitioners’ Guide to) ConvolutionalNeural Networks for Sentence Classification CNN文本分类的超参： 输入向量 卷积大小 输出通道（feature maps） 激活函数 池化策略 正则化 输入向量的影响实验表明，使用word2vec和GloVe不分伯仲，但将word2vec和GloVe简单拼接在一起并不能带来提升。 unfortunately, simply concatenating these representations does necessarily seem helpful 当句子长度很长（document classification）时，使用one-hot可能会有效果，但在句子长度不是很长时，效果不好。 建议对于新任务，可以word2vec或GloVe或者其他词向量都试一下，如果句子长，可以试着使用one-hot。 卷积大小由于卷积的长度是固定的，也就是词向量的长度，因此只需讨论宽度。实验表明，不同的数据集会有不同的最佳大小，但似乎对于长度越长的句子，最佳大小有越大的趋势。 However, for datasets comprising longer sentences, such as CR (maximum sentence length is 105, whereas it ranges from 36-56 on the other sentiment datasets used here), the optimal region size may be larger. 同时，当增加不同卷积大小作为组合时，如果组合的卷积核大小接近于最佳大小（optimal region size），有助于结果的提升；相反，如果卷积核大小离最佳大小很远时，反而会产生负面影响。 建议首先试着找到最优的卷积核大小，然后在这个基础上添加和该卷积核大小类似的卷积核。 feature maps也就是输出通道（out channel），表明该卷积核大小的卷积核有多少个。 实验表明，最佳的feature maps和数据集相关，但一般不超过600。 it would seem that increasing the number of maps beyond 600 yields at best very marginal returns, and often hurts performance. 建议在600内搜索最优，如果在600的边缘还没有明显的效果下降，那么可以尝试大于600的feature maps。 激活函数实验结果： 结果表明，tanh、ReLU和不使用激活函数效果较好。tanh的优点是以0为中心，ReLU能够加速拟合，至于为什么不使用的效果会好，可能是因为模型较为简单： This indicates that on some datasets, a linear transformation is enough to capture thecorrelation between the word embedding and the output label. 建议使用tanh、ReLU或者干脆不使用。但如果模型更为复杂，有多层的结构，还是需要使用激活函数的。 pooling策略所有的实验都表明了，1-max pooling的效果比其他好，如k-max pooling。在pooling这一步可以直接选择1-max pooling。 This may be because the location of predictive contexts does not matter, and certain n-grams in the sentence can be more predictive on their own than the entire sentence considered jointly. 正则化主要是dropout和l2 norm constraint。dropout就是随机将一些神经元置为0，l2 norm constraint是对参数矩阵W进行整体缩放，使其不超过一定阈值。（与通常的l2 regularization不同，最早可追溯到Hinton的Improving neural networks by preventingco-adaptation of feature detectors） the l2 norm of a weight vector is linearly scaled to a constraint c when it exceeds this threshold, so a smaller c implies stronger regularization 实验表明，dropout起的作用很小，l2 norm没有提升甚至还会导致下降。可能是因为模型参数不多，因此过拟合的可能性较低。 建议设置较小的dropout和较大的l2 norm，当feature maps增大时，可以试着调节较大的dropout以避免过拟合。 建议及结论 刚开始的使用使用word2vec或者GloVe，如果数据量够大，可以尝试one-hot 线性搜索最佳的卷积核大小，如果句子够长，那么可以扩大搜索范围。一旦确定了最佳卷积核大小，尝试在该卷积核大小的附近进行组合，如最佳卷积核宽度是5，那么尝试[3,4,5]或者[2,3,4,5]等 使用较小的dropout和较大的max norm constraint，然后在[100,600]范围内搜索feature maps，如果最佳的feature maps在600附近，可以试着选择比600更大的范围 尝试不同的激活函数，通常tanh和ReLU是较好的，但也可以尝试什么都不加。 使用1-max pooling。 如果模型复杂，比如feature maps很大，那么可以尝试更为严格的正则化，如更大的dropout rate和较小的max norm constraint。 ReferenceConvolutional Neural Networks for Sentence Classification A Sensitivity Analysis of (and Practitioners’ Guide to) ConvolutionalNeural Networks for Sentence Classification]]></content>
      <tags>
        <tag>CNN</tag>
        <tag>情感分析</tag>
        <tag>指南</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Pytorch中的inplace的操作]]></title>
    <url>%2F2018%2F08%2F20%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADin-place%E7%9A%84%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[最近在写Hierarchical attention network的时候遇到了如下的bug： one of the variables needed for gradient computation has been modified by an inplace operation 在查阅了文档和请教了其他人之后，最终找到了bug。 1234for i in range(seq_len): h_i = rnn_outputs[i] # batch,hidden*2 a_i = attn_weights[i].unsqueeze_(1) # take in-place opt may cause an error a_i = a_i.expand_as(h_i) # batch,hidden*2 这是我原来的逻辑，我在无意中做了inplace操作，导致了bug的发生。正确的做法应该是这样的： 12345for i in range(seq_len): h_i = rnn_outputs[i] # batch,hidden*2 # a_i = attn_weights[i].unsqueeze_(1) # take in-place opt may cause an error a_i = attn_weights[i].unsqueeze(1) # batch,1 a_i = a_i.expand_as(h_i) # batch,hidden*2 实际上，在实践过程中应当尽量避免inplace操作，在官方文档中也提到了（存疑）这点，虽然提供了inplace操作，但并不推荐使用。 具体的原因是，在Pytorch构建计算图的过程中，会记录每个节点是怎么来的，但inplace会破坏这种关系，使得在回传的时候没法正常求导。 特别地，有两种情况不应该使用inplace操作（摘自知乎）： 对于requires_grad=True的叶子张量(leaf tensor)不能使用inplace operation 对于在求梯度阶段需要用到的张量不能使用inplace operation Reference:https://zhuanlan.zhihu.com/p/38475183]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>遇到的问题</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[愿中国青年都摆脱冷气]]></title>
    <url>%2F2018%2F08%2F19%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%84%BF%E4%B8%AD%E5%9B%BD%E9%9D%92%E5%B9%B4%E9%83%BD%E6%91%86%E8%84%B1%E5%86%B7%E6%B0%94%2F</url>
    <content type="text"><![CDATA[近期的新闻常让人感到愤怒以致绝望… 愿中国青年都摆脱冷气，只是向上走，不必听自暴自弃者流的话。能做事的做事，能发声的发声。有一分热，发一分光。就令萤火一般，也可以在黑暗里发一点光，不必等候炬火。 —鲁迅《热风》]]></content>
      <tags>
        <tag>佳句分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录5]]></title>
    <url>%2F2018%2F08%2F19%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB5%2F</url>
    <content type="text"><![CDATA[1️⃣sklearn模型的保存与恢复123456789from sklearn import svmX = [[0, 0], [1, 1]]y = [0, 1]clf = svm.SVC()clf.fit(X, y) clf.fit(train_X,train_y)joblib.dump(clf, "train_model.m")clf = joblib.load("train_model.m")clf.predit(test_X) 2️⃣Dictionary类在构造字典时需要用到1234567891011121314151617181920212223242526class Dictionary(): def __init__(self): self.word2idx = &#123;&#125; self.idx2word = [] self.__vocab_size = 0 self.add_word('&lt;pad&gt;') self.add_word('&lt;UNK&gt;') def add_word(self, word): if word not in self.word2idx: self.idx2word.append(word) self.word2idx[word] = self.__vocab_size self.__vocab_size += 1 def __len__(self): return self.__vocab_size def get_index(self, word): if word in self.word2idx: return self.word2idx[word] else: return self.word2idx['&lt;UNK&gt;'] def get_word(self, idx): return self.idx2word[idx] 3️⃣对dict按元素排序的三种方法12345678910111213d=&#123;'apple':10,'orange':20,'banana':5,'watermelon':1&#125;#法1print(sorted(d.items(),key=lambda x:x[1])) #[('watermelon', 1), ('banana', 5), ('apple', 10), ('orange', 20)]#法2from operator import itemgetterprint(sorted(d.items(),key=itemgetter(1))) #[('watermelon', 1), ('banana', 5), ('apple', 10), ('orange', 20)]#法3print(sorted(d,key=d.get)) #['watermelon', 'banana', 'apple', 'orange'] 没有value了 4️⃣合并dict的三种方法1234567891011121314151617&gt;&gt;&gt; d1=&#123;'a':1&#125;&gt;&gt;&gt; d2=&#123;'b':2&#125;#法1&gt;&gt;&gt; d=&#123;**d1,**d2&#125;&gt;&gt;&gt; d&#123;'a': 1, 'b': 2&#125;#法2&gt;&gt;&gt; dd=dict(d1.items()|d2.items())&gt;&gt;&gt; dd&#123;'a': 1, 'b': 2&#125;#法3&gt;&gt;&gt; d1.update(d2)&gt;&gt;&gt; d1&#123;'a': 1, 'b': 2&#125; 5️⃣找到list最大最小值的index12345678lst = [40, 10, 20, 30]def minIndex(lst): return min(range(len(lst)),key=lst.__getitem__)def maxIndex(lst): return max(range(len(lst)),key=lst.__getitem__) print(minIndex(lst))print(maxIndex(lst))]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Pytorch中的Embedding padding]]></title>
    <url>%2F2018%2F08%2F19%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADEmbedding%E7%9A%84padding%2F</url>
    <content type="text"><![CDATA[在Pytorch中，nn.Embedding()代表embedding矩阵，其中有一个参数padding_idx指定用以padding的索引位置。所谓padding，就是在将不等长的句子组成一个batch时，对那些空缺的位置补0，以形成一个统一的矩阵。 用法：1self.embedding = nn.Embedding(vocab_size, embed_dim,padding_idx=0) #也可以是别的数值 在显式设定padding_idx=0后，在自定义的词典内也应当在相应位置添加&lt;pad&gt;作为一个词。如： 1234567class Dictionary(): def __init__(self): self.word2idx = &#123;&#125; self.idx2word = [] self.__vocab_size = 0 self.add_word('&lt;pad&gt;') # should add &lt;pad&gt; first self.add_word('&lt;UNK&gt;') 那么对于padding_idx，内部是如何操作的呢？ 在查看了Embedding的源码后，发现设置了padding_idx，类内部会有如下操作： 12345678910#-----Embedding __init__ 内部--------------if _weight is None: self.weight = Parameter(torch.Tensor(num_embeddings, embedding_dim)) self.reset_parameters() #---------reset_parameters()--------def reset_parameters(self): self.weight.data.normal_(0, 1) if self.padding_idx is not None: self.weight.data[self.padding_idx].fill_(0) 也就是说，当Embedding是随机初始化的矩阵时，会对padding_idx所在的行进行填0。保证了padding行为的正确性。 那么，还需要保证一个问题，就是在反向回传的时候，padding_idx是不会更新的. 在查看了源码后发现在Embedding类内有如下注释： .. note:: With :attr:padding_idx set, the embedding vector at :attr:padding_idx is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from :class:~torch.nn.Embedding is always zero. 并且在查阅了其他资料后，发现该行确实会不更新。有意思的是，查阅源码并没有找到如何使其不更新的机制，因为在F.embedding函数中，返回： 1return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) 但我并不能跳转到torch.embedding中，大概是因为这部分被隐藏了吧。我也没有再深究下去。我猜测有可能是在autograd内部有对该部分进行单独的处理，用mask屏蔽这部分的更新；或者一个更简单的方法，就是任其更新，但每一次都reset，将第一行手动设为全0。 附记： 假如说没有显式设置该行，是否padding就没有效果呢？我认为是的。 一般来说，我们都是以0作为padding的填充，如： 12 44 22 67 85 12 13 534 31 0 87 23 0 0 0 每一行代表一个句子，其中0作为填充。然后将该矩阵送入到embedding_lookup中，获得三维的tensor，那么0填充的部分，所获得的embedding表示应当是要全0。 假如不显式设置padding_idx=0，就可能会出现两个结果（个人推测)： ①本应该全0的地方，被词典中第一个词的词向量表示给替代了，因为将0作为索引去embedding矩阵获取到的词向量，就是第一个词的词向量，而该词并不全0。 ②词典的最后一个词被全0覆盖。F.embedding中有如下片段： 12345678if padding_idx is not None: if padding_idx &gt; 0: assert padding_idx &lt; weight.size(0), 'Padding_idx must be within num_embeddings' elif padding_idx &lt; 0: assert padding_idx &gt;= -weight.size(0), 'Padding_idx must be within num_embeddings' padding_idx = weight.size(0) + padding_idxelif padding_idx is None: padding_idx = -1 上面片段显示，padding_idx被设置为-1，也就是最后一个单词。做完这步紧接着就返回： 1return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) 还是由于torch.embedding无法查看的原因，我不知道内部是如何实现的，但应该来说，最后一个词就是被覆盖了。]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Pytorch</tag>
        <tag>Embedding</tag>
        <tag>padding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Tricks[转]]]></title>
    <url>%2F2018%2F08%2F19%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPython%20Tricks%5B%E8%BD%AC%5D%2F</url>
    <content type="text"><![CDATA[原文地址:https://hackernoon.com/python-tricks-101-2836251922e0 我觉得这个介绍Python一些tricks的文章很好，能够更加熟悉Python的一些非常方便的用法。以下是我觉得有用的几个点。 1️⃣Reverse a String/List [::-1]解释：[:]表示取所有的元素，-1表示步进。[1:5:2]表示的就是从元素1到元素5，每2个距离取一个。 2️⃣transpose 2d array zip()相当于压缩，zip(*)相当于解压。 3️⃣Chained function call 非常简洁的写法。 4️⃣Copy List 之前谈过的Python的赋值、浅拷贝、深拷贝。 5️⃣Dictionary get 避免了dict不存在该元素的问题。 6️⃣✨Sort Dictionary by Value 其中第三种返回的是[‘watermelon’, ‘banana’, ‘apple’, ‘orange’]，没有value了。 7️⃣For…else 注意到如果for在中途break了，就不会进入到else了；只有顺利循环完才会进入到else。 1234567891011121314151617&gt;&gt;&gt; a=[1,2,0]&gt;&gt;&gt; for e in a:... if e==0:... break... else:... print('hello')... #什么都没有print&gt;&gt;&gt; for e in a:... print(e)... else:... print('hello')... 120hello 8️⃣Merge dict’s 合并dict的方法。 9️⃣Min and Max index in List]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Python</tag>
        <tag>Python tricks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识4]]></title>
    <url>%2F2018%2F08%2F19%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%864%2F</url>
    <content type="text"><![CDATA[1️⃣[概率校准(Probability Calibration)]一种对机器学习算法输出结果的校准，通过几个实验可以发现，概率校准能够一定程度提高表现。几个参考资料：直观理解: http://www.bubuko.com/infodetail-2133893.htmlSVC的概率校准在sklearn上的应用: https://blog.csdn.net/ericcchen/article/details/79337716✨完全手册: Calibration of Machine Learning Models 2️⃣[Paper]Hierarchical Attention Networks for Document Classification 亮点在使用层次的RNN结构，以及使用了attention方法。 参考了其他人的代码自己也试着实现了一个，GitHub地址：https://github.com/linzehui/pytorch-hierarchical-attention-network 3️⃣[XGBoost]kaggle神器XGBoost，一篇原理的详细介绍：http://www.cnblogs.com/willnote/p/6801496.html虽然还是有好些地方没搞懂，有必要从头学起。 4️⃣[Python]关于函数列表中单星号(*)和双星号(**)单星号： 代表接收任意多个非关键字参数，将其转换成元组： 1234def one(a,*b): """a是一个普通传入参数，*b是一个非关键字星号参数""" print(b)one(1,2,3,4,5,6) #输出：(2, 3, 4, 5, 6) 对一个普通变量使用单星号，表示对该变量拆分成单个元素 1234def fun(a,b): print(a,b)l=[1,2]fun(*l) #输出 1,2 双星号： 获得字典值 1234def two(a=1,**b): """a是一个普通关键字参数，**b是一个关键字双星号参数""" print(b)two(a=1,b=2,c=3,d=4,e=5,f=6) #输出&#123;'b': 2, 'c': 3, 'e': 5, 'f': 6, 'd': 4&#125; 5️⃣[Pytorch]在Pytorch中，只要一个tensor的requires_grad是true，那么两个tensor的加减乘除后的结果的requires_grad也会是true。]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Python</tag>
        <tag>Pytorch</tag>
        <tag>Paper</tag>
        <tag>概率校准</tag>
        <tag>Probability Calibration</tag>
        <tag>HAN</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词5]]></title>
    <url>%2F2018%2F08%2F19%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D5%2F</url>
    <content type="text"><![CDATA[本周太忙了，没背什么诗词，只背（复习）了部分的《滕王阁序》。 1️⃣ 滕王阁序嗟乎！时运不齐，命途多舛。冯唐易老，李广难封。屈贾谊于长沙，非无圣主；窜梁鸿于海曲，岂乏明时？所赖君子见机，达人知命。老当益壮，宁移白首之心？穷且益坚，不坠青云之志。酌贪泉而觉爽，处涸辙以犹欢。北海虽赊，扶摇可接；东隅已逝，桑榆非晚。孟尝高洁，空馀报国之情；阮籍猖狂，岂效穷途之哭！ 勃，三尺微命，一介书生。无路请缨，等终军之弱冠；有怀投笔，慕宗慤之长风。舍簪笏于百龄，奉晨昏于万里。非谢家之宝树，接孟氏之芳邻。他日趋庭，叨陪鲤对；今兹捧袂，喜托龙门。杨意不逢，抚凌云而自惜；锺期既遇，奏流水以何惭？ 注释：冯唐：西汉人，有才能却一直不受重用。汉武帝时选求贤良，有人举荐冯唐，可是他已九十多岁，难再做官了。李广：汉武帝时的名将，多年抗击匈奴，军功很大，却终身没有封侯。 贾谊：汉文帝本想任贾谊为公卿，但因朝中权贵反对，就疏远了贾谊，任他为长沙王太傅。梁鸿：东汉人，因作诗讽刺君王，得罪了汉章帝，被迫逃到齐鲁一带躲避。 酌（zhuó）贪泉而觉爽：喝下贪泉的水，仍觉得心境清爽。古代传说广州有水名贪泉，人喝了这里的水就会变得贪婪。这句是说有德行的人在污浊的环境中也能保持纯正，不被污染。处涸辙以犹欢：处在奄奄待毙的时候，仍然乐观开朗。处河辙：原指鲋鱼处在干涸的车辙旦。比喻人陷入危急之中。 孟尝：东汉人，为官清正贤能，但不被重用，后来归田。阮籍：三国魏诗人，他有时独自驾车出行，到无路处便恸哭而返，借此宣泄不满于现实的苦闷心情。 终军：《汉书·终军传》记载，汉武帝想让南越（今广东、广西一带）王归顺，派终军前往劝说，终军请求给他长缨，必缚住南越王，带回到皇宫门前（意思是一定完成使命）。后来用“请缨”指投军报国。 宗悫（què）：南朝宋人，少年时很有抱负，说“愿乘长风破万里浪”。 簪（zān）笏（hù）：这里代指官职。晨昏：晨昏定省，出自 《礼记·曲礼上》，释义为旧时侍奉父母的日常礼节。 非谢家之宝树，接孟氏之芳邻：自己并不是像谢玄那样出色的人才，却能在今日的宴会上结识各位名士。谢家之宝树：指谢玄。《晋书·谢玄传》记载，晋朝谢安曾问子侄们：为什么人们总希望自己的子弟好？侄子谢玄回答：“譬如芝兰玉树，欲使其生于庭阶耳。”后来就称谢玄为谢家宝树。孟氏之芳邻：这里借孟子的母亲为寻找邻居而三次搬家的故事，来指赴宴的嘉宾。 他日趋庭，叨陪鲤对：过些时候自己将到父亲那里陪侍和聆听教诲。趋庭：快步走过庭院，这是表示对长辈的恭敬。叨：惭愧地承受，表示自谦。鲤对：孔鲤是孔子的儿子，鲤对指接受父亲教诲。事见《论语·季氏》：（孔子）尝独立，（孔）鲤趋而过庭。（子）曰：“学诗乎？”对曰：“未也。”“不学诗，无以言。”鲤退而学诗。他日，又独立，鲤趋而过庭。（子）曰：“学礼乎？”对曰：‘未也。”“不学礼，无以立。”鲤退而学礼。 捧袂（mèi）：举起双袖作揖，指谒见阎公。喜托龙门：（受到阎公的接待）十分高兴，好像登上龙门一样。 杨意：即蜀人杨得意，任掌管天子猎犬的官，西汉辞赋家司马相如是由他推荐给汉武帝的。凌云：这里指司马相如的赋，《史记·司马相如传》说，相如献《大人赋》，“天子大悦，飘飘有凌云之气，似游天地之间”。钟期：即钟子期。]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的拷贝]]></title>
    <url>%2F2018%2F08%2F18%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPython%E4%B8%AD%E7%9A%84%E6%8B%B7%E8%B4%9D%2F</url>
    <content type="text"><![CDATA[Python的拷贝和C/C++的差别很大，很经常就容易搞混，因此记录一下。 赋值、拷贝 赋值：实际上就是对象的引用，没有开辟新的内存空间12lst=[1,2,3]l=lst 浅拷贝:创建了新对象，但是内容是对原对象的引用，有三种形式 切片 12l=lst[:]l=[i for i in lst] 工厂 1l=list(lst) copy 12import copyl=copy.copy(lst) 深拷贝:copy中的deepcopy，生成一个全新的对象，与原来的对象无关 12import copyl=copy.deepcopy(lst) 例子12345678910111213141516171819202122232425262728293031### 引用https://www.cnblogs.com/huangbiquan/p/7795152.html 的例子###&gt;&gt;&gt; import copy&gt;&gt;&gt; a = [1,2,3,4,['a','b']] #定义一个列表a&gt;&gt;&gt; b = a #赋值&gt;&gt;&gt; c = copy.copy(a) #浅拷贝&gt;&gt;&gt; d = copy.deepcopy(a) #深拷贝&gt;&gt;&gt; a.append(5)&gt;&gt;&gt; print(a)[1, 2, 3, 4, ['a', 'b'], 5] #a添加一个元素5&gt;&gt;&gt; print(b) [1, 2, 3, 4, ['a', 'b'], 5] #b跟着添加一个元素5 &gt;&gt;&gt; print(c)[1, 2, 3, 4, ['a', 'b']] #c保持不变&gt;&gt;&gt; print(d)[1, 2, 3, 4, ['a', 'b']] #d保持不变&gt;&gt;&gt; a[4].append('c') &gt;&gt;&gt; print(a)[1, 2, 3, 4, ['a', 'b', 'c'], 5] #a中的list(即a[4])添加一个元素c&gt;&gt;&gt; print(b)[1, 2, 3, 4, ['a', 'b', 'c'], 5] #b跟着添加一个元素c&gt;&gt;&gt; print(c)[1, 2, 3, 4, ['a', 'b', 'c']] #c跟着添加一个元素c&gt;&gt;&gt; print(d)[1, 2, 3, 4, ['a', 'b']] #d保持不变#说明如下：#1.外层添加元素时， 浅拷贝c不会随原列表a变化而变化；内层list添加元素时，浅拷贝c才会变化。#2.无论原列表a如何变化，深拷贝d都保持不变。#3.赋值对象随着原列表一起变化 Referencehttps://www.cnblogs.com/huangbiquan/p/7795152.htmlhttps://www.cnblogs.com/xueli/p/4952063.html]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Python</tag>
        <tag>拷贝</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何将ELMo词向量用于中文]]></title>
    <url>%2F2018%2F08%2F12%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%A6%82%E4%BD%95%E5%B0%86ELMo%E8%AF%8D%E5%90%91%E9%87%8F%E7%94%A8%E4%BA%8E%E4%B8%AD%E6%96%87%2F</url>
    <content type="text"><![CDATA[10.10更新：ELMo已经由哈工大组用PyTorch重写了，并且提供了中文的预训练好的language model，可以直接使用。 ELMo于今年二月由AllenNLP提出，与word2vec或GloVe不同的是其动态词向量的思想，其本质即通过训练language model，对于一句话进入到language model获得不同的词向量。根据实验可得，使用了Elmo词向量之后，许多NLP任务都有了大幅的提高。 论文:Deep contextualized word representations AllenNLP一共release了两份ELMo的代码，一份是Pytorch版本的，另一份是Tensorflow版本的。Pytorch版本的只开放了使用预训练好的词向量的接口，但没有给出自己训练的接口，因此无法使用到中文语料中。Tensorflow版本有提供训练的代码，因此本文记录如何将ELMo用于中文语料中，但本文只记录使用到的部分，而不会分析全部的代码。 需求:使用预训练好的词向量作为句子表示直接传入到RNN中(也就是不使用代码中默认的先过CNN)，在训练完后，将模型保存，在需要用的时候load进来，对于一个特定的句子，首先将其转换成预训练的词向量，传入language model之后最终得到ELMo词向量。 准备工作: 将中文语料分词 训练好GloVe词向量或者word2vec 下载bilm-tf代码 生成词表 vocab_file （训练的时候要用到） optional:阅读Readme optional:通读bilm-tf的代码，对代码结构有一定的认识 思路: 将预训练的词向量读入 修改bilm-tf代码 option部分 添加给embedding weight赋初值 添加保存embedding weight的代码 开始训练，获得checkpoint和option文件 运行脚本，获得language model的weight文件 将embedding weight保存为hdf5文件形式 运行脚本，将语料转化成ELMo embedding。 训练GloVe或word2vec可参见我以前的博客或者网上的教程。注意到，如果要用gensim导入GloVe训好的词向量，需要在开头添加num_word embedding_dim。 如： 获得vocab词表文件注意到，词表文件的开头必须要有&lt;S&gt; &lt;/S&gt; &lt;UNK&gt;，且大小写敏感。并且应当按照单词的词频降序排列。可以通过手动添加这三个特殊符号。如： 代码：1234567891011121314model=gensim.models.KeyedVectors.load_word2vec_format( fname='/home/zhlin/GloVe/vectors.txt',binary=False)words=model.vocabwith open('vocab.txt','w') as f: f.write('&lt;S&gt;') f.write('\n'） f.write('&lt;/S&gt;') f.write('\n') f.write('&lt;UNK&gt;') f.write('\n') # bilm-tf 要求vocab有这三个符号，并且在最前面 for word in words: f.write(word) f.write('\n') 修改bilm-tf代码注意到，在使用该代码之前，需要安装好相应的环境。 如果使用的是conda作为默认的Python解释器，强烈建议使用conda安装，否则可能会出现一些莫名的错误。123conda install tensorflow-gpu=1.4conda install h5pypython setup.py install #应在bilm-tf的文件夹下执行该指令 然后再运行测试代码，通过说明安装成功。 修改train_elmo.pybin文件夹下的train_elmo.py是程序的入口。主要修改的地方： load_vocab的第二个参数应该改为None n_gpus CUDA_VISIBLE_DEVICES 根据自己需求改 n_train_tokens 可改可不改，影响的是输出信息。要查看自己语料的行数，可以通过wc -l corpus.txt 查看。 option的修改，将char_cnn部分都注释掉，其他根据自己需求修改 如： 修改LanguageModel类由于我需要传入预训练好的GloVe embedding，那么还需要修改embedding部分，这部分在bilm文件夹下的training.py，进入到LanguageModel类中_build_word_embeddings函数中。注意到，由于前三个是&lt;S&gt; &lt;/S&gt; &lt;UNK&gt;，而这三个字符在GloVe里面是没有的，因此这三个字符的embedding应当在训练的时候逐渐学习到，而正因此 embedding_weights的trainable应当设为True 如: 修改train函数添加代码，使得在train函数的最后保存embedding文件。 训练并获得weights文件训练需要语料文件corpus.txt，词表文件vocab.txt。 训练cd到bilm-tf文件夹下，运行12345export CUDA_VISIBLE_DEVICES=4nohup python -u bin/train_elmo.py \--train_prefix='/home/zhlin/bilm-tf/corpus.txt' \--vocab_file /home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab.txt \--save_dir /home/zhlin/bilm-tf/try &gt;bilm_out.txt 2&gt;&amp;1 &amp; 根据实际情况设定不同的值和路径。 运行情况： PS:运行过程中可能会有warning: ‘list’ object has no attribute ‘name’WARNING:tensorflow:Error encountered when serializing lstm_output_embeddings.Type is unsupported, or the types of the items don’t match field type in CollectionDef. 应该不用担心，还是能够继续运行的，后面也不受影响。 在等待了相当长的时间后，在save_dir文件夹内生成了几个文件，其中checkpoint和options是关键，checkpoint能够进一步生成language model的weights文件，而options记录language model的参数。 获得language model的weights接下来运行bin/dump_weights.py将checkpoint转换成hdf5文件。 123nohup python -u /home/zhlin/bilm-tf/bin/dump_weights.py \--save_dir /home/zhlin/bilm-tf/try \--outfile /home/zhlin/bilm-tf/try/weights.hdf5 &gt;outfile.txt 2&gt;&amp;1 &amp; 其中save_dir是checkpoint和option文件保存的地址。 接下来等待程序运行： 最终获得了想要的weights和option： 将语料转化成ELMo embedding由于我们有了vocab_file、与vocab_file一一对应的embedding h5py文件、以及language model的weights.hdf5和options.json。接下来参考usage_token.py将一句话转化成ELMo embedding。 参考代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import tensorflow as tfimport osfrom bilm import TokenBatcher, BidirectionalLanguageModel, weight_layers, \ dump_token_embeddings# Our small dataset.raw_context = [ '这 是 测试 .', '好的 .']tokenized_context = [sentence.split() for sentence in raw_context]tokenized_question = [ ['这', '是', '什么'],]vocab_file='/home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab.txt'options_file='/home/zhlin/bilm-tf/try/options.json'weight_file='/home/zhlin/bilm-tf/try/weights.hdf5'token_embedding_file='/home/zhlin/bilm-tf/glove_embedding_vocab8.10/vocab_embedding.hdf5'## Now we can do inference.# Create a TokenBatcher to map text to token ids.batcher = TokenBatcher(vocab_file)# Input placeholders to the biLM.context_token_ids = tf.placeholder('int32', shape=(None, None))question_token_ids = tf.placeholder('int32', shape=(None, None))# Build the biLM graph.bilm = BidirectionalLanguageModel( options_file, weight_file, use_character_inputs=False, embedding_weight_file=token_embedding_file)# Get ops to compute the LM embeddings.context_embeddings_op = bilm(context_token_ids)question_embeddings_op = bilm(question_token_ids)elmo_context_input = weight_layers('input', context_embeddings_op, l2_coef=0.0)with tf.variable_scope('', reuse=True): # the reuse=True scope reuses weights from the context for the question elmo_question_input = weight_layers( 'input', question_embeddings_op, l2_coef=0.0 )elmo_context_output = weight_layers( 'output', context_embeddings_op, l2_coef=0.0)with tf.variable_scope('', reuse=True): # the reuse=True scope reuses weights from the context for the question elmo_question_output = weight_layers( 'output', question_embeddings_op, l2_coef=0.0 )with tf.Session() as sess: # It is necessary to initialize variables once before running inference. sess.run(tf.global_variables_initializer()) # Create batches of data. context_ids = batcher.batch_sentences(tokenized_context) question_ids = batcher.batch_sentences(tokenized_question) # Compute ELMo representations (here for the input only, for simplicity). elmo_context_input_, elmo_question_input_ = sess.run( [elmo_context_input['weighted_op'], elmo_question_input['weighted_op']], feed_dict=&#123;context_token_ids: context_ids, question_token_ids: question_ids&#125; )print(elmo_context_input_,elmo_context_input_) 可以修改代码以适应自己的需求。 Referencehttps://github.com/allenai/bilm-tf]]></content>
      <tags>
        <tag>教程</tag>
        <tag>ELMo</tag>
        <tag>词向量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录4]]></title>
    <url>%2F2018%2F08%2F12%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB4%2F</url>
    <content type="text"><![CDATA[本周没有什么代码要记录的。 1️⃣sklearn之Pipeline例子用机器学习解决问题的流程：(去掉部分数据）—&gt; 获取feature（Tf-idf等） —&gt; （feature selection，chi2、互信息等） —&gt; （缩放/正则化） —&gt; 分类器 —&gt; GridSearch/RandomizedSearch调参 123456789101112131415161718192021222324252627282930pipe=Pipeline([ #建立pipeline ('vect',TfidfVectorizer()), ('select',SelectKBest(chi2), ('norm',MaxAbsScaler()), ('svm',svm.LinearSVC())])parameters=&#123; 'vect__ngram_range':[(1,1),(1,2),(1,3),(2,3)], 'vect__max_df':[0.6,0.7,0.8,0.9], 'vect__min_df':[1,3,5,7,9], 'vect__norm':['l1','l2'], 'svm__penalty':['l1','l2'], 'svm__loss':['squared_hinge'], 'svm__dual':[False,True], 'svm__tol':[1e-5,1e-4], 'svm__C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1], 'svm__class_weight':[None,'balanced'], 'svm__max_iter':[1000,5000]&#125;grid_search_model=GridSearchCV(pipe,parameters,error_score=0,n_jobs=5)grid_search_model.fit(train[column],train['class'])for para_name in sorted(parameters.keys()): print(para_name,grid_search_model.best_params_[para_name])print("cv_result:")print(grid_search_model.cv_results_)]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识3]]></title>
    <url>%2F2018%2F08%2F12%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%863%2F</url>
    <content type="text"><![CDATA[1️⃣[Python]在服务器上跑代码时，如 python project/folder1/a.py，如果a.py引用了一个自定义的模块但又不在folder1内，此时interpreter就会报错，提示找不到该模块。这是因为解释器默认只会在同一个folder下查找。解决方案是在运行前显式添加查找范围。如：1export PYTHONPATH=/home/zhlin/bilm-tf:$PYTHONPATH 那么python解释器就会到该目录下去找。 2️⃣[度量标准] 准确率(accuracy): $ACC=\frac{TP+TN}{TP+TN+FP+FN}$ 衡量的是分类器预测准确的比例 召回率(recall): $Recall=\frac{TP}{TP+FN}$ 正例中被分对的比例，衡量了分类器对正例的识别能力。 精确率(Precision): $P=\frac{TP}{TP+FP}$度量了被分为正例的示例中实际为正例的比例。 F-Measure: $F=\frac{(\alpha^2 +1)P*R}{\alpha^2 (P+R)}$ 其中P是Precision,R是Recall。综合考量了两种度量。 当$\alpha=1$时，称为F1值 $F1=\frac{2PR}{P+R}$ 3️⃣[调参技巧]在google发布的一份关于text-classification的guide中，提到了几个调参的trick。 在feature selection步骤中，卡方检验chi2和方差分析的F值 f_classif的表现相当，在大约选择20k的feature时，准确率达到顶峰，当feature越多，效果并没有提升甚至会下降。 在文本分类中，似乎使用normalization并没有多少用处，建议跳过。 Normalization converts all feature/sample values to small and similar values. This simplifies gradient descent convergence in learning algorithms. From what we have seen, normalization during data preprocessing does not seem to add much value in text classification problems; we recommend skipping this step. 实际上我也测试过，发现确实normalization对于准确率的提高没什么帮助，甚至还有一点下降。]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Python</tag>
        <tag>度量标准</tag>
        <tag>调参技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词4]]></title>
    <url>%2F2018%2F08%2F12%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D4%2F</url>
    <content type="text"><![CDATA[1️⃣ 灞上秋居[唐] 马戴灞原风雨定，晚见雁行频。落叶他乡树，寒灯独夜人。空园白露滴，孤壁野僧邻。寄卧郊扉久，何年致此身。 http://m.xichuangzhu.com/work/57b8a4080a2b58005c9108d9 2️⃣ 唐多令[宋] 刘过芦叶满汀洲，寒沙带浅流。二十年重过南楼。柳下系船犹未稳，能几日，又中秋。黄鹤断矶头，故人今在否？旧江山浑是新愁。欲买桂花同载酒，终不似、少年游。 http://m.xichuangzhu.com/work/57b922e7c4c9710055904842]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim常用快捷键]]></title>
    <url>%2F2018%2F08%2F10%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2FVim%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[在服务器经常要用到Vim，因此记录常用的快捷键并熟悉之。 退出:q 退出:wq 写入并退出:q! 退出并忽略所有更改:e! 放弃修改并打开原来的文件 插入i 在当前位置前插入a 在当前位置后插入 撤销:u 撤销:U 撤销整行操作Ctrl+r 重做 删除:md 删除第m行nd 删除当前行开始的n行(一共n+1行)dd 删除当前行D 删除当前字符至行尾:m,nd 删除从m到n行的内容，如: :100,10000d:m,$d 删除m行及以后所有的行:10d 移动:n 跳转到行号 如， :100gg 跳到行首G(shift+g)移动到文件尾 搜索/text 搜索text，n搜索下一个，N搜索上一个?text 反向查找:set ignorecase 忽略大小写查找:set noignorecase 不忽略大小写查找*或# 对光标处的单词搜索 复制粘贴v 从当前位置开始，光标经过的地方被选中，再按一下v结束 环境设置:set nu 显示行号:set nonu 隐藏行号:set hlsearch 设置搜索结果高亮 Referencehttps://www.cnblogs.com/wangrx/p/5907013.htmlhttps://www.cnblogs.com/yangjig/p/6014198.html]]></content>
      <tags>
        <tag>技巧</tag>
        <tag>杂七杂八</tag>
        <tag>快捷键</tag>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pycharm常用技巧]]></title>
    <url>%2F2018%2F08%2F10%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2FPycharm%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[记录Pycharm的一些技巧，让Pycharm更顺手 快捷键0️⃣Double Shift 万能搜索可以搜索文件名、类名、方法名、目录名（在关键字前面加/ ），并不能用来搜索任意关键字 1️⃣ Command+F 在页面搜索 2️⃣ Ctrl+Shift+F Find in Path 在路径下搜索 3️⃣✨Command+E 快速查找文件显示最近打开的文件 4️⃣ Shift+Enter 任意位置换行无论光标在何处都可以直接另起一行 5️⃣ Option+Enter 自动导入模块；万能提示键自动导入如何设置见小技巧#0️⃣ 6️⃣ Ctrl+F10 运行我已经添加了Ctrl+R作为另一对运行快捷键 7️⃣ Command+Shift+ +/- 展开/收缩代码 8️⃣ Option+F 在Dash中搜索 9️⃣ Ctrl+J 不跳转查看代码 小技巧0️⃣ Pycharm自动导入模块https://blog.csdn.net/lantian_123/article/details/78094148 1️⃣ ✨远程部署工程 强烈推荐两步走：配置服务器映射+配置服务器解释器 2️⃣跳转后如何回退开启toolbar即可https://segmentfault.com/a/1190000010205945 Referencehttps://foofish.net/pycharm-tips.htmlhttps://blog.csdn.net/lantian_123/article/details/78094148https://segmentfault.com/a/1190000010205945]]></content>
      <tags>
        <tag>技巧</tag>
        <tag>Pycharm</tag>
        <tag>杂七杂八</tag>
        <tag>快捷键</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无题]]></title>
    <url>%2F2018%2F08%2F06%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%97%A0%E9%A2%98%2F</url>
    <content type="text"><![CDATA[人这辈子一共会死三次。 第一次是你的心脏停止跳动，那么从生物的角度来说，你死了； 第二次是在葬礼上，认识你的人都来祭奠，那么你在社会关系上的事实存在就死了； 第三次是在最后一个记得你的人死后，那你就真的死了。]]></content>
      <tags>
        <tag>佳句分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人可以卑微如尘土,不可扭曲如蛆虫]]></title>
    <url>%2F2018%2F08%2F05%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E4%BA%BA%E5%8F%AF%E4%BB%A5%E5%8D%91%E5%BE%AE%E5%A6%82%E5%B0%98%E5%9C%9F%2C%E4%B8%8D%E5%8F%AF%E6%89%AD%E6%9B%B2%E5%A6%82%E8%9B%86%E8%99%AB%2F</url>
    <content type="text"><![CDATA[如果天总也不亮，那就摸黑过生活; 如果发出声音是危险的，那就保持沉默; 如果自觉无力发光，那就别去照亮别人。 但是——不要习惯了黑暗就为黑暗辩护; 不要为自己的苟且而得意洋洋; 不要嘲讽那些比自己更勇敢、更有热量的人们。 可以卑微如尘土，不可扭曲如蛆虫。]]></content>
      <tags>
        <tag>佳句分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python惯例[转]]]></title>
    <url>%2F2018%2F08%2F05%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2FPython%E6%83%AF%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[fork from https://github.com/jackfrued/Python-100-Days/blob/master/Python惯例.md Python惯例“惯例”这个词指的是“习惯的做法，常规的办法，一贯的做法”，与这个词对应的英文单词叫“idiom”。由于Python跟其他很多编程语言在语法和使用上还是有比较显著的差别，因此作为一个Python开发者如果不能掌握这些惯例，就无法写出“Pythonic”的代码。下面我们总结了一些在Python开发中的惯用的代码。 让代码既可以被导入又可以被执行。 1if __name__ == '__main__': 用下面的方式判断逻辑“真”或“假”。 12if x:if not x: 好的代码： 12345name = 'jackfrued'fruits = ['apple', 'orange', 'grape']owners = &#123;'1001': '骆昊', '1002': '王大锤'&#125;if name and fruits and owners: print('I love fruits!') 不好的代码： 12345name = 'jackfrued'fruits = ['apple', 'orange', 'grape']owners = &#123;'1001': '骆昊', '1002': '王大锤'&#125;if name != '' and len(fruits) &gt; 0 and owners != &#123;&#125;: print('I love fruits!') 善于使用in运算符。 12if x in items: # 包含for x in items: # 迭代 好的代码： 123name = 'Hao LUO'if 'L' in name: print('The name has an L in it.') 不好的代码： 123name = 'Hao LUO'if name.find('L') != -1: print('This name has an L in it!') 不使用临时变量交换两个值。 1a, b = b, a 用序列构建字符串。 好的代码： 123chars = ['j', 'a', 'c', 'k', 'f', 'r', 'u', 'e', 'd']name = ''.join(chars)print(name) # jackfrued 不好的代码： 12345chars = ['j', 'a', 'c', 'k', 'f', 'r', 'u', 'e', 'd']name = ''for char in chars: name += charprint(name) # jackfrued EAFP优于LBYL。 EAFP - Easier to Ask Forgiveness than Permission. LBYL - Look Before You Leap. 好的代码： 123456d = &#123;'x': '5'&#125;try: value = int(d['x']) print(value)except (KeyError, TypeError, ValueError): value = None 不好的代码： 1234567d = &#123;'x': '5'&#125;if 'x' in d and isinstance(d['x'], str) \ and d['x'].isdigit(): value = int(d['x']) print(value)else: value = None 使用enumerate进行迭代。 好的代码： 123fruits = ['orange', 'grape', 'pitaya', 'blueberry']for index, fruit in enumerate(fruits): print(index, ':', fruit) 不好的代码： 12345fruits = ['orange', 'grape', 'pitaya', 'blueberry']index = 0for fruit in fruits: print(index, ':', fruit) index += 1 用生成式生成列表。 好的代码： 123data = [7, 20, 3, 15, 11]result = [num * 3 for num in data if num &gt; 10]print(result) # [60, 45, 33] 不好的代码： 123456data = [7, 20, 3, 15, 11]result = []for i in data: if i &gt; 10: result.append(i * 3)print(result) # [60, 45, 33] 用zip组合键和值来创建字典。 好的代码： 1234keys = ['1001', '1002', '1003']values = ['骆昊', '王大锤', '白元芳']d = dict(zip(keys, values))print(d) 不好的代码： 123456keys = ['1001', '1002', '1003']values = ['骆昊', '王大锤', '白元芳']d = &#123;&#125;for i, key in enumerate(keys): d[key] = values[i]print(d) 说明：这篇文章的内容来自于网络，有兴趣的读者可以阅读原文。 注：许多原则我认为非常有意义，能够摆脱C/C++的风格，真正写出Pythonic的代码。让我有很大感触的是1、3、8，能够写出非常简洁优雅的代码。同时6我之前从没注意过，习惯了C/C++风格之后总是会在执行之前考虑所有情况，但确实不够优雅，今后可以尝试EAFP风格（什么是EAFP）。]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>杂七杂八</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何训练GloVe中文词向量]]></title>
    <url>%2F2018%2F08%2F05%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83GloVe%E4%B8%AD%E6%96%87%E8%AF%8D%E5%90%91%E9%87%8F%2F</url>
    <content type="text"><![CDATA[准备语料准备好自己的语料，保存为txt，每行一个句子或一段话，注意要分好词。 准备源码从GitHub下载代码，https://github.com/stanfordnlp/GloVe将语料corpus.txt放入到Glove的主文件夹下。 修改bash打开demo.sh，修改相应的内容 因为demo默认是下载网上的语料来训练的，因此如果要训练自己的语料，需要注释掉 修改参数设置，将CORPUS设置成语料的名字 执行bash文件进入到主文件夹下 make bash demo.sh 注意，如果训练数据较大，则训练时间较长，那么建议使用nohup来运行程序 1nohup bash demo.sh &gt;output.txt 2&gt;&amp;1 &amp; 坐等训练，最后会得到vectors.txt 以及其他的相应的文件。如果要用gensim的word2vec load进来，那么需要在vectors.txt的第一行加上vacob_size vector_size，第一个数指明一共有多少个向量，第二个数指明每个向量有多少维。 参考https://www.cnblogs.com/echo-cheng/p/8561171.html]]></content>
      <tags>
        <tag>GloVe</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识2]]></title>
    <url>%2F2018%2F08%2F05%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%862%2F</url>
    <content type="text"><![CDATA[1️⃣[Pytorch]避免写出： 1x = Variable(torch.zeros(...), requires_grad=True).cuda() 而是应该要： 1x = Variable(torch.zeros(...).cuda(), requires_grad=True) Reference:https://discuss.pytorch.org/t/variable-grad-is-always-none-when-extending-autograd/12187 2️⃣[Tf-idf]本周因为比赛的原因了解了一下各种文本建模的方法。Tf-idf能够取得不错的成绩，但有一定的缺陷。 TF-IDF用于向量空间模型，进行文档相似度计算是相当有效的。但在文本分类中单纯使用TF-IDF来判断一个特征是否有区分度是不够的。 它仅仅综合考虑了该词在文档中的重要程度和文档区分度。 它没有考虑特征词在类间的分布。特征选择所选择的特征应该在某类出现多，而其它类出现少，即考察各类的文档频率的差异。如果一个特征词，在各个类间分布比较均匀，这样的词对分类基本没有贡献；但是如果一个特征词比较集中的分布在某个类中，而在其它类中几乎不出现，这样的词却能够很好代表这个类的特征，而TF-IDF不能区分这两种情况。 它没有考虑特征词在类内部文档中的分布情况。在类内部的文档中，如果特征词均匀分布在其中，则这个特征词能够很好的代表这个类的特征，如果只在几篇文档中出现，而在此类的其它文档中不出现，显然这样的特征词不能够代表这个类的特征。 Reference:https://blog.csdn.net/mmc2015/article/details/46771791 3️⃣[卡方检验CHI]在文本分类中，用于选择最相关的特征。 Reference:https://blog.csdn.net/blockheadls/article/details/49977361 4️⃣[文本分类]各种文本分类方法的简单介绍。 Reference:https://github.com/wangjiang0624/Note/blob/master/MachineLearning/文本分类.md 5️⃣[Python]collections的两个有用的类 named_tuple：快速建立一个类，使得可以使用属性来访问而非索引，提高了代码可读性 12345from collections import namedtuplePoint = namedtuple('Point',['x','y'])p = Point(1,2)print(p.x) # 1print(p.y) # 2 Counter：统计字符出现的次数 1234567from collections import Countercount = Counter([...]).most_commom() #会按照出现的次数排序，通常可用于构建词典for c in count: # c是一个tuple，c[0]是词，c[1]是频率 if c[1]&gt;= threshold: vocab.add_word(c[0]) else: break Counter用法：https://blog.csdn.net/u014755493/article/details/69812244 6️⃣[nohup]本周在服务器上跑代码的时候遇到一个问题，使用nohup执行python程序时，发现输出文件没有显示。以为是代码的问题，但经过排查并非是代码的问题。通过查阅资料，发现问题所在：因为python输出有缓冲，导致output不能马上看到输出。实际上，在等待了一段时间后，输出文件终于显示出来了。 解决方案：使用python的参数 -u 使得python不启用缓冲。 1nohup python -u test.py &gt; nohup.out 2&gt;&amp;1 &amp; Reference:https://blog.csdn.net/sunlylorn/article/details/19127107 7️⃣[hexo配置] mathjax配置: https://www.jianshu.com/p/7ab21c7f0674 配置域名:https://www.zhihu.com/question/31377141 配置sitemap:http://www.yuan-ji.me/Hexo-优化：提交sitemap及解决百度爬虫抓取-GitHub-Pages-问题/ 8️⃣[Paper]Learning Chinese Word Representations From Glyphs Of Characters 使用图像的卷积来生成词向量:]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Pytorch</tag>
        <tag>Paper</tag>
        <tag>Tf-idf</tag>
        <tag>文本分类</tag>
        <tag>hexo</tag>
        <tag>nohup</tag>
        <tag>CHI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录3]]></title>
    <url>%2F2018%2F08%2F05%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB3%2F</url>
    <content type="text"><![CDATA[本周只有简单的代码。 1️⃣使用gensim训练word2vec12345678910111213141516#encoding=utf-8from gensim.models import word2vecsentences=word2vec.Text8Corpus(u'分词后的爽肤水评论.txt') #sentence:[ [ a b ],[c d]... ]model=word2vec.Word2Vec(sentences, size=50) #size:dim y2=model.similarity(u"好", u"还行") #计算相似度print(y2)for i in model.most_similar(u"滋润"): print i[0],i[1] #保存model.save('/model/word2vec_model')new_model=gensim.models.Word2Vec.load('/model/word2vec_model') 2️⃣使用Counter建立词表123456789def build_dict(dataset,min_freq=5): dictionary=Dictionary() count=Counter(flat(dataset)).most_common() for c in count: if c[1]&gt;=min_freq: dictionary.add_word(c[0]) else: break return dictionary]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词3]]></title>
    <url>%2F2018%2F08%2F05%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D3%2F</url>
    <content type="text"><![CDATA[本周背的都是比较简单的。 1️⃣ 把酒问月[唐] 李白青天有月来几时？我今停杯一问之。人攀明月不可得，月行却与人相随。皎如飞镜临丹阙，绿烟灭尽清辉发。但见宵从海上来，宁知晓向云间没。白兔捣药秋复春，嫦娥孤栖与谁邻？今人不见古时月，今月曾经照古人。古人今人若流水，共看明月皆如此。唯愿当歌对酒时，月光长照金樽里。 http://m.xichuangzhu.com/work/57b8f6f4165abd0054bf8c13 2️⃣ 金缕衣[唐] 杜秋娘劝君莫惜金缕衣，劝君惜取少年时。花开堪折直须折，莫待无花空折枝。 http://m.xichuangzhu.com/work/57b92bdca633bd00665eb99e 3️⃣ 北青萝[唐] 李商隐残阳西入崦，茅屋访孤僧。落叶人何在，寒云路几层。独敲初夜磬，闲倚一枝藤。世界微尘里，吾宁爱与憎。 崦（yān）：即“崦嵫（zī）”，山名，在甘肃。古时常用来指太阳落山的地方。磬（qìng）：古代打击乐器，形状像曲尺，用玉、石制成，可悬挂。 http://m.xichuangzhu.com/work/57b8ee240a2b58005c91c99e 4️⃣ 夏日绝句[宋] 李清照生当作人杰，死亦为鬼雄。至今思项羽，不肯过江东。 http://m.xichuangzhu.com/work/57b911dac4c97100558fb30e 5️⃣ 雨霖铃[宋] 柳永寒蝉凄切，对长亭晚，骤雨初歇。都门帐饮无绪，留恋处，兰舟催发。执手相看泪眼，竟无语凝噎。念去去，千里烟波，暮霭沉沉楚天阔。多情自古伤离别，更那堪、冷落清秋节。今宵酒醒何处？杨柳岸，晓风残月。此去经年，应是良辰好景虚设。便纵有千种风情，更与何人说？ http://m.xichuangzhu.com/work/57ad742f5bbb500062bc7c9c]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Pytorch中grad的理解]]></title>
    <url>%2F2018%2F08%2F03%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADgrad%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[事情起源于我写了一个CNN用于文本分类，但loss一直没降，因此我尝试print(loss.grad)的grad，发现神奇的是loss grad显示为None，接着尝试print(y_pred.grad)，同样是None，但再print loss和y_pred的requires_grad发现是正常的True。 在查阅了资料，以及问了学长之后发现原来并不是bug，而是因为，Pytorch默认不会保存中间节点(intermediate variable)的grad，此举是为了节省内存。 By default, gradients are only retained for leaf variables. non-leaf variables’ gradients are not retained to be inspected later. This was done by design, to save memory. https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94 实际上可以通过retain_grad()或者hook来查看中间节点的grad。 我后面尝试print了叶子节点，如 print(CNN_model.fc.weight.grad)，最终获得了正确的grad。 ps：所谓中间节点，是由其他节点计算所得的tensor，而叶子节点则是自己定义出来的。 最后我发现，原来loss一直没降的原因是因为我定义的CNN过于复杂，并且数据集偏小，无法快速收敛导致的。]]></content>
      <tags>
        <tag>遇到的问题</tag>
        <tag>Pytorch</tag>
        <tag>grad</tag>
        <tag>bug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2F2018%2F08%2F03%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[命令记录自己常用的命令。 1️⃣ls：显式当前目录下的文件和目录 -a 包括隐藏文件 -h 将文件的容量以易读方式列出（配合-s使用） -s 以块数形式显示每个文件分配的尺寸 -l 以较长格式列出信息，可以直接写成 ll 2️⃣cd 到达指定地址 3️⃣kill 杀死程序 -l 信息编号。当l=9时，无条件终止，其他信号可能忽略 killall -u 杀死该用户全部进程 4️⃣ps 报告当前系统的进程状态 -a 所有 -p 指定程序 -u 指定用户 -x 列出该用户的进程的详细信息(我的理解应该是) 如： 5️⃣htop 比top更优，交互更好，同时可以直观看到资源占用情况基本命令与top一致 6️⃣top：动态查看系统运行状态 -u 指定用户名 -p 指定进程 7️⃣nvidia-smi 查看显卡状态watch nvidia-smi 实时查看显卡状态，定时刷新 8️⃣tail 显示指定文件的末尾若干行 -f 显示文件最新追加的内容 -n 显示文件尾部n行内容 -c 显示文件尾部最后c个字符 如： 123tail file 显示最后10行tail -n +20 file 显示从第20行至末尾tail -c 10 file 显示文件file的最后10个字符 ------- 9️⃣echo 用于打印指定的字符串 🔟which 用于查找并显示给定命令的绝对路径，which指令会在环境变量$PATH设置的目录里查找符合条件的文件。使用which命令，可以看到某个系统命令是否存在，以及执行的是哪个位置的命令。如： 1️⃣1️⃣nohup 将程序以忽略挂起信号的方式运行，经常用于在服务器跑代码如：1nohup python xxx.py &gt;output.txt 2&gt;&amp;1 &amp; 即，将输出重定向到output.txt ；最后一个&amp;表示后台挂起 1️⃣2️⃣cp 复制文件 cp [文件] [目标文件夹] -r 递归复制，用于目录的复制 1️⃣3️⃣mv 移动文件、目录或更名 mv [文件/文件夹] [文件夹] -f 强制，当目标文件存在，直接覆盖 -i 会询问 1️⃣4️⃣rm 删除文件或目录 -f 强制删除 -r 递归删除，用于目录删除 1️⃣5️⃣file 用于判断文件的基本数据如： 1️⃣6️⃣tar 对文件打包/压缩 -t 查看打包文件的内容含有哪些文件名 -x 解压缩 -c 新建打包文件 -C 指定压缩/解压目录 -v 解压/压缩过程中将处理的文件名显示出来常用的： 123压缩：tar -jcv -f filename.tar.bz2 要被处理的文件或目录名称查询：tar -jtv -f filename.tar.bz2解压：tar -jxv -f filename.tar.bz2 -C 欲解压缩的目录 1️⃣7️⃣wc word count 统计文件内容信息，如行数、字符数 -l 显示文件行数 -c 显示字节数 -m 显示字符数 -w 显示字数 字被定义为由空白、跳格、换行字符分隔的字符串 -L 显示最长行的长度 不加参数，所有的都显示，依次是行数、单词数、字节数、文件名 1️⃣8️⃣df 显示磁盘相关信息 -h 以可读性较高的方式显示信息 1️⃣9️⃣scp 服务器之间的文件复制 如: 1scp -r /test1 zhlin@123.12.1.12:/home/zhlin ✨快捷键Ctrl+a 跳到行首Ctrl+c 退出当前进程Ctrl+e 跳到页尾Ctrl+k 删除当前光标后面的文字Ctrl+l 清屏，等价于clearCtrl+r 搜索之前打过的命令Ctrl+u 删除当前光标前面的文字✨Ctrl+左右键 单词之间跳转 在Mac上可以使用option+左右键Ctrl+y 进行恢复删除Ctrl+z 将当前进程转到后台，使用fg恢复 Referencehttps://blog.csdn.net/leo_618/article/details/53003111 ———-持续更新———-]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>碎片知识</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个关于yield的重新认识]]></title>
    <url>%2F2018%2F07%2F31%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E4%B8%80%E4%B8%AA%E5%85%B3%E4%BA%8Eyield%E7%9A%84%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86%2F</url>
    <content type="text"><![CDATA[今天遇到了一个神奇的”bug”，让我对yield的理解更深一步。 这是一个函数，我本来打算试着print一下line内部的格式和内容。 这是调用的主函数： 结果跑出的结果是： ？？？ 我尝试在函数的开头添加print： 结果仍然没有任何的输出。 我试着在main函数添加print： 结果： 也就是说，根本没有进入到get_dataset_from_txt函数啊。 我以为是pycharm的问题还重启了一遍，然而并没有任何作用。问了其他人，他们也觉得很神奇。最后一个同学看了一下函数，发现了问题所在：yield 我突然想起来，yield返回的是一个generator，只有在对generator进行遍历时，才会开始运行… 于是，我试着这么写，试着对generator遍历： 虽然报错了，但函数终于是进去了… 结论：有yield的函数会返回一个generator，当对其进行遍历时，函数才会开始运行。]]></content>
      <tags>
        <tag>遇到的问题</tag>
        <tag>Python</tag>
        <tag>yield</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录2]]></title>
    <url>%2F2018%2F07%2F29%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB2%2F</url>
    <content type="text"><![CDATA[本周主要看了AllenNLP/ELMO的代码，但并没有找到很多可复用的代码。本周也没有比较有意义的代码。 1️⃣get_time_diff获取已使用的时间123456789import timefrom datetime import timedeltastart_time=time.time()def get_time_dif(start_time): """获取已使用时间""" end_time = time.time() time_dif = end_time - start_time return timedelta(seconds=int(round(time_dif))) 2️⃣parser使用12345678parser = argparse.ArgumentParser()parser.add_argument('--save_dir', help='Location of checkpoint files')parser.add_argument('--vocab_file', help='Vocabulary file')parser.add_argument('--train_prefix', help='Prefix for train files')args = parser.parse_args() main(args) #使用]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周碎片知识1]]></title>
    <url>%2F2018%2F07%2F29%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E6%AF%8F%E5%91%A8%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%861%2F</url>
    <content type="text"><![CDATA[1️⃣[Python]assert用法： assert expression等价于if not expression: raise AssertionError 2️⃣[Pytorch]Pytorch view：创建一个新的tensor，但他们的data是共享的。 3️⃣[Pytorch]在Pytorch中，embedding的index是不能requires_grad=True的，否则会出错。https://github.com/pytorch/pytorch/issues/7021 之前看过一份代码，设置volatile=false但没有出错，是因为在Pytorch0.4之后volatile已经被弃用了，因此volatile=false不起作用，而默认requires_grad=false 4️⃣[Pytorch]在Pytorch中，nn.Linear(self.hidden_dim,self.vocab_size)的维度是vocab_sizehidden_dim，之前居然没有注意到这个问题。因为nn.Linear的*第一个参数表示输入维度，第二个参数表示输出维度 5️⃣[Pytorch]Pytorch中，使用view一般来说必须要用 .contiguous()。也即： 1batch.view(batch_size, -1).t().contiguous() contiguous()的官方解释：https://discuss.pytorch.org/t/runtimeerror-input-is-not-contiguous/930 It means that your tensor is not a single block of memory, but a block with holes. view can be only used with contiguous tensors, so if you need to use it here, just call .contiguous() before. 也就是说，contiguous会将数据存到一个连续的空间内（block）。 6️⃣[Pytorch]调用Cross_entropy时，Pytorch会帮助你加log和softmax。 7️⃣[Paper]Sliced_RNN 将RNN分块以提高并行性，甚至每层的RNN都可以不一样，达到抽取不同程度的抽象语义信息的目的。实验证明，在不同任务上都有一定的提升，但速度的提升很大。 8️⃣[Tf-idf]计算词语对于句子的重要程度 https://zh.wikipedia.org/wiki/Tf-idf tf是词频，idf是逆向文件频率。也即如果词在该句出现的次数越多，在所有文本的出现次数越少，则词对于句子的重要程度越高。 9️⃣[Numpy]在Numpy中，一个列表虽然是横着表示的，但它是列向量。我之前居然没有注意到这个问题。]]></content>
      <tags>
        <tag>碎片知识</tag>
        <tag>Python</tag>
        <tag>Pytorch</tag>
        <tag>Paper</tag>
        <tag>Tf-idf</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac配置复旦有线网]]></title>
    <url>%2F2018%2F07%2F29%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2FMac%E9%85%8D%E7%BD%AE%E5%A4%8D%E6%97%A6%E6%9C%89%E7%BA%BF%E7%BD%91%2F</url>
    <content type="text"><![CDATA[配置ip、子网掩码、DNS、路由器有线似乎不支持DHCP，因此只好自己设置。首先连接上有线，将配置iPv4选为手动。问实验室的学长具体的ip地址、子网掩码、路由器、DNS服务器。其中ip地址最后三位要自己设定，只要不和其他人冲突就好。 手动认证到认证平台，下载Mac客户端，其实就是一个.sh文件：http://10.108.255.249/srun_portal_pc.php?ac_id=1&amp;&amp;phone=1 然后，打开文件配置用户名密码，注意到等号后面要有双引号： 保存并放入终端运行，接下来就可以使用有线网了。 其他似乎，每次重新连接都要这样配置，我没有试过不清楚；有线网好像也没有比无线网快多少，但应该会稳定一些。]]></content>
      <tags>
        <tag>网络</tag>
        <tag>配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh快速登录配置]]></title>
    <url>%2F2018%2F07%2F29%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2Fssh%E5%BF%AB%E9%80%9F%E7%99%BB%E5%BD%95%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Motivation分配了服务器之后，每次要ssh进入都很麻烦：ssh user_name@ip_address 然后还要输入密码。 特别是如果分配了多个服务器，那有时候还容易忘记ip地址。因此如果能够一条命令就进入服务器能够减少麻烦。主要有三点： 创建rsa key 上传public key到服务器 设置alias 配置创建rsa key在终端输入命令： 1ssh-keygen -t rsa 当然如果以前有创建过的可以不用。 结果： 上传public key到服务器使用命令：1ssh-copy-id -i ~/.ssh/id_rsa.pub user@127.0.0.1 输入密码即可 结果： 设置alias完成以上步骤就可以不输入密码登录，但还是需要输入ip地址和用户名，为了更简化操作，给命令起个别名。需要配置 .bash_profile文件。输入命令: 1vim ~/.bash_profile 在文件后面添加以下文字： 123# alias alias ssh×××=&quot;ssh user_name@ip_address&quot;alias ssh×××=&quot;ssh user_name@ip_address&quot; 其中 ×××是你自己起的名字，可以是服务器的名字，user_name和ip_address是自己服务器的用户名和地址。保存更改退出。 然后还要使其生效: 1source ~/.bash_profile 这样，输入别名，就可以直接登录了： 参考https://www.jianshu.com/p/66d658c7cb9e]]></content>
      <tags>
        <tag>配置</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于困惑度]]></title>
    <url>%2F2018%2F07%2F29%2F%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86%2F%E5%85%B3%E4%BA%8E%E5%9B%B0%E6%83%91%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[前几天在写新手任务task3的时候，参考了Pytorch官方example的word language model，官方example在训练过程中计算困惑度是这样的： 1math.exp(cur_loss) 其中，cur_loss表示交叉熵的loss，即 $-P(\hat{x})logP(x)$，$\hat{x}$表示ground truth。 然而，在查阅了困惑度相关资料后，我发现，困惑度的定义是这样的： \begin{aligned} PP(S)= &{P(w_{1}w_{2}...w_{N})}^{-\frac{1}{N}} \\ = &\sqrt[N]{\frac{1}{p(w_1 w_2 ... w_N)}} \\ = & \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} } \end{aligned}这是另一种形式: \begin{aligned} Perplexity (W)=& 2^{H(W)} \\ = & {P(w_{1}w_{2}...w_{N})}^{-\frac{1}{N}} \\ = & \sqrt[N]{\frac{1}{p(w_1 w_2 ... w_N)}} \\ = & \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} } \end{aligned}可以看到，二者本质是一样的。 那么，为什么在代码中以e为底去计算困惑度，而不是2呢? 实际上，是因为在上述公式中，log是以2为底的，但在Pytorch中，log默认是以e为底的。因此在代码中，需要用e作为指数的底来还原成困惑度的原本形式： \begin{aligned} \sqrt[N]{\prod_{i=1}^{N}{\frac{1}{p(w_i|(w_1 w_2... w_{i-1})}} } \end{aligned}最后这是perplexity的数学推导：https://www.zhihu.com/question/58482430]]></content>
      <tags>
        <tag>困惑度</tag>
        <tag>perplexity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词2]]></title>
    <url>%2F2018%2F07%2F29%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D2%2F</url>
    <content type="text"><![CDATA[本周的诗词有两篇是已经背过的，权当是复习了一遍。 1️⃣ 下终南山过斛斯山人宿置酒[唐] 李白暮从碧山下，山月随人归。却顾所来径，苍苍横翠微。相携及田家，童稚开荆扉。绿竹入幽径，青萝拂行衣。欢言得所憩，美酒聊共挥。长歌吟松风，曲尽河星稀。我醉君复乐，陶然共忘机。 http://m.xichuangzhu.com/work/57b900307db2a20054269a2a 2️⃣ 逢入京使[唐] 岑参故园东望路漫漫，双袖龙钟泪不乾。马上相逢无纸笔，凭君传语报平安。 http://m.xichuangzhu.com/work/57b92218df0eea006335f923 3️⃣ 念奴娇·赤壁怀古[宋] 苏轼大江东去，浪淘尽、千古风流人物。故垒西边，人道是、三国周郎赤壁。乱石穿空，惊涛拍岸，卷起千堆雪。江山如画，一时多少豪杰。遥想公瑾当年，小乔初嫁了，雄姿英发。羽扇纶巾，谈笑间，樯橹灰飞烟灭。故国神游，多情应笑我，早生华发。人生如梦，一尊还酹江月。 http://m.xichuangzhu.com/work/57b8bda2df0eea006333ecd2]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码片段记录1]]></title>
    <url>%2F2018%2F07%2F23%2F%E4%BB%A3%E7%A0%81%E7%9B%B8%E5%85%B3%2F%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%E5%88%86%E4%BA%AB1%2F</url>
    <content type="text"><![CDATA[1️⃣ get_batch注意到shuffle的标准做法 123456789101112def get_batch(self,data,batch_size=32,is_shuffle): N=len(data) #获得数据的长度 if is_shuffle is True: r=random.Random() r.seed() r.shuffle(data) #如果is_shuffle为真则打乱 #开始获得batch，使用[ for in ] batch=[data[k:k+batch_size] for k in range(0,N,batch_size)] if N%batch_size!=0: #处理不整除问题，如果有显式要求丢掉则不需要处理，这里默认处理 remainder=N-N%batch_size #剩下的部分 batch.append(data[temp:N]) return batch 2️⃣使用gensim将GloVe读入实际上这份代码有点问题，在使用过程中，发现glove文件需要放在gensim的文件夹下才能被读到(7.20 updated,应该使用绝对地址)，并不好。 教程地址：gensim: scripts.glove2word2vec – Convert glove format to word2vec123456789101112131415161718#1. 使用gensim读入word2vecmodel = gensim.models.KeyedVectors.load_word2vec_format( fname='GoogleNews-vectors-negative300-SLIM.bin', binary=True)words = model.vocab #获得词表vector= model[word] #word是words里面的元素#2. 使用gensim读入glovefrom gensim.models import KeyedVectorsfrom gensim.test.utils import datapath, get_tmpfilefrom gensim.scripts.glove2word2vec import glove2word2vecglove_file=datapath('glove.txt') #最好使用绝对地址tmp_file=get_tmpfile('word2vec.txt')glove2word2vec(glove_file,tmp_file)model=KeyedVectors.load_word2vec_format(tmp_file)#接下来使用的方法是一样的 3️⃣data_split方法1234567891011121314151617181920def data_split(seed=1, proportion=0.7): data = list(iter_corpus()) ids = list(range(len(data))) N = int(len(ids) * proportion) # number of training data rng = random.Random(seed) rng.shuffle(ids) test_ids = set(ids[N:]) train_data = [] test_data = [] for x in data: if x[1] in test_ids: # x[1]: sentence id test_data.append(x) else: train_data.append(x) return train_data, test_data 4️⃣对string预处理123456789101112131415161718def clean_str(string): string = re.sub(r"[^A-Za-z0-9()!?\'\`]", "", string) string = re.sub(r"\'s", " \'s", string) string = re.sub(r"\'m", " \'m", string) string = re.sub(r"\'ve", " \'ve", string) string = re.sub(r"n\'t", " n\'t", string) string = re.sub(r"\'re", " \'re", string) string = re.sub(r"\'d", " \'d", string) string = re.sub(r"\'ll", " \'ll", string) string = re.sub(r",", " , ", string) string = re.sub(r"!", " ! ", string) string = re.sub(r"\(", " \( ", string) string = re.sub(r"\)", " \) ", string) string = re.sub(r"\?", " \? ", string) string = re.sub(r"\s&#123;2,&#125;", " ", string) string = re.sub(r"\@.*?[\s\n]", "", string) string = re.sub(r"https*://.+[\s]", "", string) return string.strip().lower() 5️⃣collate_fn(batch）重写collate_fn组建mini-batch，在NLP中常用，句子的不等长性123456789101112131415161718192021222324252627def collate_fn(batch): # rewrite collate_fn to form a mini-batch lengths = np.array([len(data['sentence']) for data in batch]) sorted_index = np.argsort(-lengths) lengths = lengths[sorted_index] # descend order max_length = lengths[0] batch_size = len(batch) sentence_tensor = torch.LongTensor(batch_size, int(max_length)).zero_() for i, index in enumerate(sorted_index): sentence_tensor[i][:lengths[i]] = torch.LongTensor(batch[index]['sentence'][:max_length]) sentiments = torch.autograd.Variable(torch.LongTensor([batch[i]['sentiment'] for i in sorted_index])) if config.use_cuda: packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(Variable(sentence_tensor.t()).cuda(), lengths) #remember to transpose sentiments = sentiments.cuda() else: packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(Variable(sentence_tensor.t()),lengths) # remember to transpose return &#123;'sentence': packed_sequences, 'sentiment': sentiments&#125;## 重写collate_fn(batch)以用于dataloader使用，使用方法如下：train_dataloader=DataLoader(train_data,batch_size=32,shuffle=True,collate_fn=collate_fn)​## 其中，train_dataloader可循环遍历​​。for data in train_dataloader: ... 6️⃣使用yield获得数据的generatoryield的用法123456789101112131415def get_dataset(txt_file): # return generator with open(txt_file,'r') as f: for line in f: if len(line.strip())==0: continue sentence=list(line.strip())+['&lt;eos&gt;'] yield sentence #在使用的时候：dataset=get_dataset(txt_file)for d in dataset: pass#如果需要还可以改成list形式dataset=list(get_dataset(txt_file)) 7️⃣动态创建RNN实例根据rnn_type动态创建对象实例，使用了getattr123# rnn in ['GRU','LSTM','RNN']self.rnn = getattr(nn, self.rnn_type)(self.embedding_dim, self.hidden_dim, self.num_layers, dropout=self.dropout)]]></content>
      <tags>
        <tag>code snippets</tag>
        <tag>代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周诗词1]]></title>
    <url>%2F2018%2F07%2F23%2F%E8%AF%97%E8%AF%8D%26%E5%8F%A5%2F%E6%AF%8F%E5%91%A8%E8%AF%97%E8%AF%8D1%2F</url>
    <content type="text"><![CDATA[本周背了四篇。 1️⃣ 临江仙·夜归临皋[宋] 苏轼夜饮东坡醒复醉，归来彷彿三更。家童鼻息已雷鸣，敲门都不应，倚杖听江声。长恨此身非我有，何时忘却营营？夜阑风静縠纹平，小舟从此逝，江海寄馀生。 縠（hú）纹皋（gao）http://m.xichuangzhu.com/work/57ae79400a2b580063150e39 2️⃣ 蝶恋花·阅尽天涯离别苦[清] 王国维阅尽天涯离别苦。不道归来，零落花如许。花底相看无一语，绿窗春与天俱莫。待把相思灯下诉。一缕新欢，旧恨千千缕。最是人间留不住，朱颜辞镜花辞树。 http://m.xichuangzhu.com/work/57b8ef70128fe10054c91d17 3️⃣ 送友人[唐] 李白青山横北郭，白水绕东城。此地一为别，孤蓬万里征。浮云游子意，落日故人情。挥手自兹去，萧萧班马鸣。 http://m.xichuangzhu.com/work/57b8facfd342d3005ac6ffb4 4️⃣ 黄鹤楼送孟浩然之广陵[唐] 李白故人西辞黄鹤楼，烟花三月下扬州。孤帆远影碧空尽，唯见长江天际流。 http://m.xichuangzhu.com/work/57b8f306128fe10054c92fb8]]></content>
      <tags>
        <tag>诗词</tag>
        <tag>诗词分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装conda错误]]></title>
    <url>%2F2018%2F07%2F23%2F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%2F%E5%AE%89%E8%A3%85conda%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[在服务器上安装conda的时候，一开始使用了pip安装pip install conda在安装好conda之后想要使用conda命令，出现： ERROR: The install method you used for conda—probably either pip install conda or easy_install conda—is not compatible with using conda as an application. If your intention is to install conda as a standalone application, currently supported install methods include the Anaconda installer and the miniconda installer. You can download the miniconda installer from https://conda.io/miniconda.html. 然后到官网下载.sh文件并bash安装，仍然没有解决该问题；接着尝试pip uninstall conda，出现 最后在查阅了网上之后，使用 which conda找到conda的地址，并删除rm ××× 最后重新bash安装即可。]]></content>
      <tags>
        <tag>杂七杂八</tag>
        <tag>conda</tag>
        <tag>遇到的问题</tag>
      </tags>
  </entry>
</search>
