<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/favicon.ico?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="人一己千">
<meta property="og:type" content="website">
<meta property="og:title" content="Weekly Review">
<meta property="og:url" content="http://www.linzehui.me/page/3/index.html">
<meta property="og:site_name" content="Weekly Review">
<meta property="og:description" content="人一己千">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Weekly Review">
<meta name="twitter:description" content="人一己千">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.linzehui.me/page/3/"/>





  <title>Weekly Review</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Weekly Review</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/11/11/碎片知识/每周碎片知识11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/11/碎片知识/每周碎片知识11/" itemprop="url">每周碎片知识11</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-11T10:56:14+08:00">
                2018-11-11
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-01T15:35:06+08:00">
                2018-12-01
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-Optimizer"><a href="#1️⃣-Optimizer" class="headerlink" title="1️⃣[Optimizer]"></a>1️⃣[Optimizer]</h3><p><a href="https://zhuanlan.zhihu.com/p/32262540" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32262540</a><br><a href="https://zhuanlan.zhihu.com/p/32338983" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32338983</a></p>
<p>Adam等自适应学习率算法对于稀疏数据具有优势，且收敛速度很快；但精调参数的SGD（+Momentum）往往能够取得更好的最终结果。</p>
<p>建议：<br>前期用Adam，享受Adam快速收敛的优势；后期切换到SGD，慢慢寻找最优解。<br>什么时候从Adam切换到SGD？当SGD的相应学习率的移动平均值基本不变的时候。</p>
<hr>
<h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>LongTensor除以浮点数，会对除数进行取整，再做除法。<br><img src="/images/2018-11-11-15419055399325.jpg" width="30%" height="50%"></p>
<hr>
<h3 id="3️⃣-Pytorch"><a href="#3️⃣-Pytorch" class="headerlink" title="3️⃣[Pytorch]"></a>3️⃣[Pytorch]</h3><p>使用Pytorch的DataParallel</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda:'</span> + str(</span><br><span class="line">    config.CUDA_VISIBLE_DEVICES[<span class="number">0</span>]) <span class="keyword">if</span> config.use_cuda <span class="keyword">else</span> <span class="string">'cpu'</span>)   <span class="comment"># 指定第一个设备</span></span><br><span class="line"></span><br><span class="line">model = ClassifyModel(</span><br><span class="line">    vocab_size=len(vocab), max_seq_len=config.max_sent_len,</span><br><span class="line">    embed_dim=config.embed_dim, n_layers=config.n_layers,</span><br><span class="line">    n_head=config.n_head, d_k=config.d_k,</span><br><span class="line">    d_v=config.d_v,</span><br><span class="line">    d_model=config.d_model, d_inner=config.d_inner_hid,</span><br><span class="line">    n_label=config.n_label,</span><br><span class="line">    dropout=config.dropout</span><br><span class="line">).to(device)</span><br><span class="line"></span><br><span class="line">model = DataParallel(model, device_ids=config.CUDA_VISIBLE_DEVICES)  <span class="comment"># 显式定义device_ids</span></span><br></pre></td></tr></table></figure>
<p>注意到：device_ids的起始编号要与之前定义的device中的“cuda:0”相一致，不然会报错。</p>
<p>如果不显式在代码中的DataParallel指定设备，那么需要在命令行内指定。如果是在命令行里面运行的，且device不是从0开始，应当显式设置GPU_id，否则会出错‘AssertionError: Invalid device id’，正确的命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=4,5  python -u classify_main.py --gpu_id 0,1</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/11/11/碎片知识/关于sparse gradient/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/11/碎片知识/关于sparse gradient/" itemprop="url">关于sparse gradient</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-11T10:35:24+08:00">
                2018-11-11
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-19T23:09:12+08:00">
                2018-11-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>前几天在看AllenAI在EMNLP的ppt时，有一页写道：<br><img src="/images/2018-11-11-15419037448379.jpg" width="70%" height="50%"></p>
<p>为什么会出现这种情况？</p>
<p>Embedding是一个很大的矩阵，每一次其实都只有一个小部分进行了更新，对于一些词来说，出现的频率不高，或者说，其实大部分的词在一个loop/epoch中，被更新的次数是较少的。但是，注意到一般的optimizer算法，是以matrix为单位进行更新的，也就是每一次都是$W^{t+1}=W^{t}-\eta \frac{\partial L}{\partial{W}}$</p>
<p>而Adam算法：<br><img src="/images/2018-11-11-15419038346958.jpg" width="70%" height="50%"></p>
<p>动量占了主导。但这样，每次batch更新，那些没被更新的词（也即gradient=0）的动量仍然会被衰减，所以这样当到这个词更新的时候，他的动量已经被衰减完了，所以更新的gradient就很小。</p>
<p>解决方案：</p>
<p>①在PyTorch中，Embedding的API：<br><code>torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False, _weight=None)</code></p>
<p>其中sparse (bool, optional) – if True, gradient w.r.t. weight matrix will be a sparse tensor.</p>
<p>将sparse设为True即可。</p>
<p>②针对sparse矩阵，使用不同的optimizer，如torch.optim.SparseAdam：</p>
<blockquote>
<p>Implements lazy version of Adam algorithm suitable for sparse tensors.<br>In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/11/10/论文/每周论文5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/10/论文/每周论文5/" itemprop="url">每周论文5</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-10T23:10:30+08:00">
                2018-11-10
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-19T11:21:11+08:00">
                2018-11-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1️⃣-Neural-Turing-Machine"><a href="#1️⃣-Neural-Turing-Machine" class="headerlink" title="1️⃣[Neural Turing Machine]"></a>1️⃣[Neural Turing Machine]</h2><p>通过模仿冯诺依曼机，引入外部内存(externel memory)。<br><img src="/images/2018-11-10-15418626837026.jpg" width="70%" height="50%"></p>
<p>和普通神经网络一样，与外界交互，获得一个输入，产生一个输出。但不同的是，内部还有一个memory进行读写。<br>假设memory是一个N × M的矩阵，N是内存的位置数量。</p>
<h3 id="读写memory"><a href="#读写memory" class="headerlink" title="读写memory"></a>读写memory</h3><p>①读<br><img src="/images/2018-11-10-15418627403268.jpg" width="25%" height="50%"><br>其中读的时候对各内存位置线性加权。w是归一化权重。</p>
<p>②写<br>$e_t$是擦除向量（erase vector）<br><img src="/images/2018-11-10-15418627941358.jpg" width="35%" height="50%"></p>
<p>$a_t$是加和向量(add vector)<br><img src="/images/2018-11-10-15418628323343.jpg" width="30%" height="50%"></p>
<p>具体如何获得权重就不说了。</p>
<h3 id="Controller-network"><a href="#Controller-network" class="headerlink" title="Controller network"></a>Controller network</h3><p>中间的controller network可以是一个普通的feed forward或者RNN。</p>
<p>在实际中NTM用得并不多。</p>
<hr>
<h2 id="2️⃣-Efficient-Contextualized-Representation-Language-Model-Pruning-for-Sequence-Labeling"><a href="#2️⃣-Efficient-Contextualized-Representation-Language-Model-Pruning-for-Sequence-Labeling" class="headerlink" title="2️⃣[Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling]"></a>2️⃣[Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling]</h2><p>ELMo的精简版，通过即插即用的方法来压缩语言模型，对特定任务剪枝不同的层，使得能够减少inference的时间。<br>这篇的idea挺有创新的，但似乎有些trivial的感觉。</p>
<p><img src="/images/2018-11-11-15418977712883.jpg" width="70%" height="50%"></p>
<h3 id="RNN-and-Dense-Connectivity"><a href="#RNN-and-Dense-Connectivity" class="headerlink" title="RNN and Dense Connectivity"></a>RNN and Dense Connectivity</h3><p>每一层的输出都会传到所有层作为输入，因此对于L层的输入：<br><img src="/images/2018-11-11-15418979328482.jpg" width="35%" height="50%"></p>
<p>这样我们就能够随意地去掉任意中间层了。同时一些语言信息也分散到各个层，即使去掉某些层也没有关系。</p>
<p>则最终的output为：<br><img src="/images/2018-11-11-15418991345288.jpg" width="33%" height="50%"></p>
<p>最终作projection到正常维度（在每层都会这么做，将输入降维到正常维度再输入）：<br><img src="/images/2018-11-11-15418992517849.jpg" width="37%" height="50%"></p>
<p>再做一个softmax：<br><img src="/images/2018-11-11-15418993146246.jpg" width="32%" height="50%"></p>
<p>由于 $h^{※}$ 用于softmax，所以可能和target word，也即下一个词比较相似，<strong>因此可能没有很多的上下文信息</strong>。</p>
<p>所以最终我们使用$h_t$，以及反向的$h_t^r$，再过一层线性层获得最终的embedding（和ELMo有些不同，ELMo是直接拼起来）：<br><img src="/images/2018-11-11-15418994541442.jpg" width="40%" height="50%"></p>
<h3 id="Layer-Selection"><a href="#Layer-Selection" class="headerlink" title="Layer Selection"></a>Layer Selection</h3><p>我们在每层的output都加一个权重系数。<br><img src="/images/2018-11-11-15418996081852.jpg" width="30%" height="50%"></p>
<p>我们希望在target task上用的时候，部分z能够变成0，达到layer selection的效果，加快inference的速度。</p>
<p>亦即：<br><img src="/images/2018-11-11-15418996697208.jpg" width="20%" height="50%"></p>
<p>一种理想的方法是L0正则化：<br><img src="/images/2018-11-11-15418997294756.jpg" width="17%" height="50%"></p>
<p>但由于没办法求导，因此，采用L1正则化：<br><img src="/images/2018-11-11-15418997747882.jpg" width="15%" height="50%"><br>但使用L1正则化有一定的风险，因为如果让所有z都远离1，那么会影响performance。</p>
<p>引入新的正则化方法$R_2 =\delta(|z|_0&gt;\lambda_1) |z|_1$<br>亦即，只有在非零z的个数大于某个阈值时，才能有正则化效果，保证非零的个数。’it can be “turned-off” after achieving a satisfying sparsity’.</p>
<p>进一步引入$R_3=\delta(|z|_0&gt;\lambda_1) |z|_1 + |z(1-z)|_1$<br>其中第二项为了鼓励z向0或1走。</p>
<h3 id="Layer-wise-Dropout"><a href="#Layer-wise-Dropout" class="headerlink" title="Layer-wise Dropout"></a>Layer-wise Dropout</h3><p>随机删除部分layer，这些layer的输出不会传入之后的层，但仍然会参与最后的representation计算。<br><img src="/images/2018-11-11-15419000928057.jpg" width="70%" height="50%"></p>
<p>这种dropout会让perplexity更高，但对生成更好的representation有帮助。</p>
<hr>
<h2 id="3️⃣-Constituency-Parsing-with-a-Self-Attentive-Encoder"><a href="#3️⃣-Constituency-Parsing-with-a-Self-Attentive-Encoder" class="headerlink" title="3️⃣[Constituency Parsing with a Self-Attentive Encoder]"></a>3️⃣[Constituency Parsing with a Self-Attentive Encoder]</h2><p>其中的positional encoding我比较感兴趣。<br>原版的positional encoding是直接和embedding相加的。<br>亦即：<br><img src="/images/2018-11-11-15419002563338.jpg" width="22%" height="50%"><br>那么在selt-attention时，有：<br><img src="/images/2018-11-11-15419002855901.jpg" width="45%" height="50%"><br>这样会有交叉项：<br><img src="/images/2018-11-11-15419003111684.jpg" width="13%" height="50%"><br>该项没有什么意义，且可能会带来过拟合。</p>
<p>因此在这边将positional encoding和embedding拼起来，亦即：<br><img src="/images/2018-11-11-15419003740409.jpg" width="23%" height="50%"></p>
<p>并且，在进入multi-head时的线性层也做改变：<br><img src="/images/2018-11-11-15419004269693.jpg" width="24%" height="50%"></p>
<p>这样在相乘的时候就不会有交叉项了。</p>
<p>实验证明，该方法有一定的提升。</p>
<hr>
<h2 id="4️⃣-DropBlock-A-regularization-method-for-convolutional-networks"><a href="#4️⃣-DropBlock-A-regularization-method-for-convolutional-networks" class="headerlink" title="4️⃣[DropBlock: A regularization method for convolutional networks]"></a>4️⃣[DropBlock: A regularization method for convolutional networks]</h2><p>大致翻了一下。<br>Motivation:在CNN中，dropout对convolutional layer的作用不大，一般都只用在全连接层。作者推测，因为每个feature map都有一个感受野范围，仅仅对单个像素进行dropout并不能降低feature map学习的特征范围，亦即网络仍可以通过该位置的相邻位置元素去学习对应的语义信息，也就不会促使网络去学习更加鲁棒的特征。</p>
<p>因此作者的做法是，dropout一整块位置。<br><img src="/images/2018-11-11-15419007355875.jpg" width="80%" height="50%"></p>
<hr>
<h2 id="5️⃣-Accelerating-Neural-Transformer-via-an-Average-Attention-Network"><a href="#5️⃣-Accelerating-Neural-Transformer-via-an-Average-Attention-Network" class="headerlink" title="5️⃣[Accelerating Neural Transformer via an Average Attention Network]"></a>5️⃣[Accelerating Neural Transformer via an Average Attention Network]</h2><p>提出了AAN(average attention network)，对transformer翻译模型的decode部分进行改进，加速了过程。</p>
<p>由于Transformer在decode阶段需要用到前面所有的y，也即自回归(auto-regressive)的性质，所以无法并行：</p>
<p><img src="/images/2018-11-11-15419009098650.jpg" width="50%" height="50%"></p>
<h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><p>给定y：<br><img src="/images/2018-11-11-15419010325049.jpg" width="27%" height="50%"></p>
<p>首先将他们加起来，过一层全连接：<br><img src="/images/2018-11-11-15419010603010.jpg" width="27%" height="50%"><br>这也相当于就是让所有的y有相同的权重，此时g就是上下文相关的表示。</p>
<p>接下来添加一个gating：<br><img src="/images/2018-11-11-15419011154221.jpg" width="27%" height="50%"><br>控制了从过去保存多少信息和获取多少新的信息。</p>
<p>和Transformer原版论文一样，添加一个residual connection：<br><img src="/images/2018-11-11-15419011595237.jpg" width="30%" height="50%"></p>
<p>如图整个过程：<br><img src="/images/2018-11-11-15419011840751.jpg" width="55%" height="50%"></p>
<p>总结：AAN=average layer+gating layer</p>
<h3 id="加速"><a href="#加速" class="headerlink" title="加速"></a>加速</h3><p>①考虑到加和操作是序列化的，只能一个一个来，不能并行，在这里使用一个mask的trick，使得在训练时也能够并行：<br><img src="/images/2018-11-11-15419013219526.jpg" width="60%" height="50%"></p>
<p>②在inference时的加速：<br><img src="/images/2018-11-11-15419019335926.jpg" width="20%" height="50%"></p>
<p>这样Transformer就能够类似RNN，只考虑前一个的state，而不是前面所有的state。</p>
<p>最终的模型：<br><img src="/images/2018-11-11-15419023032628.jpg" width="60%" height="50%"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/11/10/诗词&句/每周诗词15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/10/诗词&句/每周诗词15/" itemprop="url">每周诗词15</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-10T23:08:14+08:00">
                2018-11-10
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-10T23:08:57+08:00">
                2018-11-10
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣蜀相"><a href="#1️⃣蜀相" class="headerlink" title="1️⃣蜀相"></a>1️⃣蜀相</h3><p>[唐] 杜甫<br>丞相祠堂何处寻，锦官城外柏森森。<br>映阶碧草自春色，隔叶黄鹂空好音。<br>三顾频烦天下计，两朝开济老臣心。<br><strong>出师未捷身先死，长使英雄泪满襟</strong>。</p>
<p><a href="http://m.xichuangzhu.com/work/57b93410a633bd00665efd4a" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b93410a633bd00665efd4a</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/11/04/论文/每周论文4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/04/论文/每周论文4/" itemprop="url">每周论文4</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-04T08:18:30+08:00">
                2018-11-04
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-19T11:21:12+08:00">
                2018-11-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1️⃣-Character-Level-Language-Modeling-with-Deeper-Self-Attention"><a href="#1️⃣-Character-Level-Language-Modeling-with-Deeper-Self-Attention" class="headerlink" title="1️⃣[Character-Level Language Modeling with Deeper Self-Attention]"></a>1️⃣[Character-Level Language Modeling with Deeper Self-Attention]</h2><p>将transformer用于character-level的语言模型中，通过添加多个loss来提高其表现以及加快拟合速度，同时加深transformer的层数，极大提升表现，12层的transformer layer能达到SOTA，而64层则有更多的提升。</p>
<p>普通RNN用于character-level language model：<br>将句子按character为单位组成多个batch，每个batch预测最后一个词，然后将该batch的隐状态传入下一个batch。也即“truncated backpropagation through time” (TBTT)。</p>
<p>如果用在Transformer，如下图，我们只预测$t_4$。<br><img src="/images/2018-11-04-15412915431327.jpg" width="90%" height="50%"></p>
<p>本文的一大贡献是多加了三种loss，并且有些loss的权值会随着训练的过程而逐渐减小，每个loss都会自己的schedule。这些loss加快了拟合速度，同时也提升了表现。</p>
<h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><h4 id="Multiple-Positions"><a href="#Multiple-Positions" class="headerlink" title="Multiple Positions"></a>Multiple Positions</h4><p>对于batch内而言，每个时间步t都要预测下一个词。<br><img src="/images/2018-11-04-15412916429104.jpg" width="90%" height="50%"></p>
<h4 id="Intermediate-Layer-Losses"><a href="#Intermediate-Layer-Losses" class="headerlink" title="Intermediate Layer Losses"></a>Intermediate Layer Losses</h4><p>要求中间层也做出预测：<br><img src="/images/2018-11-04-15412916704097.jpg" width="95%" height="50%"></p>
<p>在这里，越底层的layer其loss权值越低。</p>
<h4 id="Multiple-Targets"><a href="#Multiple-Targets" class="headerlink" title="Multiple Targets"></a>Multiple Targets</h4><p>每一个position，不仅仅要预测下一个词，还要预测下几个词，预测下一个词和预测下几个词的分类器是独立的。</p>
<p><img src="/images/2018-11-04-15412917374689.jpg" width="70%" height="50%"></p>
<h3 id="Positional-embedding"><a href="#Positional-embedding" class="headerlink" title="Positional embedding"></a>Positional embedding</h3><p>每一层的都添加一个不共享的可学习的positional embedding。</p>
<hr>
<h2 id="2️⃣-Self-Attention-with-Relative-Position-Representations"><a href="#2️⃣-Self-Attention-with-Relative-Position-Representations" class="headerlink" title="2️⃣[Self-Attention with Relative Position Representations]"></a>2️⃣[Self-Attention with Relative Position Representations]</h2><p>提出使用相对位置替代Transformer的绝对位置信息，并在NMT上有一定的提升。</p>
<p>分解：<br>在原先的self-attention中，输出为：<br><img src="/images/2018-11-04-15412923510664.jpg" width="25%" height="50%"></p>
<p>其中：<br><img src="/images/2018-11-04-15412923744647.jpg" width="25%" height="50%"><br><img src="/images/2018-11-04-15412923773686.jpg" width="25%" height="50%"></p>
<p>现在我们考虑添加相对位置，其中相对位置信息在各层都是共享的：<br><img src="/images/2018-11-04-15412924279426.jpg" width="30%" height="50%"><br><img src="/images/2018-11-04-15412924396468.jpg" width="30%" height="50%"></p>
<p>$a_{ij}^K$的具体形式：<br><img src="/images/2018-11-04-15412925792994.jpg" width="40%" height="50%"><br><img src="/images/2018-11-04-15412925910424.jpg" width="55%" height="50%"><br>上式为了降低复杂度，不考虑长于k的相对位置信息。</p>
<p>考虑到transformer的并行性，为了并行性，我们考虑如下式子：<br><img src="/images/2018-11-04-15412926687951.jpg" width="50%" height="50%"><br>其中，第一项和原来的Transformer一致；第二项，通过reshape可以达到并行的效果，然后两项直接加起来。</p>
<p>实验证明，使用相对位置效果是有一定的提升的，而同时使用绝对位置和相对位置并没有提升。<br><img src="/images/2018-11-04-15412930642978.jpg" width="90%" height="50%"></p>
<hr>
<h2 id="3️⃣-WEIGHTED-TRANSFORMER-NETWORK-FOR-MACHINE-TRANSLATION"><a href="#3️⃣-WEIGHTED-TRANSFORMER-NETWORK-FOR-MACHINE-TRANSLATION" class="headerlink" title="3️⃣[WEIGHTED TRANSFORMER NETWORK FOR MACHINE TRANSLATION]"></a>3️⃣[WEIGHTED TRANSFORMER NETWORK FOR MACHINE TRANSLATION]</h2><p>这篇被ICLR拒了，但有审稿人打了9分的高分。</p>
<p>对Transformer进行改进，拥有更好的效果和更小的计算代价。</p>
<p>传统的Transformer：</p>
<script type="math/tex; mode=display">Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><script type="math/tex; mode=display">head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)</script><script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat_i (head_i)W^O</script><script type="math/tex; mode=display">FFN(x)=max(0,xW_1+b_1)W_2 + b_2</script><p>在本文中，先对head进行升维并乘以权重，过了FNN后，再乘以另一个权重。其中权重$\alpha$ $ \kappa$为可学习参数：</p>
<script type="math/tex; mode=display">head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)</script><script type="math/tex; mode=display">\overline{head_i}=head_i W^{O_i} \times \kappa_i</script><script type="math/tex; mode=display">BranchedAttention(Q,K,V)=\sum_{i=1}^{M} \alpha_i FFN(\overline{head}_i)</script><p>其中要求权重之和为1。即$\sum_{i=1}^{M}\alpha_i=1$,$\sum_{i=1}^{M}\kappa_i=1$。</p>
<p><img src="/images/2018-11-04-15412939412047.jpg" width="90%" height="50%"></p>
<p>文中对$\kappa$和$\alpha$作了解释。</p>
<blockquote>
<p>κ can be interpreted as a learned concatenation weight and α as the learned addition weight</p>
</blockquote>
<p>通过实验，发现该模型会有更好的正则化特性。同时效果也有一定提升，收敛速度更快：<br><img src="/images/2018-11-04-15412940966579.jpg" width="80%" height="50%"></p>
<hr>
<h2 id="4️⃣-You-May-Not-Need-Attention"><a href="#4️⃣-You-May-Not-Need-Attention" class="headerlink" title="4️⃣[You May Not Need Attention]"></a>4️⃣[You May Not Need Attention]</h2><p>粗略地过了一遍，一些细节没有弄明白。</p>
<p>提出一种将encoder-decoder融合起来的模型，也即eager translation model，不需要attention，能够实现即时的翻译，也即读入一个词就能翻译一个词，同时不需要记录encoder的所有输出，因此需要很少的内存。</p>
<p><img src="/images/2018-11-04-15412942175720.jpg" width="50%" height="50%"></p>
<p>分为三步：<br>①pre-processing<br>进行预处理，使得源句子和目标句子满足<strong>eager feasible</strong> for every aligned pair of words $(s_i , t_j ), i ≤ j$。</p>
<p>首先通过现成的工具进行对齐操作(alignment)，然后对于那些不符合eager feasible的有具体算法（没认真看）进行补padding。如图<br><img src="/images/2018-11-04-15412945231042.jpg" width="60%" height="50%"></p>
<p>我们还可以在target sentence的开头添加b个padding，使得模型能够在开始预测之前获取更多的source sentence的词。</p>
<p>②模型<br>两层的LSTM，输入是上一次的y和当前的x拼接起来直接传进去。</p>
<p>③post processing<br>在最终结果之前，将padding去掉。</p>
<p>在inference（也即beam search）时，还有几个操作/trick：</p>
<ul>
<li>Padding limit</li>
<li>Source padding injection SPI</li>
</ul>
<p>实验表明，eager model在长的句子表现超过传统带attention的NMT，而长句子的建模正是attention-based 的模型的一大挑战；而在短句子上就不如attention-based的NMT。<br><img src="/images/2018-11-04-15412946442983.jpg" width="50%" height="50%"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/11/04/诗词&句/每周诗词14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/04/诗词&句/每周诗词14/" itemprop="url">每周诗词14</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-04T08:15:14+08:00">
                2018-11-04
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-04T08:16:37+08:00">
                2018-11-04
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣鹤冲天"><a href="#1️⃣鹤冲天" class="headerlink" title="1️⃣鹤冲天"></a>1️⃣鹤冲天</h3><p>[宋] 柳永<br>黄金榜上，偶失龙头望。明代暂遗贤，如何向？未遂风云便，争不恣游狂荡。何须论得丧？才子词人，自是白衣卿相。<br>烟花巷陌，依约丹靑屛障。幸有意中人，堪寻访。且恁偎红倚翠，风流事，平生畅。靑春都一饷。<strong>忍把浮名，换了浅斟低唱</strong>！</p>
<p>恣（zì）：放纵，随心所欲。<br>恁（nèn）：如此。</p>
<p><a href="http://m.xichuangzhu.com/work/57aeff68a633bd0057f7d406" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57aeff68a633bd0057f7d406</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/11/04/代码相关/代码记录9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/04/代码相关/代码记录9/" itemprop="url">代码片段记录9</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-04T08:09:30+08:00">
                2018-11-04
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-04T08:11:16+08:00">
                2018-11-04
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-collate-fn"><a href="#1️⃣-collate-fn" class="headerlink" title="1️⃣[collate_fn]"></a>1️⃣[collate_fn]</h3><p>将不等长句子组合成batch。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(insts)</span>:</span></span><br><span class="line">    <span class="string">''' Pad the instance to the max seq length in batch '''</span></span><br><span class="line"></span><br><span class="line">    max_len = max(len(inst) <span class="keyword">for</span> inst <span class="keyword">in</span> insts)</span><br><span class="line"></span><br><span class="line">    batch_seq = np.array([</span><br><span class="line">        inst + [Constants.PAD] * (max_len - len(inst))</span><br><span class="line">        <span class="keyword">for</span> inst <span class="keyword">in</span> insts])</span><br><span class="line"></span><br><span class="line">    batch_pos = np.array([</span><br><span class="line">        [pos_i + <span class="number">1</span> <span class="keyword">if</span> w_i != Constants.PAD <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">         <span class="keyword">for</span> pos_i, w_i <span class="keyword">in</span> enumerate(inst)] <span class="keyword">for</span> inst <span class="keyword">in</span> batch_seq]) <span class="comment"># 位置信息</span></span><br><span class="line"></span><br><span class="line">    batch_seq = torch.LongTensor(batch_seq)</span><br><span class="line">    batch_pos = torch.LongTensor(batch_pos)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> batch_seq, batch_pos</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/10/30/见闻&想法/“莱斯杯”挑战赛有感/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/30/见闻&想法/“莱斯杯”挑战赛有感/" itemprop="url">“莱斯杯”挑战赛有感</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-30T08:44:14+08:00">
                2018-10-30
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-19T11:21:13+08:00">
                2018-11-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>历时三个月的“莱斯杯”全国第一届“军事智能·机器阅读”挑战赛终于落下帷幕，前几日（10.26-10.28）有幸在南京青旅宾馆参与决赛，体验多多，收获满满，心中亦有一些感想。</p>
<p>一个是南京总带给我一种回家的感觉，对南京的事物总有亲切感。第一次来南京是一年半前，也是来参加比赛。周五晚上的夜游秦淮，让我感受到许久未曾感受到的烟火气息。</p>
<p><img src="/images/2018-10-30-511540860855_.pic_hd.jpg" width="90%" height="50%"></p>
<p>第二个是此次主办方提供的食宿令人惊喜。一开始听到青旅宾馆，我已经做好了艰苦奋战的准备了，然而酒店是星级酒店的，吃方面直接到楼下的自助。可以看出主办方此次确实用心在举办这次比赛。</p>
<p><img src="/images/2018-10-30-15408644190676.jpg" width="100%" height="50%"></p>
<p>第三点是关于比赛的，关于比赛的整个历程我还是颇有感触。<br>我们是以第9名的成绩挺进决赛，其实在后期比赛中，我们都有所懈怠了，几乎没有花时间在这上面，10月初发布决赛的数据集，而我们在10月20日才得知这一事情，此时离决赛只剩一周时间。因此我们确实准备不足。当然我们也没有预料到我们的决赛成绩会这么靠前，否则我们肯定会更加充分去准备。这确实是我们的失误。</p>
<p>我们在比赛过程中，一直尝试在使用ELMo，这正是我负责的部分。一开始使用官方TensorFlow的代码，费了九牛二虎之力我才跑通代码，但因为队长使用的是pytorch，而二者在cuda版本上不兼容，因此在初赛我们没有使用ELMo。而在最后几天，我尝试使用哈工大的pytorch训练代码，但因为inference速度实在太慢，我们最终还是弃用了这个方案。而在决赛现场，我们发现也确实是因为速度和资源的原因，大家都没有使用ELMo，除了一组。该组正是凭借了ELMo弯道超车从第7升到了第一，拿走了20万大奖。这也是我们非常遗憾的一个地方，我们在遇到困难时没有尝试解决，而是直接弃用，最终没有取得更好的成绩。</p>
<p>此次我们的成绩排名第4(三等奖)，是有一定的进步的，但有一点遗憾的是，我们仅差0.18百分点，就能超过第三名拿到5万的奖金了。后面我们分析了一下，还是因为我们对比赛懈怠的态度，其他组都对数据进行了分析并有针对性的改进，而我们并没有做这一步。</p>
<p>Anyway，第一次组队参加比赛就有收获，增长了见识，从交流中也获得了许多。这个比赛之后，就得好好看paper了。 __(:з」∠)_</p>
<p><img src="/images/2018-10-30-521540861008_.pic_hd.jpg" width="80%" height="50%"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/10/29/诗词&句/每周诗词13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/29/诗词&句/每周诗词13/" itemprop="url">每周诗词13</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-29T10:20:14+08:00">
                2018-10-29
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-10-29T10:36:40+08:00">
                2018-10-29
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣行路难三首"><a href="#1️⃣行路难三首" class="headerlink" title="1️⃣行路难三首"></a>1️⃣行路难三首</h3><p>[唐] 李白<br>【其一】<br>金樽清酒斗十千，玉盘珍羞直万钱。<br><strong>停杯投箸不能食，拔剑四顾心茫然</strong>。<br>欲渡黄河冰塞川，将登太行雪满山。<br>闲来垂钓碧溪上，忽复乘舟梦日边。<br>行路难，行路难，多歧路，今安在？<br><strong>长风破浪会有时，直挂云帆济沧海</strong>！</p>
<p><strong>注释</strong>：<br>「闲来垂钓碧溪上，忽复乘舟梦日边。」句：暗用典故：姜太公吕尚曾在渭水的磻溪上钓鱼，得遇周文王，助周灭商；伊尹曾梦见自己乘船从日月旁边经过，后被商汤聘请，助商灭夏。这两句表示诗人自己对从政仍有所期待。碧，一作「坐」。</p>
<hr>
<h3 id="2️⃣登科后"><a href="#2️⃣登科后" class="headerlink" title="2️⃣登科后"></a>2️⃣登科后</h3><p>[唐] 孟郊<br>昔日龌龊不足夸，今朝放荡思无涯。<br><strong>春风得意马蹄疾，一日看尽长安花</strong>。</p>
<p><strong>注释</strong>：<br>龌龊（wò chuò）：原意是肮脏，这里指不如意的处境。不足夸：不值得提起。<br>放荡（dàng）：自由自在，不受约束。<br>思无涯：兴致高涨。</p>
<p><a href="http://m.xichuangzhu.com/work/57add198a633bd0057eefa8a" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57add198a633bd0057eefa8a</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/10/29/论文/每周论文3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/29/论文/每周论文3/" itemprop="url">每周论文3</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-29T10:17:30+08:00">
                2018-10-29
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-19T11:21:12+08:00">
                2018-11-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1️⃣-A-Neural-Probabilistic-Language-Model"><a href="#1️⃣-A-Neural-Probabilistic-Language-Model" class="headerlink" title="1️⃣[A Neural Probabilistic Language Model]"></a>1️⃣[A Neural Probabilistic Language Model]</h2><p>第一篇使用神经网络获得词向量的paper。</p>
<p>通过对language model建模，将词映射到低维表示，在训练过程中同时训练语言模型以及每个词的词向量。</p>
<p><img src="/images/2018-10-29-15407808716787.jpg" width="50%" height="50%"></p>
<p>将中心词的前n个拼接起来 $x=(C(w_{t-1},C(w_{t-2}),…,C(w_{t-n+1}))$<br>将$x$送入神经网络中获得$y=b+Wx+Utanh(d+Hx)$，最后做一个softmax即可。</p>
<hr>
<h2 id="2️⃣-Adaptive-Computation-Time-for-Recurrent-Neural-Networks"><a href="#2️⃣-Adaptive-Computation-Time-for-Recurrent-Neural-Networks" class="headerlink" title="2️⃣[Adaptive Computation Time for Recurrent Neural Networks]"></a>2️⃣[Adaptive Computation Time for Recurrent Neural Networks]</h2><p>一种允许RNN动态堆叠层数的算法。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>证据证明，RNN的堆叠层数多，效果会有提升。但是，对于不同的任务，要求不同的计算复杂度。我们需要先验来决定特定任务的计算复杂度。当然我们可以粗暴地直接堆叠深层的网络。ACT(Adaptive Computation Time)能够动态决定每个输入t所需的计算次数。</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>将RNN每一步的输出过一个网络+sigmoid层，获得一个概率分布，也即什么时候应当停止不再继续往上堆叠，直到概率加和为1。同时为了尽可能抑制层数的无限增长，在loss添加一项惩罚。</p>
<h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>对于普通的RNN：<br><img src="/images/2018-10-29-15408103221289.jpg" width="30%" height="50%"></p>
<p>s是隐藏层；y是输出。</p>
<p>对于ACT的RNN，有：<br><img src="/images/2018-10-29-15408103823681.jpg" width="40%" height="50%"></p>
<p>上标n是指的t时刻的层数；其中：<br><img src="/images/2018-10-29-15408104201718.jpg" width="20%" height="50%"></p>
<p>$δ$是flat，指示x是第几次输入。</p>
<p>引入新的网络，输入时隐状态，输出是一个概率分布：<br><img src="/images/2018-10-29-15408105451770.jpg" width="30%" height="50%"></p>
<p>那么每一层的概率是：<br><img src="/images/2018-10-29-15408105687677.jpg" width="35%" height="50%"></p>
<p>其中$R(t)$是在每一层概率求和超过1时的剩余概率（为了保证概率和为1，可以试着举一个例子来证明）<br><img src="/images/2018-10-29-15408106099743.jpg" width="45%" height="50%"></p>
<p><img src="/images/2018-10-29-15408106125837.jpg" width="25%" height="50%"></p>
<p>ε是为了解决第一次输出时就超过1-ε的情况，ε一般取很小。</p>
<p>最终，加权求和，作为最终的结果，传入下一个时间步：<br><img src="/images/2018-10-29-15408106649319.jpg" width="45%" height="50%"></p>
<p>普通RNN与ACT的RNN对比：<br><img src="/images/2018-10-29-15408106950342.jpg" width="90%" height="50%"></p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>为了防止模型层数无限增长，添加一项惩罚项以抑制。</p>
<p>记每一步的惩罚项为：<br><img src="/images/2018-10-29-15408107184035.jpg" width="23%" height="50%"></p>
<p>总的惩罚项则为：<br><img src="/images/2018-10-29-15408107351871.jpg" width="19%" height="50%"></p>
<p>Loss function则为：<br><img src="/images/2018-10-29-15408108024183.jpg" width="35%" height="50%"></p>
<p>因为N(t)是不可导的，我们在实际过程中只去最小化R(t)  （<del>我觉得不甚合理</del>，一种解读是如果我们不断最小化R(t)直到变成0，那么相当于N(t)少了一层，接着R(t)就会变得很大，然后又继续最小化R(t)…）</p>
<hr>
<h2 id="3️⃣-Universal-Transformers"><a href="#3️⃣-Universal-Transformers" class="headerlink" title="3️⃣[Universal Transformers]"></a>3️⃣[Universal Transformers]</h2><p>提出一种新型通用的transformer。</p>
<h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>Transformer的问题：RNN的归纳偏置(inductive bias)在一些任务上很重要，也即RNN的循环学习的过程；Transformer在一些问题上表现不好，可能是归纳偏置的原因。</p>
<blockquote>
<p>Notably, however, the Transformer foregoes the RNN’s inductive bias towards learning iterative or recursive transformations.Our experiments indicate that this inductive bias may be crucial for several algorithmic and language understanding tasks of varying complexity: in contrast to models such as the Neural Turing Machine [13], the Neural GPU [17] or Stack RNNs [16], the Transformer does not generalize well to input lengths not encountered during training.</p>
</blockquote>
<p>因此在Transformer内引入归纳偏置</p>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li>每一层的权重是共享的，也即multi-head上的权重以及transition function在每一层是一致的。这一点和RNN、CNN一致。</li>
<li>动态层数（ACT mechanism ）：对于每个词都会有不同的循环次数；也即有些词需要更多的refine；而有些词不需要。和固定层数的transformer相比，会有更好的通用性。</li>
</ul>
<h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><h4 id="总体架构"><a href="#总体架构" class="headerlink" title="总体架构"></a>总体架构</h4><p><img src="/images/2018-10-29-15408125098915.jpg" width="90%" height="50%"></p>
<p>过程：<br><img src="/images/2018-10-29-15408125469899.jpg" width="45%" height="50%"></p>
<p><img src="/images/2018-10-29-15408125685038.jpg" width="70%" height="50%"></p>
<p><img src="/images/2018-10-29-15408125974795.jpg" width="70%" height="50%"></p>
<p><img src="/images/2018-10-29-15408126143943.jpg" width="60%" height="50%"></p>
<p>和普通Transformer不同的地方在于：</p>
<ul>
<li>加了一层Transition层，Transition可以是depth-wise separable convolution（<a href="https://www.cnblogs.com/adong7639/p/7918527.html" target="_blank" rel="noopener">是什么？</a>）或者全连接层。</li>
<li>每层都添加了position embedding；以及timestep embedding，用以指示层数。</li>
</ul>
<h4 id="ACT"><a href="#ACT" class="headerlink" title="ACT"></a>ACT</h4><p>由于一个句子中间，有些词比其他词更难学会，需要更多计算量，但堆叠太多层会大大增加计算量，为了节省计算量，我们可以引入ACT来动态分配计算量。</p>
<p>ACT原来用于RNN，在Transformer中，当halting unit指示词t应当停止时，直接讲该词的状态复制到下一个time step，直到所有的词都停止。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg"
                alt="林泽辉" />
            
              <p class="site-author-name" itemprop="name">林泽辉</p>
              <p class="site-description motion-element" itemprop="description">人一己千</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">110</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">129</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">林泽辉</span>

  
</div>






  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
