<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/favicon.ico?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="人一己千">
<meta property="og:type" content="website">
<meta property="og:title" content="Weekly Review">
<meta property="og:url" content="http://www.linzehui.me/page/2/index.html">
<meta property="og:site_name" content="Weekly Review">
<meta property="og:description" content="人一己千">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Weekly Review">
<meta name="twitter:description" content="人一己千">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.linzehui.me/page/2/"/>





  <title>Weekly Review</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Weekly Review</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/09/碎片知识/Python中的+=操作/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/09/碎片知识/Python中的+=操作/" itemprop="url">Python中的+=操作</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-09T20:14:11+08:00">
                2018-12-09
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-09T20:35:29+08:00">
                2018-12-09
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>前几日在写一段Pytorch代码时，又一次遇到了in-place操作的问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output+=pos  <span class="comment"># pos是不可更新的tensor，output是可更新的tensor</span></span><br></pre></td></tr></table></figure>
<p>程序报错：“one of the variables needed for gradient computation has been modified by an inplace operation”。</p>
<p>无意中将代码改成<code>output=output+pos</code>，程序就不会报错了。</p>
<p>在查阅了相关资料后，将我的思考整理下来。</p>
<p>在Python中，<code>i=i+1</code>和<code>i+=1</code>是不同的，如果被操作数没有部署 ’<strong>iadd</strong>‘方法，则<code>i=i+1</code>和<code>i+=1</code>是等价的，’+=‘并不会产生in-place操作；当被操作数有部署该方法且正确部署，则是会产生in-place操作的。当没有in-place操作时，<code>i=i+1</code>表示对i重分配，也即i指向了另一个空间而不是原来的空间。</p>
<p>所以，这样的例子就能解释清楚了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A = np.arange(<span class="number">12</span>).reshape(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> A:</span><br><span class="line">    a = a + <span class="number">1</span></span><br><span class="line"><span class="comment"># A并没有被改变</span></span><br><span class="line">B = np.arange(<span class="number">12</span>).reshape(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> b <span class="keyword">in</span> B:</span><br><span class="line">    b += <span class="number">1</span></span><br><span class="line"><span class="comment"># B被改变了</span></span><br></pre></td></tr></table></figure>
<p>在Pytorch中，也有部署’<strong>iadd</strong>()‘操作，所以对于<code>output+=pos</code>，output内部的值被改变了，也即在计算图中引入了环，在反向求导时则会出错。</p>
<p>因此，在Pytorch中，应当避免in-place的操作。</p>
<p>Reference:<br><a href="https://stackoverflow.com/questions/41446833/what-is-the-difference-between-i-i-1-and-i-1-in-a-for-loop" target="_blank" rel="noopener">https://stackoverflow.com/questions/41446833/what-is-the-difference-between-i-i-1-and-i-1-in-a-for-loop</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/09/论文/每周论文8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/09/论文/每周论文8/" itemprop="url">每周论文8</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-09T09:57:30+08:00">
                2018-12-09
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-16T11:08:16+08:00">
                2018-12-16
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1️⃣-DiSAN-Directional-Self-Attention-Network-for-RNN-CNN-Free-Language-Understanding"><a href="#1️⃣-DiSAN-Directional-Self-Attention-Network-for-RNN-CNN-Free-Language-Understanding" class="headerlink" title="1️⃣[DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding]"></a>1️⃣[DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding]</h2><p>提出了两种attention机制，即 multi-dimentional attention和directional self-attention，在此基础上提出有向自注意力网络（directional self-attention network)</p>
<h3 id="Multi-dimensional-Attention"><a href="#Multi-dimensional-Attention" class="headerlink" title="Multi-dimensional Attention"></a>Multi-dimensional Attention</h3><p>与传统的方法不同的是，对于每个词对，attention出来的不是标量而是向量。<br><img src="/images/15443239891184.jpg" width="60%" height="50%"></p>
<p>计算公式：<br><img src="/images/15443240335179.jpg" width="40%" height="50%"></p>
<p>$f$的维度与$q$相同，每一维代表的是$x_i$在该维对$q$的重要性。也即feature-wise的attention。因此对于$q$而言，其获得的加权求和向量为：<br><img src="/images/15443241367138.jpg" width="55%" height="50%"></p>
<p>使用feature-wise的attention能够解决一次多义的问题，因为能够计算每一维的重要性，在不同的context下有不同的重要性。</p>
<p>将其应用于self-attention中，有两种变体：<br>①token2token<br><img src="/images/15443242128118.jpg" width="58%" height="50%"></p>
<p><img src="/images/15443242249947.jpg" width="20%" height="50%"></p>
<p>因此x在交互完有：<br><img src="/images/15443242733208.jpg" width="35%" height="50%"></p>
<p>②source2token<br><img src="/images/15443243090328.jpg" width="40%" height="50%"></p>
<p>也即$x_i$没有和其他元素有交互。<br>可用作获得sentence encoding：<br><img src="/images/15443243968731.jpg" width="20%" height="50%"></p>
<h3 id="Directional-Self-Attention"><a href="#Directional-Self-Attention" class="headerlink" title="Directional Self-Attention"></a>Directional Self-Attention</h3><p>使用mask达到有向性这一目的：<strong>通过mask矩阵将位置/方向编码进attention，解决时序丢失问题</strong>。<br>首先将x过一层获得新的h表示：<br><img src="/images/15443244421489.jpg" width="27%" height="50%"></p>
<p>接着使用token2token求attention，这里为了减少参数作了一定改动，将W换成c，tanh替换σ。<br><img src="/images/15443245099821.jpg" width="53%" height="50%"></p>
<p>$\textbf{1}$是全1的向量。M就是mask矩阵，代表i与j是否连通，Mask矩阵有：<br><img src="/images/15443248745747.jpg" width="28%" height="50%"></p>
<p><img src="/images/15443248908199.jpg" width="31%" height="50%"></p>
<p>也即：<br><img src="/images/15443249388350.jpg" width="50%" height="50%"></p>
<p>首先mask掉自己，第二：分别mask掉forward和backward，类似biLSTM，只和前面或后面的交互。</p>
<h3 id="Directional-Self-Attention-Network"><a href="#Directional-Self-Attention-Network" class="headerlink" title="Directional Self-Attention Network"></a>Directional Self-Attention Network</h3><p>在上述两个方法的基础上，此时已获得了上下文相关的$s_i$，再引入fusion gate：<br><img src="/images/15443250617220.jpg" width="45%" height="50%"></p>
<p>整个流程：<br><img src="/images/15443250338890.jpg" width="50%" height="50%"></p>
<p>将前向和反向的表示拼接起来，获得最终的表示$[u^{fw};u^{bw}]$：<br><img src="/images/15443251919096.jpg" width="50%" height="50%"></p>
<p>对于所获得的每一个表示，通过source2token，获得最终的句子表示。</p>
<p>这一点论文也提到了，非常类似bi-LSTM。</p>
<hr>
<h2 id="2️⃣-Targeted-Dropout"><a href="#2️⃣-Targeted-Dropout" class="headerlink" title="2️⃣[Targeted Dropout]"></a>2️⃣[Targeted Dropout]</h2><p>一种网络剪枝方法，想法简单易实现。<br>简单说，在每次更新时对最不重要的weight或者unit进行随机dropout。</p>
<h3 id="Targeted-Dropout"><a href="#Targeted-Dropout" class="headerlink" title="Targeted Dropout"></a>Targeted Dropout</h3><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>给定输入X，权重W，输出Y M为dropout的mask矩阵。<br>unit dropout：<br><img src="/images/15443218016315.jpg" width="22%" height="50%"></p>
<p>weight dropout：<br><img src="/images/15443218315803.jpg" width="22%" height="50%"></p>
<p>也即drop掉的是layer之间的connection。</p>
<h4 id="Magnitude-based-pruning"><a href="#Magnitude-based-pruning" class="headerlink" title="Magnitude-based pruning"></a>Magnitude-based pruning</h4><p>剪枝通常对权重最小的进行剪枝，也即保留topk个最大的权重。</p>
<p>Unit pruning：直接剪掉的是一整列，也即一个unit<br><img src="/images/15443218793541.jpg" width="43%" height="50%"></p>
<p>Weight pruning：对W的每个元素进行剪枝。注意是对每行的topk进行保留<br><img src="/images/15443219325533.jpg" width="58%" height="50%"></p>
<p>可以理解成对一个unit来说，保留最高的k个connection。</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>结合dropout和剪枝。<br>主要思想：首先选择N-k最不重要的element，由于我们希望这些low-value的元素有机会在训练过程中变得重要，因此我们对这些element进行随机dropout。</p>
<p>引入targeting proportion γ和drop probability α，亦即：选择最低的γ|θ|个weight，再根据α进行dropout。<br>这样做的结果是：减少重要的子网络对不重要的子网络的依赖。</p>
<h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><p>①dropout的intuition：减少unit之间的相互适应。when dropout is applied to a unit, the remaining network can no longer depend on that unit’s contribution to the function and must learn to propagate that unit’s information through a more reliable channel。<br>也可以理解成：使得unit之间的交互信息达到最大，在失去某个unit的时候影响不会那么大。</p>
<p>②targeted dropout intuition：the important subnetwork is completely separated from the unimportant one。假设一个网络由两个不相交的子网络组成，每个都能输出正确的结果，总的网络是这两个网络的平均。我们通过对不重要的子网络进行dropout（也即往子网络里加noise，会破坏该子网络的输出，由于重要的子网络已经能够输出正确的结果，因此为了减少损失，我们需要减少不重要网络的输出到0，也即kill掉该子网络，并且加强这两个网络的分离。（为什么不直接舍弃呢？因为是在训练过程中，有可能会有变化）<br>这个解释还是没完全懂。</p>
<hr>
<h2 id="3️⃣-A2-Nets-Double-Attention-Networks"><a href="#3️⃣-A2-Nets-Double-Attention-Networks" class="headerlink" title="3️⃣[A2-Nets: Double Attention Networks]"></a>3️⃣[A2-Nets: Double Attention Networks]</h2><p>发表于NIPS2018，个人认为很有启发。提出一种新的attention机制，基于“收集-分发”的思想，能够让CNN获得更大的感受野。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>CNN本身主要是捕获局部特征与关系，但对于长距离之间的关系只能通过堆叠多几层才能实现。但这样需要更高的计算量，且容易过拟合；同时，远处的特征实际上是来自好几层的延迟，导致推理的困难。</p>
<p>通过将feature收集起来，然后分发下去，使得feature之间有交互，让CNN获得更大的感受野，能够捕获长距离的特征。</p>
<h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p><img src="/images/15443223814542.jpg" width="80%" height="50%"></p>
<p>也即：<br><img src="/images/15443224194279.jpg" width="34%" height="50%"></p>
<p>X是所有输入，$v_i是$local feature。</p>
<h4 id="The-First-Attention-Step-Feature-Gathering"><a href="#The-First-Attention-Step-Feature-Gathering" class="headerlink" title="The First Attention Step: Feature Gathering"></a>The First Attention Step: Feature Gathering</h4><p>对于两个feature map A,B，有：<br><img src="/images/15443225280678.jpg" width="40%" height="50%"></p>
<p>其中：<br><img src="/images/15443226145868.jpg" width="35%" height="50%"></p>
<p><img src="/images/15443226262919.jpg" width="35%" height="50%"></p>
<p>如果A、B都来自同一个X，将B归一化softmax，就类似transformer的attention。其中上式的最右边是外积的形式。</p>
<p>我们将G拆分成向量形式：<br><img src="/images/15443226708983.jpg" width="33%" height="50%"><br>同时将B重写成行向量形式，则有：<br><img src="/images/15443227241595.jpg" width="22%" height="50%"></p>
<p>则会有：<br><img src="/images/15443227868015.jpg" width="28%" height="50%"></p>
<p>上式让我们有一个新的理解角度：G实际上就是 a bag of visual primitives。每个$g_i$是所有local feature加权求和，其中$b_i$是求和的weight。</p>
<p>因此我们对B做softmax，保证权重为1：<br><img src="/images/15443228682403.jpg" width="28%" height="50%"></p>
<h4 id="The-Second-Attention-Step-Feature-Distribution"><a href="#The-Second-Attention-Step-Feature-Distribution" class="headerlink" title="The Second Attention Step: Feature Distribution"></a>The Second Attention Step: Feature Distribution</h4><p>在获得了全局的feature G后，现在根据local feature去获取全局feature的部分，这通过一个权重控制，也即$v_i$（local feature)的每一维作为权重。可以不将local feature $v_i$归一化，但归一化能更好地converge。</p>
<h4 id="The-Double-Attention-Block"><a href="#The-Double-Attention-Block" class="headerlink" title="The Double Attention Block"></a>The Double Attention Block</h4><p>最终得到double attention block：<br><img src="/images/15443230115907.jpg" width="68%" height="50%"></p>
<p>整个流程：<br><img src="/images/15443230641691.jpg" width="80%" height="50%"></p>
<p>所以其实是有三个convolution layer。</p>
<p>上式还可以写成：<br><img src="/images/15443232642402.jpg" width="70%" height="50%"><br>数学上等价，但计算上差很多。第一个式子会有更低的复杂度。</p>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>虽然用了attention，但这里和Transformer还是有非常大的区别的。Transformer每个元素都和其他元素有交互，通过直接的计算得到权重。而这边的权重由feature本身来决定。并没有直接的交互。</p>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/09/碎片知识/每周碎片知识14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/09/碎片知识/每周碎片知识14/" itemprop="url">每周碎片知识14</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-09T09:41:14+08:00">
                2018-12-09
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-09T09:55:28+08:00">
                2018-12-09
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-Pytorch"><a href="#1️⃣-Pytorch" class="headerlink" title="1️⃣[Pytorch]"></a>1️⃣[Pytorch]</h3><p>Pytorch的tensor和Tensor是有区别的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.tensor(<span class="number">2</span>)  <span class="comment"># 是标量，size为[]</span></span><br><span class="line">b = torch.Tensor(<span class="number">2</span>)  <span class="comment"># 是向量，size为[2]</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/09/诗词&句/每周诗词17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/09/诗词&句/每周诗词17/" itemprop="url">每周诗词17</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-09T09:38:14+08:00">
                2018-12-09
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-09T09:39:18+08:00">
                2018-12-09
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣虞美人"><a href="#1️⃣虞美人" class="headerlink" title="1️⃣虞美人"></a>1️⃣虞美人</h3><p>[宋] 叶梦得<br>落花已作风前舞，又送黄昏雨。晓来庭院半残红，惟有游丝，千丈袅晴空。<br>殷勤花下同携手，更尽杯中酒。美人不用敛蛾眉，<strong>我亦多情，无奈酒阑时</strong>。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/02/碎片知识/每周碎片知识13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/02/碎片知识/每周碎片知识13/" itemprop="url">每周碎片知识13</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-02T15:32:14+08:00">
                2018-12-02
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-02T11:34:20+08:00">
                2018-12-02
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-attention"><a href="#1️⃣-attention" class="headerlink" title="1️⃣[attention]"></a>1️⃣[attention]</h3><p>所有attention的总结：<br><img src="/images/15437180657954.jpg" width="70%" height="50%"><br><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank" rel="noopener">Attention? Attention!</a></p>
<hr>
<h3 id="2️⃣-Pytorch"><a href="#2️⃣-Pytorch" class="headerlink" title="2️⃣[Pytorch]"></a>2️⃣[Pytorch]</h3><p>①torch.no_grad能够显著减少内存使用，model.eval不能。因为eval不会关闭历史追踪。</p>
<blockquote>
<p>model.eval() will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval model instead of training mode.<br>torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script).</p>
</blockquote>
<p>Reference:<br><a href="https://discuss.pytorch.org/t/does-model-eval-with-torch-set-grad-enabled-is-train-have-the-same-effect-for-grad-history/17183/3" target="_blank" rel="noopener">Does model.eval() &amp; with torch.set_grad_enabled(is_train) have the same effect for grad history?</a></p>
<p><a href="https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615" target="_blank" rel="noopener">‘model.eval()’ vs ‘with torch.no_grad()’</a></p>
<p>②torch.full(…) returns a tensor filled with value.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/02/代码相关/代码记录11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/02/代码相关/代码记录11/" itemprop="url">代码片段记录11</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-02T10:48:30+08:00">
                2018-12-02
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-16T11:00:39+08:00">
                2018-12-16
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="①"><a href="#①" class="headerlink" title="①"></a>①</h2><p>需求：对于两个向量$a$、$b$，$a,b \in R^d$，定义一种减法，有：</p>
<script type="math/tex; mode=display">a-b=M</script><p>其中$M \in R^{d\times d}$，$M_{ij}=a_i-b_j$</p>
<p>在代码中实际的维度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=torch.rand(batch_size,sequence_len,dim)</span><br><span class="line">b=torch.rand(batch_size,sequence_len,dim)</span><br></pre></td></tr></table></figure>
<p>方法①：for循环</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">M=torch.zeros(bz,seq_len,seq_len)</span><br><span class="line"><span class="keyword">for</span> b_i <span class="keyword">in</span> range(bz):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(seq_len):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(seq_len):</span><br><span class="line">            M_ij=torch.norm(a[b_i][i]-b[b_i][j])</span><br><span class="line">            M[b][i][j]=M_ij</span><br></pre></td></tr></table></figure>
<p>方法②：矩阵运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=a.unsqueeze(<span class="number">2</span>)  <span class="comment"># bz,seq_len,1,dim</span></span><br><span class="line">b=b.unsqueeze(<span class="number">1</span>)  <span class="comment"># bz,1,seq_lens,dim</span></span><br><span class="line">M=torch.norm(a-b,dim=<span class="number">-1</span>)   <span class="comment"># will broadcast</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="②"><a href="#②" class="headerlink" title="②"></a>②</h2><p>需求，生成一个mask矩阵，每一行有一段连续的位置填充1，其中每一行填充1的开始位置和结束位置都不同。具体来说，先生成一个中心位置center，则开始位置为center-window；结束位置为center+window。其中开始位置和结束位置不能越界，也即不小于0和大于行的总长度。<br>如：<br><img src="/images/15437208061953.jpg" width="25%" height="50%"></p>
<p>思路：<br>①先生成n行每行对应的随机中心位置，然后再获得左和右边界</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">centers=torch.randint(low=<span class="number">0</span>,high=query_len,size=(query_len,),dtype=torch.long)</span><br><span class="line"></span><br><span class="line">left=centers-self.window</span><br><span class="line">left=torch.max(left,torch.LongTensor([<span class="number">0</span>])).unsqueeze(<span class="number">1</span>)   <span class="comment"># query_len,1</span></span><br><span class="line"></span><br><span class="line">right=centers+self.window</span><br><span class="line">right=torch.min(right,torch.LongTensor([query_len<span class="number">-1</span>])).unsqueeze(<span class="number">1</span>)  <span class="comment"># query_len,1</span></span><br></pre></td></tr></table></figure>
<p>②生成一个每行都用[0,n-1]填充的矩阵，[0,n-1]表示的是该元素的index，亦即：<br><img src="/images/15437212363142.jpg" width="25%" height="50%"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">range_matrix=torch.range(<span class="number">0</span>,query_len<span class="number">-1</span>,dtype=torch.long).unsqueeze(<span class="number">0</span>).expand(query_len,<span class="number">-1</span>)  <span class="comment"># query_len,query_len</span></span><br></pre></td></tr></table></figure>
<p>③利用&lt;=和&gt;=获得一个左边界和右边界矩阵，左边界矩阵表示在该左边界的左边都是填充的1；右边界矩阵表示在该右边界右边都是填充的1。再进行异或操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">range_matrix=torch.range(<span class="number">0</span>,query_len<span class="number">-1</span>,dtype=torch.long).unsqueeze(<span class="number">0</span>).expand(query_len,<span class="number">-1</span>)  <span class="comment"># query_len,query_len</span></span><br><span class="line">left_matrix=range_matrix&lt;=left</span><br><span class="line">right_matrix=range_matrix&lt;=right</span><br><span class="line">final_matrix=left_matrix^right_matrix</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/02/论文/每周论文7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/02/论文/每周论文7/" itemprop="url">每周论文7</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-02T08:48:30+08:00">
                2018-12-02
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-02T11:47:07+08:00">
                2018-12-02
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1️⃣-Convolutional-Self-Attention-Network"><a href="#1️⃣-Convolutional-Self-Attention-Network" class="headerlink" title="1️⃣[Convolutional Self-Attention Network]"></a>1️⃣[Convolutional Self-Attention Network]</h2><p>对self-attention进行改进，引入CNN的local-bias，也即对query的邻近词进行attention而不是所有词；将self-attention扩展到2D，也即让不同的head之间也有attention交互。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>1️⃣the normalization in Softmax may inhibits the attention to neighboring information 也即邻居的信息更重要，要加强邻居的重要性</p>
<p>2️⃣features can be better captured by modeling dependencies across different channels 对于不同的channel/head也增加他们之间的交互。</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="/images/15437126353758.jpg" width="80%" height="50%"></p>
<p>对于1D的convolution：选取中心词周围一个window：<br><img src="/images/15437128149700.jpg" width="28%" height="50%"></p>
<p>对于2D的convolution，则有：<br><img src="/images/15437128476725.jpg" width="45%" height="50%"></p>
<p>在具体实践中，只对前三层添加local bias，这是因为modeling locality在底层更有效，对于高层应该捕获更远的信息。</p>
<hr>
<h2 id="2️⃣-Modeling-Localness-for-Self-Attention-Networks"><a href="#2️⃣-Modeling-Localness-for-Self-Attention-Networks" class="headerlink" title="2️⃣[Modeling Localness for Self-Attention Networks]"></a>2️⃣[Modeling Localness for Self-Attention Networks]</h2><p>和上文一样，引入local bias对self-attention进行改进，从而提升了翻译表现。和上文是同一作者，发在EMNLP上。</p>
<h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>1️⃣self-attention存在的问题：虽然能够增加长程关注，但因此会导致注意力的分散，对邻居的信号会忽略。实践证明，对local bias建模在self-attention有提升。</p>
<p>2️⃣从直觉上来说，在翻译模型中，当目标词i与源语言词j有对齐关系时，我们希望词i能同时对词j周围的词进行对齐，使得能够捕获上下文信息，如phrase的信息。</p>
<h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>在原来的公式上添加G：<br><img src="/images/15437133932791.jpg" width="45%" height="50%"><br>也即：<br><img src="/images/15437134105761.jpg" width="70%" height="50%"></p>
<p>G是一个alignment position matrix（对齐位置矩阵），元素ij代表目标词i与源语言词j之间的紧密程度。<br>我们每次根据目标词i预测一个源语言的中心词，则$G_{ij}$则为：</p>
<p><img src="/images/15437135769000.jpg" width="23%" height="50%"></p>
<p>$P_i$就是对于目标词j而言源语言的中心词。 $\sigma$ 手动设定，通常是$\frac{D}{2}$，D代表窗口大小。</p>
<p>也即最终我们需要计算的是，中心词$P_i$和窗口$D$。</p>
<h4 id="计算-P-i"><a href="#计算-P-i" class="headerlink" title="计算$P_i$"></a>计算$P_i$</h4><p>利用对应的目标词i的query即可：<br><img src="/images/15437138514005.jpg" width="28%" height="50%"><br>$p_i$是一个实数。</p>
<h4 id="计算window-size"><a href="#计算window-size" class="headerlink" title="计算window size"></a>计算window size</h4><p>①固定窗口，将其作为一个超参。</p>
<p>②Layer-Speciﬁc Window<br>将该层所有的key平均，计算出一个共享的window size：<br><img src="/images/15437139914993.jpg" width="28%" height="50%"></p>
<p>③Query-Speciﬁc Window<br>每个query都有自己的window size<br><img src="/images/15437140367683.jpg" width="30%" height="50%"></p>
<h3 id="实验分析与结论"><a href="#实验分析与结论" class="headerlink" title="实验分析与结论"></a>实验分析与结论</h3><p>①将model locality用于低层效果会更好，这是因为低层对相邻建模，而越高层越关注更远的词。</p>
<p><img src="/images/15437141387365.jpg" width="50%" height="50%"></p>
<p>②将model locality放在encoder和encoder-decoder部分会更好（transformer有三个地方可以放）</p>
<p><img src="/images/15437141719564.jpg" width="50%" height="50%"><br>因为decoder本身就倾向关注临近的词，如果继续让其关注临近的词，那么就难以进行长程建模。</p>
<p>③越高层，window size（scope）越大。</p>
<p><img src="/images/15437142078121.jpg" width="70%" height="50%"></p>
<p>也即，在底层更倾向于捕获邻近词的语义；而高层倾向捕获长程依赖。但这不包括第一层，第一层是embedding，还没有上下文信息，因此倾向于捕获全局信息。</p>
<hr>
<h2 id="3️⃣-Effective-Approaches-to-Attention-based-Neural-Machine-Translation"><a href="#3️⃣-Effective-Approaches-to-Attention-based-Neural-Machine-Translation" class="headerlink" title="3️⃣[Effective Approaches to Attention-based Neural Machine Translation]"></a>3️⃣[Effective Approaches to Attention-based Neural Machine Translation]</h2><p>提出两种attention机制的翻译模型，global和local。</p>
<p>本文与原版的翻译模型略有不同：<br><img src="/images/15437143753188.jpg" width="40%" height="50%"><br><img src="/images/15437143893418.jpg" width="30%" height="50%"></p>
<p>c是context，h是decode的隐层。</p>
<h3 id="global-attention"><a href="#global-attention" class="headerlink" title="global attention"></a>global attention</h3><p><img src="/images/15437144396133.jpg" width="45%" height="50%"></p>
<p>计算attention分数：<br><img src="/images/15437145076271.jpg" width="40%" height="50%"></p>
<p>score有多种选择：<br><img src="/images/15437145588496.jpg" width="52%" height="50%"></p>
<p>注意到该模型与第一个提出attention based的模型不同之处：<br>$h_t -&gt; a_t -&gt; c_t -&gt; \tilde{h_t}$<br>原版是：<br>$h_{t-1} -&gt; a_t -&gt; c_t -&gt; h_t$</p>
<h3 id="local-attention"><a href="#local-attention" class="headerlink" title="local attention"></a>local attention</h3><p><img src="/images/15437147612512.jpg" width="45%" height="50%"></p>
<p>由于global attention计算代价高，且对于长句效果不好，我们可以选择一部分来做attention。<br>首先生成一个对齐位置$p_t$，再选择一个窗口$[p_t - D,p_t + D]$，其中D是超参。</p>
<p>如何获得$p_t$?<br>①直接假设$p_t=t$，也即source和target的位置大致一一对应。</p>
<p>②做预测：<br><img src="/images/15437150115321.jpg" width="43%" height="50%"><br>其中S是source的句子长度。</p>
<p>接着，以$p_t$为中心，添加一个高斯分布。最终attention计算公式：<br><img src="/images/15437150721538.jpg" width="50%" height="50%"></p>
<p>其中align和上面一致：<br><img src="/images/15437151043916.jpg" width="45%" height="50%"></p>
<p>也就是说，将位置信息也考虑进来。</p>
<h3 id="Input-feeding-Approach"><a href="#Input-feeding-Approach" class="headerlink" title="Input-feeding Approach"></a>Input-feeding Approach</h3><p>motivation：在下一次的alignment（也就是计算attention）之前，应当知道之前的alignment情况，所以应当作为输入信息传进下一层：<br><img src="/images/15437152269151.jpg" width="50%" height="50%"></p>
<p>注意这里和Bahdanau的不同。Bahdanau是直接用上下文去构造隐层。这里提出的模型相对更为通用，也可以被应用于非attention的模型中（也就是每次将encoder的最后一层作为输入在每个time step都输入）</p>
<hr>
<h2 id="4️⃣-Towards-Linear-Time-Neural-Machine-Translation-with-Capsule-Networks"><a href="#4️⃣-Towards-Linear-Time-Neural-Machine-Translation-with-Capsule-Networks" class="headerlink" title="4️⃣[Towards Linear Time Neural Machine Translation with Capsule Networks]"></a>4️⃣[Towards Linear Time Neural Machine Translation with Capsule Networks]</h2><p>思想：利用capsule提前生成source sentence的固定长度的表示，在decode的时候直接使用，而不需要attention，以达到线性时间NMT的目的。</p>
<p>Motivation：attention-based的NMT时间复杂度为$|S|\times |T|$，而本文希望能够将NMT减少到线性时间。而传统不加attention的NMT通常使用LSTM最后一层隐层作为源语言的encode信息传入decode，但这样的信息并不能很好地代表整个句子，因此本文使用capsule作为提取source sentence信息的方法，利用capsule生成固定长度表示，直接传入decode端，以达到线性时间的目的。</p>
<p><img src="/images/15437164176973.jpg" width="50%" height="50%"></p>
<h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><p>对于embedding：<br><img src="/images/15437164440500.jpg" width="37%" height="50%"><br>希望能够转换成固定长度的表示C：<br><img src="/images/15437164654931.jpg" width="37%" height="50%"></p>
<p>我们首先通过一个双向的LSTM：<br><img src="/images/15437165067165.jpg" width="28%" height="50%"></p>
<p>一种简单的获取C的方法：<br><img src="/images/15437165382025.jpg" width="30%" height="50%"><br>其中$h_1$和$h_L$有互补关系。</p>
<p>本文使用capsule提取更丰富的信息。</p>
<p>在decode阶段，由于拥有固定表示，那么就不需要attention：</p>
<p><img src="/images/15437166827481.jpg" width="35%" height="50%"><br><img src="/images/15437167374470.jpg" width="37%" height="50%"></p>
<p>总体架构：<br><img src="/images/15437167607085.jpg" width="60%" height="50%"></p>
<h3 id="Aggregation-layers-with-Capsule-Networks"><a href="#Aggregation-layers-with-Capsule-Networks" class="headerlink" title="Aggregation layers with Capsule Networks"></a>Aggregation layers with Capsule Networks</h3><p><img src="/images/15437168111687.jpg" width="65%" height="50%"><br>实际上就是dynamic routing那一套，对信息进行提取（论文公式有误就不贴图了）</p>
<p>算法：<br><img src="/images/15437168668191.jpg" width="55%" height="50%"></p>
<p>最终获得了：<br><img src="/images/15437168888967.jpg" width="27%" height="50%"></p>
<hr>
<h2 id="5️⃣-DropBlock-A-regularization-method-for-convolutional-networks"><a href="#5️⃣-DropBlock-A-regularization-method-for-convolutional-networks" class="headerlink" title="5️⃣[DropBlock: A regularization method for convolutional networks]"></a>5️⃣[DropBlock: A regularization method for convolutional networks]</h2><p>重读了一遍。<br>介绍一种新型的dropout，可用于卷积层提高表现。通过大量的实验得出许多有意义的结论。本文发表于NIPS2018。</p>
<h3 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h3><p>由于卷积层的feature相互之间有联系，即使使用了dropout，信息也能够根据周围的feature传到下一层。因此使用dropblock，一次将一个方块内的都drop掉。</p>
<p><img src="/images/15437170173072.jpg" width="50%" height="50%"></p>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p><img src="/images/15437170840909.jpg" width="80%" height="50%"></p>
<p>其中有两个超参：①block_size表示块的大小；γ表示有多少个unit要drop掉，等价传统的dropout的p。当block_size=1时等价dropout；当block size=整个feature map，等价于spatial dropout。</p>
<p>在实践中，通过以下公式计算γ：<br><img src="/images/15437172746112.jpg" width="55%" height="50%"></p>
<p>(why? 通过计算期望的方式将传统dropout的keep_prob与当前的γ联系起来，得到一个等式，整理即可获得上式）</p>
<p>在实验中，还可以逐渐减小keep_prob使得更加鲁棒性。</p>
<h3 id="实验-amp-结论"><a href="#实验-amp-结论" class="headerlink" title="实验&amp;结论"></a>实验&amp;结论</h3><p>①效果:dropout&lt; spatial dropout &lt; dropblock</p>
<p>②dropblock能有效去掉semantic information</p>
<p>③dropblock是一个更加强的regularization</p>
<p>④使用dropblock的模型，能够学习更多的区域，而不是只专注于一个区域<br><img src="/images/15437174940381.jpg" width="70%" height="50%"></p>
<p>对于resnet，直接将dropblock应用于添加完skip connection后的feature能够有更高的表现。</p>
<hr>
<h2 id="6️⃣-Contextual-String-Embeddings-for-Sequence-Labeling"><a href="#6️⃣-Contextual-String-Embeddings-for-Sequence-Labeling" class="headerlink" title="6️⃣[Contextual String Embeddings for Sequence Labeling]"></a>6️⃣[Contextual String Embeddings for Sequence Labeling]</h2><p>提出一种建立在character基础上的新型的上下文embedding(contextualized embedding）。用于sequence labeling。本文发表于coling2018。</p>
<h3 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h3><p>整体架构：<br><img src="/images/15437175991019.jpg" width="100%" height="50%"></p>
<p>首先将character作为基本单位，过一个双向LSTM，进行language model的建模。</p>
<p>如何提取一个词的词向量：<br><img src="/images/15437176650871.jpg" width="100%" height="50%"><br>提取前向LSTM中该词的最后一个character的后一个hidden state，以及后向LSTM中第一个词的前一个hidden state， 如上图所示。最终拼起来即可：<br><img src="/images/15437177090697.jpg" width="28%" height="50%"><br>因此该词不仅与词内部的character相关，还跟其周围的context有关。</p>
<p>sequence labeling我不感兴趣，该部分没看。</p>
<h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>相比word level的language model，character-level独立于tokenization和fixed vocabulary，模型更容易被训练，因为词表小且训练时间短。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/01/诗词&句/每周诗词16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/01/诗词&句/每周诗词16/" itemprop="url">每周诗词16</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-01T23:08:14+08:00">
                2018-12-01
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-01T23:08:49+08:00">
                2018-12-01
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣菩萨蛮"><a href="#1️⃣菩萨蛮" class="headerlink" title="1️⃣菩萨蛮"></a>1️⃣菩萨蛮</h3><p>[五代十国] 李煜<br>人生愁恨何能免，销魂独我情何限！故国梦重归，觉来双泪垂。<br>髙楼谁与上？长记秋晴望。<strong>往事已成空，还如一梦中</strong>。</p>
<p>觉(jue)来：醒来。</p>
<hr>
<h3 id="2️⃣南乡子-·-和杨元素，时移守密州"><a href="#2️⃣南乡子-·-和杨元素，时移守密州" class="headerlink" title="2️⃣南乡子 · 和杨元素，时移守密州"></a>2️⃣南乡子 · 和杨元素，时移守密州</h3><p>[宋] 苏轼<br>东武望馀杭，云海天涯两杳茫。<strong>何日功成名遂了，还乡，醉笑陪公三万场</strong>。<br><strong>不用诉离觞，痛饮从来别有肠</strong>。今夜送归灯火冷，河塘，堕泪羊公却姓杨。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/12/01/诗词&句/人不可能经历世界上所有热闹/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/01/诗词&句/人不可能经历世界上所有热闹/" itemprop="url">无题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-01T11:13:15+08:00">
                2018-12-01
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-02T11:33:38+08:00">
                2018-12-02
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>人不可能经历世界上所有热闹，但可以用眼睛看，用心感受，用胸怀扩张。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/11/19/碎片知识/每周碎片知识12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/19/碎片知识/每周碎片知识12/" itemprop="url">每周碎片知识12</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-19T15:32:14+08:00">
                2018-11-19
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-19T23:21:17+08:00">
                2018-11-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-Transformer"><a href="#1️⃣-Transformer" class="headerlink" title="1️⃣[Transformer]"></a>1️⃣[Transformer]</h3><p>对Transformer新理解：</p>
<ul>
<li>可以将Transformer理解成一张全连接图，其中每个节点与其他节点的关系通过attention权重表现。图关系是序列关系或者树关系的一般化。</li>
<li>为什么要有multi-head？不仅仅是论文的解释，或许还可以理解成，对一个向量的不同部分（如第1维到20维，第21维到40维等）施以不同的attention权重，如果不使用multi-head，那么对于一个query，就只会有一个权重，而不同的维度有不同的重要性。</li>
</ul>
<hr>
<h3 id="2️⃣-attention-amp-capsule"><a href="#2️⃣-attention-amp-capsule" class="headerlink" title="2️⃣[attention&amp;capsule]"></a>2️⃣[attention&amp;capsule]</h3><p>attention是收信息，query从value按权重获取信息，其中所有value的权重和是1。<br>capsule是发信息，对于$l-1$层的一个capsule来说，在传入到$l$层的k个capsule的信息，其权重和为1。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg"
                alt="林泽辉" />
            
              <p class="site-author-name" itemprop="name">林泽辉</p>
              <p class="site-description motion-element" itemprop="description">人一己千</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">113</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">130</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">林泽辉</span>

  
</div>






  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
