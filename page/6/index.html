<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/favicon.ico?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="人一己千">
<meta property="og:type" content="website">
<meta property="og:title" content="Weekly Review">
<meta property="og:url" content="http://www.linzehui.me/page/6/index.html">
<meta property="og:site_name" content="Weekly Review">
<meta property="og:description" content="人一己千">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Weekly Review">
<meta name="twitter:description" content="人一己千">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.linzehui.me/page/6/"/>





  <title>Weekly Review</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Weekly Review</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/10/29/诗词&句/每周诗词13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/29/诗词&句/每周诗词13/" itemprop="url">每周诗词13</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-29T10:20:14+08:00">
                2018-10-29
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-10-29T10:36:40+08:00">
                2018-10-29
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣行路难三首"><a href="#1️⃣行路难三首" class="headerlink" title="1️⃣行路难三首"></a>1️⃣行路难三首</h3><p>[唐] 李白<br>【其一】<br>金樽清酒斗十千，玉盘珍羞直万钱。<br><strong>停杯投箸不能食，拔剑四顾心茫然</strong>。<br>欲渡黄河冰塞川，将登太行雪满山。<br>闲来垂钓碧溪上，忽复乘舟梦日边。<br>行路难，行路难，多歧路，今安在？<br><strong>长风破浪会有时，直挂云帆济沧海</strong>！</p>
<p><strong>注释</strong>：<br>「闲来垂钓碧溪上，忽复乘舟梦日边。」句：暗用典故：姜太公吕尚曾在渭水的磻溪上钓鱼，得遇周文王，助周灭商；伊尹曾梦见自己乘船从日月旁边经过，后被商汤聘请，助商灭夏。这两句表示诗人自己对从政仍有所期待。碧，一作「坐」。</p>
<hr>
<h3 id="2️⃣登科后"><a href="#2️⃣登科后" class="headerlink" title="2️⃣登科后"></a>2️⃣登科后</h3><p>[唐] 孟郊<br>昔日龌龊不足夸，今朝放荡思无涯。<br><strong>春风得意马蹄疾，一日看尽长安花</strong>。</p>
<p><strong>注释</strong>：<br>龌龊（wò chuò）：原意是肮脏，这里指不如意的处境。不足夸：不值得提起。<br>放荡（dàng）：自由自在，不受约束。<br>思无涯：兴致高涨。</p>
<p><a href="http://m.xichuangzhu.com/work/57add198a633bd0057eefa8a" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57add198a633bd0057eefa8a</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/10/29/论文/每周论文3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/29/论文/每周论文3/" itemprop="url">每周论文3</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-29T10:17:30+08:00">
                2018-10-29
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-19T11:21:12+08:00">
                2018-11-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1️⃣-A-Neural-Probabilistic-Language-Model"><a href="#1️⃣-A-Neural-Probabilistic-Language-Model" class="headerlink" title="1️⃣[A Neural Probabilistic Language Model]"></a>1️⃣[A Neural Probabilistic Language Model]</h2><p>第一篇使用神经网络获得词向量的paper。</p>
<p>通过对language model建模，将词映射到低维表示，在训练过程中同时训练语言模型以及每个词的词向量。</p>
<p><img src="/images/2018-10-29-15407808716787.jpg" width="50%" height="50%"></p>
<p>将中心词的前n个拼接起来 $x=(C(w_{t-1},C(w_{t-2}),…,C(w_{t-n+1}))$<br>将$x$送入神经网络中获得$y=b+Wx+Utanh(d+Hx)$，最后做一个softmax即可。</p>
<hr>
<h2 id="2️⃣-Adaptive-Computation-Time-for-Recurrent-Neural-Networks"><a href="#2️⃣-Adaptive-Computation-Time-for-Recurrent-Neural-Networks" class="headerlink" title="2️⃣[Adaptive Computation Time for Recurrent Neural Networks]"></a>2️⃣[Adaptive Computation Time for Recurrent Neural Networks]</h2><p>一种允许RNN动态堆叠层数的算法。</p>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>证据证明，RNN的堆叠层数多，效果会有提升。但是，对于不同的任务，要求不同的计算复杂度。我们需要先验来决定特定任务的计算复杂度。当然我们可以粗暴地直接堆叠深层的网络。ACT(Adaptive Computation Time)能够动态决定每个输入t所需的计算次数。</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>将RNN每一步的输出过一个网络+sigmoid层，获得一个概率分布，也即什么时候应当停止不再继续往上堆叠，直到概率加和为1。同时为了尽可能抑制层数的无限增长，在loss添加一项惩罚。</p>
<h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>对于普通的RNN：<br><img src="/images/2018-10-29-15408103221289.jpg" width="30%" height="50%"></p>
<p>s是隐藏层；y是输出。</p>
<p>对于ACT的RNN，有：<br><img src="/images/2018-10-29-15408103823681.jpg" width="40%" height="50%"></p>
<p>上标n是指的t时刻的层数；其中：<br><img src="/images/2018-10-29-15408104201718.jpg" width="20%" height="50%"></p>
<p>$δ$是flat，指示x是第几次输入。</p>
<p>引入新的网络，输入时隐状态，输出是一个概率分布：<br><img src="/images/2018-10-29-15408105451770.jpg" width="30%" height="50%"></p>
<p>那么每一层的概率是：<br><img src="/images/2018-10-29-15408105687677.jpg" width="35%" height="50%"></p>
<p>其中$R(t)$是在每一层概率求和超过1时的剩余概率（为了保证概率和为1，可以试着举一个例子来证明）<br><img src="/images/2018-10-29-15408106099743.jpg" width="45%" height="50%"></p>
<p><img src="/images/2018-10-29-15408106125837.jpg" width="25%" height="50%"></p>
<p>ε是为了解决第一次输出时就超过1-ε的情况，ε一般取很小。</p>
<p>最终，加权求和，作为最终的结果，传入下一个时间步：<br><img src="/images/2018-10-29-15408106649319.jpg" width="45%" height="50%"></p>
<p>普通RNN与ACT的RNN对比：<br><img src="/images/2018-10-29-15408106950342.jpg" width="90%" height="50%"></p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>为了防止模型层数无限增长，添加一项惩罚项以抑制。</p>
<p>记每一步的惩罚项为：<br><img src="/images/2018-10-29-15408107184035.jpg" width="23%" height="50%"></p>
<p>总的惩罚项则为：<br><img src="/images/2018-10-29-15408107351871.jpg" width="19%" height="50%"></p>
<p>Loss function则为：<br><img src="/images/2018-10-29-15408108024183.jpg" width="35%" height="50%"></p>
<p>因为N(t)是不可导的，我们在实际过程中只去最小化R(t)  （<del>我觉得不甚合理</del>，一种解读是如果我们不断最小化R(t)直到变成0，那么相当于N(t)少了一层，接着R(t)就会变得很大，然后又继续最小化R(t)…）</p>
<hr>
<h2 id="3️⃣-Universal-Transformers"><a href="#3️⃣-Universal-Transformers" class="headerlink" title="3️⃣[Universal Transformers]"></a>3️⃣[Universal Transformers]</h2><p>提出一种新型通用的transformer。</p>
<h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>Transformer的问题：RNN的归纳偏置(inductive bias)在一些任务上很重要，也即RNN的循环学习的过程；Transformer在一些问题上表现不好，可能是归纳偏置的原因。</p>
<blockquote>
<p>Notably, however, the Transformer foregoes the RNN’s inductive bias towards learning iterative or recursive transformations.Our experiments indicate that this inductive bias may be crucial for several algorithmic and language understanding tasks of varying complexity: in contrast to models such as the Neural Turing Machine [13], the Neural GPU [17] or Stack RNNs [16], the Transformer does not generalize well to input lengths not encountered during training.</p>
</blockquote>
<p>因此在Transformer内引入归纳偏置</p>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li>每一层的权重是共享的，也即multi-head上的权重以及transition function在每一层是一致的。这一点和RNN、CNN一致。</li>
<li>动态层数（ACT mechanism ）：对于每个词都会有不同的循环次数；也即有些词需要更多的refine；而有些词不需要。和固定层数的transformer相比，会有更好的通用性。</li>
</ul>
<h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><h4 id="总体架构"><a href="#总体架构" class="headerlink" title="总体架构"></a>总体架构</h4><p><img src="/images/2018-10-29-15408125098915.jpg" width="90%" height="50%"></p>
<p>过程：<br><img src="/images/2018-10-29-15408125469899.jpg" width="45%" height="50%"></p>
<p><img src="/images/2018-10-29-15408125685038.jpg" width="70%" height="50%"></p>
<p><img src="/images/2018-10-29-15408125974795.jpg" width="70%" height="50%"></p>
<p><img src="/images/2018-10-29-15408126143943.jpg" width="60%" height="50%"></p>
<p>和普通Transformer不同的地方在于：</p>
<ul>
<li>加了一层Transition层，Transition可以是depth-wise separable convolution（<a href="https://www.cnblogs.com/adong7639/p/7918527.html" target="_blank" rel="noopener">是什么？</a>）或者全连接层。</li>
<li>每层都添加了position embedding；以及timestep embedding，用以指示层数。</li>
</ul>
<h4 id="ACT"><a href="#ACT" class="headerlink" title="ACT"></a>ACT</h4><p>由于一个句子中间，有些词比其他词更难学会，需要更多计算量，但堆叠太多层会大大增加计算量，为了节省计算量，我们可以引入ACT来动态分配计算量。</p>
<p>ACT原来用于RNN，在Transformer中，当halting unit指示词t应当停止时，直接讲该词的状态复制到下一个time step，直到所有的词都停止。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/10/21/诗词&句/每周诗词12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/21/诗词&句/每周诗词12/" itemprop="url">每周诗词12</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-21T10:36:14+08:00">
                2018-10-21
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-10-21T14:53:18+08:00">
                2018-10-21
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣望海潮"><a href="#1️⃣望海潮" class="headerlink" title="1️⃣望海潮"></a>1️⃣望海潮</h3><p>[宋] 柳永<br>东南形胜，三吴都会，钱塘自古繁华。烟柳画桥，风帘翠幕，参差十万人家。云树绕堤沙，怒涛卷霜雪，天堑无涯。市列珠玑，户盈罗绮，竞豪奢。<br>重湖叠巘清嘉，有三秋桂子，十里荷花。羌管弄晴，菱歌泛夜，嬉嬉钓叟莲娃。千骑拥高牙，乘醉听箫鼓，吟赏烟霞。异日图将好景，归去凤池夸。</p>
<p>叠巘（yǎn）：层层叠叠的山峦。</p>
<p><a href="http://m.xichuangzhu.com/work/57b318228ac247005f2223db" target="_blank" rel="noopener">http://m.xichuangzhu.com/work/57b318228ac247005f2223db</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/10/21/代码相关/代码记录8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/21/代码相关/代码记录8/" itemprop="url">代码片段记录8</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-21T10:33:30+08:00">
                2018-10-21
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-10-21T10:34:37+08:00">
                2018-10-21
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-batchify"><a href="#1️⃣-batchify" class="headerlink" title="1️⃣[batchify]"></a>1️⃣[batchify]</h3><p>快速将数据分成batch。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchify</span><span class="params">(data, bsz)</span>:</span></span><br><span class="line">    <span class="comment"># Work out how cleanly we can divide the dataset into bsz parts.</span></span><br><span class="line">    nbatch = data.size(<span class="number">0</span>) // bsz</span><br><span class="line">    <span class="comment"># Trim off any extra elements that wouldn't cleanly fit (remainders).</span></span><br><span class="line">    data = data.narrow(<span class="number">0</span>, <span class="number">0</span>, nbatch * bsz)</span><br><span class="line">    <span class="comment"># Evenly divide the data across the bsz batches.</span></span><br><span class="line">    data = data.view(bsz, <span class="number">-1</span>).t().contiguous()</span><br><span class="line">    <span class="keyword">return</span> data.to(device)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/10/21/机器学习与深度学习算法知识/PRML第四章 分类的线性模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/21/机器学习与深度学习算法知识/PRML第四章 分类的线性模型/" itemprop="url">PRML第四章 分类的线性模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-21T09:21:24+08:00">
                2018-10-21
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-19T11:21:24+08:00">
                2018-11-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="判别函数"><a href="#判别函数" class="headerlink" title="判别函数"></a>判别函数</h1><p><img src="/images/2018-10-21-Xnip2018-10-21_09-26-42.jpg" alt="0"></p>
<hr>
<p><img src="/images/2018-10-21-Xnip2018-10-21_09-27-57.jpg" alt="1"></p>
<p>—-未完—-</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/10/20/论文/每周论文2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/20/论文/每周论文2/" itemprop="url">每周论文2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-20T10:07:30+08:00">
                2018-10-20
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-19T11:21:12+08:00">
                2018-11-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-An-Empirical-Evaluation-of-Generic-Convolutional-and-Recurrent-Networks-for-Sequence-Modeling"><a href="#1️⃣-An-Empirical-Evaluation-of-Generic-Convolutional-and-Recurrent-Networks-for-Sequence-Modeling" class="headerlink" title="1️⃣[An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling]"></a>1️⃣[An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling]</h3><p>本文贡献：提出一种新的模型<strong>TCN（Temporal Convolutional Networks）</strong>进行language model建模。</p>
<h4 id="Dilated-convolution"><a href="#Dilated-convolution" class="headerlink" title="Dilated convolution"></a>Dilated convolution</h4><p>每一层的感受野都可以是不同的，也即，同样的kernel size，高层的可以跳着看。<br><img src="/images/2018-10-20-15400016170606.jpg" width="60%" height="50%"></p>
<p>每层的d逐渐增大（也即跳的步数），一般按指数增大。（我觉得这样很有道理，如果每一层的d都是一样的，那capture到的信息就会有重复，能看到的视野也不如逐渐增大的多）</p>
<h4 id="Residual-block"><a href="#Residual-block" class="headerlink" title="Residual block"></a>Residual block</h4><p><img src="/images/2018-10-20-15400017320092.jpg" width="70%" height="50%"></p>
<p>这边的residual block比较复杂；一个值得主意的细节是，因为感受野的不同，上层的感受野总是比下层的大很多，因此不应该直接将下层的加到上层，而是可以使用一个1*1的convolution对下层的x进行卷积，这就类似scale对输入进行放缩。</p>
<hr>
<h3 id="2️⃣-Dissecting-Contextual-Word-Embeddings：-Architecture-and-Representation"><a href="#2️⃣-Dissecting-Contextual-Word-Embeddings：-Architecture-and-Representation" class="headerlink" title="2️⃣[Dissecting Contextual Word Embeddings： Architecture and Representation]"></a>2️⃣[Dissecting Contextual Word Embeddings： Architecture and Representation]</h3><p>一篇分析的文章。ELMo作者的又一篇文章。</p>
<p>对比三种不同的建模方式（LSTM/GCNN/Transformer）获得的词向量，以及在不同任务上的表现；以及不同层获得的不同信息…获得了不同的结论。</p>
<p>①biLM 专注于word morphology词的形态；底层的LM关注local syntax；而高层的LM关注semantic content；</p>
<p>②不同的任务会有不同的正则化s的倾向。</p>
<hr>
<h3 id="3️⃣-Transformer-XL-Language-modeling-with-longer-term-dependency"><a href="#3️⃣-Transformer-XL-Language-modeling-with-longer-term-dependency" class="headerlink" title="3️⃣[Transformer-XL: Language modeling with longer-term dependency]"></a>3️⃣[Transformer-XL: Language modeling with longer-term dependency]</h3><p>利用Transformer进行language model，与普通的Transformer建模不同的是，Transformer-XL添加了历史信息，能够显著提升表现。这篇还在ICLR2019审稿中。</p>
<p>贡献：本文提出了能够进行长程依赖的基于Transformer的语言模型 Transformer-XL；引入相对位置的positional encoding。</p>
<h4 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h4><p>原先的transformer language model是将句子分为一个一个segment。segment之间是没有联系的。（为什么不直接按原版的Transformer那样所有的词都相互做self-attention？因为考虑到效率问题，句子长度可能会很长）</p>
<p>训练阶段：<br><img src="/images/2018-10-20-15400023645268.jpg" width="35%" height="50%"></p>
<p>而在测试阶段，每次向右滑动一格：<br><img src="/images/2018-10-20-15400024115822.jpg" width="80%" height="50%"><br>这样每一个时间步都要重新计算一遍，历史信息没有利用到。显然速度很慢。</p>
<p>在Transformer引入recurrence，也即引入历史信息。基于这样的想法，提出的新模型Transformer-XL。在结构上同样分为每个segment，但在每个阶段都接收上一个（甚至上L个）历史信息。</p>
<p>训练阶段：<br><img src="/images/2018-10-20-15400026059113.jpg" width="80%" height="50%"></p>
<p>而在测试阶段，同样分为segment，但因为接收了历史信息，不需要每次滑动一格也能获得大量信息。<br><img src="/images/2018-10-20-15400027040526.jpg" width="45%" height="50%"></p>
<p>具体来说：<br><img src="/images/2018-10-20-15400027302545.jpg" width="120%" height="50%"><br>SG代表stop gradient，和该阶段的hidden state进行拼接。</p>
<h4 id="RELATIVE-POSITIONAL-ENCODINGS"><a href="#RELATIVE-POSITIONAL-ENCODINGS" class="headerlink" title="RELATIVE POSITIONAL ENCODINGS"></a>RELATIVE POSITIONAL ENCODINGS</h4><p>如果我们使用了absolute positional encodings（也即原版的positional encodings）那么会出现这种情况</p>
<p><img src="/images/2018-10-20-15400027991211.jpg" width="70%" height="50%"></p>
<p>在同一层之间的前一个segment和后一个segment使用了同样的绝对位置信息，对于当前segment的高层，对于同一个位置i，无法区分该位置信息是来自当前segment的还是上一个segment的（因为都是同样的绝对位置）。</p>
<p>因此我们引入相对位置信息R，其中第i行代表相对距离i的encoding。</p>
<p>具体来说：</p>
<p>首先我们在传统的计算$query_i$和$key_j$的attention分数时，可以拆解成：</p>
<p><img src="/images/2018-10-20-15400030310583.jpg" width="80%" height="50%"><br>（因为query=(embedding E +positional embedding U），key也一样，将式子拆开就能获得上述式子)</p>
<p>我们将该式子进行修改：</p>
<p><img src="/images/2018-10-20-15400031662378.jpg" width="80%" height="50%"></p>
<p>第一，将出现了absolute positional embedding $U$的地方，统统改成$R_{i-j}$，也即在b和d项。其中这里的R和原版的Transformer的位置计算公式相同。</p>
<p>第二，在c项中，使用一个$u$替代了$U_i W_q$，这一项原本的意义在于，$query_i$的positional encoding对$key_j$的embedding进行attention，也就是说，该项表现了$query_i$位置对哪些$key_j$的内容有兴趣，作者认为query不管在哪个位置上都是一样的，也就是说query的位置信息应当没影响，所以统统替换成一个可学习的$u$。基于类似的理由d项换成了$v$。</p>
<p>第三，将$W_k$细分成了两个$W_{k,E}$和$W_{k,R}$。这是根据query是Embedding还是positional encoding来区分的。for producing the content-based key vectors and location-based key vectors respectively</p>
<p>每一项现在都有了不同的意义：</p>
<blockquote>
<p>Under the new parameterization, each term has an intuitive meaning: term (a) represents contentbased addressing, term (b) captures a content-dependent positional bias, term (c) governs a global content bias, and (d) encodes a global positional bias.</p>
</blockquote>
<p>最后总结一下整个结构：</p>
<p><img src="/images/2018-10-20-15400040342520.jpg" width="120%" height="50%"></p>
<p>与原版Transformer不同的是，Transformer-XL在每一层都添加了位置信息。</p>
<hr>
<h3 id="4️⃣-Trellis-Networks-for-Sequence-Modeling"><a href="#4️⃣-Trellis-Networks-for-Sequence-Modeling" class="headerlink" title="4️⃣[Trellis Networks for Sequence Modeling]"></a>4️⃣[Trellis Networks for Sequence Modeling]</h3><p>一种结合RNN和CNN的语言建模方式。</p>
<p>最小的单元结构：</p>
<p><img src="/images/2018-10-21-15400858162232.jpg" width="40%" height="50%"></p>
<p>也即：<br><img src="/images/2018-10-21-15400860605560.jpg" width="40%" height="50%"></p>
<p>接下来再处理非线性：<br><img src="/images/2018-10-21-15400861618655.jpg" width="30%" height="50%"></p>
<p>因为每层都要输入x，且W是共享的，所以我们可以提前计算好这一项，后面直接用即可。<br><img src="/images/2018-10-21-15400861870898.jpg" width="35%" height="50%"></p>
<p>最终在实现的时候是：<br><img src="/images/2018-10-21-15400862184335.jpg" width="40%" height="50%"></p>
<p><img src="/images/2018-10-21-15400862303741.jpg" width="40%" height="50%"></p>
<p>总体框架：<br><img src="/images/2018-10-21-15400874987498.jpg" width="70%" height="50%"></p>
<p>与TCN（temporal convolution network）不同之处：①filter weight不仅在time step之间共享，在不同层之间也共享；②在每一层都添加了输入</p>
<p>优点：共享了W，显著减少了参数；‘Weight tying can be viewed as a form of regularization that can stabilize training’</p>
<p>我们还可以扩展该网络，引入gate：<br><img src="/images/2018-10-21-15400875805208.jpg" width="40%" height="50%"></p>
<hr>
<h3 id="5️⃣-Towards-Decoding-as-Continuous-Optimisation-in-Neural-Machine-Translation"><a href="#5️⃣-Towards-Decoding-as-Continuous-Optimisation-in-Neural-Machine-Translation" class="headerlink" title="5️⃣[Towards Decoding as Continuous Optimisation in Neural Machine Translation]"></a>5️⃣[Towards Decoding as Continuous Optimisation in Neural Machine Translation]</h3><p>一篇很有意思的paper。用于NMT decode的inference阶段。这篇有一定的难度，以下只是我的理解。</p>
<h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><p>Motivation：<br>NMT中的decode inference阶段，通常都是从左到右的，这样有个缺点，就是整体的target之间的依赖是没有被充分利用到的，比如说生成的词的右边是没有用到的。那么我们为什么不直接全部生成呢？然后不断更新。也就是说我们将离散（discrete）的decode过程变成一个连续的过程（continuous optimization）。</p>
<p>假设我们已经训练好模型，给定一个句子，我们要翻译成目标句子，且假设我们已知要生成的句子长度是l，那么我们有：<br><img src="/images/2018-10-21-15400876953609.jpg" width="45%" height="50%"><br>我们要找到一个最优的序列$y$，使得$-log$最小。</p>
<p>等价于：<br><img src="/images/2018-10-21-15400877226851.jpg" width="55%" height="50%"><br>其中$\widetilde{y}_i$是one-hot。其实这里就是假设有这么一个ground truth，但实际上是没有的。</p>
<p>我们将$\widetilde{y}_i$是one-hot这个条件放宽一些，变成是一个概率单纯型（其实就是所有元素加起来是1，且都大于等于0）。</p>
<p>那么就变成了：<br><img src="/images/2018-10-21-15400879019592.jpg" width="50%" height="50%"></p>
<p>这个改变的本质是：<br><img src="/images/2018-10-21-15400879379023.jpg" width="50%" height="50%"></p>
<p>就是说原来one-hot的$\widetilde{y}_i$生成后丢到下一个时间步，取了一个词向量，接着计算。现在是一个概率分布$\hat{y}_i$丢进来，就相当于取了多个词向量的加权求和。</p>
<p>在利用下述的更新算法更新完$\hat{y}_i$之后，对于每个时间步t，我们找$\hat{y}_i$中元素最大的值对应的词作为生成的词。</p>
<p>有两种方法Exponentiated Gradient 和 SGD。实际上方法倒在其次了，主要是前面所述的continuous optimization这种思想。</p>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><h5 id="Exponentiated-Gradient"><a href="#Exponentiated-Gradient" class="headerlink" title="Exponentiated Gradient"></a>Exponentiated Gradient</h5><p><img src="/images/2018-10-21-15400881918713.jpg" width="80%" height="50%"><br>具体见论文</p>
<h5 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h5><p>因为我们要保证单纯形的约束不变，因此我们引入一个r，然后做一个softmax<br><img src="/images/2018-10-21-15400882306948.jpg" width="80%" height="50%"></p>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>这种连续decode可以用在哪？</p>
<h5 id="Bidirectional-Ensemble"><a href="#Bidirectional-Ensemble" class="headerlink" title="Bidirectional Ensemble"></a>Bidirectional Ensemble</h5><p>可以很方便地进行双向的生成：</p>
<p><img src="/images/2018-10-21-15400883321474.jpg" width="45%" height="50%"><br>而在传统的方法中没办法（很难）做到</p>
<h5 id="Bilingual-Ensemble"><a href="#Bilingual-Ensemble" class="headerlink" title="Bilingual Ensemble"></a>Bilingual Ensemble</h5><p>我们希望源语言到目标语言和目标到源语言都生成得好</p>
<p><img src="/images/2018-10-21-15400883583228.jpg" width="50%" height="50%"></p>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>$\hat{y}_i$的初始化很重要，一不小心就会陷入local minima；生成的速度慢</p>
<hr>
<h3 id="6️⃣-Universal-Language-Model-Fine-tuning-for-Text-Classiﬁcation"><a href="#6️⃣-Universal-Language-Model-Fine-tuning-for-Text-Classiﬁcation" class="headerlink" title="6️⃣[Universal Language Model Fine-tuning for Text Classiﬁcation]"></a>6️⃣[Universal Language Model Fine-tuning for Text Classiﬁcation]</h3><p>和ELMo、OpenAI GPT一样，都是预训练语言模型，迁移到其他任务上（这里是分类任务）。可以在非常小的数据集上有很好的效果。</p>
<p>贡献：</p>
<ol>
<li>迁移学习模型ULMFiT</li>
<li>提出几种trick：discriminative ﬁne-tuning, slanted triangular learning rates,gradual unfreezing ，最大保证知识的保留。</li>
</ol>
<h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p><img src="/images/2018-10-21-15401043145373.jpg" width="90%" height="50%"></p>
<p>三部曲：</p>
<ol>
<li>通用语言模型预训练</li>
<li>目标任务的语言模型fine-tuning</li>
<li>目标任务的分类fine-tuning</li>
</ol>
<h4 id="trick"><a href="#trick" class="headerlink" title="trick"></a>trick</h4><h5 id="Discriminative-ﬁne-tuning"><a href="#Discriminative-ﬁne-tuning" class="headerlink" title="Discriminative ﬁne-tuning"></a>Discriminative ﬁne-tuning</h5><p>Motivation：不同层有不同的信息；应当fine-tune 不同程度，也即使用不同的learning rate。</p>
<p><img src="/images/2018-10-21-15401044160803.jpg" width="35%" height="50%"></p>
<p>作者发现上一层的学习率是下一层的2.6倍时效果比较好。</p>
<h5 id="Slanted-triangular-learning-rates-STLR"><a href="#Slanted-triangular-learning-rates-STLR" class="headerlink" title="Slanted triangular learning rates (STLR)"></a>Slanted triangular learning rates (STLR)</h5><p><img src="/images/2018-10-21-15401045153164.jpg" width="60%" height="50%"></p>
<p>具体公式：<br><img src="/images/2018-10-21-15401045316305.jpg" width="50%" height="50%"></p>
<h5 id="Gradual-unfreezing"><a href="#Gradual-unfreezing" class="headerlink" title="Gradual unfreezing"></a>Gradual unfreezing</h5><p>从顶层到底层，一步一步unfreeze，也即从上到下fine-tune。这是因为最上一层有最少的general knowledge。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/10/14/论文/每周论文1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/14/论文/每周论文1/" itemprop="url">每周论文1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-14T11:09:30+08:00">
                2018-10-14
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-19T11:21:13+08:00">
                2018-11-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1️⃣-Learned-in-Translation-Contextualized-Word-Vectors"><a href="#1️⃣-Learned-in-Translation-Contextualized-Word-Vectors" class="headerlink" title="1️⃣[Learned in Translation: Contextualized Word Vectors]"></a>1️⃣[Learned in Translation: Contextualized Word Vectors]</h3><p>CoVe是第一个引入动态词向量的模型。<br>Motivation：翻译模型能够保存最多的信息，因为如果保存信息不够多，decoder接收到的信息不足，翻译效果就不会好。（但实际上，我个人认为，decoder的表现还和language model有关，如果decoder是一个好的language model，也有可能翻译出不错的结果）</p>
<p>做法：使用传统NMT的encoder-decoder的做法翻译模型，只是将(bi)LSTM所得到的隐层状态表示取出来和embedding拼接起来，作为一个词的表示：</p>
<script type="math/tex; mode=display">w=[GloVe(w); CoVe(w)]</script><hr>
<h3 id="2️⃣-Language-Modeling-with-Gated-Convolutional-Networks"><a href="#2️⃣-Language-Modeling-with-Gated-Convolutional-Networks" class="headerlink" title="2️⃣[Language Modeling with Gated Convolutional Networks]"></a>2️⃣[Language Modeling with Gated Convolutional Networks]</h3><p>使用CNN对语言模型进行建模，提高并行性。</p>
<p>贡献：使用了CNN进行language model建模；提出了简化版的gate机制应用在CNN中。</p>
<p>做法：<br><img src="/images/2018-10-14-15394870930257.jpg" width="50%" height="50%"></p>
<p>实际上就是一个输入两个filter，卷积出来的做一个gate的操作$H_0 = A⊗σ(B)$，控制流向下一层的数据。</p>
<p>一个小细节是，为了不让language model看到下一个词，每一层在开始卷积的时候会在左边添加kernel_size-1个padding。</p>
<p>扩展：因为CNN的并行性高，可以使用CNN来对language model建模替代ELMo，同样可以获得动态词向量。这个想法已经由提出ELMo的团队做出来并进行对比了。论文：Dissecting Contextual Word Embeddings: Architecture and Representation</p>
<p>目前正在<a href="https://github.com/linzehui/Gated-Convolutional-Networks" target="_blank" rel="noopener">复现</a>该论文 。</p>
<hr>
<h3 id="3️⃣-Attention-is-All-you-need"><a href="#3️⃣-Attention-is-All-you-need" class="headerlink" title="3️⃣[Attention is All you need]"></a>3️⃣[Attention is All you need]</h3><p>非常经典的论文。提出了Transformer。为了读BERT重温了一遍。<br><img src="/images/2018-10-14-15394876881322.jpg" width="70%" height="50%"></p>
<p><img src="/images/2018-10-14-15394877200390.jpg" width="70%" height="50%"></p>
<p><img src="/images/2018-10-14-15394877478814.jpg" width="70%" height="50%"></p>
<hr>
<h3 id="4️⃣-Improving-Language-Understanding-by-Generative-Pre-Training"><a href="#4️⃣-Improving-Language-Understanding-by-Generative-Pre-Training" class="headerlink" title="4️⃣[Improving Language Understanding by Generative Pre-Training]"></a>4️⃣[Improving Language Understanding by Generative Pre-Training]</h3><p>BERT就是follow这篇文章的工作。<br>使用Transformer预训练一个language model进行迁移学习。</p>
<p>训练过程分为两步：①使用未标记数据训练language model；②使用有标记数据进行fine-tune</p>
<p>Motivation：ELMo是训练好language model，然后获得动态词向量再用到其他任务上，这样就会多了很多参数。和ELMo不同的是，这里使用一个Transformer模型解决多种任务（利用迁移学习）。</p>
<p>贡献：使用Transformer进行language model建模；尝试利用language model进行迁移学习而不是另一种思路（ELMo）只提取词向量。</p>
<p>①无监督学习language model<br><img src="/images/2018-10-14-15395044176746.jpg" width="40%" height="50%"></p>
<p>具体到Transformer就是：<br><img src="/images/2018-10-14-15395044608239.jpg" width="50%" height="50%"></p>
<p>②监督学习（fine-tune）<br>根据输入预测标签<br><img src="/images/2018-10-14-15395045508824.jpg" width="35%" height="50%"></p>
<p>具体就是：<br><img src="/images/2018-10-14-15395045734859.jpg" width="40%" height="50%"></p>
<p>将两个任务一起训练，则有：<br><img src="/images/2018-10-14-15395045932795.jpg" width="30%" height="50%"></p>
<p>对于不同任务，对输入进行一定的改动以适应Transformer结构：<br><img src="/images/2018-10-14-15395046364928.jpg" width="90%" height="50%"></p>
<hr>
<h3 id="5️⃣-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding"><a href="#5️⃣-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding" class="headerlink" title="5️⃣[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]"></a>5️⃣[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]</h3><p>刷爆各榜单的一篇神文。使用Transformer预训练一个language model进行迁移学习。</p>
<p>Motivation：之前的language model只能根据前面的词来预测下一个（即使ELMo是双向的LSTM，也是分别训练一个前向和一个后向的），限制了双向的context；因此提出了双向的language model。</p>
<h4 id="做法："><a href="#做法：" class="headerlink" title="做法："></a>做法：</h4><p>模型分为两个部分：<br>①masked LM：因为使用了两边的context，而language model的目的是预测下一个词，这样模型会提前看到下一个词，为了解决该问题，训练的时候讲部分词mask掉，最终只预测被mask掉的词。</p>
<p>②Next Sentence Prediction：随机50%生成两个句子是有上下句关系的，50%两个句子是没有关系的，然后做分类；具体来说是拿第一个词[CLS]（这是手动添加的）的表示，过一个softmax层得到。<br><img src="/images/2018-10-14-15394891973653.jpg" width="50%" height="50%"></p>
<p>联合训练这两个任务。</p>
<p>接下来是通过具体的任务进行fine-tune。一个模型解决多种问题：<br><img src="/images/2018-10-14-15395038593955.jpg" width="80%" height="50%"></p>
<p>本文贡献：使用Transformer进行双向的language model建模。论文提到的一些细节/tricks非常值得讨论，比如对token embedding添加了许多信息，非常简单粗暴。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/10/14/机器学习与深度学习算法知识/Lecture 18: Deep Reinforcement Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/14/机器学习与深度学习算法知识/Lecture 18: Deep Reinforcement Learning/" itemprop="url">Lecture 18:Deep Reinforcement Learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-14T10:17:24+08:00">
                2018-10-14
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-10-14T10:59:14+08:00">
                2018-10-14
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>记号： $a$是action，$s$即外部状态state，$\pi_{\theta}(s)$也即从$s$映射到$a$的函数；$r$是reward，每采取一个动作，会有一个reward，则总的reward为</p>
<script type="math/tex; mode=display">R_\theta = \sum_{t=1}^{T} r_t</script><p>我们使用神经网络来拟合$\pi$，一个eposide $\tau$是一个流程下来的的所有state、action和reward的集合。</p>
<script type="math/tex; mode=display">\tau = \{s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T \}</script><p>如果我们使用相同的actor运行n次，则每个$\tau$会有一定的概率被采样到，采样概率记为$P(\tau|\theta)$，则我们可以通过采样的方式来对期望reward进行估计：</p>
<script type="math/tex; mode=display">\overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta) ≈ \frac{1}{N} \sum_{n=1}^{N} R(\tau^n)</script><p>那么我们接下来的<strong>目标</strong>就是最大化期望reward，其中期望reward是：</p>
<script type="math/tex; mode=display">\overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta)</script><p>我们同样使用梯度上升：其中与$θ$相关的是$P$，则可以写成：</p>
<script type="math/tex; mode=display">\nabla \overline{R}_\theta = \sum_\tau R(\tau) \nabla P(\tau|\theta)= \sum_\tau R(\tau) P(\tau|\theta) \frac{\nabla P(\tau|\theta)}{P(\tau|\theta)}</script><p>由于$\dfrac {d\log \left( f\left( x\right) \right) }{dx}=\dfrac {1}{f\left( x\right) }\dfrac {df(x)}{dx}$，则前式可写成：</p>
<script type="math/tex; mode=display">\nabla \overline{R}_\theta = \sum_\tau R(\tau) P(\tau|\theta) \nabla log P(\tau | \theta) ≈ \frac{1}{N} \sum_{n=1}^{N} R(\tau^n) log P(\tau ^n| \theta)</script><p>如何求梯度？<br>由于：</p>
<script type="math/tex; mode=display">P(\tau | \theta)=p(s_1)p(a_1|s_1,\theta)p(r_1,s_2|s_1,a_1)p(a_2|s_2,\theta)p(r_2,s_3|s_2,a_2)...
\\=p(s_1)\prod_{t=1}^{T}p(a_t|s_t,\theta)p(r_t , s_{t+1}| s_t,a_t)</script><p>实际上，其中与梯度相关的只有中间项$p(a_t|s_t,\theta)$，该项也即$π$函数，从state到action的映射。<br>取log并求导，有：</p>
<script type="math/tex; mode=display">\nabla log P(\tau | \theta)= \sum_{t=1}^{T} \nabla log p(a_t|s_t,\theta)</script><p>代回，因此最终$\overline{R}_\theta$的梯度为：</p>
<script type="math/tex; mode=display">\nabla \overline{R}_\theta = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^n) \nabla log p(a_{t}^n | s_t^n,\theta)</script><p>注意到该式子告诉我们，应考虑整体的reward而不应该只考虑每一步的reward；并且取log的原因可以理解成是对action取归一化，因为：</p>
<script type="math/tex; mode=display">\frac{\nabla p(a_t^n | s_t^n,\theta)}{p(a_t^n | s_t^n,\theta)}</script><p>也就是说对于那些出现次数较多的action，要衡量他们对reward的真正影响，应当对他们归一化。</p>
<p>为了让那些出现可能性较低的action不会因为没被sample到而在更新后被降低他们的概率，可以添加一个baseline，只有超过$b$的reward才会增加他们出现的概率。</p>
<script type="math/tex; mode=display">\nabla \overline{R}_\theta = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} (R(\tau^n)-b) \nabla log p(a_{t}^n | s_t^n,\theta)</script>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/10/14/机器学习与深度学习算法知识/Lecture 17: Ensemble/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/14/机器学习与深度学习算法知识/Lecture 17: Ensemble/" itemprop="url">Lecture 17:Ensemble</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-14T10:09:24+08:00">
                2018-10-14
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-19T11:21:29+08:00">
                2018-11-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>对于复杂模型，往往variance会大，通过对多个模型的平均，能够减小variance：<br><img src="/images/2018-10-14-15394832736339.jpg" width="50%" height="50%"></p>
<p>bagging的思想是多次有放回地采样N’个点（通常N’=N），然后对采样的几个数据集分别训练一个模型<br><img src="/images/2018-10-14-15394833007835.jpg" width="50%" height="50%"></p>
<p>测试的时候再对几个模型进行平均或投票<br><img src="/images/2018-10-14-15394833255306.jpg" width="50%" height="50%"></p>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>基本思想是对几个弱分类器线性加权，得到强分类器。分类器按先后顺序训练，每次训练完，对新模型分类错误的数据进行调高权重，而正确的数据则降低权重。</p>
<p>可以保证：只要分类器的错误率小于50%，在boosting后能够有100%的正确率（在训练集）。</p>
<p>证明过程略。</p>
<h2 id="Ensemble-Stacking"><a href="#Ensemble-Stacking" class="headerlink" title="Ensemble: Stacking"></a>Ensemble: Stacking</h2><p>基本思想：使用训练数据训练多个初级分类器，将初级分类器的输出作为次级分类器的输入，获得最终的输出。我们应当使用不同的训练数据来训练次级分类器<br><img src="/images/2018-10-14-15394833971028.jpg" width="50%" height="50%"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.linzehui.me/2018/10/12/碎片知识/浅谈mask矩阵/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="林泽辉">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weekly Review">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/12/碎片知识/浅谈mask矩阵/" itemprop="url">浅谈mask矩阵</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-12T23:10:24+08:00">
                2018-10-12
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-11-19T11:21:14+08:00">
                2018-11-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>个人目前对mask矩阵的一点理解。</p>
<hr>
<h2 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h2><p>mask矩阵是什么？是一个由0和1组成的矩阵。一个例子是，在自然语言处理(NLP)中，句子的长度是不等长的，但因为我们经常将句子组成mini-batch用以训练，因此那些长度较短的句子都会在句尾进行填充0，也即padding的操作。一个mask矩阵即用以指示哪些是真正的数据，哪些是padding。如：<br><img src="/images/2018-10-12-15393574958961.jpg" width="50%" height="50%"><br>图片来源：<a href="https://www.cnblogs.com/neopenx/p/4806006.html" target="_blank" rel="noopener">Theano：LSTM源码解析</a></p>
<p>其中mask矩阵中1代表真实数据；0代表padding数据。</p>
<h2 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h2><p>为什么要使用mask矩阵？使用mask矩阵是为了让那些被mask掉的tensor不会被更新。考虑一个tensor T的size(a,b)，同样大小的mask矩阵M，相乘后，在反向回传的时候在T对应mask为0的地方，0的梯度仍为0。因此不会被更新。</p>
<h2 id="怎么做"><a href="#怎么做" class="headerlink" title="怎么做"></a>怎么做</h2><p>接下来介绍几种（可能不全）使用mask的场景。</p>
<h3 id="对输入进行mask"><a href="#对输入进行mask" class="headerlink" title="对输入进行mask"></a>对输入进行mask</h3><p>考虑NLP中常见的句子不等长的情况。设我们的输入的batch I:(batch_size,max_seqlen)，我们在过一层Embedding层之前，<br>在过了一层Embedding层，则有 E:(batch_size,max_seqlen,embed_dim)，如果我们希望Embedding是更新的(比如我们的Embedding是随机初始化的，那当然Embedding需要更新)，但我们又不希望padding更新。<br>一种方法即令E与M相乘。其中M是mask矩阵(batch_size,max_seqlen,1) (1是因为要broadcast），这样在Embedding更新梯度时，因为mask矩阵的关系，padding位置上的梯度就是0。<br>当然在Pytorch中还可以直接显式地写：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.embedding = nn.Embedding(vocab_size, embed_dim,padding_idx=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<p>而此时应当将padding显式添加到词典的第一个。</p>
<h3 id="对模型中间进行mask"><a href="#对模型中间进行mask" class="headerlink" title="对模型中间进行mask"></a>对模型中间进行mask</h3><p>一个很经典的场景就是dropout。<br>对于参数矩阵W:(h,w)，同样大小的mask矩阵M，在前向传播时令W’=W*M，则在反向传播时，M中为0的部分不被更新。<br>当然，我们可以直接调用PyTorch中的包<code>nn.Dropout()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Dropout(p=<span class="number">0.2</span>)</span><br><span class="line">input = torch.randn(<span class="number">20</span>, <span class="number">16</span>)</span><br><span class="line">output = m(input)</span><br></pre></td></tr></table></figure>
<h3 id="对loss进行mask"><a href="#对loss进行mask" class="headerlink" title="对loss进行mask"></a>对loss进行mask</h3><p>考虑NLP中的language model，每个词都需要预测下一个词，在一个batch中句子总是有长有短，对于一个短句，此时在计算loss的时候，会出现这样的场景：<code>&lt;pad&gt;</code>词要预测下一个<code>&lt;pad&gt;</code>词。举个例子：三个句子[a,b,c,d],[e,f,g],[h,i]，在组成batch后，会变成<br>X：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td>b</td>
<td>c</td>
<td>d</td>
</tr>
<tr>
<td>e</td>
<td>f</td>
<td>g</td>
<td><code>&lt;pad&gt;</code></td>
</tr>
<tr>
<td>h</td>
<td>i</td>
<td><code>&lt;pad&gt;</code></td>
<td><code>&lt;pad&gt;</code></td>
</tr>
</tbody>
</table>
</div>
<p>Y：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>b</td>
<td>c</td>
<td>d</td>
<td><code>&lt;pad&gt;</code></td>
</tr>
<tr>
<td>f</td>
<td>g</td>
<td><code>&lt;eos&gt;</code></td>
<td><code>&lt;pad&gt;</code></td>
</tr>
<tr>
<td>i</td>
<td><code>&lt;eos&gt;</code></td>
<td><code>&lt;pad&gt;</code></td>
<td><code>&lt;pad&gt;</code></td>
</tr>
</tbody>
</table>
</div>
<p>X是输入，Y是预测。那么从第三行可以看出，<code>&lt;pad&gt;</code>在预测下一个<code>&lt;pad&gt;</code>。这显然是有问题的。<br>一种解决方案就是使用mask矩阵，在loss的计算时，将那些本不应该计算的mask掉，使得其loss为0，这样就不会反向回传了。<br>具体实践：在PyTorch中，以CrossEntropy为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">nn</span>.<span class="title">CrossEntropyLoss</span><span class="params">(weight=None, size_average=None, ignore_index=<span class="number">-100</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">reduce=None, reduction=’elementwise_mean’</span></span></span><br></pre></td></tr></table></figure>
<p>如果<code>reduction=None</code>则会返回一个与输入同样大小的矩阵。在与mask矩阵相乘后，再对新矩阵进行mean操作。<br>在PyTorch实践上还可以可以这么写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">masked_outputs = torch.masked_select(dec_outputs, mask)</span><br><span class="line">masked_targets = torch.masked_select(targets, mask)</span><br><span class="line">loss = my_criterion(masked_outputs, masked_targets)</span><br></pre></td></tr></table></figure>
<p>另一种更为简单的解决方案是，直接在CrossEntropy中设<code>ignore_index=0</code>，这样，在计算loss的时候，发现target=0时，会自动不对其进行loss的计算。其本质和mask矩阵是一致的。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>mask矩阵可以用在任何地方，只要希望与之相乘的tensor相对应的地方不更新就可以进行mask操作。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://ws4.sinaimg.cn/large/006tNc79gy1fthknxagbjj3069069jr8.jpg"
                alt="林泽辉" />
            
              <p class="site-author-name" itemprop="name">林泽辉</p>
              <p class="site-description motion-element" itemprop="description">人一己千</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">132</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">150</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">林泽辉</span>

  
</div>






  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
